{"text": "Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension\n\nAbstract:\nWe present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.\n\n\n1 Introduction\nMachine reading comprehension (MRC) is a central task in natural language understanding, with techniques lately driven by a surge of large-scale datasets (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016), usually formalized as a task of answering questions given a passage. An increasing number of analyses (Jia and Liang, 2017; Rajpurkar et al., 2018; Kaushik and Lipton, 2018) have revealed that a large portion of questions in these datasets can be answered by simply matching the patterns between the question and the answer sentence in the passage. While systems may match or even outperform humans on these datasets, our intuition suggests that there are at least some instances in human reading comprehension that require more than what existing challenge tasks are emphasizing. One primary type of questions these datasets lack are the ones that require reasoning over common sense or understanding across multiple sentences in the passage (Rajpurkar et al., 2016; Trischler et al., 2017).\nTo overcome this limitation, we introduce a large-scale dataset for reading comprehension, ReCoRD ([\"rEk@rd]), which consists of over 120,000 examples, most of which require * Work done when Sheng Zhang was visiting Microsoft.\n\nPassage (Cloze-style) Query\nAccording to claims in the suit, \"Parts of 'Stairway to Heaven,' instantly recognizable to the music fans across the world, sound almost identical to significant portions of 'X.'\"\n\nReference Answers\nTaurus (CNN) --A lawsuit has been filed claiming that the iconic Led Zeppelin song \"Stairway to Heaven\" was far from original. The suit, filed on May 31 in the United States District Court Eastern District of Pennsylvania, was brought by the estate of the late musician Randy California against the surviving members of Led Zeppelin and their record label. The copyright infringement case alleges that the Zeppelin song was taken from the single \"Taurus\" by the 1960s band Spirit, for whom California served as lead guitarist. \"Late in 1968, a then new band named Led Zeppelin began touring in the United States, opening for Spirit,\" the suit states. \"It was during this time that Jimmy Page, Led Zeppelin's guitarist, grew familiar with 'Taurus' and the rest of Spirit's catalog. Page stated in interviews that he found Spirit to be 'very good' and that the band's performances struck him 'on an emotional level.' \"\n\u2022 Suit claims similarities between two songs \u2022 Randy California was guitarist for the group Spirit \u2022 Jimmy Page has called the accusation \"ridiculous\" Figure 1 : An example from ReCoRD. The passage is a snippet from a news article followed by some bullet points which summarize the news event. Named entities highlighted in the passage are possible answers to the query. The query is a statement that is factually supported by the passage. X in the statement indicates a missing named entity. The goal is to find the correct entity in the passage that best fits X.\ndeep commonsense reasoning. ReCoRD is an acronym for the Reading Comprehension with Commonsense Reasoning Dataset.\nFigure 1 shows a ReCoRD example: the passage describes a lawsuit claiming that the band \"Led Zeppelin\" had plagiarized the song \"Taurus\" to their most iconic song, \"Stairway to Heaven\". The cloze-style query asks what does \"Stairway to Heaven\" sound similar to. To find the correct answer, we need to understand from the passage that \"a copyright infringement case alleges that 'Stairway to Heaven' was taken from 'Taurus'\", and from the bullet point that \"these two songs are claimed similar\". Then based on the commonsense knowledge that \"if two songs are claimed similar, it is likely that (parts of) these songs sound almost identical\", we can reasonably infer that the answer is \"Taurus\".\nDiffering from most of the existing MRC datasets, all queries and passages in ReCoRD are automatically mined from news articles, which maximally reduces the human elicitation bias (Gordon and Van Durme, 2013; Misra et al., 2016; Zhang et al., 2017), and the data collection method we propose is cost-efficient. Further analysis shows that a large portion of ReCoRD requires commonsense reasoning.\nExperiments on ReCoRD demonstrate that human readers are able to achieve a high performance at 91.69 F1, whereas the state-of-the-art MRC models fall far behind at 46.65 F1. Thus, ReCoRD presents a real challenge for future research to bridge the gap between human and machine commonsense reading comprehension.\n\n2 Task Motivation\nA program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows. - McCarthy (1959) Commonsense Reasoning in MRC As illustrated by the example in Figure 1, the commonsense knowledge \"if two songs are claimed similar, it is likely that (parts of) these songs sound almost identica\" is not explicitly described in the passage, but is necessary to acquire in order to generate the answer. Human is able to infer the answer because the commonsense knowledge is commonly known by nearly all people. Our goal is to evaluate whether a machine is able to learn such knowledge. However, since commonsense knowledge is massive and mostly implicit, defining an explicit free-form evaluation is challenging (Levesque et al., 2011). Motivated by Mc-Carthy (1959), we instead evaluate a machine's ability of commonsense reasoning -a reasoning process requiring commonsense knowledge; that is, if a machine has common sense, it can deduce for itself the likely consequences or details of anything it is told and what it already knows rather than the unlikely ones. To formalize it in MRC, given a passage p (i.e., \"anything it is told\" and \"what it already knows\"), and a set of consequences or details C which are factually supported by the passage p with different likelihood, if a machine M has common sense, it can choose the most likely consequence or detail c * from C, i.e.,c * = arg max c\u2208C P (c | p, M).\n(1)\nTask Definition With the above discussion, we propose a specific task to evaluate a machine's ability of commonsense reasoning in MRC: as shown in Figure 1, given a passage p describing an event, a set of text spans E marked in p, and a cloze-style query Q(X) with a missing text span indicated by X, a machine M is expected to act like human, reading the passage p and then using its hidden commonsense knowledge to choose a text span e \u2208 E that best fits X, i.e.,e * = arg max e\u2208E P (Q(e) | p, M).\nOnce the cloze-style query Q(X) is filled in by a text span e, the resulted statement Q(e) becomes a consequence or detail c as described in Equation (1), which is factually supported by the passage with certain likelihood.\n\n3 Data Collection\nWe describe the framework for automatically generating the dataset, ReCoRD, for our task defined in Equation ( 2), which consists of passages with text spans marked, cloze-style queries, and reference answers. We collect ReCoRD in four stages as shown in Figure 2: (1) curating CNN/Daily Mail news articles, (2) generating passage-queryanswers triples based on the news articles, (3) filtering out the queries that can be easily answered by state-of-the-art MRC models, and (4) filtering out the queries ambiguous to human readers.\n\n3.1 News Article Curation\nWe choose to create ReCoRD by exploiting news articles, because the structure of news makes it a good source for our task: normally, the first few paragraphs of a news article summarize the news event, which can be used to generate passages of the task; and the rest of the news article provides consequences or details of the news event, which can be used to generate queries of the task. In addition, news providers such as CNN and Daily Mail supplement their articles with a number of bullet points (Svore et al., 2007; Woodsend and Lapata, 2010; Hermann et al., 2015), which outline the highlights of the news and hence form a supplemental source for generating passages.\nWe first downloaded CNN and Daily Mail news articles using the script 1 provided by Hermann et al. (2015), and then sampled 148K articles from CNN and Daily Mail. In these articles, named entities and their coreference information have been annotated by a Google NLP pipeline, and will be used in the second stage of our data collection. Since these articles can be easily downloaded using the public script, we are concerned about potential cheating if using them as the source for generating the dev./test datasets. Therefore, we crawled additional 22K news articles from the CNN and Daily Mail websites. These crawled articles have no overlap with the articles used in Hermann et al. (2015). We then ran the stateof-the-art named entity recognition model (Peters et al., 2018) and the end-to-end coreference resolution model (Lee et al., 2017) provided by AllenNLP (Gardner et al., 2018) to annotate the crawled articles. Overall, we have collected 170K CNN/Daily Mail news articles with their named entities and coreference information annotated.\n1 https://github.com/deepmind/rc-data 3.2 Passage-Query-Answers Generation All passages, queries and answers in ReCoRD were automatically generated from the curated news articles. Figure 3 illustrates the generation process. (1) we split each news article into two parts as described in Section 3.1: the first few paragraphs which summarize the news event, and the rest of the news which provides the details or consequences of the news event. These two parts make a good source for generating passages and queries of our task respectively. (2) we enriched the first part of news article with the bullet points provided by the news editors. The first part of news article, together with the bullet points, is considered as a candidate passage. To ensure that the candidate passages are informative enough, we required the first part of news article to have at least 100 tokens and contain at least four different entities. (3) for each candidate passage, the second part of its corresponding news article was split into sentences by Stanford CoreNLP (Manning et al., 2014). Then we selected the sentences that satisfy the following conditions as potential details or consequences of the news event described by the passage:\n\u2022 Sentences should have at least 10 tokens, as longer sentences contain more information and thus are more likely to be inferrable details or consequences.\n\u2022 Sentences should not be questions, as we only consider details or consequences of a news event, not questions.\n\u2022 Sentences should not have 3-gram overlap with the corresponding passage, so they are less likely to be paraphrase of sentences in the passage.\n\u2022 Sentences should have at least one named entity, so that we can replace it with X to generate a cloze-style query.\n\u2022 All named entities in sentences should have precedents in the passage according to coreference, so that the sentences are not too disconnected from the passage, and the correct entity can be found in the passage to fill in X.\nFinally, we generated queries by replacing entities in the selected sentences with X. We only replaced one entity in the selected sentence each time, and generated one cloze-style query. Based on coreference, the precedents of the replaced en-\n\nSTORY HIGHLIGHTS\n\u2022 Suit claims similarity between two songs\n\u2022 Randy California was guitarist for the group Spirit\n\u2022 Jimmy Page has called the accusation \"ridiculous\"\n(CNN) --A lawsuit has been filed claiming that the iconic Led Zeppelin song \"Stairway to Heaven\" was far from original.\nThe suit, filed on May 31 in the United States District Court Eastern District of Pennsylvania, was brought by the estate of the late musician Randy California against the surviving members of Led Zeppelin and their record label. The copyright infringement case alleges that the Zeppelin song was taken from the single \"Taurus\" by the 1960s band Spirit, for whom California served as lead guitarist.\n\"Late in 1968, a then new band named Led Zeppelin began touring in the United States, opening for Spirit,\" the suit states. \"It was during this time that Jimmy Page, Led Zeppelin's guitarist, grew familiar with 'Taurus' and the rest of Spirit's catalog. Page stated in interviews that he found Spirit to be 'very good' and that the band's performances struck him 'on an emotional level.' \"\nOne of the causes of action for the suit is listed as \"Falsification of Rock N' Roll History\" and the typeface in the section headings of the filing resembles that used for Led Zeppelin album covers. According to claims in the suit, \"Parts of 'Stairway to Heaven,' instantly recognizable to the music fans across the world, sound almost identical to significant portions of 'Taurus.' \" \u2026\u2026.\n\nPassage (Cloze-style) Query\nAccording to claims in the suit, \"Parts of 'Stairway to Heaven,' instantly recognizable to the music fans across the world, sound almost identical to significant portions of 'X.'\" The first few paragraphs and the bullet points of the news article summarize the news event.\n\nReference Answers\nThe rest of the news article provides details or concequences of the new event.\nThe hidden commonsense is used in comprehension of the underlined sentence (If two songs are claimed similar, it is likely that (parts of) these songs sound almost identical.) tity in the passage became reference answers to the query. The passage-query-answers generation process matched our task definition in Section 2, and therefore created queries that require some aspect of reasoning beyond immediate pattern matching. In total, we generated 770k (passage, query, answers) triples.\n\n3.3 Machine Filtering\nAs discussed in Jia and Liang (2017); Rajpurkar et al. (2018); Wang and Bansal (2018); Kaushik and Lipton (2018), existing MRC models mostly learn to predict the answer by simply paraphrasing questions into declarative forms, and then matching them with the sentences in the passages. To overcome this limitation, we filtered out triples whose queries can be easily answered by the stateof-the-art MRC architecture, Stochastic Answer Networks (SAN) (Liu et al., 2018). We choose SAN because it is competitive on existing MRC datasets, and it has components widely used in many MRC architectures such that low bias was anticipated in the filtering (which is confirmed by evaluation in Section 5). We used SAN to perform a five-fold cross validation on all 770k triples. The SAN models correctly answered 68% of these triples. We excluded those triples, and only kept 244k triples that could not be answered by SAN. These triples contain queries which could not be answered by simple paraphrasing, and other types of reasoning such as commonsense reasoning and multi-sentence reasoning are needed.\n\n3.4 Human Filtering\nSince the first three stages of data collection were fully automated, the resulted triples could be noisy and ambiguous to human readers. Therefore, we employed crowdworkers to validate these triples. We used Amazon Mechanical Turk for validation. Crowdworkers were required to: 1) have a 95% HIT acceptance rate, 2) a minimum of 50 HITs, 3) be located in the United States, Canada, or Great Britain, and 4) not be granted the qualification of poor quality (which we will explain later in this section). Workers were asked to spend at least 30 seconds on each assignment, and paid $3.6 per hour on average.\nFigure 4 shows the crowdsourcing web interface. Each HIT corresponds to a triple in our data collection. In each HIT assignment, we first showed the expandable instructions for first-time workers, to help them better understand our task (see the Appendix A.2). Then we presented workers with a passage in which the named entities are highlighted and clickable. After reading the passage, workers were given a supported statement with a placeholder (i.e., a cloze-style query) indicating a missing entity. Based on their understanding of the events that might be inferred from the passage, workers were asked to find the correct entity in the passage that best fits the placeholder. If workers thought the answer is not obvious, they were allowed to guess one, and were required to report that case in the feedback box. Workers were also encouraged to write other feedback. To ensure quality and prevent spamming, we used the reference answers in the triples to compute workers' average performance after every 1000 submissions. While there might be coreference or named entity recognition errors in the reference answers, as reported in Chen et al. (2016) (also confirmed by our analysis in Section 4), they only accounted for a very small portion of all the reference answers. Thus, the reference answers could be used for comparing workers' performance. Specifically, if a worker's performance was significantly lower than the average performance of all workers, we blocked the worker by granting the qualification of poor quality. In practice, workers were able to correctly answer about 50% of all queries. We blocked workers if their average accuracy was lower than 20%, and then republished their HIT assignments. Overall, 2,257 crowdworkers have participated in our task, and 51 of them have been granted the qualification of poor quality. Train / Dev. / Test Splits Among all the 244k triples collected from the third stage, we first obtained one worker answer for each triple. Compared to the reference answers, workers correctly answered queries in 122k triples. We then selected around 100k correctly-answered triples as the training set, restricting the origins of these triples to the news articles used in Hermann et al. (2015). As for the development and test sets, we solicited another worker answer to further ensure their quality. Therefore, each of the rest 22k triples has been validated by two workers. We only kept 20k triples that were correctly answered by both workers. The origins of these triples are either articles used in Hermann et al. (2015) or articles crawled by us (as described in Section 3.1), with a ratio of 3:7. Finally, we randomly split the 20k triples into development and test sets, with 10k triples for each set.\n\n4 Data Analysis\nReCoRD differs from other reading comprehension datasets due to its unique requirement for reasoning more than just paraphrasing. In this section, we provide a qualitative analysis of ReCoRD which highlights its unique features.\nReasoning Types We sampled 100 examples from the development set, and then manually categorized them into types shown in table 2. The results show that significantly different from existing datasets such as SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2017), ReCoRD requires commonsense reasoning to answer 75% of queries. Owing to the machine filtering stage, only 3% queries could be answered by paraphrasing. The small percentage (6%) of ambiguous queries demonstrate the benefit of the human filtering stage. We also noticed that 10% queries can be answered through partial clues. As the example shows, some of partial clues were caused by the incompleteness of named entity recognition in the stage of news article curation.\nTypes of Commonsense Reasoning Formalizing the commonsense knowledge needed for even simple reasoning problems is a huge undertaking.\nBased on the observation of the sampled queries that required commonsense reasoning, we roughly categorized them into the following four coarsegained types:  Conceptual Knowledge: the presumed knowledge of properties of concepts (Miller, 1995; Liu and Singh, 2004; Pas \u00b8ca and Van Durme, 2008; Zhang et al., 2017).\nCausal Reasoning: the causal bridging inference invoked between two events, which is validated against common sense (Singer et al., 1992; Roemmele et al., 2011).\nNa\u00efve Psychology: the predictable human mental states in reaction to events (Stich and Ravenscroft, 1994).\nOther: Other types of common sense, such as social norms, planning, spatial reasoning, etc. We annotated one or more types to each of these queries, and computed the percentage of them in these queries as shown in Table 3.\n\n5 Evaluation\nWe are interested in the performance of existing MRC architectures on ReCoRD. According to the task definition in Section 2, ReCoRD can be formalized as two types of machine reading comprehension (MRC) datasets: passages with clozestyle queries, or passages with queries whose answers are spans in the passage. Therefore, we can evaluate two types of MRC models on ReCoRD, and compare them with human performance. All the evaluation is carried out based on the train /dev. /test split as illustrated in Table 1.\n\n5.1 Methods\nDocQA 2 (Clark and Gardner, 2018) is a strong baseline model for queries with extractive answers. It consists of components such as bidirectional attention flow (Seo et al., 2016) and self attention which are widely used in MRC models. We also evaluate DocQA with ELMo (Peters et al., 2018) to analyze the impact of largely pretrained encoder on our dataset.  (Vaswani et al., 2017). Through QANet, we can evaluate the reasoning ability of transformer on our dataset. SAN 4 (Liu et al., 2018) is also a top-rank MRC model. It shares many components with DocQA, and employs a stochastic answer module. Since we used SAN to filter out easy queries in our data collection, it is necessary to verify that the queries we collect is hard for not only SAN but also other MRC architectures.\nASReader 5 (Kadlec et al., 2016) is a strong baseline model for cloze-style datasets such as (Her-mann et al., 2015; Hill et al., 2015). Unlike other baseline models which search among all text spans in the passage, ASReader directly predicts answers from the candidate named entities. Language Models 6 (LMs) (Trinh and Le, 2018) trained on large corpora recently achieved the state-of-the-art scores on the Winograd Schema Challenge (Levesque et al., 2011). Following in the same manner, we first concatenate the passage and the query together as a long sequence, and substitute X in the long sequence with each candidate entity; we use LMs to compute the probability of each resultant sequence and the substitution that results in the most probable sequence will be the predicted answer.\nRandom Guess acts as the lower bound of the evaluated models. It considers the queries in our dataset as cloze style, and randomly picks a candidate entity from the passage as the answer.\n\n5.2 Human Performance\nAs described in Section 3.4, we obtained two worker answers for each query in the development and test sets, and confirmed that each query has been correctly answered by two different workers.\nTo get human performance, we obtained an additional worker answer for each query, and compare it with the reference answers.\n\n5.3 Metrics\nWe use two evaluation metrics similar to those used by SQuAD (Rajpurkar et al., 2016). Both ignore punctuations and articles (e.g., a, an, the).\nExact Match (EM) measures the percentage of predictions that match any one of the reference answers exactly.\n(Macro-averaged) F1 measures the average overlap between the prediction and the reference answers. We treat the prediction and the reference answer as bags of tokens, and compute their F1. We take the maximum F1 over all of the reference answers for a given query, and then average over all of the queries.\n\n5.4 Results\nWe show the evaluation results in Table 4. Humans are able to get 91.31 EM and 91.69 F1 on the set, with similar results on the development set. In contrast, the best automatic method -DocQA with ELMo -achieves 45.44 EM and 46.65 F1 on the test set, illustrating a significant gap between human and machine reading comprehension on ReCoRD. All other methods without ELMo get EM/F1 scores significantly lower than DocQA with ELMo, which shows the positive impact of ELMo (see in Section 5.5). We also note that SAN leads to a result comparable with other strong baseline methods. This confirms that since SAN shares general components with many MRC models, using it to do machine filtering does help us filter out queries that are relatively easy to all the methods we evaluate. Finally, to our surprise, the unsupervised method (i.e., LM) which achieved the state-of-the-art scores on the Winograd Schema Challenge only leads to a result similar to the random guess baseline: a potential explanation is the lack of domain knowledge on our dataset. We leave this question for future work.\n\n5.5 Analysis\nHuman Errors About 8% dev./test queries have not been correctly answered in the human evaluation. We analyzed samples from these queries, and found that in most queries human was able to narrow down the set of possible candidate entities, but not able to find a unique answer. In many cases, two candidate entities equally fit X unless human has the specific background knowledge. We show an example in the Appendix A.1.\nFor the method analysis, we mainly analyzed the results of three representative methods: DocQA w/ ELMo, DocQA, and QANet. Impact of ELMo As shown in Figure 5, among all three methods the correct predictions of DocQA w/ ELMo have the largest overlap (92.6%) with the human predictions. As an ablation study, we analyzed queries which were only correctly answered after ELMo was added. We found that in some cases ELMo helped the prediction by incorporating the knowledge of language models. We show an example in the Appendix A.1.\nPredictions of QANet Figure 5 shows that QANet correctly answered some ambiguous queries, which we think was due to the randomness of parameter initialization and did not reflect the true reasoning ability. Since QANet uses the transformer-based encoder and DocQA uses the LSTM-based encoder, we see a significant difference of predictions between QANet and DocQA.\n\nMethod\nOOC Rate DocQA w/ ELMo 6.27% DocQA 6.37% QANet 6.41% Impact of Cloze-style Setting Except ASReader, all the MRC models were evaluated under the extractive setting, which means the information of candidate named entities was not used. Instead, extractive models searched answers from all possible text spans in passages. To show the potential benefit of using the candidate entities in these models, we computed the percentage of model predictions that could not be found in the candidate entities. As shown in Table 5, all three methods have about 6% OOC predictions. Making use of the candidate entities would potentially help them increase the performance by 6%.\nIn Section 4, we manually labeled 100 randomly sampled queries with different types of reasoning. In Figure 6 and 7, we show the performance of three analyzed methods on these queries. Figure 6 shows that three methods performed poorly on queries requiring commonsense reasoning, multi-sentence reasoning and partial clue.\nCompared to DocQA, QANet performed better on multi-sentence reasoning queries probably due to the use of transformer. Also, QANet outperformed DocQA on paraphrased queries probably because we used SAN to filtering queries and SAN has an architecture similar to DocQA. As we expect, ELMo improved the performance of DocQA on paraphrased queries. Among the 75% sampled queries that require commonsense reasoning, we see that ELMo significantly improved the performance of commonsense reasoning with presumed knowledge. For all other types of commonsense reasoning, all three methods have relatively poor performance.\n\n6 Related Datasets\nReCoRD relates to two strands of research in datasets: data for reading comprehension, and that for commonsense reasoning.\n\nReading Comprehension\nThe CNN/Daily Mail Corpus (Hermann et al., 2015), The Children's Book Test (CBT) (Hill et al., 2015), and LAM-BADA (Paperno et al., 2016) are closely related to ReCoRD: (1) The CNN/Daily Mail Corpus constructed queries from the bullet points, most of which required limited reasoning ability (Chen et al., 2016). ( 2) CBT is a collection of 21 consecutive sentences from book excerpts, with one word randomly removed from the last sentence. Since CBT has no machine or human filtering to ensure quality, only a small portion of the CBT examples really probes machines' ability to understand the context. (3) Built in a similar manner to CBT, LAMBADA was filtered to be humanguessable in the broader context only. Differing from ReCoRD, LAMBADA was designed to be a language modeling problem where contexts were not required to be event summaries, and answers were not necessarily in the context.\nSince all candidate answers were extracted from in the passage, ReCoRD can also be formalized as a extractive MRC dataset, similar to SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). The difference is that questions in these datasets were curated from crowdworkers. Since it is hard to control the quality of crowdsourced questions, a large portion of questions in these datasets can be answered by word matching or paraphrasing (Jia and Liang, 2017; Rajpurkar et al., 2018; Wang and Bansal, 2018). There are other large-scale datasets (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017; Dunn et al., 2017; Kocisky et al., 2018; Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018) targeting different aspects of reading comprehension. See (Gao et al., 2018) for a recent survey. Commonsense Reasoning ROCStories Corpus (Mostafazadeh et al., 2016), SWAG (Zellers et al., 2018), and The Winograd Schema Challenge (WSC) (Levesque et al., 2011) are related ReCoRD: (1) ROCStories assesses commonsense reasoning in story understanding by choosing the correct story ending from only two candidates. Stories in the corpus were all curated from crowdworkers, which could suffer from human elicitation bias (Gordon and Van Durme, 2013; Misra et al., 2016; Zhang et al., 2017).\n(2) SWAG unifies commonsense reasoning and natural language inference. It selects an ending from multiple choices which is most likely to be anticipated from the situation describe in the premise.\nThe counterfactual endings in SWAG were generated using language models with adversarial filtering. (3) WSC foucses on intra-sentential pronoun disambiguation problems that require commonsense reasoning. There are other datasets (Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b) targeting different aspects of commonsense reasoning.\n\n7 Conclusion\nWe introduced ReCoRD, a large-scale reading comprehension dataset requiring commonsense reasoning. Unlike existing machine reading comprehension (MRC) datasets, ReCoRD contains a large portion of queries that require commonsense reasoning to be answered. Our baselines, including top performers on existing MRC datasets, are no match for human competence on ReCoRD. We hope that ReCoRD will spur more research in MRC with commonsense reasoning.\n\nA Appendices\nA.1 Case Study Human Error Table 6 shows an example where the ambiguous query caused human error. The passage in this example describes \"ambiverts\", and there are two experts studying it: \"Vanessa Van Edwards\" and \"Adam Grant\". Both of them fit in the query asking who gave advice to ambiverts. There is no further information to help human choose a unique answer for this query.\nPassage: Your colleagues think you're quiet, but your friends think you're a party animal. If that sounds like you, then you may be what psychologists describe as an 'ambivert'. Scientists believe around two-thirds of people are ambiverts; a personality category that has, up until now, been given relatively little attention. 'Most people who are ambiverts have been told the wrong category their whole life,' Vanessa Van Edwards, an Orgeon-based behavioural expert, told DailyMail.com 'You hear extrovert and you hear introvert, and you think 'ugh, that's not me'.' Ambiversion is a label that has been around for some time, but gained popularity in 2013 with a paper in the journal Psychological Science, by Adam Grant the University of Pennsylvania.\n\u2022 Most ambiverts have been labelled incorrectly their whole life\n\u2022 They slide up and down personality spectrum depending on the situation\n\u2022 Ambiverts are good at gaining people's trust and making their point heard\n\u2022 They often feel pressure to mirror personality of the person they are with Query: 'Read each situation more carefully,' X advised ambiverts, 'and ask yourself, 'What do I need to do right now to be most happy or successful?\" Reference answers: Adam Grant Table 6 : An example illustrating a ambiguous query.\nImpact of ELMo Table 7 shows an example where DocQA w/ ELMo correctly answered but DocQA failed. The passage in this example describes a woman artist \"Sarah Milne\" who launched a public appeal to find a handsome stranger \"William Scott Chalmers\", and invited him to meet her. The query asks the missing information in the greetings from \"William Scott Chalmers\" when he went to meet \"Sarah Milne\".\nOur common sense about social norms tells us when two people meet each other for the first time, they are very likely to first introduce themselves.\nIn the query of this example, when Mr. Chalmers said \"Hello, I'm . . . \", it is very likely that he was introducing himself. Therefore, the name of Mr Chalmer fit X best.\nIn this example, the prediction of DocQA without ELMo is \"New Zealand\" which is not even close to the reference answer. The benefit of using ELMo in this example is that its language model will help exclude \"New Zealand\" from the likely candidate answers, because \"I'm . . . \" is usually followed by a person name rather than a location name. Such a pattern learnt by ELMo is useful in narrowing down candidiate answers in ReCoRD.\nPassage: A British backpacker who wrote a romantic note to locate a handsome stranger after spotting him on a New Zealand beach has finally met her Romeo for the first time. Sarah Milne, from Glasgow, left a handmade poster for the man, who she saw in Picton on Friday and described as 'shirtless, wearing black shorts with stars tattooed on his torso and running with a curly, bouncy and blonde dog'. In her note, entitled 'Is this you? ', she invited the mystery stranger to meet her on the same beach on Tuesday. But the message soon became a source of huge online interest with the identity of both the author and its intended target generating unexpected publicity.\n\u2022 Sarah Milne, a Glasgow artist, launched a public appeal to find the mystery man\n\u2022 She wrote a heart-warming message and drew a picture of him with his dog\n\u2022 She said she would return to the same spot in Picton, New Zealand, on Tuesday in search for him\n\u2022 William Scott Chalmers revealed himself as the man and went to meet her\n\nA.2 HIT Instructions\nWe show the instructions for Amazon Mechanical Turk HITs in Figure 8.\n\nFootnotes:\n2: https://github.com/allenai/ document-qa\n6: https://github.com/tensorflow/models/ tree/master/research/lm_commonsense\n\nReferences:\n\n- Danqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 2358-2367, Berlin, Germany. Asso- ciation for Computational Linguistics.- Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen- tau Yih, Yejin Choi, Percy Liang, and Luke Zettle- moyer. 2018. Quac: Question answering in context. arXiv preprint arXiv:1808.07036.\n\n- Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 845-855. Association for Computational Linguistics.\n\n- Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.\n\n- Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational ai. arXiv preprint arXiv:1809.08267.\n\n- Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language pro- cessing platform. arXiv preprint arXiv:1803.07640.\n\n- Jonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In Proceed- ings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC '13, pages 25-30, New York, NY, USA. ACM.\n\n- Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1693- 1701.\n\n- Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representa- tions. arXiv preprint arXiv:1511.02301.\n\n- Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.\n\n- Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.\n\n- Rudolf Kadlec, Martin Schmid, Ond\u0159ej Bajgar, and Jan Kleindienst. 2016. Text understanding with the at- tention sum reader network. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 908-918. Association for Computational Linguis- tics.\n\n- Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks.\n\n- Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Asso- ciation for Computational Linguistics, 6:317-328.\n\n- Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In Pro- ceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, pages 785- 794, Copenhagen, Denmark. Association for Com- putational Linguistics.\n\n- Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference reso- lution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 188-197, Copenhagen, Denmark. Asso- ciation for Computational Linguistics.\n\n- Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning.\n\n- H. Liu and P. Singh. 2004. Conceptnet &mdash; a practical commonsense reasoning tool-kit. BT Tech- nology Journal, 22(4):211-226.\n\n- Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2018. Stochastic answer networks for ma- chine reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1694-1704. Association for Computational Linguis- tics.\n\n- Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Association for Compu- tational Linguistics (ACL) System Demonstrations, pages 55-60.\n\n- John McCarthy. 1959. Programs with common sense. In Proceedings of the Teddington Conference on the Mechanization of Thought Processes, London: Her Majesty's Stationery Office.\n\n- George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41.\n\n- Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. 2016. Seeing through the human reporting bias: Visual classifiers from noisy human- centric labels. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 2930-2939.\n\n- Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguis- tics.\n\n- Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.\n\n- Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari- dou, Ngoc Quan Pham, Raffaella Bernardi, San- dro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534. Association for Computational Linguistics.\n\n- Marius Pas \u00b8ca and Benjamin Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of ACL-08: HLT, pages 19-27. Association for Computational Linguistics.\n\n- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227- 2237. Association for Computational Linguistics.\n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 784-789. Association for Computational Linguistics.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\n- Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, and Yejin Choi. 2018a. Modeling naive psychology of characters in simple common- sense stories. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2289- 2299. Association for Computational Linguistics.\n\n- Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, and Yejin Choi. 2018b. Event2mind: Commonsense inference on events, intents, and reactions. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 463-473. Association for Computational Linguistics.\n\n- Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042.\n\n- Melissa Roemmele, Cosmin Adrian Bejan, and An- drew S Gordon. 2011. Choice of plausible alterna- tives: An evaluation of commonsense causal reason- ing. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning, pages 90-95.\n\n- Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.\n\n- Murray Singer, Michael Halldorson, Jeffrey C Lear, and Peter Andrusiak. 1992. Validation of causal bridging inferences in discourse understanding. Journal of Memory and Language, 31(4):507 -524.\n\n- Stephen Stich and Ian Ravenscroft. 1994. What is folk psychology? Cognition, 50(1-3):447-468.\n\n- Krysta Svore, Lucy Vanderwende, and Christopher Burges. 2007. Enhancing single-document sum- marization by combining RankNet and third-party sources. In Proceedings of the 2007 Joint Con- ference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning (EMNLP-CoNLL), pages 448-457, Prague, Czech Republic. Association for Computa- tional Linguistics.\n\n- Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.\n\n- Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017. Newsqa: A machine compre- hension dataset. In Proceedings of the 2nd Work- shop on Representation Learning for NLP, pages 191-200, Vancouver, Canada. Association for Com- putational Linguistics.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998-6008.\n\n- Yicheng Wang and Mohit Bansal. 2018. Robust ma- chine comprehension models via adversarial train- ing. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 575-581. Association for Computational Linguistics.\n\n- Kristian Woodsend and Mirella Lapata. 2010. Auto- matic generation of story highlights. In Proceed- ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565-574, Up- psala, Sweden. Association for Computational Lin- guistics.\n\n- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\n- Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. arXiv preprint arXiv:1804.09541.\n\n- Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\n- Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben- jamin Van Durme. 2017. Ordinal common-sense in- ference. Transactions of the Association for Com- putational Linguistics, 5:379-395.\n\n", "annotations": {"Abstract": [{"begin": 90, "end": 515, "idx": 0}], "Head": [{"begin": 518, "end": 532, "n": "1", "idx": 0}, {"begin": 1820, "end": 1847, "idx": 1}, {"begin": 2029, "end": 2046, "idx": 2}, {"begin": 5048, "end": 5065, "n": "2", "idx": 3}, {"begin": 7295, "end": 7312, "n": "3", "idx": 4}, {"begin": 7846, "end": 7871, "n": "3.1", "idx": 5}, {"begin": 11827, "end": 11843, "idx": 6}, {"begin": 13294, "end": 13321, "idx": 7}, {"begin": 13596, "end": 13613, "idx": 8}, {"begin": 14183, "end": 14204, "n": "3.3", "idx": 9}, {"begin": 15302, "end": 15321, "n": "3.4", "idx": 10}, {"begin": 18688, "end": 18703, "n": "4", "idx": 11}, {"begin": 20622, "end": 20634, "n": "5", "idx": 12}, {"begin": 21148, "end": 21159, "n": "5.1", "idx": 13}, {"begin": 22923, "end": 22944, "n": "5.2", "idx": 14}, {"begin": 23264, "end": 23275, "n": "5.3", "idx": 15}, {"begin": 23838, "end": 23849, "n": "5.4", "idx": 16}, {"begin": 24939, "end": 24951, "n": "5.5", "idx": 17}, {"begin": 26269, "end": 26275, "idx": 18}, {"begin": 27880, "end": 27898, "n": "6", "idx": 19}, {"begin": 28023, "end": 28044, "idx": 20}, {"begin": 30792, "end": 30804, "n": "7", "idx": 21}, {"begin": 31251, "end": 31263, "idx": 22}, {"begin": 35072, "end": 35092, "idx": 23}], "ReferenceToBib": [{"begin": 687, "end": 709, "target": "#b7", "idx": 0}, {"begin": 710, "end": 728, "target": "#b8", "idx": 1}, {"begin": 729, "end": 752, "target": "#b29", "idx": 2}, {"begin": 753, "end": 776, "target": "#b39", "idx": 3}, {"begin": 777, "end": 797, "target": "#b24", "idx": 4}, {"begin": 901, "end": 922, "target": "#b9", "idx": 5}, {"begin": 923, "end": 946, "target": "#b28", "idx": 6}, {"begin": 947, "end": 972, "target": "#b12", "idx": 7}, {"begin": 1542, "end": 1566, "target": "#b29", "idx": 8}, {"begin": 1567, "end": 1590, "target": "#b39", "idx": 9}, {"begin": 4518, "end": 4546, "target": "#b6", "idx": 10}, {"begin": 4547, "end": 4566, "target": "#b22", "idx": 11}, {"begin": 4567, "end": 4586, "target": "#b46", "idx": 12}, {"begin": 5236, "end": 5251, "target": "#b20", "idx": 13}, {"begin": 5863, "end": 5886, "target": "#b16", "idx": 14}, {"begin": 5901, "end": 5917, "idx": 15}, {"begin": 8374, "end": 8394, "target": "#b37", "idx": 16}, {"begin": 8395, "end": 8421, "target": "#b42", "idx": 17}, {"begin": 8422, "end": 8443, "target": "#b7", "idx": 18}, {"begin": 8632, "end": 8653, "target": "#b7", "idx": 19}, {"begin": 9220, "end": 9241, "target": "#b7", "idx": 20}, {"begin": 9306, "end": 9327, "target": "#b27", "idx": 21}, {"begin": 9376, "end": 9394, "target": "#b15", "idx": 22}, {"begin": 9416, "end": 9438, "target": "#b5", "idx": 23}, {"begin": 10649, "end": 10671, "target": "#b19", "idx": 24}, {"begin": 14243, "end": 14266, "target": "#b28", "idx": 25}, {"begin": 14268, "end": 14290, "target": "#b41", "idx": 26}, {"begin": 14292, "end": 14317, "target": "#b12", "idx": 27}, {"begin": 14654, "end": 14672, "target": "#b18", "idx": 28}, {"begin": 17066, "end": 17084, "target": "#b0", "idx": 29}, {"begin": 18164, "end": 18170, "idx": 30}, {"begin": 18481, "end": 18502, "target": "#b7", "idx": 31}, {"begin": 19146, "end": 19170, "target": "#b29", "idx": 32}, {"begin": 19183, "end": 19207, "target": "#b39", "idx": 33}, {"begin": 20043, "end": 20057, "target": "#b21", "idx": 34}, {"begin": 20058, "end": 20078, "target": "#b17", "idx": 35}, {"begin": 20079, "end": 20107, "target": "#b26", "idx": 36}, {"begin": 20108, "end": 20127, "target": "#b46", "idx": 37}, {"begin": 20245, "end": 20266, "target": "#b35", "idx": 38}, {"begin": 20267, "end": 20289, "target": "#b33", "idx": 39}, {"begin": 20367, "end": 20396, "target": "#b36", "idx": 40}, {"begin": 21321, "end": 21339, "target": "#b34", "idx": 41}, {"begin": 21429, "end": 21450, "target": "#b27", "idx": 42}, {"begin": 21520, "end": 21542, "target": "#b40", "idx": 43}, {"begin": 21634, "end": 21652, "target": "#b18", "idx": 44}, {"begin": 21954, "end": 21974, "target": "#b11", "idx": 45}, {"begin": 22036, "end": 22059, "idx": 46}, {"begin": 22060, "end": 22078, "target": "#b8", "idx": 47}, {"begin": 22253, "end": 22273, "target": "#b38", "idx": 48}, {"begin": 22378, "end": 22401, "target": "#b16", "idx": 49}, {"begin": 23337, "end": 23361, "target": "#b29", "idx": 50}, {"begin": 28071, "end": 28093, "target": "#b7", "idx": 51}, {"begin": 28126, "end": 28145, "target": "#b8", "idx": 52}, {"begin": 28160, "end": 28182, "target": "#b25", "idx": 53}, {"begin": 28337, "end": 28356, "target": "#b0", "idx": 54}, {"begin": 29081, "end": 29105, "target": "#b29", "idx": 55}, {"begin": 29117, "end": 29141, "target": "#b39", "idx": 56}, {"begin": 29389, "end": 29410, "target": "#b9", "idx": 57}, {"begin": 29411, "end": 29434, "target": "#b28", "idx": 58}, {"begin": 29435, "end": 29457, "target": "#b41", "idx": 59}, {"begin": 29496, "end": 29517, "target": "#b24", "idx": 60}, {"begin": 29518, "end": 29537, "target": "#b10", "idx": 61}, {"begin": 29538, "end": 29555, "target": "#b14", "idx": 62}, {"begin": 29556, "end": 29574, "target": "#b3", "idx": 63}, {"begin": 29575, "end": 29596, "target": "#b13", "idx": 64}, {"begin": 29597, "end": 29616, "target": "#b32", "idx": 65}, {"begin": 29617, "end": 29635, "target": "#b1", "idx": 66}, {"begin": 29636, "end": 29654, "target": "#b43", "idx": 67}, {"begin": 29713, "end": 29731, "target": "#b4", "idx": 68}, {"begin": 29793, "end": 29820, "target": "#b23", "idx": 69}, {"begin": 29827, "end": 29849, "target": "#b45", "idx": 70}, {"begin": 29891, "end": 29914, "target": "#b16", "idx": 71}, {"begin": 30172, "end": 30200, "target": "#b6", "idx": 72}, {"begin": 30201, "end": 30220, "target": "#b22", "idx": 73}, {"begin": 30221, "end": 30240, "target": "#b46", "idx": 74}, {"begin": 30668, "end": 30691, "target": "#b33", "idx": 75}, {"begin": 30692, "end": 30711, "target": "#b46", "idx": 76}, {"begin": 30712, "end": 30736, "idx": 77}], "ReferenceToFootnote": [{"begin": 21166, "end": 21167, "target": "#foot_0", "idx": 0}, {"begin": 22245, "end": 22246, "target": "#foot_1", "idx": 1}], "SectionFootnote": [{"begin": 35164, "end": 35294, "idx": 0}], "ReferenceString": [{"begin": 35311, "end": 35644, "id": "b0", "idx": 0}, {"begin": 35646, "end": 35831, "id": "b1", "idx": 1}, {"begin": 35835, "end": 36114, "id": "b2", "idx": 2}, {"begin": 36118, "end": 36318, "id": "b3", "idx": 3}, {"begin": 36322, "end": 36444, "id": "b4", "idx": 4}, {"begin": 36448, "end": 36692, "id": "b5", "idx": 5}, {"begin": 36696, "end": 36912, "id": "b6", "idx": 6}, {"begin": 36916, "end": 37160, "id": "b7", "idx": 7}, {"begin": 37164, "end": 37353, "id": "b8", "idx": 8}, {"begin": 37357, "end": 37632, "id": "b9", "idx": 9}, {"begin": 37636, "end": 37992, "id": "b10", "idx": 10}, {"begin": 37996, "end": 38305, "id": "b11", "idx": 11}, {"begin": 38309, "end": 38455, "id": "b12", "idx": 12}, {"begin": 38459, "end": 38707, "id": "b13", "idx": 13}, {"begin": 38711, "end": 39026, "id": "b14", "idx": 14}, {"begin": 39030, "end": 39312, "id": "b15", "idx": 15}, {"begin": 39316, "end": 39485, "id": "b16", "idx": 16}, {"begin": 39489, "end": 39618, "id": "b17", "idx": 17}, {"begin": 39622, "end": 39927, "id": "b18", "idx": 18}, {"begin": 39931, "end": 40192, "id": "b19", "idx": 19}, {"begin": 40196, "end": 40372, "id": "b20", "idx": 20}, {"begin": 40376, "end": 40467, "id": "b21", "idx": 21}, {"begin": 40471, "end": 40742, "id": "b22", "idx": 22}, {"begin": 40746, "end": 41193, "id": "b23", "idx": 23}, {"begin": 41197, "end": 41399, "id": "b24", "idx": 24}, {"begin": 41403, "end": 41819, "id": "b25", "idx": 25}, {"begin": 41823, "end": 42063, "id": "b26", "idx": 26}, {"begin": 42067, "end": 42458, "id": "b27", "idx": 27}, {"begin": 42462, "end": 42752, "id": "b28", "idx": 28}, {"begin": 42756, "end": 43057, "id": "b29", "idx": 29}, {"begin": 43061, "end": 43396, "id": "b30", "idx": 30}, {"begin": 43400, "end": 43726, "id": "b31", "idx": 31}, {"begin": 43730, "end": 43872, "id": "b32", "idx": 32}, {"begin": 43876, "end": 44118, "id": "b33", "idx": 33}, {"begin": 44122, "end": 44287, "id": "b34", "idx": 34}, {"begin": 44291, "end": 44485, "id": "b35", "idx": 35}, {"begin": 44489, "end": 44582, "id": "b36", "idx": 36}, {"begin": 44586, "end": 44980, "id": "b37", "idx": 37}, {"begin": 44984, "end": 45094, "id": "b38", "idx": 38}, {"begin": 45098, "end": 45413, "id": "b39", "idx": 39}, {"begin": 45417, "end": 45647, "id": "b40", "idx": 40}, {"begin": 45651, "end": 45985, "id": "b41", "idx": 41}, {"begin": 45989, "end": 46249, "id": "b42", "idx": 42}, {"begin": 46253, "end": 46550, "id": "b43", "idx": 43}, {"begin": 46554, "end": 46781, "id": "b44", "idx": 44}, {"begin": 46785, "end": 47026, "id": "b45", "idx": 45}, {"begin": 47030, "end": 47212, "id": "b46", "idx": 46}], "ReferenceToTable": [{"begin": 20618, "end": 20619, "target": "#tab_4", "idx": 0}, {"begin": 21144, "end": 21145, "target": "#tab_0", "idx": 1}, {"begin": 23890, "end": 23891, "target": "#tab_5", "idx": 2}, {"begin": 26792, "end": 26793, "target": "#tab_6", "idx": 3}, {"begin": 31297, "end": 31298, "idx": 4}, {"begin": 32875, "end": 32876, "idx": 5}, {"begin": 32943, "end": 32944, "target": "#tab_8", "idx": 6}], "Footnote": [{"begin": 35175, "end": 35217, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 35218, "end": 35294, "id": "foot_1", "n": "6", "idx": 1}], "ReferenceToFormula": [{"begin": 7424, "end": 7425, "target": "#formula_1", "idx": 0}, {"begin": 28360, "end": 28361, "target": "#formula_1", "idx": 1}], "Paragraph": [{"begin": 100, "end": 515, "idx": 0}, {"begin": 533, "end": 1591, "idx": 1}, {"begin": 1592, "end": 1818, "idx": 2}, {"begin": 1848, "end": 2027, "idx": 3}, {"begin": 2047, "end": 2963, "idx": 4}, {"begin": 2964, "end": 3528, "idx": 5}, {"begin": 3529, "end": 3643, "idx": 6}, {"begin": 3644, "end": 4337, "idx": 7}, {"begin": 4338, "end": 4734, "idx": 8}, {"begin": 4735, "end": 5046, "idx": 9}, {"begin": 5066, "end": 6534, "idx": 10}, {"begin": 6566, "end": 6569, "idx": 11}, {"begin": 6570, "end": 7035, "idx": 12}, {"begin": 7070, "end": 7293, "idx": 13}, {"begin": 7313, "end": 7844, "idx": 14}, {"begin": 7872, "end": 8547, "idx": 15}, {"begin": 8548, "end": 9598, "idx": 16}, {"begin": 9599, "end": 10822, "idx": 17}, {"begin": 10823, "end": 10978, "idx": 18}, {"begin": 10979, "end": 11091, "idx": 19}, {"begin": 11092, "end": 11236, "idx": 20}, {"begin": 11237, "end": 11353, "idx": 21}, {"begin": 11354, "end": 11581, "idx": 22}, {"begin": 11582, "end": 11825, "idx": 23}, {"begin": 11844, "end": 11886, "idx": 24}, {"begin": 11887, "end": 11940, "idx": 25}, {"begin": 11941, "end": 11992, "idx": 26}, {"begin": 11993, "end": 12112, "idx": 27}, {"begin": 12113, "end": 12512, "idx": 28}, {"begin": 12513, "end": 12902, "idx": 29}, {"begin": 12903, "end": 13292, "idx": 30}, {"begin": 13322, "end": 13594, "idx": 31}, {"begin": 13614, "end": 13693, "idx": 32}, {"begin": 13694, "end": 14181, "idx": 33}, {"begin": 14205, "end": 15300, "idx": 34}, {"begin": 15322, "end": 15928, "idx": 35}, {"begin": 15929, "end": 18686, "idx": 36}, {"begin": 18704, "end": 18932, "idx": 37}, {"begin": 18933, "end": 19679, "idx": 38}, {"begin": 19680, "end": 19813, "idx": 39}, {"begin": 19814, "end": 20128, "idx": 40}, {"begin": 20129, "end": 20290, "idx": 41}, {"begin": 20291, "end": 20397, "idx": 42}, {"begin": 20398, "end": 20620, "idx": 43}, {"begin": 20635, "end": 21146, "idx": 44}, {"begin": 21160, "end": 21942, "idx": 45}, {"begin": 21943, "end": 22733, "idx": 46}, {"begin": 22734, "end": 22921, "idx": 47}, {"begin": 22945, "end": 23137, "idx": 48}, {"begin": 23138, "end": 23262, "idx": 49}, {"begin": 23276, "end": 23420, "idx": 50}, {"begin": 23421, "end": 23529, "idx": 51}, {"begin": 23530, "end": 23836, "idx": 52}, {"begin": 23850, "end": 24937, "idx": 53}, {"begin": 24952, "end": 25372, "idx": 54}, {"begin": 25373, "end": 25902, "idx": 55}, {"begin": 25903, "end": 26267, "idx": 56}, {"begin": 26276, "end": 26940, "idx": 57}, {"begin": 26941, "end": 27263, "idx": 58}, {"begin": 27264, "end": 27878, "idx": 59}, {"begin": 27899, "end": 28021, "idx": 60}, {"begin": 28045, "end": 28940, "idx": 61}, {"begin": 28941, "end": 30241, "idx": 62}, {"begin": 30242, "end": 30438, "idx": 63}, {"begin": 30439, "end": 30790, "idx": 64}, {"begin": 30805, "end": 31249, "idx": 65}, {"begin": 31264, "end": 31643, "idx": 66}, {"begin": 31644, "end": 32397, "idx": 67}, {"begin": 32398, "end": 32462, "idx": 68}, {"begin": 32463, "end": 32535, "idx": 69}, {"begin": 32536, "end": 32611, "idx": 70}, {"begin": 32612, "end": 32921, "idx": 71}, {"begin": 32922, "end": 33319, "idx": 72}, {"begin": 33320, "end": 33468, "idx": 73}, {"begin": 33469, "end": 33639, "idx": 74}, {"begin": 33640, "end": 34070, "idx": 75}, {"begin": 34071, "end": 34741, "idx": 76}, {"begin": 34742, "end": 34823, "idx": 77}, {"begin": 34824, "end": 34898, "idx": 78}, {"begin": 34899, "end": 34996, "idx": 79}, {"begin": 34997, "end": 35070, "idx": 80}, {"begin": 35093, "end": 35162, "idx": 81}], "SectionHeader": [{"begin": 0, "end": 515, "idx": 0}], "SectionReference": [{"begin": 35296, "end": 47214, "idx": 0}], "Sentence": [{"begin": 100, "end": 208, "idx": 0}, {"begin": 209, "end": 336, "idx": 1}, {"begin": 337, "end": 465, "idx": 2}, {"begin": 466, "end": 515, "idx": 3}, {"begin": 533, "end": 867, "idx": 4}, {"begin": 868, "end": 1147, "idx": 5}, {"begin": 1148, "end": 1379, "idx": 6}, {"begin": 1380, "end": 1591, "idx": 7}, {"begin": 1592, "end": 1818, "idx": 8}, {"begin": 1848, "end": 2027, "idx": 9}, {"begin": 2047, "end": 2173, "idx": 10}, {"begin": 2174, "end": 2403, "idx": 11}, {"begin": 2404, "end": 2573, "idx": 12}, {"begin": 2574, "end": 2697, "idx": 13}, {"begin": 2698, "end": 2827, "idx": 14}, {"begin": 2828, "end": 2963, "idx": 15}, {"begin": 2964, "end": 3149, "idx": 16}, {"begin": 3150, "end": 3257, "idx": 17}, {"begin": 3258, "end": 3334, "idx": 18}, {"begin": 3335, "end": 3403, "idx": 19}, {"begin": 3404, "end": 3456, "idx": 20}, {"begin": 3457, "end": 3528, "idx": 21}, {"begin": 3529, "end": 3556, "idx": 22}, {"begin": 3557, "end": 3643, "idx": 23}, {"begin": 3644, "end": 3829, "idx": 24}, {"begin": 3830, "end": 3905, "idx": 25}, {"begin": 3906, "end": 4138, "idx": 26}, {"begin": 4139, "end": 4337, "idx": 27}, {"begin": 4338, "end": 4648, "idx": 28}, {"begin": 4649, "end": 4734, "idx": 29}, {"begin": 4735, "end": 4908, "idx": 30}, {"begin": 4909, "end": 5046, "idx": 31}, {"begin": 5066, "end": 5233, "idx": 32}, {"begin": 5234, "end": 5553, "idx": 33}, {"begin": 5554, "end": 5661, "idx": 34}, {"begin": 5662, "end": 5736, "idx": 35}, {"begin": 5737, "end": 5887, "idx": 36}, {"begin": 5888, "end": 6217, "idx": 37}, {"begin": 6218, "end": 6534, "idx": 38}, {"begin": 6566, "end": 6569, "idx": 39}, {"begin": 6570, "end": 7035, "idx": 40}, {"begin": 7070, "end": 7293, "idx": 41}, {"begin": 7313, "end": 7522, "idx": 42}, {"begin": 7523, "end": 7844, "idx": 43}, {"begin": 7872, "end": 8261, "idx": 44}, {"begin": 8262, "end": 8547, "idx": 45}, {"begin": 8548, "end": 8710, "idx": 46}, {"begin": 8711, "end": 8885, "idx": 47}, {"begin": 8886, "end": 9065, "idx": 48}, {"begin": 9066, "end": 9154, "idx": 49}, {"begin": 9155, "end": 9242, "idx": 50}, {"begin": 9243, "end": 9472, "idx": 51}, {"begin": 9473, "end": 9598, "idx": 52}, {"begin": 9599, "end": 9636, "idx": 53}, {"begin": 9637, "end": 9778, "idx": 54}, {"begin": 9779, "end": 9823, "idx": 55}, {"begin": 9824, "end": 10042, "idx": 56}, {"begin": 10043, "end": 10139, "idx": 57}, {"begin": 10140, "end": 10239, "idx": 58}, {"begin": 10240, "end": 10342, "idx": 59}, {"begin": 10343, "end": 10521, "idx": 60}, {"begin": 10522, "end": 10672, "idx": 61}, {"begin": 10673, "end": 10822, "idx": 62}, {"begin": 10823, "end": 10978, "idx": 63}, {"begin": 10979, "end": 11091, "idx": 64}, {"begin": 11092, "end": 11236, "idx": 65}, {"begin": 11237, "end": 11353, "idx": 66}, {"begin": 11354, "end": 11581, "idx": 67}, {"begin": 11582, "end": 11667, "idx": 68}, {"begin": 11668, "end": 11768, "idx": 69}, {"begin": 11769, "end": 11825, "idx": 70}, {"begin": 11844, "end": 11886, "idx": 71}, {"begin": 11887, "end": 11940, "idx": 72}, {"begin": 11941, "end": 11992, "idx": 73}, {"begin": 11993, "end": 12112, "idx": 74}, {"begin": 12113, "end": 12342, "idx": 75}, {"begin": 12343, "end": 12512, "idx": 76}, {"begin": 12513, "end": 12636, "idx": 77}, {"begin": 12637, "end": 12766, "idx": 78}, {"begin": 12767, "end": 12902, "idx": 79}, {"begin": 12903, "end": 13102, "idx": 80}, {"begin": 13103, "end": 13292, "idx": 81}, {"begin": 13322, "end": 13501, "idx": 82}, {"begin": 13502, "end": 13594, "idx": 83}, {"begin": 13614, "end": 13693, "idx": 84}, {"begin": 13694, "end": 13869, "idx": 85}, {"begin": 13870, "end": 13928, "idx": 86}, {"begin": 13929, "end": 14118, "idx": 87}, {"begin": 14119, "end": 14181, "idx": 88}, {"begin": 14205, "end": 14489, "idx": 89}, {"begin": 14490, "end": 14673, "idx": 90}, {"begin": 14674, "end": 14900, "idx": 91}, {"begin": 14901, "end": 14973, "idx": 92}, {"begin": 14974, "end": 15029, "idx": 93}, {"begin": 15030, "end": 15118, "idx": 94}, {"begin": 15119, "end": 15300, "idx": 95}, {"begin": 15322, "end": 15459, "idx": 96}, {"begin": 15460, "end": 15522, "idx": 97}, {"begin": 15523, "end": 15569, "idx": 98}, {"begin": 15570, "end": 15825, "idx": 99}, {"begin": 15826, "end": 15928, "idx": 100}, {"begin": 15929, "end": 15976, "idx": 101}, {"begin": 15977, "end": 16033, "idx": 102}, {"begin": 16034, "end": 16289, "idx": 103}, {"begin": 16290, "end": 16433, "idx": 104}, {"begin": 16434, "end": 16610, "idx": 105}, {"begin": 16611, "end": 16747, "idx": 106}, {"begin": 16748, "end": 16801, "idx": 107}, {"begin": 16802, "end": 16956, "idx": 108}, {"begin": 16957, "end": 17206, "idx": 109}, {"begin": 17207, "end": 17284, "idx": 110}, {"begin": 17285, "end": 17462, "idx": 111}, {"begin": 17463, "end": 17539, "idx": 112}, {"begin": 17540, "end": 17648, "idx": 113}, {"begin": 17649, "end": 17775, "idx": 114}, {"begin": 17776, "end": 17788, "idx": 115}, {"begin": 17789, "end": 17914, "idx": 116}, {"begin": 17915, "end": 18001, "idx": 117}, {"begin": 18002, "end": 18171, "idx": 118}, {"begin": 18172, "end": 18277, "idx": 119}, {"begin": 18278, "end": 18352, "idx": 120}, {"begin": 18353, "end": 18423, "idx": 121}, {"begin": 18424, "end": 18580, "idx": 122}, {"begin": 18581, "end": 18686, "idx": 123}, {"begin": 18704, "end": 18833, "idx": 124}, {"begin": 18834, "end": 18932, "idx": 125}, {"begin": 18933, "end": 19272, "idx": 126}, {"begin": 19273, "end": 19361, "idx": 127}, {"begin": 19362, "end": 19462, "idx": 128}, {"begin": 19463, "end": 19534, "idx": 129}, {"begin": 19535, "end": 19679, "idx": 130}, {"begin": 19680, "end": 19813, "idx": 131}, {"begin": 19814, "end": 20128, "idx": 132}, {"begin": 20129, "end": 20290, "idx": 133}, {"begin": 20291, "end": 20397, "idx": 134}, {"begin": 20398, "end": 20489, "idx": 135}, {"begin": 20490, "end": 20620, "idx": 136}, {"begin": 20635, "end": 20712, "idx": 137}, {"begin": 20713, "end": 20945, "idx": 138}, {"begin": 20946, "end": 21048, "idx": 139}, {"begin": 21049, "end": 21107, "idx": 140}, {"begin": 21108, "end": 21146, "idx": 141}, {"begin": 21160, "end": 21257, "idx": 142}, {"begin": 21258, "end": 21395, "idx": 143}, {"begin": 21396, "end": 21518, "idx": 144}, {"begin": 21519, "end": 21543, "idx": 145}, {"begin": 21544, "end": 21627, "idx": 146}, {"begin": 21628, "end": 21682, "idx": 147}, {"begin": 21683, "end": 21760, "idx": 148}, {"begin": 21761, "end": 21942, "idx": 149}, {"begin": 21943, "end": 22079, "idx": 150}, {"begin": 22080, "end": 22228, "idx": 151}, {"begin": 22229, "end": 22402, "idx": 152}, {"begin": 22403, "end": 22733, "idx": 153}, {"begin": 22734, "end": 22795, "idx": 154}, {"begin": 22796, "end": 22921, "idx": 155}, {"begin": 22945, "end": 23137, "idx": 156}, {"begin": 23138, "end": 23262, "idx": 157}, {"begin": 23276, "end": 23362, "idx": 158}, {"begin": 23363, "end": 23420, "idx": 159}, {"begin": 23421, "end": 23529, "idx": 160}, {"begin": 23530, "end": 23628, "idx": 161}, {"begin": 23629, "end": 23718, "idx": 162}, {"begin": 23719, "end": 23836, "idx": 163}, {"begin": 23850, "end": 23921, "idx": 164}, {"begin": 23922, "end": 23994, "idx": 165}, {"begin": 23995, "end": 24189, "idx": 166}, {"begin": 24190, "end": 24341, "idx": 167}, {"begin": 24342, "end": 24428, "idx": 168}, {"begin": 24429, "end": 24627, "idx": 169}, {"begin": 24628, "end": 24897, "idx": 170}, {"begin": 24898, "end": 24937, "idx": 171}, {"begin": 24952, "end": 25049, "idx": 172}, {"begin": 25050, "end": 25228, "idx": 173}, {"begin": 25229, "end": 25332, "idx": 174}, {"begin": 25333, "end": 25372, "idx": 175}, {"begin": 25373, "end": 25494, "idx": 176}, {"begin": 25495, "end": 25657, "idx": 177}, {"begin": 25658, "end": 25756, "idx": 178}, {"begin": 25757, "end": 25862, "idx": 179}, {"begin": 25863, "end": 25902, "idx": 180}, {"begin": 25903, "end": 26109, "idx": 181}, {"begin": 26110, "end": 26267, "idx": 182}, {"begin": 26276, "end": 26509, "idx": 183}, {"begin": 26510, "end": 26595, "idx": 184}, {"begin": 26596, "end": 26773, "idx": 185}, {"begin": 26774, "end": 26843, "idx": 186}, {"begin": 26844, "end": 26940, "idx": 187}, {"begin": 26941, "end": 27038, "idx": 188}, {"begin": 27039, "end": 27125, "idx": 189}, {"begin": 27126, "end": 27263, "idx": 190}, {"begin": 27264, "end": 27381, "idx": 191}, {"begin": 27382, "end": 27531, "idx": 192}, {"begin": 27532, "end": 27608, "idx": 193}, {"begin": 27609, "end": 27780, "idx": 194}, {"begin": 27781, "end": 27878, "idx": 195}, {"begin": 27899, "end": 28021, "idx": 196}, {"begin": 28045, "end": 28357, "idx": 197}, {"begin": 28358, "end": 28485, "idx": 198}, {"begin": 28486, "end": 28648, "idx": 199}, {"begin": 28649, "end": 28757, "idx": 200}, {"begin": 28758, "end": 28940, "idx": 201}, {"begin": 28941, "end": 29142, "idx": 202}, {"begin": 29143, "end": 29225, "idx": 203}, {"begin": 29226, "end": 29458, "idx": 204}, {"begin": 29459, "end": 29708, "idx": 205}, {"begin": 29709, "end": 29752, "idx": 206}, {"begin": 29753, "end": 30066, "idx": 207}, {"begin": 30067, "end": 30241, "idx": 208}, {"begin": 30242, "end": 30312, "idx": 209}, {"begin": 30313, "end": 30438, "idx": 210}, {"begin": 30439, "end": 30538, "idx": 211}, {"begin": 30539, "end": 30642, "idx": 212}, {"begin": 30643, "end": 30790, "idx": 213}, {"begin": 30805, "end": 30903, "idx": 214}, {"begin": 30904, "end": 31059, "idx": 215}, {"begin": 31060, "end": 31170, "idx": 216}, {"begin": 31171, "end": 31249, "idx": 217}, {"begin": 31264, "end": 31361, "idx": 218}, {"begin": 31362, "end": 31491, "idx": 219}, {"begin": 31492, "end": 31558, "idx": 220}, {"begin": 31559, "end": 31643, "idx": 221}, {"begin": 31644, "end": 31734, "idx": 222}, {"begin": 31735, "end": 31821, "idx": 223}, {"begin": 31822, "end": 31970, "idx": 224}, {"begin": 31971, "end": 32130, "idx": 225}, {"begin": 32131, "end": 32397, "idx": 226}, {"begin": 32398, "end": 32462, "idx": 227}, {"begin": 32463, "end": 32535, "idx": 228}, {"begin": 32536, "end": 32611, "idx": 229}, {"begin": 32612, "end": 32921, "idx": 230}, {"begin": 32922, "end": 33018, "idx": 231}, {"begin": 33019, "end": 33197, "idx": 232}, {"begin": 33198, "end": 33319, "idx": 233}, {"begin": 33320, "end": 33468, "idx": 234}, {"begin": 33469, "end": 33539, "idx": 235}, {"begin": 33540, "end": 33593, "idx": 236}, {"begin": 33594, "end": 33639, "idx": 237}, {"begin": 33640, "end": 33759, "idx": 238}, {"begin": 33760, "end": 33914, "idx": 239}, {"begin": 33915, "end": 33982, "idx": 240}, {"begin": 33983, "end": 34070, "idx": 241}, {"begin": 34071, "end": 34244, "idx": 242}, {"begin": 34245, "end": 34472, "idx": 243}, {"begin": 34473, "end": 34508, "idx": 244}, {"begin": 34509, "end": 34586, "idx": 245}, {"begin": 34587, "end": 34741, "idx": 246}, {"begin": 34742, "end": 34823, "idx": 247}, {"begin": 34824, "end": 34898, "idx": 248}, {"begin": 34899, "end": 34996, "idx": 249}, {"begin": 34997, "end": 35070, "idx": 250}, {"begin": 35093, "end": 35162, "idx": 251}], "ReferenceToFigure": [{"begin": 3122, "end": 3123, "idx": 0}, {"begin": 3651, "end": 3652, "idx": 1}, {"begin": 5321, "end": 5322, "idx": 2}, {"begin": 6724, "end": 6725, "idx": 3}, {"begin": 7575, "end": 7577, "target": "#fig_0", "idx": 4}, {"begin": 9786, "end": 9787, "target": "#fig_2", "idx": 5}, {"begin": 15936, "end": 15937, "target": "#fig_3", "idx": 6}, {"begin": 25529, "end": 25530, "target": "#fig_4", "idx": 7}, {"begin": 25931, "end": 25932, "target": "#fig_4", "idx": 8}, {"begin": 27049, "end": 27050, "target": "#fig_5", "idx": 9}, {"begin": 27133, "end": 27134, "target": "#fig_5", "idx": 10}, {"begin": 35160, "end": 35161, "target": "#fig_7", "idx": 11}], "Div": [{"begin": 100, "end": 515, "idx": 0}, {"begin": 518, "end": 1818, "idx": 1}, {"begin": 1820, "end": 2027, "idx": 2}, {"begin": 2029, "end": 5046, "idx": 3}, {"begin": 5048, "end": 7293, "idx": 4}, {"begin": 7295, "end": 7844, "idx": 5}, {"begin": 7846, "end": 11825, "idx": 6}, {"begin": 11827, "end": 13292, "idx": 7}, {"begin": 13294, "end": 13594, "idx": 8}, {"begin": 13596, "end": 14181, "idx": 9}, {"begin": 14183, "end": 15300, "idx": 10}, {"begin": 15302, "end": 18686, "idx": 11}, {"begin": 18688, "end": 20620, "idx": 12}, {"begin": 20622, "end": 21146, "idx": 13}, {"begin": 21148, "end": 22921, "idx": 14}, {"begin": 22923, "end": 23262, "idx": 15}, {"begin": 23264, "end": 23836, "idx": 16}, {"begin": 23838, "end": 24937, "idx": 17}, {"begin": 24939, "end": 26267, "idx": 18}, {"begin": 26269, "end": 27878, "idx": 19}, {"begin": 27880, "end": 28021, "idx": 20}, {"begin": 28023, "end": 30790, "idx": 21}, {"begin": 30792, "end": 31249, "idx": 22}, {"begin": 31251, "end": 35070, "idx": 23}, {"begin": 35072, "end": 35162, "idx": 24}], "SectionMain": [{"begin": 515, "end": 35162, "idx": 0}], "ScholarlyEntity": [{"label": "Dataset", "begin": 134, "end": 140, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9991059899330139, 0.9991538524627686, 0.9991965889930725], "text": "ReCoRD", "score": 0.999152143796285, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 146, "end": 175, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.9972817897796631, 0.9937924742698669, 0.9947925209999084], "text": "machine reading comprehension", "score": 0.9952889283498129, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 186, "end": 207, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9869787693023682, 0.9893711805343628, 0.980211615562439], "text": "commonsense reasoning", "score": 0.9855205217997233, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 290, "end": 293, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9946122169494629, 0.990481436252594], "text": "MRC", "score": 0.9925468266010284, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 337, "end": 343, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9989039897918701, 0.9990779161453247, 0.9990849494934082], "text": "ReCoRD", "score": 0.9990222851435343, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 466, "end": 472, "seq_label": ["I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120machine", "\u0120commons", "ense", "\u0120reading", "\u0120comprehension", "\u0120Re", "Co", "RD"], "seq_scores": [0.550743043422699, 0.821185827255249, 0.9954226613044739, 0.9957283139228821, 0.995554506778717, 0.997837245464325, 0.9981244206428528, 0.9983351826667786], "text": "ReCoRD", "score": 0.9191164001822472, "type": "ScholarlyEntity"}, {"label": "URL", "begin": 489, "end": 514, "seq_label": ["B-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL"], "seq_token": ["\u0120http", "://", "nl", "p", ".", "j", "hu", ".", "edu", "/", "record"], "seq_scores": [0.7782537341117859, 0.9947038292884827, 0.985471248626709, 0.9950022101402283, 0.9966674447059631, 0.9926801919937134, 0.995433509349823, 0.9973999261856079, 0.9960189461708069, 0.9971201419830322, 0.9936619997024536], "text": "http://nlp.jhu.edu/record", "score": 0.9747648347507823, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 111, "end": 132, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "-", "scale", "\u0120dataset"], "seq_scores": [0.9994584918022156, 0.9993298053741455, 0.9996556043624878, 0.9997879862785339, 0.9997474551200867], "text": "a large-scale dataset", "score": 0.9995958685874939, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 224, "end": 236, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9937415719032288, 0.9986660480499268], "text": "this dataset", "score": 0.9962038099765778, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 533, "end": 562, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["Machine", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.9979089498519897, 0.9969203472137451, 0.99744713306427], "text": "Machine reading comprehension", "score": 0.9974254767100016, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 564, "end": 567, "seq_label": ["B-Task", "I-Task"], "seq_token": ["M", "RC"], "seq_scores": [0.998314619064331, 0.9961718916893005], "text": "MRC", "score": 0.9972432553768158, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 590, "end": 620, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120natural", "\u0120language", "\u0120understanding"], "seq_scores": [0.7951003313064575, 0.8157094120979309, 0.8452749252319336], "text": "natural language understanding", "score": 0.8186948895454407, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 688, "end": 708, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "erman", "n", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.999222993850708, 0.9965498447418213, 0.9981908202171326, 0.9985277652740479, 0.9990383386611938, 0.9984560012817383, 0.9979939460754395], "text": "Hermann et al., 2015", "score": 0.9982828157288688, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 710, "end": 727, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hill", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9994401335716248, 0.998513400554657, 0.9990390539169312, 0.9986608028411865, 0.9983658194541931], "text": "Hill et al., 2015", "score": 0.9988038420677186, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 729, "end": 751, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Raj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9994727969169617, 0.999269425868988, 0.9991016387939453, 0.9987377524375916, 0.999019980430603, 0.9988200068473816, 0.9987030029296875], "text": "Rajpurkar et al., 2016", "score": 0.9990178006035941, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 753, "end": 775, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Tr", "isch", "ler", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.999452531337738, 0.998796820640564, 0.9988829493522644, 0.9987397789955139, 0.9990170001983643, 0.9988627433776855, 0.9988535642623901], "text": "Trischler et al., 2017", "score": 0.9989436268806458, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 777, "end": 796, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Nguyen", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993419051170349, 0.998677670955658, 0.9990546107292175, 0.9988178610801697, 0.9988264441490173], "text": "Nguyen et al., 2016", "score": 0.9989436984062194, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 902, "end": 921, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["J", "ia", "\u0120and", "\u0120Liang", ",", "\u01202017"], "seq_scores": [0.999421238899231, 0.9983144998550415, 0.9987186193466187, 0.9988861680030823, 0.9989820122718811, 0.9989006519317627], "text": "Jia and Liang, 2017", "score": 0.9988705317179362, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 923, "end": 945, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Raj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9993458390235901, 0.9994389414787292, 0.9993917942047119, 0.9992045760154724, 0.9994020462036133, 0.9992285966873169, 0.9993233680725098], "text": "Rajpurkar et al., 2018", "score": 0.9993335945265633, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 947, "end": 971, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ka", "ush", "ik", "\u0120and", "\u0120Li", "pton", ",", "\u01202018"], "seq_scores": [0.9991509914398193, 0.9993215799331665, 0.9994128942489624, 0.9993531107902527, 0.9995152950286865, 0.9994463324546814, 0.9993036985397339, 0.999352753162384], "text": "Kaushik and Lipton, 2018", "score": 0.9993570819497108, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1282, "end": 1309, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120human", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.5586894750595093, 0.7287354469299316, 0.8581734895706177], "text": "human reading comprehension", "score": 0.7151994705200195, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1456, "end": 1483, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120reasoning", "\u0120over", "\u0120common", "\u0120sense"], "seq_scores": [0.9515791535377502, 0.9698160290718079, 0.9729461073875427, 0.9681835770606995], "text": "reasoning over common sense", "score": 0.9656312167644501, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1543, "end": 1565, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993410706520081, 0.9990021586418152, 0.9993385672569275, 0.999197781085968, 0.9991222023963928, 0.9992868304252625, 0.9990910291671753, 0.9989645481109619], "text": "Rajpurkar et al., 2016", "score": 0.9991680234670639, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1567, "end": 1589, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Tr", "isch", "ler", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9994439482688904, 0.9988449811935425, 0.9989583492279053, 0.9990758895874023, 0.9992688298225403, 0.9990988969802856, 0.9991143345832825], "text": "Trischler et al., 2017", "score": 0.9991150328091213, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 655, "end": 656, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.7602812051773071], "text": "a", "score": 0.7602812051773071, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 841, "end": 850, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120datasets", "\u0120questions"], "seq_scores": [0.7080623507499695, 0.9992456436157227, 0.999369204044342, 0.9994076490402222, 0.993115246295929], "text": "questions", "score": 0.9398400187492371, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 992, "end": 1020, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120portion", "\u0120of", "\u0120questions"], "seq_scores": [0.9391224384307861, 0.9684920907020569, 0.9854108095169067, 0.9868200421333313, 0.9883952140808105], "text": "a large portion of questions", "score": 0.9736481189727784, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1024, "end": 1038, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9930979609489441, 0.9987480640411377], "text": "these datasets", "score": 0.9959230124950409, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1201, "end": 1215, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9929003715515137, 0.9973254203796387], "text": "these datasets", "score": 0.9951128959655762, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1400, "end": 1409, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.6365872621536255], "text": "questions", "score": 0.6365872621536255, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1410, "end": 1424, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9967857599258423, 0.9982142448425293], "text": "these datasets", "score": 0.9975000023841858, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1660, "end": 1681, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120reading", "\u0120comprehension"], "seq_scores": [0.9927817583084106, 0.9926751255989075], "text": "reading comprehension", "score": 0.9927284419536591, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1683, "end": 1689, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9953936338424683, 0.9940266609191895, 0.9931647181510925], "text": "ReCoRD", "score": 0.9941950043042501, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1634, "end": 1655, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "-", "scale", "\u0120dataset"], "seq_scores": [0.9991627931594849, 0.9995300769805908, 0.9997407793998718, 0.9997881054878235, 0.9998113512992859], "text": "a large-scale dataset", "score": 0.9996066212654113, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1721, "end": 1742, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120over", "\u0120120", ",", "000", "\u0120examples"], "seq_scores": [0.7450069189071655, 0.9734636545181274, 0.9997884631156921, 0.9997279047966003, 0.9994683861732483], "text": "over 120,000 examples", "score": 0.9434910655021668, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3142, "end": 3148, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.963320791721344, 0.9756686687469482, 0.9672255516052246], "text": "ReCoRD", "score": 0.968738337357839, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3529, "end": 3555, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["deep", "\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9584800601005554, 0.9761303067207336, 0.961137056350708, 0.9790118336677551], "text": "deep commonsense reasoning", "score": 0.968689814209938, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3557, "end": 3563, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9944781064987183, 0.9960095882415771, 0.9947277903556824], "text": "ReCoRD", "score": 0.9950718283653259, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3586, "end": 3634, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Reading", "\u0120Comp", "rehens", "ion", "\u0120with", "\u0120Commons", "ense", "\u0120Reason", "ing"], "seq_scores": [0.9836834073066711, 0.9872477054595947, 0.9891293048858643, 0.9900694489479065, 0.956450343132019, 0.9563735723495483, 0.9667524695396423, 0.9365167617797852, 0.9023653864860535], "text": "Reading Comprehension with Commonsense Reasoning", "score": 0.9631764888763428, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3661, "end": 3667, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9349949955940247, 0.9766070246696472, 0.9552965760231018], "text": "ReCoRD", "score": 0.9556328654289246, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4374, "end": 4377, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9882764220237732, 0.983479380607605], "text": "MRC", "score": 0.9858779013156891, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4416, "end": 4422, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9969282746315002, 0.9987930059432983, 0.9982171654701233], "text": "ReCoRD", "score": 0.997979482014974, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 4452, "end": 4456, "seq_label": ["B-Datasource"], "seq_token": ["\u0120news"], "seq_scores": [0.6577596068382263], "text": "news", "score": 0.6577596068382263, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4495, "end": 4512, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120elic", "itation"], "seq_scores": [0.7242716550827026, 0.7643120884895325, 0.7424780130386353], "text": "human elicitation", "score": 0.7436872522036234, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4519, "end": 4545, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gordon", "\u0120and", "\u0120Van", "\u0120Dur", "me", ",", "\u01202013"], "seq_scores": [0.9985384941101074, 0.9979776740074158, 0.9991375207901001, 0.999099850654602, 0.9989627599716187, 0.9983920454978943, 0.998150110244751], "text": "Gordon and Van Durme, 2013", "score": 0.9986083507537842, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4547, "end": 4565, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mis", "ra", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9991218447685242, 0.9987267851829529, 0.9982271790504456, 0.998649537563324, 0.9982010126113892, 0.9981603026390076], "text": "Misra et al., 2016", "score": 0.9985144436359406, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4567, "end": 4585, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Zhang", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9992784857749939, 0.9983017444610596, 0.9987322688102722, 0.9985424280166626, 0.9987292885780334], "text": "Zhang et al., 2017", "score": 0.9987168431282043, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4596, "end": 4611, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120data", "\u0120collection"], "seq_scores": [0.5621622204780579, 0.797059178352356], "text": "data collection", "score": 0.6796106994152069, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4696, "end": 4702, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9967749714851379, 0.9987624883651733, 0.9980350136756897], "text": "ReCoRD", "score": 0.9978574911753336, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4712, "end": 4733, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9952671527862549, 0.9959557056427002, 0.99666827917099], "text": "commonsense reasoning", "score": 0.9959637125333151, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4353, "end": 4386, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120most", "\u0120of", "\u0120the", "\u0120existing", "\u0120M", "RC", "\u0120datasets"], "seq_scores": [0.5110028386116028, 0.7048090696334839, 0.6693789958953857, 0.9993478655815125, 0.99953293800354, 0.9996241331100464, 0.9994592070579529], "text": "most of the existing MRC datasets", "score": 0.8404507211276463, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4388, "end": 4399, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120queries"], "seq_scores": [0.8697957992553711, 0.8171289563179016], "text": "all queries", "score": 0.8434623777866364, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4452, "end": 4465, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.9745069146156311, 0.981054425239563], "text": "news articles", "score": 0.977780669927597, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4750, "end": 4756, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9082323908805847, 0.820623517036438, 0.643805742263794], "text": "ReCoRD", "score": 0.7908872167269388, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4869, "end": 4872, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.8354114890098572, 0.5718629360198975], "text": "MRC", "score": 0.7036372125148773, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4915, "end": 4921, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.8976447582244873, 0.7885029911994934, 0.5851309299468994], "text": "ReCoRD", "score": 0.7570928931236267, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5012, "end": 5045, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.7386190891265869, 0.9948956370353699, 0.9932763576507568, 0.9943713545799255], "text": "commonsense reading comprehension", "score": 0.9302906095981598, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4848, "end": 4879, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120state", "-", "of", "-", "the", "-", "art", "\u0120M", "RC", "\u0120models"], "seq_scores": [0.9795439839363098, 0.955710232257843, 0.9990072846412659, 0.9989411234855652, 0.999053418636322, 0.9988113641738892, 0.9990423321723938, 0.9987972974777222, 0.995701014995575, 0.9992087483406067, 0.998691976070404], "text": "the state-of-the-art MRC models", "score": 0.9929553432898088, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5080, "end": 5092, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120common", "\u0120sense"], "seq_scores": [0.7508936524391174, 0.567433774471283], "text": "common sense", "score": 0.6591637134552002, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5236, "end": 5251, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120McCarthy", "\u0120(", "1959", ")"], "seq_scores": [0.9886237382888794, 0.9946005344390869, 0.9918538331985474, 0.9927799105644226], "text": "McCarthy (1959)", "score": 0.9919645041227341, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5252, "end": 5273, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Commons", "ense", "\u0120Reason", "ing"], "seq_scores": [0.9723818302154541, 0.9762342572212219, 0.9814080595970154, 0.9619366526603699], "text": "Commonsense Reasoning", "score": 0.9729901999235153, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5277, "end": 5280, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9865198135375977, 0.9775295257568359], "text": "MRC", "score": 0.9820246696472168, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5864, "end": 5885, "seq_label": ["I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["ense", "Le", "ves", "que", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.5870072841644287, 0.9984103441238403, 0.9992006421089172, 0.9992333650588989, 0.9992092251777649, 0.999413013458252, 0.9991850256919861, 0.9993663430213928], "text": "Levesque et al., 2011", "score": 0.9476281553506851, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5901, "end": 5918, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mc", "-", "Car", "thy", "\u0120(", "1959", "),"], "seq_scores": [0.9931281805038452, 0.9837645888328552, 0.9751535058021545, 0.9864673018455505, 0.9933834671974182, 0.9940152764320374, 0.996527373790741], "text": "Mc-Carthy (1959),", "score": 0.9889199563435146, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5962, "end": 5983, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9705852270126343, 0.9715975522994995, 0.9591012001037598], "text": "commonsense reasoning", "score": 0.9670946598052979, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6015, "end": 6036, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120commons", "ense", "\u0120knowledge"], "seq_scores": [0.7902669906616211, 0.6693748831748962, 0.7702507376670837], "text": "commonsense knowledge", "score": 0.7432975371678671, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6064, "end": 6076, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120common", "\u0120sense"], "seq_scores": [0.7400750517845154, 0.6537328958511353], "text": "common sense", "score": 0.6969039738178253, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6237, "end": 6240, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9759572148323059, 0.9664889574050903], "text": "MRC", "score": 0.9712230861186981, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6451, "end": 6463, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120common", "\u0120sense"], "seq_scores": [0.6468146443367004, 0.7191963791847229], "text": "common sense", "score": 0.6830055117607117, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6675, "end": 6696, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9870203733444214, 0.9738432765007019, 0.9650030136108398], "text": "commonsense reasoning", "score": 0.9752888878186544, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6700, "end": 6703, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9947655200958252, 0.9913607239723206], "text": "MRC", "score": 0.9930631220340729, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 6881, "end": 6882, "seq_label": ["B-MLModel"], "seq_token": ["\u0120M"], "seq_scores": [0.512939453125], "text": "M", "score": 0.512939453125, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6951, "end": 6983, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120its", "\u0120hidden", "\u0120commons", "ense", "\u0120knowledge"], "seq_scores": [0.703478217124939, 0.8411184549331665, 0.9458461403846741, 0.9702299237251282, 0.9547332525253296], "text": "its hidden commonsense knowledge", "score": 0.8830811977386475, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7343, "end": 7367, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120automatically", "\u0120generating"], "seq_scores": [0.7283754944801331, 0.4594819247722626], "text": "automatically generating", "score": 0.5939287096261978, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7381, "end": 7387, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9970623850822449, 0.998910665512085, 0.9987969398498535], "text": "ReCoRD", "score": 0.9982566634813944, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7534, "end": 7540, "seq_label": ["I-Task", "I-Task", "B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["ze", "style", "\u0120Re", "Co", "RD"], "seq_scores": [0.578412652015686, 0.49720099568367004, 0.9986860156059265, 0.9993546605110168, 0.9990718364715576], "text": "ReCoRD", "score": 0.8145452320575715, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7582, "end": 7590, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120cur", "ating"], "seq_scores": [0.8729722499847412, 0.8456586003303528], "text": "curating", "score": 0.859315425157547, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7591, "end": 7619, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail", "\u0120news", "\u0120articles"], "seq_scores": [0.927664577960968, 0.9465382099151611, 0.9325411915779114, 0.9554564952850342, 0.8504104614257812, 0.6460124254226685], "text": "CNN/Daily Mail news articles", "score": 0.876437226931254, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7771, "end": 7774, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9746159315109253, 0.9505355954170227], "text": "MRC", "score": 0.962575763463974, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7791, "end": 7800, "seq_label": ["B-Method"], "seq_token": ["\u0120filtering"], "seq_scores": [0.5043458938598633], "text": "filtering", "score": 0.5043458938598633, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7368, "end": 7379, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9984942674636841, 0.9989288449287415], "text": "the dataset", "score": 0.9987115561962128, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7446, "end": 7454, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120passages"], "seq_scores": [0.6860498785972595], "text": "passages", "score": 0.6860498785972595, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7479, "end": 7498, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120clo", "ze", "-", "style", "\u0120queries"], "seq_scores": [0.8609380125999451, 0.8038408160209656, 0.8632269501686096, 0.7671487331390381, 0.6579339504241943], "text": "cloze-style queries", "score": 0.7906176924705506, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7504, "end": 7521, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120reference", "\u0120answers"], "seq_scores": [0.9004499316215515, 0.8834899663925171], "text": "reference answers", "score": 0.8919699490070343, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7591, "end": 7619, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail", "\u0120news", "\u0120articles"], "seq_scores": [0.6795634031295776, 0.9135473370552063, 0.7604977488517761, 0.772310197353363, 0.7691246271133423, 0.7622131109237671], "text": "CNN/Daily Mail news articles", "score": 0.7762094040711721, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7636, "end": 7664, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120passage", "-", "query", "ans", "w", "ers", "\u0120tri", "ples"], "seq_scores": [0.9954574108123779, 0.9989138841629028, 0.9979314804077148, 0.9988431930541992, 0.9989690780639648, 0.9989824891090393, 0.9986743927001953, 0.9988395571708679], "text": "passage-queryanswers triples", "score": 0.9983264356851578, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7674, "end": 7691, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120news", "\u0120articles"], "seq_scores": [0.8689964413642883, 0.919031023979187, 0.9535473585128784], "text": "the news articles", "score": 0.9138582746187845, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7711, "end": 7722, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120queries"], "seq_scores": [0.9882376790046692, 0.9802609086036682], "text": "the queries", "score": 0.9842492938041687, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7754, "end": 7781, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120state", "-", "of", "-", "the", "-", "art", "\u0120M", "RC", "\u0120models"], "seq_scores": [0.9907031059265137, 0.9985213875770569, 0.9983416795730591, 0.9988182187080383, 0.9984703660011292, 0.9988092184066772, 0.9984453320503235, 0.9959484934806824, 0.9992664456367493, 0.998464822769165], "text": "state-of-the-art MRC models", "score": 0.9975789070129395, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7805, "end": 7816, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120queries"], "seq_scores": [0.974946141242981, 0.9701787829399109], "text": "the queries", "score": 0.9725624620914459, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7892, "end": 7898, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.59978848695755, 0.6824666857719421, 0.6096294522285461], "text": "ReCoRD", "score": 0.6306282083193461, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8298, "end": 8301, "seq_label": ["B-Datasource"], "seq_token": ["\u0120CNN"], "seq_scores": [0.9792405366897583], "text": "CNN", "score": 0.9792405366897583, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8306, "end": 8316, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Daily", "\u0120Mail"], "seq_scores": [0.9584485292434692, 0.9490445852279663], "text": "Daily Mail", "score": 0.9537465572357178, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8375, "end": 8393, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["S", "v", "ore", "\u0120et", "\u0120al", ".,", "\u01202007"], "seq_scores": [0.9988371729850769, 0.9981722831726074, 0.998591959476471, 0.9988248944282532, 0.9990848302841187, 0.9986294507980347, 0.9986479878425598], "text": "Svore et al., 2007", "score": 0.9986840827124459, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8395, "end": 8420, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Woods", "end", "\u0120and", "\u0120Lap", "ata", ",", "\u01202010"], "seq_scores": [0.9988619089126587, 0.9989811778068542, 0.998849630355835, 0.9991673231124878, 0.9991151690483093, 0.9988497495651245, 0.9988237023353577], "text": "Woodsend and Lapata, 2010", "score": 0.9989498087338039, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8422, "end": 8442, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Herman", "n", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9991307854652405, 0.9986043572425842, 0.9986224174499512, 0.9989499449729919, 0.9986282587051392, 0.9985550045967102], "text": "Hermann et al., 2015", "score": 0.9987484614054362, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7913, "end": 7926, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.9401552081108093, 0.9650023579597473], "text": "news articles", "score": 0.9525787830352783, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8568, "end": 8591, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120CNN", "\u0120and", "\u0120Daily", "\u0120Mail", "\u0120news"], "seq_scores": [0.9499361515045166, 0.6554448008537292, 0.8174455165863037, 0.961499035358429, 0.5953820943832397], "text": "CNN and Daily Mail news", "score": 0.7959415197372437, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8632, "end": 8645, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Herman", "n", "\u0120et", "\u0120al"], "seq_scores": [0.9994179010391235, 0.9994913339614868, 0.9996803998947144, 0.9996994733810425], "text": "Hermann et al", "score": 0.9995722770690918, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8691, "end": 8694, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-Datasource"], "seq_token": ["\u0120(", "2015", "),", "\u0120CNN"], "seq_scores": [0.9996036887168884, 0.9996316432952881, 0.9981459379196167, 0.9763944745063782], "text": "CNN", "score": 0.9934439361095428, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8699, "end": 8709, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Daily", "\u0120Mail"], "seq_scores": [0.96833735704422, 0.9834354519844055], "text": "Daily Mail", "score": 0.9758864045143127, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8802, "end": 8823, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120Google", "\u0120N", "LP", "\u0120pipeline"], "seq_scores": [0.7207454442977905, 0.5544692277908325, 0.9884748458862305, 0.9928629994392395, 0.9215019345283508], "text": "a Google NLP pipeline", "score": 0.8356108903884888, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9126, "end": 9129, "seq_label": ["I-Method", "I-Method", "B-Datasource"], "seq_token": ["\u0120data", "\u0120collection", "\u0120CNN"], "seq_scores": [0.4808388650417328, 0.8237013816833496, 0.9695335626602173], "text": "CNN", "score": 0.7580246031284332, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9134, "end": 9153, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Daily", "\u0120Mail", "\u0120websites"], "seq_scores": [0.8879619240760803, 0.9808222651481628, 0.7680584192276001], "text": "Daily Mail websites", "score": 0.8789475361506144, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9220, "end": 9242, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Herman", "n", "\u0120et", "\u0120al", ".", "\u0120(", "2015", ")."], "seq_scores": [0.9994294047355652, 0.9993450045585632, 0.9995834231376648, 0.9995937943458557, 0.9995449185371399, 0.9994890689849854, 0.9995974898338318, 0.9975947737693787], "text": "Hermann et al. (2015).", "score": 0.9992722347378731, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9307, "end": 9326, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["P", "eters", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9991618394851685, 0.9982494711875916, 0.9988318085670471, 0.9991888403892517, 0.9990110397338867, 0.999082088470459], "text": "Peters et al., 2018", "score": 0.9989208479722341, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9377, "end": 9393, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Lee", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.998309850692749, 0.9971535205841064, 0.9980015158653259, 0.9976072311401367, 0.9981740713119507], "text": "Lee et al., 2017", "score": 0.9978492379188537, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9407, "end": 9415, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Allen", "N", "LP"], "seq_scores": [0.9788315892219543, 0.9946838021278381, 0.9950892329216003], "text": "AllenNLP", "score": 0.989534874757131, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9417, "end": 9437, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "ard", "ner", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9556259512901306, 0.97569340467453, 0.9795929193496704, 0.9791029095649719, 0.9813600778579712, 0.9861568212509155, 0.9780961275100708], "text": "Gardner et al., 2018", "score": 0.9765183159283229, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9505, "end": 9508, "seq_label": ["B-Datasource"], "seq_token": ["\u0120CNN"], "seq_scores": [0.8314381241798401], "text": "CNN", "score": 0.8314381241798401, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8568, "end": 8600, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120CNN", "\u0120and", "\u0120Daily", "\u0120Mail", "\u0120news", "\u0120articles"], "seq_scores": [0.9980260729789734, 0.998969554901123, 0.9985068440437317, 0.9992499947547913, 0.9977729916572571, 0.9982234835624695], "text": "CNN and Daily Mail news articles", "score": 0.9984581569830576, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8672, "end": 8685, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120148", "K", "\u0120articles"], "seq_scores": [0.9979591369628906, 0.9980239868164062, 0.9975996613502502], "text": "148K articles", "score": 0.9978609283765157, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8892, "end": 8906, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120articles"], "seq_scores": [0.8594457507133484, 0.8454388976097107], "text": "these articles", "score": 0.8524423241615295, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9042, "end": 9064, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dev", "./", "test", "\u0120datasets"], "seq_scores": [0.9940744042396545, 0.9959195852279663, 0.9997267127037048, 0.9994334578514099, 0.9995972514152527], "text": "the dev./test datasets", "score": 0.9977502822875977, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9099, "end": 9116, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012022", "K", "\u0120news", "\u0120articles"], "seq_scores": [0.9949277639389038, 0.9954138994216919, 0.995712161064148, 0.9956263303756714], "text": "22K news articles", "score": 0.9954200387001038, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9155, "end": 9177, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120These", "\u0120crawled", "\u0120articles"], "seq_scores": [0.8786568641662598, 0.9350233674049377, 0.9671237468719482], "text": "These crawled articles", "score": 0.9269346594810486, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9199, "end": 9211, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120articles"], "seq_scores": [0.9937517046928406, 0.9964026212692261], "text": "the articles", "score": 0.9950771629810333, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9255, "end": 9305, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120state", "of", "-", "the", "-", "art", "\u0120named", "\u0120entity", "\u0120recognition", "\u0120model"], "seq_scores": [0.9972779154777527, 0.9972822666168213, 0.9996368885040283, 0.9997116923332214, 0.9994526505470276, 0.9997076392173767, 0.9996682405471802, 0.9986061453819275, 0.9998550415039062, 0.9998553991317749, 0.9996587038040161], "text": "the stateof-the-art named entity recognition model", "score": 0.9991556893695485, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9332, "end": 9375, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120end", "-", "to", "-", "end", "\u0120core", "ference", "\u0120resolution", "\u0120model"], "seq_scores": [0.9976876974105835, 0.9956884980201721, 0.9997597336769104, 0.9997056126594543, 0.9997877478599548, 0.9997860789299011, 0.9996510744094849, 0.9998468160629272, 0.9998251795768738, 0.9997096657752991], "text": "the end-to-end coreference resolution model", "score": 0.9991448104381562, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9500, "end": 9533, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120170", "K", "\u0120CNN", "/", "Daily", "\u0120Mail", "\u0120news", "\u0120articles"], "seq_scores": [0.9977097511291504, 0.9983657002449036, 0.9954114556312561, 0.9996059536933899, 0.9995867609977722, 0.9996581077575684, 0.9992992877960205, 0.9989838004112244], "text": "170K CNN/Daily Mail news articles", "score": 0.9985776022076607, "type": "ScholarlyEntity"}, {"label": "URL", "begin": 9601, "end": 9615, "seq_label": ["B-URL", "I-URL", "I-URL"], "seq_token": ["\u0120https", "://", "github"], "seq_scores": [0.7617328763008118, 0.9389240145683289, 0.8748006820678711], "text": "https://github", "score": 0.8584858576456705, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 9641, "end": 9673, "seq_label": ["I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["com", "/", "deep", "mind", "/", "rc", "-", "data", "\u0120Passage", "-", "Query", "-", "An", "swers", "\u0120Generation"], "seq_scores": [0.9428258538246155, 0.9650644063949585, 0.9238929748535156, 0.9514433741569519, 0.9678438901901245, 0.9507917165756226, 0.9741612672805786, 0.9513940811157227, 0.7535475492477417, 0.8460980653762817, 0.8859042525291443, 0.8725757598876953, 0.8302154541015625, 0.8345861434936523, 0.5365347266197205], "text": "Passage-Query-Answers Generation", "score": 0.8791253010431925, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 9711, "end": 9717, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.8728664517402649, 0.8355264067649841, 0.8699334263801575], "text": "ReCoRD", "score": 0.8594420949618021, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 10632, "end": 10648, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120generation", "\u0120news", "\u0120editors", "\u0120Stanford", "\u0120Core", "N", "LP"], "seq_scores": [0.5117645263671875, 0.47603553533554077, 0.7236605286598206, 0.9438698291778564, 0.9854037165641785, 0.9913320541381836, 0.9923032522201538], "text": "Stanford CoreNLP", "score": 0.8034813489232745, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 10650, "end": 10670, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Man", "ning", "\u0120et", "\u0120al", ".,", "\u01202014"], "seq_scores": [0.9886382222175598, 0.9835774898529053, 0.984103798866272, 0.9893691539764404, 0.9893457293510437, 0.9844687581062317], "text": "Manning et al., 2014", "score": 0.9865838587284088, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9688, "end": 9695, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["swers", "\u0120passages", "\u0120queries"], "seq_scores": [0.5373795628547668, 0.5133801698684692, 0.8869932889938354], "text": "queries", "score": 0.6459176739056905, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9700, "end": 9707, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120answers"], "seq_scores": [0.767068088054657], "text": "answers", "score": 0.767068088054657, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9752, "end": 9777, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120curated", "\u0120news", "\u0120articles"], "seq_scores": [0.9686968922615051, 0.9920765161514282, 0.9896395206451416, 0.9927814602851868], "text": "the curated news articles", "score": 0.9857985973358154, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10093, "end": 10101, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120passages"], "seq_scores": [0.857871949672699], "text": "passages", "score": 0.857871949672699, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10106, "end": 10113, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.7541871666908264], "text": "queries", "score": 0.7541871666908264, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11439, "end": 11450, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120core", "ference"], "seq_scores": [0.9399613738059998, 0.9535885453224182], "text": "coreference", "score": 0.946774959564209, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11778, "end": 11789, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120core", "ference"], "seq_scores": [0.7779951095581055, 0.7966750860214233], "text": "coreference", "score": 0.7873350977897644, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14074, "end": 14083, "seq_label": ["B-Task"], "seq_token": ["\u0120reasoning"], "seq_scores": [0.6681868433952332], "text": "reasoning", "score": 0.6681868433952332, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14142, "end": 14180, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120770", "k", "\u0120(", "pass", "age", ",", "\u0120query", ",", "\u0120answers", ")", "\u0120tri", "ples"], "seq_scores": [0.9847831130027771, 0.9953536987304688, 0.989297091960907, 0.9896719455718994, 0.9928731918334961, 0.9777467846870422, 0.9809967279434204, 0.9778702259063721, 0.9883602857589722, 0.9935047626495361, 0.9951816201210022, 0.99513840675354], "text": "770k (passage, query, answers) triples", "score": 0.9883981545766195, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14221, "end": 14242, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120J", "ia", "\u0120and", "\u0120Liang", "\u0120(", "2017", ");"], "seq_scores": [0.9995869994163513, 0.9994754195213318, 0.9995695948600769, 0.9996236562728882, 0.9995717406272888, 0.9995810389518738, 0.981257438659668], "text": "Jia and Liang (2017);", "score": 0.9969522697584969, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14243, "end": 14258, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Raj", "pur", "kar", "\u0120et", "\u0120al"], "seq_scores": [0.9993663430213928, 0.9998026490211487, 0.9997443556785583, 0.9997660517692566, 0.9998034834861755], "text": "Rajpurkar et al", "score": 0.9996965765953064, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14268, "end": 14291, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120(", "2018", ");", "\u0120Wang", "\u0120and", "\u0120Bans", "al", "\u0120(", "2018", ");"], "seq_scores": [0.999535083770752, 0.9997225403785706, 0.9805388450622559, 0.9991581439971924, 0.9997135996818542, 0.9998020529747009, 0.9998001456260681, 0.999727189540863, 0.9997561573982239, 0.9843357801437378], "text": "Wang and Bansal (2018);", "score": 0.9962089538574219, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14292, "end": 14318, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ka", "ush", "ik", "\u0120and", "\u0120Li", "pton", "\u0120(", "2018", "),"], "seq_scores": [0.9989023208618164, 0.9997730851173401, 0.9997767806053162, 0.9997604489326477, 0.9998077750205994, 0.9998223185539246, 0.999729335308075, 0.9997690320014954, 0.9913941025733948], "text": "Kaushik and Lipton (2018),", "score": 0.9987483554416232, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14328, "end": 14331, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9771501421928406, 0.9238914251327515], "text": "MRC", "score": 0.950520783662796, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14603, "end": 14604, "seq_label": ["B-Task"], "seq_token": ["\u0120M"], "seq_scores": [0.600143313407898], "text": "M", "score": 0.600143313407898, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14621, "end": 14647, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120St", "och", "astic", "\u0120Answer", "\u0120Networks"], "seq_scores": [0.7981152534484863, 0.7937911152839661, 0.7634751796722412, 0.7775692939758301, 0.6584752202033997], "text": "Stochastic Answer Networks", "score": 0.7582852125167847, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14649, "end": 14652, "seq_label": ["B-MLModel"], "seq_token": ["SAN"], "seq_scores": [0.7918189167976379], "text": "SAN", "score": 0.7918189167976379, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14655, "end": 14671, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["L", "iu", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9994957447052002, 0.9992520213127136, 0.9992651343345642, 0.9994701743125916, 0.9994615912437439, 0.9994277358055115], "text": "Liu et al., 2018", "score": 0.9993954002857208, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14684, "end": 14687, "seq_label": ["B-MLModel"], "seq_token": ["\u0120SAN"], "seq_scores": [0.6806579828262329], "text": "SAN", "score": 0.6806579828262329, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14726, "end": 14729, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9508392214775085, 0.926042914390564], "text": "MRC", "score": 0.9384410679340363, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14909, "end": 14912, "seq_label": ["B-MLModel"], "seq_token": ["\u0120SAN"], "seq_scores": [0.6974099278450012], "text": "SAN", "score": 0.6974099278450012, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14978, "end": 14981, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "B-MLModel"], "seq_token": ["\u0120five", "-", "fold", "\u0120cross", "\u0120validation", "\u0120SAN"], "seq_scores": [0.4864978790283203, 0.7096154093742371, 0.7176824808120728, 0.7446305155754089, 0.7195613980293274, 0.529961109161377], "text": "SAN", "score": 0.6513247986634573, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 15114, "end": 15117, "seq_label": ["B-MLModel"], "seq_token": ["\u0120SAN"], "seq_scores": [0.6519505977630615], "text": "SAN", "score": 0.6519505977630615, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 15238, "end": 15259, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9858400821685791, 0.9861316680908203, 0.9849808216094971], "text": "commonsense reasoning", "score": 0.9856508572896322, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 15264, "end": 15288, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120multi", "-", "sent", "ence", "\u0120reasoning"], "seq_scores": [0.9893292784690857, 0.9883434176445007, 0.9885995388031006, 0.9893524050712585, 0.985192060470581], "text": "multi-sentence reasoning", "score": 0.9881633400917054, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14319, "end": 14338, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120existing", "\u0120M", "RC", "\u0120models"], "seq_scores": [0.9941360950469971, 0.9923436045646667, 0.9997784495353699, 0.9996622800827026], "text": "existing MRC models", "score": 0.9964801073074341, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14397, "end": 14406, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9798433184623718], "text": "questions", "score": 0.9798433184623718, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14476, "end": 14488, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120passages"], "seq_scores": [0.6646838188171387, 0.795620858669281], "text": "the passages", "score": 0.7301523387432098, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14535, "end": 14542, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120tri", "ples"], "seq_scores": [0.982743501663208, 0.9990781545639038], "text": "triples", "score": 0.9909108281135559, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14549, "end": 14556, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.9413318037986755], "text": "queries", "score": 0.9413318037986755, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14583, "end": 14619, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120state", "of", "-", "the", "-", "art", "\u0120M", "RC", "\u0120architecture"], "seq_scores": [0.994380533695221, 0.9972884654998779, 0.9984346032142639, 0.9984305500984192, 0.9983500242233276, 0.9982280135154724, 0.9985658526420593, 0.9956081509590149, 0.9993434548377991, 0.99832683801651], "text": "the stateof-the-art MRC architecture", "score": 0.9976956486701966, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14717, "end": 14738, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120existing", "\u0120M", "RC", "\u0120datasets"], "seq_scores": [0.9988905787467957, 0.9956307411193848, 0.9997391104698181, 0.9994230270385742], "text": "existing MRC datasets", "score": 0.9984208643436432, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14956, "end": 14972, "seq_label": ["I-MLModelGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["RC", "\u0120all", "\u0120770", "k", "\u0120tri", "ples"], "seq_scores": [0.6956316828727722, 0.9915196299552917, 0.9963774085044861, 0.9998761415481567, 0.999642014503479, 0.9997027516365051], "text": "all 770k triples", "score": 0.9471249381701151, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14974, "end": 14988, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120SAN", "\u0120models"], "seq_scores": [0.9928591251373291, 0.9976571798324585, 0.9995835423469543], "text": "The SAN models", "score": 0.9966999491055807, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15008, "end": 15014, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012068", "%", "\u0120of"], "seq_scores": [0.9822627305984497, 0.9850120544433594, 0.6782782673835754], "text": "68% of", "score": 0.8818510174751282, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15015, "end": 15028, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120tri", "ples"], "seq_scores": [0.5119763612747192, 0.9982174038887024, 0.9990895986557007], "text": "these triples", "score": 0.8364277879397074, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15042, "end": 15055, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120those", "\u0120tri", "ples"], "seq_scores": [0.9532715082168579, 0.9899038076400757, 0.9977390766143799], "text": "those triples", "score": 0.9803047974904379, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15071, "end": 15083, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120244", "k", "\u0120tri", "ples"], "seq_scores": [0.9949136972427368, 0.9996902942657471, 0.9995424747467041, 0.9994557499885559], "text": "244k triples", "score": 0.998400554060936, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15119, "end": 15132, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120These", "\u0120tri", "ples"], "seq_scores": [0.9959946870803833, 0.9981614947319031, 0.9989387392997742], "text": "These triples", "score": 0.9976983070373535, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15141, "end": 15148, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.7990097403526306], "text": "queries", "score": 0.7990097403526306, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15483, "end": 15495, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120crowd", "workers"], "seq_scores": [0.7736103534698486, 0.8227018713951111], "text": "crowdworkers", "score": 0.7981561124324799, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15531, "end": 15553, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Amazon", "\u0120Mechanical", "\u0120Turk"], "seq_scores": [0.8505402207374573, 0.8942099809646606, 0.9107374548912048], "text": "Amazon Mechanical Turk", "score": 0.8851625521977743, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15570, "end": 15582, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Crowd", "workers"], "seq_scores": [0.7237380146980286, 0.7511932849884033], "text": "Crowdworkers", "score": 0.7374656498432159, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15392, "end": 15412, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120resulted", "\u0120tri", "ples"], "seq_scores": [0.9821166396141052, 0.9771488904953003, 0.9919547438621521, 0.9876700639724731], "text": "the resulted triples", "score": 0.9847225844860077, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15508, "end": 15521, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120tri", "ples"], "seq_scores": [0.7477769255638123, 0.9531115293502808, 0.9050108194351196], "text": "these triples", "score": 0.8686330914497375, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15948, "end": 15965, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120crowds", "ourcing", "\u0120web"], "seq_scores": [0.5380100607872009, 0.6855522990226746, 0.5641450881958008], "text": "crowdsourcing web", "score": 0.5959024826685587, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 17066, "end": 17084, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Chen", "\u0120et", "\u0120al", ".", "\u0120(", "2016", ")"], "seq_scores": [0.9994394183158875, 0.9997032284736633, 0.9997907280921936, 0.9997976422309875, 0.9997300505638123, 0.9997811913490295, 0.9989966750144958], "text": "Chen et al. (2016)", "score": 0.9996055620057243, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18149, "end": 18171, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Herman", "n", "\u0120et", "\u0120al", ".", "\u0120(", "2015", ")."], "seq_scores": [0.9993883371353149, 0.9994932413101196, 0.9995148181915283, 0.9996253252029419, 0.9996778964996338, 0.9995686411857605, 0.99965500831604, 0.9979907274246216], "text": "Hermann et al. (2015).", "score": 0.9993642494082451, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18481, "end": 18502, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Herman", "n", "\u0120et", "\u0120al", ".", "\u0120(", "2015", ")"], "seq_scores": [0.9992221593856812, 0.9993234872817993, 0.9992350339889526, 0.9994322657585144, 0.9989700317382812, 0.9993752837181091, 0.9995736479759216, 0.9929556846618652], "text": "Hermann et al. (2015)", "score": 0.9985109493136406, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16001, "end": 16009, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120triple"], "seq_scores": [0.7000220417976379, 0.6869112253189087], "text": "a triple", "score": 0.6934666335582733, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16013, "end": 16032, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120data", "\u0120collection"], "seq_scores": [0.908279538154602, 0.8302814960479736, 0.6816329956054688], "text": "our data collection", "score": 0.8067313432693481, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16850, "end": 16871, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9280551671981812, 0.9158850908279419, 0.8192176222801208], "text": "the reference answers", "score": 0.8877192934354147, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16875, "end": 16886, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120tri", "ples"], "seq_scores": [0.9586942195892334, 0.9860031604766846, 0.9753215312957764], "text": "the triples", "score": 0.9733396371205648, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17028, "end": 17049, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9135408401489258, 0.8997161388397217, 0.8285130858421326], "text": "the reference answers", "score": 0.88059002161026, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17184, "end": 17205, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.45133209228515625, 0.8964048027992249, 0.8143129944801331], "text": "the reference answers", "score": 0.7206832965215048, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17213, "end": 17234, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9287243485450745, 0.9172423481941223, 0.8197218775749207], "text": "the reference answers", "score": 0.8885628581047058, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17527, "end": 17538, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120queries"], "seq_scores": [0.7960695028305054, 0.9060621857643127], "text": "all queries", "score": 0.8510658442974091, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17776, "end": 17787, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Train", "\u0120/", "\u0120Dev"], "seq_scores": [0.9676560163497925, 0.9991819262504578, 0.9982786178588867], "text": "Train / Dev", "score": 0.9883721868197123, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17813, "end": 17829, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120/", "\u0120Test", "\u0120Spl", "its", "\u0120the", "\u0120244", "k", "\u0120tri", "ples"], "seq_scores": [0.9986679553985596, 0.992938756942749, 0.9986063838005066, 0.9982990622520447, 0.8998490571975708, 0.998494029045105, 0.9989940524101257, 0.9992969036102295, 0.9980430603027344], "text": "the 244k triples", "score": 0.9870210289955139, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17927, "end": 17948, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120triple", "\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.6427943706512451, 0.8764998912811279, 0.8107831478118896, 0.7316392064094543], "text": "the reference answers", "score": 0.7654291540384293, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17977, "end": 17984, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.5483131408691406], "text": "queries", "score": 0.5483131408691406, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17988, "end": 18000, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120122", "k", "\u0120tri", "ples"], "seq_scores": [0.9965277314186096, 0.998514711856842, 0.99919193983078, 0.9986842274665833], "text": "122k triples", "score": 0.9982296526432037, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18026, "end": 18057, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120100", "k", "\u0120correctly", "-", "answered", "\u0120tri", "ples"], "seq_scores": [0.8402074575424194, 0.9995762705802917, 0.9987660646438599, 0.9997482895851135, 0.9995957016944885, 0.9996169805526733, 0.9996309280395508], "text": "100k correctly-answered triples", "score": 0.9767345275197711, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18061, "end": 18077, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.99518221616745, 0.996780276298523, 0.997209370136261], "text": "the training set", "score": 0.9963906208674113, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18106, "end": 18119, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120tri", "ples"], "seq_scores": [0.9802228808403015, 0.992156982421875, 0.9949969053268433], "text": "these triples", "score": 0.9891255895296732, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18179, "end": 18208, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120articles", "\u0120the", "\u0120development", "\u0120and", "\u0120test", "\u0120sets"], "seq_scores": [0.6206012964248657, 0.9956269264221191, 0.9936908483505249, 0.9992032647132874, 0.9985601305961609, 0.9979998469352722], "text": "the development and test sets", "score": 0.9342803855737051, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18297, "end": 18317, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120rest", "\u012022", "k", "\u0120tri", "ples"], "seq_scores": [0.9722594618797302, 0.9823525547981262, 0.8647887110710144, 0.9995243549346924, 0.9993434548377991, 0.9992069602012634], "text": "the rest 22k triples", "score": 0.9695792496204376, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18366, "end": 18377, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012020", "k", "\u0120tri", "ples"], "seq_scores": [0.9913661479949951, 0.9994578957557678, 0.9995183944702148, 0.9993688464164734], "text": "20k triples", "score": 0.9974278211593628, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18439, "end": 18452, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120tri", "ples"], "seq_scores": [0.9817688465118408, 0.9930413961410522, 0.9937909245491028], "text": "these triples", "score": 0.9895337224006653, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18704, "end": 18710, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Re", "Co", "RD"], "seq_scores": [0.9961442947387695, 0.9985173344612122, 0.9974774718284607], "text": "ReCoRD", "score": 0.9973797003428141, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18888, "end": 18894, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9960435628890991, 0.9980344176292419, 0.9966358542442322], "text": "ReCoRD", "score": 0.9969046115875244, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18724, "end": 18760, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120reading", "\u0120comprehension", "\u0120datasets"], "seq_scores": [0.9933264255523682, 0.9406505823135376, 0.9993058443069458, 0.9995926022529602], "text": "other reading comprehension datasets", "score": 0.9832188636064529, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19140, "end": 19145, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.999432384967804, 0.9996664524078369, 0.9997085928916931], "text": "SQuAD", "score": 0.999602476755778, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 19147, "end": 19169, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.999529242515564, 0.9995280504226685, 0.999599277973175, 0.9995355606079102, 0.9994725584983826, 0.9995185136795044, 0.9994602799415588, 0.9994539618492126], "text": "Rajpurkar et al., 2016", "score": 0.999512180685997, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19176, "end": 19182, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120News", "Q", "A"], "seq_scores": [0.9992929697036743, 0.9996839761734009, 0.9996539354324341], "text": "NewsQA", "score": 0.9995436271031698, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 19184, "end": 19206, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["T", "ris", "ch", "ler", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9993855953216553, 0.9994944334030151, 0.9995717406272888, 0.9994962215423584, 0.999494194984436, 0.9995107650756836, 0.9994538426399231, 0.999489426612854], "text": "Trischler et al., 2017", "score": 0.9994870275259018, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19209, "end": 19215, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9812182784080505, 0.9894730448722839, 0.977627694606781], "text": "ReCoRD", "score": 0.9827730059623718, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19225, "end": 19246, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9937501549720764, 0.9782987833023071, 0.9633487462997437], "text": "commonsense reasoning", "score": 0.9784658948580424, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19286, "end": 19309, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120machine", "\u0120filtering", "\u0120stage"], "seq_scores": [0.5570523142814636, 0.9845526218414307, 0.703342080116272], "text": "machine filtering stage", "score": 0.748315672079722, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19440, "end": 19461, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120filtering", "\u0120stage"], "seq_scores": [0.4955460727214813, 0.9899354577064514, 0.6792174577713013], "text": "human filtering stage", "score": 0.7215663293997446, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19616, "end": 19640, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120named", "\u0120entity", "\u0120recognition"], "seq_scores": [0.9557709693908691, 0.9522375464439392, 0.9615097045898438], "text": "named entity recognition", "score": 0.956506073474884, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19657, "end": 19678, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120news", "\u0120article", "\u0120cur", "ation"], "seq_scores": [0.5168462991714478, 0.4014784097671509, 0.439584344625473, 0.5131425857543945], "text": "news article curation", "score": 0.46776290982961655, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18960, "end": 18972, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120100", "\u0120examples"], "seq_scores": [0.987852931022644, 0.9924357533454895], "text": "100 examples", "score": 0.9901443421840668, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18978, "end": 18997, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120development", "\u0120set"], "seq_scores": [0.9985595345497131, 0.9992437362670898, 0.9983773231506348], "text": "the development set", "score": 0.9987268646558126, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19029, "end": 19033, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120them"], "seq_scores": [0.630096971988678], "text": "them", "score": 0.630096971988678, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19114, "end": 19131, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120existing", "\u0120datasets"], "seq_scores": [0.9981460571289062, 0.9995237588882446], "text": "existing datasets", "score": 0.9988349080085754, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19689, "end": 19710, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Commons", "ense", "\u0120Reason", "ing"], "seq_scores": [0.8542162179946899, 0.8103636503219604, 0.9109036326408386, 0.7919232249259949], "text": "Commonsense Reasoning", "score": 0.841851681470871, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19772, "end": 19781, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Task"], "seq_token": ["\u0120commons", "ense", "\u0120knowledge", "\u0120reasoning"], "seq_scores": [0.6240041255950928, 0.9833341240882874, 0.976753830909729, 0.6259945631027222], "text": "reasoning", "score": 0.8025216609239578, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 19876, "end": 19897, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9944900274276733, 0.9968442916870117, 0.9966980218887329], "text": "commonsense reasoning", "score": 0.9960107803344727, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20044, "end": 20056, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Miller", ",", "\u01201995"], "seq_scores": [0.9948374629020691, 0.9976754784584045, 0.9973981380462646], "text": "Miller, 1995", "score": 0.9966370264689127, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20058, "end": 20077, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Liu", "\u0120and", "\u0120Singh", ",", "\u01202004"], "seq_scores": [0.99845290184021, 0.9987009763717651, 0.9990155696868896, 0.9986308217048645, 0.9986634254455566], "text": "Liu and Singh, 2004", "score": 0.9986927390098572, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20079, "end": 20082, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u0120Pas"], "seq_scores": [0.9975212216377258], "text": "Pas", "score": 0.9975212216377258, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20108, "end": 20126, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["ca", "\u0120and", "\u0120Van", "\u0120Dur", "me", ",", "\u01202008", "\u0120Zhang", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9939596652984619, 0.9976961016654968, 0.9984808564186096, 0.9989988207817078, 0.9989437460899353, 0.99873286485672, 0.9985148310661316, 0.9987175464630127, 0.9987055063247681, 0.9987958669662476, 0.9988228678703308, 0.9985463619232178], "text": "Zhang et al., 2017", "score": 0.99824291964372, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19842, "end": 19861, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120sampled", "\u0120queries"], "seq_scores": [0.9990719556808472, 0.9996781349182129, 0.9995531439781189], "text": "the sampled queries", "score": 0.9994344115257263, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20129, "end": 20145, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Ca", "usal", "\u0120Reason", "ing"], "seq_scores": [0.9057487845420837, 0.9730386734008789, 0.9692555069923401, 0.9732173681259155], "text": "Causal Reasoning", "score": 0.9553150832653046, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20151, "end": 20176, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120causal", "\u0120brid", "ging", "\u0120inference"], "seq_scores": [0.7834611535072327, 0.9422709941864014, 0.9389285445213318, 0.7753631472587585], "text": "causal bridging inference", "score": 0.8600059598684311, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20246, "end": 20265, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["S", "inger", "\u0120et", "\u0120al", ".,", "\u01201992"], "seq_scores": [0.9984935522079468, 0.9916816353797913, 0.997198224067688, 0.9972610473632812, 0.9968461394309998, 0.9945282340049744], "text": "Singer et al., 1992", "score": 0.9960014720757803, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20267, "end": 20288, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ro", "em", "me", "le", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.9989235997200012, 0.9974809288978577, 0.9980630278587341, 0.9982465505599976, 0.9979912042617798, 0.9976544976234436, 0.9978395700454712, 0.99730384349823], "text": "Roemmele et al., 2011", "score": 0.9979379028081894, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20291, "end": 20307, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["Na", "\u00c3\u00afve", "\u0120Psychology"], "seq_scores": [0.9395442605018616, 0.9709750413894653, 0.9307464361190796], "text": "Na\u00efve Psychology", "score": 0.9470885793368021, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20368, "end": 20395, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["St", "ich", "\u0120and", "\u0120Raven", "sc", "ro", "ft", ",", "\u01201994"], "seq_scores": [0.9985923171043396, 0.9985179305076599, 0.9987550973892212, 0.9990226030349731, 0.9991693496704102, 0.9991787075996399, 0.9992104768753052, 0.998063862323761, 0.9979776740074158], "text": "Stich and Ravenscroft, 1994", "score": 0.9987208909458585, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20684, "end": 20687, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.900478184223175, 0.6735689640045166], "text": "MRC", "score": 0.7870235741138458, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20705, "end": 20711, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9985979199409485, 0.9991963505744934, 0.9986420273780823], "text": "ReCoRD", "score": 0.9988120992978414, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20760, "end": 20766, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9980081915855408, 0.9989884495735168, 0.9982796907424927], "text": "ReCoRD", "score": 0.9984254439671835, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20801, "end": 20830, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.9974896907806396, 0.9952527284622192, 0.9858291149139404], "text": "machine reading comprehension", "score": 0.9928571780522665, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20832, "end": 20835, "seq_label": ["B-Task", "I-Task"], "seq_token": ["M", "RC"], "seq_scores": [0.9795827865600586, 0.9882033467292786], "text": "MRC", "score": 0.9838930666446686, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20861, "end": 20871, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120cl", "oz", "estyle"], "seq_scores": [0.48580092191696167, 0.48733481764793396, 0.5945018529891968], "text": "clozestyle", "score": 0.5225458641846975, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20986, "end": 20989, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.888492226600647, 0.7238131761550903], "text": "MRC", "score": 0.8061527013778687, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21000, "end": 21006, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9985466599464417, 0.9990426898002625, 0.9983932375907898], "text": "ReCoRD", "score": 0.9986608624458313, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21030, "end": 21035, "seq_label": ["B-Method"], "seq_token": ["\u0120human"], "seq_scores": [0.5617798566818237], "text": "human", "score": 0.5617798566818237, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20675, "end": 20701, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120existing", "\u0120M", "RC", "\u0120architectures"], "seq_scores": [0.9008545279502869, 0.877708911895752, 0.9762559533119202, 0.92868572473526], "text": "existing MRC architectures", "score": 0.9208762794733047, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20801, "end": 20845, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120machine", "\u0120reading", "\u0120comprehension", "\u0120(", "M", "RC", ")", "\u0120datasets"], "seq_scores": [0.9778101444244385, 0.9998075366020203, 0.9998573064804077, 0.9998053908348083, 0.9989755153656006, 0.9998637437820435, 0.9998137354850769, 0.9997267127037048], "text": "machine reading comprehension (MRC) datasets", "score": 0.9969575107097626, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20847, "end": 20879, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120passages", "\u0120with", "\u0120cl", "oz", "estyle", "\u0120queries"], "seq_scores": [0.9987779259681702, 0.922145664691925, 0.9834651947021484, 0.9997740387916565, 0.9995604157447815, 0.9996036887168884], "text": "passages with clozestyle queries", "score": 0.9838878214359283, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20884, "end": 20892, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120passages"], "seq_scores": [0.9984917640686035], "text": "passages", "score": 0.9984917640686035, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20898, "end": 20905, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.7021921277046204], "text": "queries", "score": 0.7021921277046204, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20986, "end": 20996, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120answers", "\u0120M", "RC", "\u0120models"], "seq_scores": [0.6922786831855774, 0.9297318458557129, 0.9996086955070496, 0.9995967745780945], "text": "MRC models", "score": 0.9053039997816086, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21096, "end": 21101, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120train"], "seq_scores": [0.6160393953323364], "text": "train", "score": 0.6160393953323364, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21160, "end": 21167, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Doc", "Q", "A", "\u01202"], "seq_scores": [0.9989930987358093, 0.9997530579566956, 0.999701201915741, 0.9995423555374146], "text": "DocQA 2", "score": 0.9994974285364151, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21169, "end": 21192, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Clark", "\u0120and", "\u0120Gardner", ",", "\u01202018"], "seq_scores": [0.9987369179725647, 0.9961336851119995, 0.9971304535865784, 0.9966880679130554, 0.9966322779655457], "text": "Clark and Gardner, 2018", "score": 0.9970642805099488, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21292, "end": 21320, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120bid", "irection", "al", "\u0120attention", "\u0120flow"], "seq_scores": [0.936439037322998, 0.92304527759552, 0.9378091096878052, 0.9345032572746277, 0.946478545665741], "text": "bidirectional attention flow", "score": 0.9356550455093384, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21322, "end": 21338, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Se", "o", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9970545768737793, 0.9964359998703003, 0.9949421286582947, 0.9962477087974548, 0.9953587651252747, 0.9952139854431152], "text": "Seo et al., 2016", "score": 0.9958755274613699, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21344, "end": 21358, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120self", "\u0120attention"], "seq_scores": [0.9839304089546204, 0.9757594466209412], "text": "self attention", "score": 0.9798449277877808, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21384, "end": 21387, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.937915563583374, 0.7120997309684753], "text": "MRC", "score": 0.8250076472759247, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21413, "end": 21418, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9991899132728577, 0.999599039554596, 0.9994921684265137], "text": "DocQA", "score": 0.9994270404179891, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21424, "end": 21428, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9959028363227844, 0.9927272796630859], "text": "ELMo", "score": 0.9943150579929352, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21430, "end": 21449, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["P", "eters", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9990372657775879, 0.9975425004959106, 0.998062789440155, 0.998704195022583, 0.9981130361557007, 0.9984373450279236], "text": "Peters et al., 2018", "score": 0.9983161886533102, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21521, "end": 21541, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["V", "as", "w", "ani", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9992295503616333, 0.9993343949317932, 0.9993665814399719, 0.9993683695793152, 0.999010443687439, 0.9992807507514954, 0.9990261793136597, 0.9992139339447021], "text": "Vaswani et al., 2017", "score": 0.9992287755012512, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21552, "end": 21557, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.9985501170158386, 0.9995953440666199, 0.9997037053108215], "text": "QANet", "score": 0.9992830554644266, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 21600, "end": 21611, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120transformer"], "seq_scores": [0.5918475985527039], "text": "transformer", "score": 0.5918475985527039, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21628, "end": 21633, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120SAN", "\u01204"], "seq_scores": [0.9877327680587769, 0.9927884936332703], "text": "SAN 4", "score": 0.9902606308460236, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21635, "end": 21651, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["L", "iu", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9986348748207092, 0.9972092509269714, 0.9978725910186768, 0.9985764026641846, 0.9978508949279785, 0.9980547428131104], "text": "Liu et al., 2018", "score": 0.9980331261952718, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21672, "end": 21675, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9731642603874207, 0.8640547394752502], "text": "MRC", "score": 0.9186094999313354, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21714, "end": 21719, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9987260699272156, 0.9995182752609253, 0.9994696974754333], "text": "DocQA", "score": 0.9992380142211914, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21733, "end": 21759, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120sto", "ch", "astic", "\u0120answer", "\u0120module"], "seq_scores": [0.953181803226471, 0.9623855948448181, 0.995243489742279, 0.9956647753715515, 0.994685173034668, 0.988987386226654], "text": "a stochastic answer module", "score": 0.9816913704077402, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21775, "end": 21778, "seq_label": ["B-MLModel"], "seq_token": ["\u0120SAN"], "seq_scores": [0.9582570195198059], "text": "SAN", "score": 0.9582570195198059, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21782, "end": 21788, "seq_label": ["B-Task"], "seq_token": ["\u0120filter"], "seq_scores": [0.5096163153648376], "text": "filter", "score": 0.5096163153648376, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21905, "end": 21908, "seq_label": ["B-MLModel"], "seq_token": ["\u0120SAN"], "seq_scores": [0.8095904588699341], "text": "SAN", "score": 0.8095904588699341, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21924, "end": 21927, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9629034399986267, 0.7779971361160278], "text": "MRC", "score": 0.8704502880573273, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21197, "end": 21220, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120strong", "\u0120baseline", "\u0120model"], "seq_scores": [0.9991438388824463, 0.9997259974479675, 0.9997789263725281, 0.9997431635856628], "text": "a strong baseline model", "score": 0.9995979815721512, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21225, "end": 21232, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.9239833354949951], "text": "queries", "score": 0.9239833354949951, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21238, "end": 21256, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120extract", "ive", "\u0120answers"], "seq_scores": [0.9341469407081604, 0.9504308104515076, 0.8912951350212097], "text": "extractive answers", "score": 0.9252909620602926, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21384, "end": 21394, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120M", "RC", "\u0120models"], "seq_scores": [0.9976468682289124, 0.9991918206214905, 0.998785674571991], "text": "MRC models", "score": 0.9985414544741312, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21476, "end": 21502, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120largely", "\u0120pret", "rained", "\u0120enc", "oder"], "seq_scores": [0.9586127400398254, 0.9452486038208008, 0.9833489656448364, 0.9869979619979858, 0.9849081039428711], "text": "largely pretrained encoder", "score": 0.9718232750892639, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21506, "end": 21517, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120dataset"], "seq_scores": [0.9986314177513123, 0.9988724589347839], "text": "our dataset", "score": 0.9987519383430481, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21600, "end": 21611, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120transformer"], "seq_scores": [0.6822919845581055], "text": "transformer", "score": 0.6822919845581055, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21615, "end": 21626, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120dataset"], "seq_scores": [0.9987757802009583, 0.9989330172538757], "text": "our dataset", "score": 0.998854398727417, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21661, "end": 21681, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120top", "-", "rank", "\u0120M", "RC", "\u0120model"], "seq_scores": [0.9989790916442871, 0.9988623857498169, 0.9997195601463318, 0.9996451139450073, 0.9996300935745239, 0.9997987151145935, 0.999665379524231], "text": "a top-rank MRC model", "score": 0.9994714770998273, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21809, "end": 21817, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120data"], "seq_scores": [0.8916307091712952, 0.8200486898422241], "text": "our data", "score": 0.8558396995067596, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21861, "end": 21872, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120queries"], "seq_scores": [0.8982732892036438, 0.8460536003112793], "text": "the queries", "score": 0.8721634447574615, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21918, "end": 21923, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120other"], "seq_scores": [0.6160557866096497], "text": "other", "score": 0.6160557866096497, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21924, "end": 21941, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120M", "RC", "\u0120architectures"], "seq_scores": [0.5778362154960632, 0.9926666617393494, 0.9647356271743774], "text": "MRC architectures", "score": 0.84507950146993, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21943, "end": 21951, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["AS", "Reader"], "seq_scores": [0.9860081076622009, 0.9957045912742615], "text": "ASReader", "score": 0.9908563494682312, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 21955, "end": 21974, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["K", "adle", "c", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993771910667419, 0.9992814660072327, 0.9992202520370483, 0.9990262985229492, 0.9991902709007263, 0.9989468455314636, 0.9989196062088013], "text": "Kadlec et al., 2016", "score": 0.999137418610709, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22037, "end": 22058, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Her", "-", "mann", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9991269707679749, 0.9988390803337097, 0.9990542531013489, 0.9987088441848755, 0.9989759922027588, 0.9985837936401367, 0.9985049962997437], "text": "Her-mann et al., 2015", "score": 0.9988277043615069, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22060, "end": 22077, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hill", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9989727735519409, 0.998542308807373, 0.9988862872123718, 0.9986298084259033, 0.9983886480331421], "text": "Hill et al., 2015", "score": 0.9986839652061462, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 22159, "end": 22167, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120AS", "Reader"], "seq_scores": [0.9922338724136353, 0.9972917437553406], "text": "ASReader", "score": 0.9947628080844879, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22254, "end": 22272, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Tr", "in", "h", "\u0120and", "\u0120Le", ",", "\u01202018"], "seq_scores": [0.9992703795433044, 0.9987201690673828, 0.9988413453102112, 0.9989030361175537, 0.9992145299911499, 0.9989638328552246, 0.9990050196647644], "text": "Trinh and Le, 2018", "score": 0.9989883303642273, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 22352, "end": 22377, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.913499653339386, 0.9790974259376526, 0.9852237701416016, 0.985880434513092, 0.9865989685058594, 0.9315982460975647], "text": "Winograd Schema Challenge", "score": 0.9636497497558594, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22379, "end": 22400, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Le", "ves", "que", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.9992291927337646, 0.9994470477104187, 0.9992067217826843, 0.9990803003311157, 0.9993102550506592, 0.999138355255127, 0.9992601275444031], "text": "Levesque et al., 2011", "score": 0.9992388572011676, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21979, "end": 22002, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120strong", "\u0120baseline", "\u0120model"], "seq_scores": [0.9990697503089905, 0.9997945427894592, 0.9997254014015198, 0.999686598777771], "text": "a strong baseline model", "score": 0.9995690733194351, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22007, "end": 22027, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120clo", "ze", "-", "style", "\u0120datasets"], "seq_scores": [0.9980379939079285, 0.999502420425415, 0.999605119228363, 0.9997579455375671, 0.9997231364250183], "text": "cloze-style datasets", "score": 0.9993253231048584, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22087, "end": 22108, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120other", "\u0120baseline", "\u0120models"], "seq_scores": [0.9974363446235657, 0.9987397789955139, 0.999433696269989], "text": "other baseline models", "score": 0.9985366066296896, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22229, "end": 22244, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Language", "\u0120Models"], "seq_scores": [0.9953662157058716, 0.9902015328407288], "text": "Language Models", "score": 0.9927838742733002, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22248, "end": 22251, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["L", "Ms"], "seq_scores": [0.9957101345062256, 0.9929232597351074], "text": "LMs", "score": 0.9943166971206665, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22285, "end": 22298, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120corpor", "a"], "seq_scores": [0.9970422387123108, 0.9991482496261597, 0.9972636699676514], "text": "large corpora", "score": 0.9978180527687073, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22582, "end": 22585, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9952868223190308, 0.9938299059867859], "text": "LMs", "score": 0.9945583641529083, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22839, "end": 22850, "seq_label": ["I-MLModel", "B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Guess", "\u0120clo", "ze", "\u0120style"], "seq_scores": [0.48717647790908813, 0.7370674014091492, 0.8067477345466614, 0.6033610105514526], "text": "cloze style", "score": 0.6585881561040878, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22774, "end": 22794, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120evaluated", "\u0120models"], "seq_scores": [0.9946086406707764, 0.9971634745597839, 0.994051992893219], "text": "the evaluated models", "score": 0.9952747027079264, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22809, "end": 22820, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120queries"], "seq_scores": [0.7242927551269531, 0.6650585532188416], "text": "the queries", "score": 0.6946756541728973, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22824, "end": 22835, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120dataset"], "seq_scores": [0.9982750415802002, 0.9986977577209473], "text": "our dataset", "score": 0.9984863996505737, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23023, "end": 23052, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120development", "\u0120and", "\u0120test", "\u0120sets"], "seq_scores": [0.995819091796875, 0.9940351843833923, 0.9969347715377808, 0.9953526258468628, 0.9948640465736389], "text": "the development and test sets", "score": 0.99540114402771, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23145, "end": 23162, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120performance"], "seq_scores": [0.9431257247924805, 0.8762007355690002], "text": "human performance", "score": 0.9096632301807404, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23176, "end": 23178, "seq_label": ["B-Method"], "seq_token": ["\u0120an"], "seq_scores": [0.5863854885101318], "text": "an", "score": 0.5863854885101318, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23331, "end": 23336, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9970082640647888, 0.9984931945800781, 0.9987175464630127], "text": "SQuAD", "score": 0.9980730017026266, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 23338, "end": 23360, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9988310933113098, 0.9986635446548462, 0.9988067150115967, 0.9987288117408752, 0.9987068176269531, 0.9986565113067627, 0.998552143573761, 0.9968133568763733], "text": "Rajpurkar et al., 2016", "score": 0.9984698742628098, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23499, "end": 23520, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9972345232963562, 0.9984934329986572, 0.9967431426048279], "text": "the reference answers", "score": 0.9974903662999471, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23606, "end": 23627, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9885770082473755, 0.9911655783653259, 0.9901782274246216], "text": "the reference answers", "score": 0.9899736046791077, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23754, "end": 23775, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120reference", "\u0120answers"], "seq_scores": [0.9671490788459778, 0.9974599480628967, 0.9958159327507019], "text": "the reference answers", "score": 0.9868083198865255, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23817, "end": 23835, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120of", "\u0120the", "\u0120queries"], "seq_scores": [0.9870184063911438, 0.968812108039856, 0.9805137515068054, 0.9993240833282471], "text": "all of the queries", "score": 0.9839170873165131, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24035, "end": 24040, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Doc", "Q", "A"], "seq_scores": [0.5915024280548096, 0.8162906169891357, 0.6778481602668762], "text": "DocQA", "score": 0.6952137351036072, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 24046, "end": 24050, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9800164103507996, 0.9722672700881958], "text": "ELMo", "score": 0.9761418402194977, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24139, "end": 24144, "seq_label": ["B-Method"], "seq_token": ["\u0120human"], "seq_scores": [0.4309487044811249], "text": "human", "score": 0.4309487044811249, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24182, "end": 24188, "seq_label": ["I-Method", "I-Task", "I-Task", "B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120machine", "\u0120reading", "\u0120comprehension", "\u0120Re", "Co", "RD"], "seq_scores": [0.7431259155273438, 0.706369161605835, 0.8077029585838318, 0.9980554580688477, 0.9985200762748718, 0.9964919686317444], "text": "ReCoRD", "score": 0.8750442564487457, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 24216, "end": 24220, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9845705628395081, 0.979127824306488], "text": "ELMo", "score": 0.981849193572998, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24263, "end": 24268, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.630564272403717, 0.8253439664840698, 0.7546440362930298], "text": "DocQA", "score": 0.7368507583936056, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 24274, "end": 24278, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9830095171928406, 0.9781278967857361], "text": "ELMo", "score": 0.9805687069892883, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 24315, "end": 24319, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9836982488632202, 0.9834077954292297], "text": "ELMo", "score": 0.983553022146225, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24360, "end": 24363, "seq_label": ["B-Method"], "seq_token": ["\u0120SAN"], "seq_scores": [0.8880529999732971], "text": "SAN", "score": 0.8880529999732971, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24454, "end": 24457, "seq_label": ["B-Method"], "seq_token": ["\u0120SAN"], "seq_scores": [0.9383569359779358], "text": "SAN", "score": 0.9383569359779358, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 24494, "end": 24497, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9946263432502747, 0.9922206997871399], "text": "MRC", "score": 0.9934235215187073, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24521, "end": 24538, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120machine", "\u0120filtering"], "seq_scores": [0.836567759513855, 0.914789617061615], "text": "machine filtering", "score": 0.875678688287735, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24685, "end": 24687, "seq_label": ["B-Method"], "seq_token": ["\u0120LM"], "seq_scores": [0.6163781881332397], "text": "LM", "score": 0.6163781881332397, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24739, "end": 24764, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.8355792164802551, 0.6282549500465393, 0.6949824094772339, 0.6745879054069519, 0.504340410232544, 0.7513637542724609], "text": "Winograd Schema Challenge", "score": 0.6815181076526642, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23941, "end": 23948, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120set"], "seq_scores": [0.9949836730957031, 0.990680992603302], "text": "the set", "score": 0.9928323328495026, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23974, "end": 23993, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120development", "\u0120set"], "seq_scores": [0.998847484588623, 0.9991318583488464, 0.9988085031509399], "text": "the development set", "score": 0.9989292820294698, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24008, "end": 24033, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120best", "\u0120automatic", "\u0120method"], "seq_scores": [0.8944368362426758, 0.9150760173797607, 0.8532035946846008, 0.8168556094169617], "text": "the best automatic method", "score": 0.8698930144309998, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24086, "end": 24098, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120test", "\u0120set"], "seq_scores": [0.9977477192878723, 0.9983489513397217, 0.9944730401039124], "text": "the test set", "score": 0.9968565702438354, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24190, "end": 24207, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120All", "\u0120other", "\u0120methods"], "seq_scores": [0.6700220704078674, 0.6377010941505432, 0.7540838718414307], "text": "All other methods", "score": 0.6872690121332804, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24398, "end": 24427, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120other", "\u0120strong", "\u0120baseline", "\u0120methods"], "seq_scores": [0.7396316528320312, 0.6969925165176392, 0.969477117061615, 0.9198538661003113], "text": "other strong baseline methods", "score": 0.8314887881278992, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24489, "end": 24493, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120many"], "seq_scores": [0.6536261439323425], "text": "many", "score": 0.6536261439323425, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24494, "end": 24504, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120M", "RC", "\u0120models"], "seq_scores": [0.4911954700946808, 0.9931640028953552, 0.9897633790969849], "text": "MRC models", "score": 0.8247076173623403, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24654, "end": 24677, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120un", "super", "vised", "\u0120method"], "seq_scores": [0.9533537030220032, 0.911998450756073, 0.8841009736061096, 0.9084744453430176, 0.897682785987854], "text": "the unsupervised method", "score": 0.9111220717430115, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24799, "end": 24824, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120random", "\u0120guess", "\u0120baseline"], "seq_scores": [0.9050408005714417, 0.911980152130127, 0.871199905872345, 0.7786585092544556], "text": "the random guess baseline", "score": 0.8667198419570923, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24885, "end": 24896, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120dataset"], "seq_scores": [0.9976423382759094, 0.9981028437614441], "text": "our dataset", "score": 0.9978725910186768, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25028, "end": 25031, "seq_label": ["B-Method"], "seq_token": ["\u0120the"], "seq_scores": [0.549011766910553], "text": "the", "score": 0.549011766910553, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25032, "end": 25048, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120evaluation"], "seq_scores": [0.5998092889785767, 0.9798651337623596], "text": "human evaluation", "score": 0.7898372113704681, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24971, "end": 24991, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01208", "%", "\u0120dev", "./", "test", "\u0120queries"], "seq_scores": [0.9454038143157959, 0.9772615432739258, 0.7660207748413086, 0.9933479428291321, 0.9950487017631531, 0.9924693703651428], "text": "8% dev./test queries", "score": 0.9449253578980764, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25075, "end": 25088, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120queries"], "seq_scores": [0.8793937563896179, 0.94980788230896], "text": "these queries", "score": 0.9146008193492889, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25462, "end": 25475, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-MLModel", "I-Method", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Doc", "Q", "A", "\u0120w", "/", "\u0120EL", "Mo"], "seq_scores": [0.9772751331329346, 0.9950276017189026, 0.9935864806175232, 0.4018940031528473, 0.4873150885105133, 0.38650450110435486, 0.8266299962997437], "text": "DocQA w/ ELMo", "score": 0.7240332577909742, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25477, "end": 25482, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9898177981376648, 0.9966801404953003, 0.9955487251281738], "text": "DocQA", "score": 0.9940155545870463, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25488, "end": 25493, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.9955074787139893, 0.9982970356941223, 0.9970347881317139], "text": "QANet", "score": 0.9969464341799418, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 25505, "end": 25509, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.966590404510498, 0.9301348328590393], "text": "ELMo", "score": 0.9483626186847687, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25583, "end": 25591, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-MLModel", "I-Method"], "seq_token": ["\u0120Doc", "Q", "A", "\u0120w", "/"], "seq_scores": [0.977390706539154, 0.9939665794372559, 0.9923920035362244, 0.4050197899341583, 0.5139203071594238], "text": "DocQA w/", "score": 0.7765378773212432, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 25592, "end": 25596, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.4298534393310547, 0.7849888205528259], "text": "ELMo", "score": 0.6074211299419403, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25639, "end": 25644, "seq_label": ["B-Method"], "seq_token": ["\u0120human"], "seq_scores": [0.36318835616111755], "text": "human", "score": 0.36318835616111755, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 25741, "end": 25745, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9519861936569214, 0.8972368240356445], "text": "ELMo", "score": 0.924611508846283, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 25785, "end": 25789, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9401668906211853, 0.8834142088890076], "text": "ELMo", "score": 0.9117905497550964, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 25801, "end": 25811, "seq_label": ["B-Task"], "seq_token": ["\u0120prediction"], "seq_scores": [0.9759666323661804], "text": "prediction", "score": 0.9759666323661804, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 25432, "end": 25460, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120three", "\u0120representative", "\u0120methods"], "seq_scores": [0.9388551712036133, 0.9646172523498535, 0.9633399248123169], "text": "three representative methods", "score": 0.9556041161219279, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 25538, "end": 25555, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120three", "\u0120methods"], "seq_scores": [0.8366420865058899, 0.7893261909484863, 0.8478454947471619], "text": "all three methods", "score": 0.8246045907338461, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25692, "end": 25699, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.9963763356208801], "text": "queries", "score": 0.9963763356208801, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 25846, "end": 25861, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.997312605381012, 0.9974913597106934], "text": "language models", "score": 0.9974019825458527, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25918, "end": 25923, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.8680015206336975, 0.8374115824699402, 0.7692877054214478], "text": "QANet", "score": 0.8249002695083618, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 25944, "end": 25949, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.8425986766815186, 0.7653961181640625, 0.6416772603988647], "text": "QANet", "score": 0.7498906850814819, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26116, "end": 26121, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.937912106513977, 0.8791397213935852, 0.795250654220581], "text": "QANet", "score": 0.8707674940427145, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 26131, "end": 26156, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120transformer", "-", "based", "\u0120enc", "oder"], "seq_scores": [0.9312574863433838, 0.9676756262779236, 0.9364727139472961, 0.7783551216125488, 0.9320815205574036], "text": "transformer-based encoder", "score": 0.9091684937477111, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26161, "end": 26166, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9993157386779785, 0.9995947480201721, 0.9996843338012695], "text": "DocQA", "score": 0.99953160683314, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 26176, "end": 26194, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120L", "ST", "M", "-", "based", "\u0120enc", "oder"], "seq_scores": [0.9761804938316345, 0.9730761051177979, 0.97027188539505, 0.9461690783500671, 0.8880847692489624, 0.788017988204956, 0.9003735184669495], "text": "LSTM-based encoder", "score": 0.920310548373631, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26251, "end": 26256, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.9358043074607849, 0.877936601638794, 0.7954472899436951], "text": "QANet", "score": 0.8697293996810913, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26261, "end": 26266, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9992327690124512, 0.9995410442352295, 0.9995906949043274], "text": "DocQA", "score": 0.9994548360506693, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26285, "end": 26290, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9963288903236389, 0.9966188669204712, 0.9975643157958984], "text": "DocQA", "score": 0.9968373576800028, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 26294, "end": 26298, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9921412467956543, 0.9953475594520569], "text": "ELMo", "score": 0.9937444031238556, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26305, "end": 26310, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9930585622787476, 0.9943442344665527, 0.9969236254692078], "text": "DocQA", "score": 0.9947754740715027, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 26317, "end": 26322, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.9804312586784363, 0.9793154001235962, 0.9890746474266052], "text": "QANet", "score": 0.9829404354095459, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26339, "end": 26358, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Clo", "ze", "-", "style", "\u0120Setting"], "seq_scores": [0.8299911022186279, 0.9154227375984192, 0.8796126246452332, 0.8745056390762329, 0.6378135085105896], "text": "Cloze-style Setting", "score": 0.8274691224098205, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 26366, "end": 26374, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120AS", "Reader"], "seq_scores": [0.9535653591156006, 0.9770117402076721], "text": "ASReader", "score": 0.9652885496616364, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 26384, "end": 26387, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9898666143417358, 0.9725725054740906], "text": "MRC", "score": 0.9812195599079132, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26376, "end": 26394, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120the", "\u0120M", "RC", "\u0120models"], "seq_scores": [0.9147998094558716, 0.8307493925094604, 0.9990793466567993, 0.9998158812522888, 0.9997699856758118], "text": "all the MRC models", "score": 0.9488428831100464, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26519, "end": 26536, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120extract", "ive", "\u0120models"], "seq_scores": [0.998645007610321, 0.9998012185096741, 0.9997434020042419], "text": "extractive models", "score": 0.999396542708079, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26661, "end": 26673, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120models"], "seq_scores": [0.961694061756134, 0.9841127991676331], "text": "these models", "score": 0.9729034304618835, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26795, "end": 26812, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120three", "\u0120methods"], "seq_scores": [0.7651177048683167, 0.7702484726905823, 0.7130072712898254], "text": "all three methods", "score": 0.7494578162829081, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27198, "end": 27219, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.996001660823822, 0.998807430267334, 0.9988027811050415], "text": "commonsense reasoning", "score": 0.9978706240653992, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27221, "end": 27245, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120multi", "-", "sent", "ence", "\u0120reasoning"], "seq_scores": [0.9971827268600464, 0.9989118576049805, 0.9989150762557983, 0.9987963438034058, 0.9985948204994202], "text": "multi-sentence reasoning", "score": 0.9984801650047302, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27250, "end": 27262, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120partial", "\u0120clue"], "seq_scores": [0.9962335228919983, 0.9946871995925903], "text": "partial clue", "score": 0.9954603612422943, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 26975, "end": 27003, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120100", "\u0120randomly", "\u0120sampled", "\u0120queries"], "seq_scores": [0.9985479712486267, 0.9990399479866028, 0.9995449185371399, 0.9993158578872681], "text": "100 randomly sampled queries", "score": 0.9991121739149094, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27085, "end": 27107, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120three", "\u0120analyzed", "\u0120methods"], "seq_scores": [0.982587993144989, 0.9886470437049866, 0.9822575449943542], "text": "three analyzed methods", "score": 0.9844975272814432, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27111, "end": 27124, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120queries"], "seq_scores": [0.9884888529777527, 0.9845579266548157], "text": "these queries", "score": 0.9865233898162842, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27146, "end": 27159, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120three", "\u0120methods"], "seq_scores": [0.9712156653404236, 0.9879875183105469], "text": "three methods", "score": 0.9796015918254852, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27276, "end": 27281, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9981912970542908, 0.9993289709091187, 0.9992163181304932], "text": "DocQA", "score": 0.9989121953646342, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27283, "end": 27288, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.9866876006126404, 0.989574134349823, 0.9863278865814209], "text": "QANet", "score": 0.9875298738479614, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27309, "end": 27333, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120multi", "-", "sent", "ence", "\u0120reasoning"], "seq_scores": [0.68338543176651, 0.6152495741844177, 0.7926517128944397, 0.7915225028991699, 0.8469212055206299], "text": "multi-sentence reasoning", "score": 0.7459460854530334, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27369, "end": 27380, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120transformer"], "seq_scores": [0.6338119506835938], "text": "transformer", "score": 0.6338119506835938, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27388, "end": 27393, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Q", "AN", "et"], "seq_scores": [0.990481436252594, 0.9908281564712524, 0.9903737306594849], "text": "QANet", "score": 0.9905611077944437, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27407, "end": 27412, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9982428550720215, 0.9993020296096802, 0.9992069602012634], "text": "DocQA", "score": 0.998917281627655, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27461, "end": 27464, "seq_label": ["B-Method"], "seq_token": ["\u0120SAN"], "seq_scores": [0.6629429459571838], "text": "SAN", "score": 0.6629429459571838, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27490, "end": 27493, "seq_label": ["B-Method"], "seq_token": ["\u0120SAN"], "seq_scores": [0.38761845231056213], "text": "SAN", "score": 0.38761845231056213, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27525, "end": 27530, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9954793453216553, 0.9964886903762817, 0.9967922568321228], "text": "DocQA", "score": 0.9962534308433533, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 27546, "end": 27550, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.8992317318916321, 0.8996381163597107], "text": "ELMo", "score": 0.8994349241256714, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27579, "end": 27584, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9985968470573425, 0.9993799924850464, 0.9993257522583008], "text": "DocQA", "score": 0.9991008639335632, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27652, "end": 27673, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9969769716262817, 0.9974963068962097, 0.9966362714767456], "text": "commonsense reasoning", "score": 0.9970365166664124, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 27687, "end": 27691, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.8738098740577698, 0.8775942921638489], "text": "ELMo", "score": 0.8757020831108093, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27734, "end": 27755, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9869065284729004, 0.9913015961647034, 0.9800012707710266], "text": "commonsense reasoning", "score": 0.9860697984695435, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27761, "end": 27779, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120presumed", "\u0120knowledge"], "seq_scores": [0.44283169507980347, 0.49190959334373474], "text": "presumed knowledge", "score": 0.4673706442117691, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27804, "end": 27825, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9933083057403564, 0.9938165545463562, 0.9917997717857361], "text": "commonsense reasoning", "score": 0.9929748773574829, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27309, "end": 27341, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120multi", "-", "sent", "ence", "\u0120reasoning", "\u0120queries"], "seq_scores": [0.9875531196594238, 0.9896687865257263, 0.9923156499862671, 0.9936589598655701, 0.9885969758033752, 0.9828996062278748], "text": "multi-sentence reasoning queries", "score": 0.9891155163447062, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27416, "end": 27435, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120paraph", "r", "ased", "\u0120queries"], "seq_scores": [0.9973008036613464, 0.9972303509712219, 0.9979076385498047, 0.9984423518180847], "text": "paraphrased queries", "score": 0.9977202862501144, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27588, "end": 27607, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120paraph", "r", "ased", "\u0120queries"], "seq_scores": [0.9969636797904968, 0.9972991347312927, 0.9981579184532166, 0.9986684322357178], "text": "paraphrased queries", "score": 0.997772291302681, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27615, "end": 27638, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u012075", "%", "\u0120sampled", "\u0120queries"], "seq_scores": [0.9982786178588867, 0.9997114539146423, 0.9997968077659607, 0.9998028874397278, 0.9996048808097839], "text": "the 75% sampled queries", "score": 0.9994389295578003, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27827, "end": 27844, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120three", "\u0120methods"], "seq_scores": [0.5380141139030457, 0.5726631879806519, 0.5957716107368469], "text": "all three methods", "score": 0.5688163042068481, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27899, "end": 27905, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Re", "Co", "RD"], "seq_scores": [0.8664938807487488, 0.8637920618057251, 0.8126453757286072], "text": "ReCoRD", "score": 0.847643772761027, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27963, "end": 27984, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120reading", "\u0120comprehension"], "seq_scores": [0.9947143197059631, 0.9952367544174194], "text": "reading comprehension", "score": 0.9949755370616913, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27999, "end": 28020, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9956849813461304, 0.9988446235656738, 0.9985142350196838], "text": "commonsense reasoning", "score": 0.9976812799771627, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27944, "end": 27952, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9834728240966797], "text": "datasets", "score": 0.9834728240966797, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27954, "end": 27984, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120data", "\u0120for", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.9885385632514954, 0.825801432132721, 0.9263179898262024, 0.9554873108863831], "text": "data for reading comprehension", "score": 0.9240363240242004, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27990, "end": 27994, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120that"], "seq_scores": [0.817296028137207], "text": "that", "score": 0.817296028137207, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28049, "end": 28070, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail", "\u0120Corpus"], "seq_scores": [0.9984985589981079, 0.9987020492553711, 0.9991104006767273, 0.9989197254180908, 0.9918185472488403], "text": "CNN/Daily Mail Corpus", "score": 0.9974098563194275, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28072, "end": 28092, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "erman", "n", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9994029998779297, 0.9990153312683105, 0.9991938471794128, 0.9992402791976929, 0.9994099140167236, 0.9991784691810608, 0.9991040825843811], "text": "Hermann et al., 2015", "score": 0.9992207033293588, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28099, "end": 28119, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Children", "'s", "\u0120Book", "\u0120Test"], "seq_scores": [0.939350426197052, 0.9985076785087585, 0.9982155561447144, 0.991441011428833], "text": "Children's Book Test", "score": 0.9818786680698395, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28121, "end": 28124, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["C", "BT"], "seq_scores": [0.9978514909744263, 0.998399555683136], "text": "CBT", "score": 0.9981255233287811, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28127, "end": 28144, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Hill", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9990725517272949, 0.9990981817245483, 0.999393105506897, 0.9991031885147095, 0.9991626739501953], "text": "Hill et al., 2015", "score": 0.999165940284729, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28151, "end": 28159, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "-", "B", "ADA"], "seq_scores": [0.9975112676620483, 0.9987297654151917, 0.9986447691917419, 0.9988454580307007, 0.9989226460456848], "text": "LAM-BADA", "score": 0.9985307812690735, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28161, "end": 28181, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["P", "ap", "erno", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993195533752441, 0.9992417097091675, 0.999402642250061, 0.9993738532066345, 0.9994862079620361, 0.9993010759353638, 0.9993120431900024], "text": "Paperno et al., 2016", "score": 0.999348155089787, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28206, "end": 28212, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9899641871452332, 0.9970653653144836, 0.9961618185043335], "text": "ReCoRD", "score": 0.9943971236546835, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28222, "end": 28243, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail", "\u0120Corpus"], "seq_scores": [0.9983903169631958, 0.9986158609390259, 0.9989224672317505, 0.9987263083457947, 0.9938859343528748], "text": "CNN/Daily Mail Corpus", "score": 0.9977081775665283, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 28319, "end": 28328, "seq_label": ["B-Task"], "seq_token": ["\u0120reasoning"], "seq_scores": [0.43544551730155945], "text": "reasoning", "score": 0.43544551730155945, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28338, "end": 28355, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["C", "hen", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9995046854019165, 0.9989713430404663, 0.999135434627533, 0.9993113279342651, 0.9990500807762146, 0.9990310668945312], "text": "Chen et al., 2016", "score": 0.9991673231124878, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28363, "end": 28366, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120CB", "T"], "seq_scores": [0.9972684383392334, 0.9982631802558899], "text": "CBT", "score": 0.9977658092975616, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28492, "end": 28495, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120CB", "T"], "seq_scores": [0.9943830966949463, 0.9963756203651428], "text": "CBT", "score": 0.9953793585300446, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 28503, "end": 28529, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120machine", "\u0120or", "\u0120human", "\u0120filtering"], "seq_scores": [0.9682399034500122, 0.8693413734436035, 0.8312573432922363, 0.9932258725166321], "text": "machine or human filtering", "score": 0.915516123175621, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 28577, "end": 28580, "seq_label": ["B-Task", "I-Dataset"], "seq_token": ["\u0120CB", "T"], "seq_scores": [0.7086925506591797, 0.40837347507476807], "text": "CBT", "score": 0.5585330128669739, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28682, "end": 28685, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120CB", "T"], "seq_scores": [0.9923529028892517, 0.9962297081947327], "text": "CBT", "score": 0.9942913055419922, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28687, "end": 28694, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "B", "ADA"], "seq_scores": [0.997031569480896, 0.9987607002258301, 0.998986542224884, 0.9988671541213989], "text": "LAMBADA", "score": 0.9984114915132523, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28773, "end": 28779, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9893656969070435, 0.9961503744125366, 0.9949902892112732], "text": "ReCoRD", "score": 0.993502120176951, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28781, "end": 28788, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "B", "ADA"], "seq_scores": [0.996170699596405, 0.9985411167144775, 0.9987516403198242, 0.9986482262611389], "text": "LAMBADA", "score": 0.9980279207229614, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 28810, "end": 28827, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.9661659002304077, 0.9545733332633972], "text": "language modeling", "score": 0.9603696167469025, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28256, "end": 28263, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.925597608089447], "text": "queries", "score": 0.925597608089447, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28269, "end": 28272, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.5313804149627686], "text": "the", "score": 0.5313804149627686, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28370, "end": 28410, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120collection", "\u0120of", "\u012021", "\u0120consecutive", "\u0120sentences"], "seq_scores": [0.9787802696228027, 0.9837477803230286, 0.9371513724327087, 0.9016031622886658, 0.9859344363212585, 0.9844549894332886], "text": "a collection of 21 consecutive sentences", "score": 0.9619453350702921, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28416, "end": 28429, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120book", "\u0120excerpts"], "seq_scores": [0.9125210642814636, 0.9931382536888123], "text": "book excerpts", "score": 0.9528296589851379, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28554, "end": 28569, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120small", "\u0120portion"], "seq_scores": [0.8927121162414551, 0.7290693521499634, 0.742281436920166], "text": "a small portion", "score": 0.7880209684371948, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28573, "end": 28589, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120CB", "T", "\u0120examples"], "seq_scores": [0.9789788126945496, 0.9990695118904114, 0.9989783763885498, 0.9977185726165771], "text": "the CBT examples", "score": 0.993686318397522, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28842, "end": 28850, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120contexts"], "seq_scores": [0.9229192733764648], "text": "contexts", "score": 0.9229192733764648, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28896, "end": 28903, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["\u0120summ", "\u0120answers"], "seq_scores": [0.6967328786849976, 0.9574514031410217], "text": "answers", "score": 0.8270921409130096, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29005, "end": 29011, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9958903193473816, 0.99897301197052, 0.9987517595291138], "text": "ReCoRD", "score": 0.9978716969490051, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 29051, "end": 29054, "seq_label": ["I-Task", "B-Task", "I-Task"], "seq_token": ["ive", "\u0120M", "RC"], "seq_scores": [0.5199533104896545, 0.7320668697357178, 0.9230020642280579], "text": "MRC", "score": 0.7250074148178101, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29075, "end": 29080, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9992972612380981, 0.9996286630630493, 0.9996517896652222], "text": "SQuAD", "score": 0.9995259046554565, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29082, "end": 29104, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993205070495605, 0.9994091987609863, 0.9995579123497009, 0.9994832277297974, 0.9994101524353027, 0.9995076656341553, 0.9993364214897156, 0.9994088411331177], "text": "Rajpurkar et al., 2016", "score": 0.999429240822792, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29110, "end": 29116, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120News", "Q", "A"], "seq_scores": [0.998946487903595, 0.9996445178985596, 0.9994205236434937], "text": "NewsQA", "score": 0.9993371764818827, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29118, "end": 29140, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["T", "ris", "ch", "ler", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9990886449813843, 0.9994747042655945, 0.999545156955719, 0.9994839429855347, 0.9994171857833862, 0.9995231628417969, 0.9993627667427063, 0.9994572997093201], "text": "Trischler et al., 2017", "score": 0.9994191080331802, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 29212, "end": 29224, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120crowd", "workers"], "seq_scores": [0.8391980528831482, 0.7441025376319885], "text": "crowdworkers", "score": 0.7916502952575684, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 29269, "end": 29275, "seq_label": ["B-Method"], "seq_token": ["\u0120crowds"], "seq_scores": [0.48732054233551025], "text": "crowds", "score": 0.48732054233551025, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29390, "end": 29409, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["J", "ia", "\u0120and", "\u0120Liang", ",", "\u01202017"], "seq_scores": [0.9994839429855347, 0.998713493347168, 0.9989303946495056, 0.9990400671958923, 0.9991133809089661, 0.999062716960907], "text": "Jia and Liang, 2017", "score": 0.9990573326746622, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29411, "end": 29433, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Raj", "pur", "kar", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9996019005775452, 0.999496340751648, 0.9994639754295349, 0.9992800354957581, 0.9994589686393738, 0.999265730381012, 0.9993925094604492], "text": "Rajpurkar et al., 2018", "score": 0.9994227801050458, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29435, "end": 29456, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Wang", "\u0120and", "\u0120Bans", "al", ",", "\u01202018"], "seq_scores": [0.9995306730270386, 0.9990090131759644, 0.9994856119155884, 0.9994920492172241, 0.9993101358413696, 0.9993869066238403], "text": "Wang and Bansal, 2018", "score": 0.9993690649668375, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29497, "end": 29516, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["N", "guyen", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9994736313819885, 0.9932007193565369, 0.9988512992858887, 0.9991680383682251, 0.9987636804580688, 0.9986914992332458], "text": "Nguyen et al., 2016", "score": 0.9980248113473257, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29518, "end": 29536, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Josh", "i", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.999283492565155, 0.9988992214202881, 0.9989957213401794, 0.9992741942405701, 0.9990277290344238, 0.9991681575775146], "text": "Joshi et al., 2017", "score": 0.9991080860296885, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29538, "end": 29554, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120L", "ai", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.999512791633606, 0.9982830286026001, 0.9988778233528137, 0.9992551207542419, 0.9989435076713562, 0.999091625213623], "text": "Lai et al., 2017", "score": 0.9989939828713735, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29556, "end": 29573, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Dunn", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.999139666557312, 0.9988008737564087, 0.9991472959518433, 0.9989017248153687, 0.9989743232727051], "text": "Dunn et al., 2017", "score": 0.9989927768707275, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29575, "end": 29595, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120K", "oc", "isky", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9992589354515076, 0.9984889030456543, 0.9989917874336243, 0.9988821148872375, 0.9990862607955933, 0.9989286065101624, 0.9989388585090637], "text": "Kocisky et al., 2018", "score": 0.9989393523761204, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29597, "end": 29615, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Red", "dy", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9992827773094177, 0.9992615580558777, 0.9988110065460205, 0.9991340041160583, 0.9989803433418274, 0.9989939332008362], "text": "Reddy et al., 2018", "score": 0.9990772704283396, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29617, "end": 29634, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Choi", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9991810917854309, 0.9986376166343689, 0.9989956021308899, 0.9988877177238464, 0.9988521337509155], "text": "Choi et al., 2018", "score": 0.9989108324050904, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29636, "end": 29653, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9991675615310669, 0.9986696243286133, 0.9990983009338379, 0.9989125728607178, 0.9989545345306396], "text": "Yang et al., 2018", "score": 0.9989605188369751, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 29686, "end": 29707, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120reading", "\u0120comprehension"], "seq_scores": [0.9508410692214966, 0.9165621995925903], "text": "reading comprehension", "score": 0.9337016344070435, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29714, "end": 29730, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "ao", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9993178844451904, 0.9984777569770813, 0.9990044236183167, 0.9992738366127014, 0.999005138874054, 0.9990501999855042], "text": "Gao et al., 2018", "score": 0.9990215400854746, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29753, "end": 29792, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Commons", "ense", "\u0120Reason", "ing", "\u0120R", "OC", "St", "ories", "\u0120Corpus"], "seq_scores": [0.9971715807914734, 0.997352123260498, 0.9985920786857605, 0.9984265565872192, 0.9665252566337585, 0.99699866771698, 0.9983962178230286, 0.9981626868247986, 0.953600287437439], "text": "Commonsense Reasoning ROCStories Corpus", "score": 0.9894694950845506, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29794, "end": 29819, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Most", "af", "az", "ade", "h", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9987794756889343, 0.9992984533309937, 0.99931800365448, 0.9994984865188599, 0.9994527697563171, 0.9993769526481628, 0.9994422793388367, 0.9992815852165222, 0.9993330836296082], "text": "Mostafazadeh et al., 2016", "score": 0.9993090099758573, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29822, "end": 29826, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120SW", "AG"], "seq_scores": [0.9988781809806824, 0.9991717338562012], "text": "SWAG", "score": 0.9990249574184418, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29828, "end": 29848, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Z", "ell", "ers", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9991267323493958, 0.9995372295379639, 0.9995315074920654, 0.9993602633476257, 0.9994809031486511, 0.9993125200271606, 0.9994032382965088], "text": "Zellers et al., 2018", "score": 0.9993931991713387, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29886, "end": 29889, "seq_label": ["I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "B-Dataset", "I-Dataset"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge", "W", "SC"], "seq_scores": [0.6850066781044006, 0.9975159168243408, 0.9977978467941284, 0.9986020922660828, 0.9986849427223206, 0.997175931930542, 0.9983931183815002, 0.9986324906349182], "text": "WSC", "score": 0.9589761272072792, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29892, "end": 29913, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Le", "ves", "que", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.9989155530929565, 0.9993315935134888, 0.9990515112876892, 0.9992401599884033, 0.9994390606880188, 0.9992222785949707, 0.9993253946304321], "text": "Levesque et al., 2011", "score": 0.9992179359708514, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29939, "end": 29949, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120R", "OC", "St", "ories"], "seq_scores": [0.9987552165985107, 0.9978421926498413, 0.9988093376159668, 0.9987223744392395], "text": "ROCStories", "score": 0.9985322803258896, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 29959, "end": 29980, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9588922262191772, 0.9602981805801392, 0.9481310844421387], "text": "commonsense reasoning", "score": 0.9557738304138184, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 29984, "end": 30003, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120story", "\u0120understanding"], "seq_scores": [0.9655401706695557, 0.9687376022338867], "text": "story understanding", "score": 0.9671388864517212, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 30111, "end": 30123, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120crowd", "workers"], "seq_scores": [0.8880150318145752, 0.8152279853820801], "text": "crowdworkers", "score": 0.8516215085983276, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 30149, "end": 30159, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120elic"], "seq_scores": [0.5524603128433228, 0.5903270244598389], "text": "human elic", "score": 0.5713936686515808, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30173, "end": 30199, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gordon", "\u0120and", "\u0120Van", "\u0120Dur", "me", ",", "\u01202013"], "seq_scores": [0.9990519881248474, 0.9986030459403992, 0.9994028806686401, 0.999338686466217, 0.9991487264633179, 0.9988892674446106, 0.9985554814338684], "text": "Gordon and Van Durme, 2013", "score": 0.9989985823631287, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30201, "end": 30219, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mis", "ra", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9993208646774292, 0.999038577079773, 0.9990183115005493, 0.9992014765739441, 0.9989860653877258, 0.9990691542625427], "text": "Misra et al., 2016", "score": 0.9991057415803274, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30221, "end": 30239, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Zhang", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9991853833198547, 0.9988001585006714, 0.9991256594657898, 0.9988812804222107, 0.9990731477737427], "text": "Zhang et al., 2017", "score": 0.9990131258964539, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29038, "end": 29062, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120extract", "ive", "\u0120M", "RC", "\u0120dataset"], "seq_scores": [0.9992684721946716, 0.9992493987083435, 0.999751627445221, 0.9995142221450806, 0.9998767375946045, 0.9998664855957031], "text": "a extractive MRC dataset", "score": 0.9995878239472707, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29166, "end": 29175, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9858333468437195], "text": "questions", "score": 0.9858333468437195, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29179, "end": 29193, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9669196605682373, 0.9911608695983887], "text": "these datasets", "score": 0.979040265083313, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29269, "end": 29291, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120crowds", "ourced", "\u0120questions"], "seq_scores": [0.9766318202018738, 0.9459401965141296, 0.9806755185127258], "text": "crowdsourced questions", "score": 0.9677491784095764, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29325, "end": 29339, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120portion", "\u0120questions", "\u0120these", "\u0120datasets"], "seq_scores": [0.7191007137298584, 0.7336602210998535, 0.9908719658851624, 0.996198832988739], "text": "these datasets", "score": 0.8599579334259033, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29469, "end": 29495, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120large", "-", "scale", "\u0120datasets"], "seq_scores": [0.9646293520927429, 0.6681998372077942, 0.9993593096733093, 0.9994763731956482, 0.9993745684623718], "text": "other large-scale datasets", "score": 0.9262078881263733, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29736, "end": 29751, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120recent", "\u0120survey"], "seq_scores": [0.7764891982078552, 0.886870265007019, 0.7678704261779785], "text": "a recent survey", "score": 0.8104099631309509, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30067, "end": 30074, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120Stories"], "seq_scores": [0.7810617685317993], "text": "Stories", "score": 0.7810617685317993, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30078, "end": 30088, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120corpus"], "seq_scores": [0.9435195922851562, 0.9491037726402283], "text": "the corpus", "score": 0.9463116824626923, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 30246, "end": 30250, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120SW", "AG"], "seq_scores": [0.9284834265708923, 0.9660855531692505], "text": "SWAG", "score": 0.9472844898700714, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30259, "end": 30280, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9661152958869934, 0.8800436854362488, 0.8628134727478027], "text": "commonsense reasoning", "score": 0.9029908180236816, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30285, "end": 30311, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120natural", "\u0120language", "\u0120inference"], "seq_scores": [0.9819694757461548, 0.9413692355155945, 0.9644252061843872], "text": "natural language inference", "score": 0.9625879724820455, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 30469, "end": 30473, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120SW", "AG"], "seq_scores": [0.9983062744140625, 0.9980646967887878], "text": "SWAG", "score": 0.9981854856014252, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 30516, "end": 30537, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120advers", "arial", "\u0120filtering"], "seq_scores": [0.9786952137947083, 0.9835154414176941, 0.9893670678138733], "text": "adversarial filtering", "score": 0.9838592410087585, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 30543, "end": 30546, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC"], "seq_scores": [0.9981069564819336, 0.9969738721847534], "text": "WSC", "score": 0.9975404143333435, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30558, "end": 30606, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120intra", "-", "sent", "ential", "\u0120pronoun", "\u0120dis", "amb", "ig", "uation", "\u0120problems"], "seq_scores": [0.9956468939781189, 0.9976980090141296, 0.9988287091255188, 0.9992710947990417, 0.9987909197807312, 0.9992125034332275, 0.9993281364440918, 0.9986355900764465, 0.9991506338119507, 0.8323633670806885], "text": "intra-sentential pronoun disambiguation problems", "score": 0.9818925857543945, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30620, "end": 30641, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9973927736282349, 0.9985004663467407, 0.9985509514808655], "text": "commonsense reasoning", "score": 0.9981480638186137, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30669, "end": 30690, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ro", "em", "me", "le", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.9995243549346924, 0.9992533326148987, 0.9995158910751343, 0.9995495676994324, 0.9994014501571655, 0.9995656609535217, 0.9994020462036133, 0.9993517994880676], "text": "Roemmele et al., 2011", "score": 0.9994455128908157, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30692, "end": 30710, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Zhang", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9994944334030151, 0.9992014765739441, 0.9995341300964355, 0.9993439316749573, 0.9994736313819885], "text": "Zhang et al., 2017", "score": 0.9994095206260681, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30712, "end": 30735, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Rash", "kin", "\u0120et", "\u0120al", ".,", "\u01202018", "a", ",", "b"], "seq_scores": [0.9995519518852234, 0.9995453953742981, 0.9995400905609131, 0.9996356964111328, 0.9995617270469666, 0.9995459914207458, 0.9955927729606628, 0.9138097167015076, 0.9863858222961426], "text": "Rashkin et al., 2018a,b", "score": 0.9881299071841769, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30768, "end": 30789, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9965274930000305, 0.9982194304466248, 0.9985448122024536], "text": "commonsense reasoning", "score": 0.9977639118830363, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 30495, "end": 30510, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9971824884414673, 0.997848629951477], "text": "language models", "score": 0.9975155591964722, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30653, "end": 30667, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120datasets"], "seq_scores": [0.9983042478561401, 0.9997455477714539], "text": "other datasets", "score": 0.999024897813797, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 30819, "end": 30825, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9982074499130249, 0.9995099306106567, 0.9994944334030151], "text": "ReCoRD", "score": 0.9990706046422323, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30841, "end": 30848, "seq_label": ["B-Task"], "seq_token": ["\u0120reading"], "seq_scores": [0.5619490742683411], "text": "reading", "score": 0.5619490742683411, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30881, "end": 30902, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.8770659565925598, 0.9295394420623779, 0.943373441696167], "text": "commonsense reasoning", "score": 0.9166596134503683, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30920, "end": 30949, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120reading", "\u0120comprehension"], "seq_scores": [0.9935430288314819, 0.9918782711029053, 0.9744019508361816], "text": "machine reading comprehension", "score": 0.9866077502568563, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 30951, "end": 30954, "seq_label": ["B-Task", "I-Task"], "seq_token": ["M", "RC"], "seq_scores": [0.979698896408081, 0.9862776398658752], "text": "MRC", "score": 0.9829882681369781, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 30966, "end": 30972, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9962371587753296, 0.9991476535797119, 0.9991568326950073], "text": "ReCoRD", "score": 0.9981805483500162, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 31022, "end": 31043, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.8282610177993774, 0.8909860849380493, 0.8693744540214539], "text": "commonsense reasoning", "score": 0.8628738522529602, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 31112, "end": 31115, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.954856812953949, 0.9597114324569702], "text": "MRC", "score": 0.9572841227054596, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 31163, "end": 31169, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9972463846206665, 0.9991650581359863, 0.9992852807044983], "text": "ReCoRD", "score": 0.9985655744870504, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 31184, "end": 31190, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.9974948167800903, 0.9993544220924377, 0.9993956089019775], "text": "ReCoRD", "score": 0.9987482825915018, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 31218, "end": 31221, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120M", "RC"], "seq_scores": [0.9933759570121765, 0.9926180243492126], "text": "MRC", "score": 0.9929969906806946, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 31227, "end": 31248, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9284378886222839, 0.9657304286956787, 0.965128481388092], "text": "commonsense reasoning", "score": 0.9530989329020182, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30827, "end": 30870, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "-", "scale", "\u0120reading", "\u0120comprehension", "\u0120dataset"], "seq_scores": [0.9991025924682617, 0.999659538269043, 0.9997517466545105, 0.9997320771217346, 0.999836802482605, 0.9998371601104736, 0.9997618794441223], "text": "a large-scale reading comprehension dataset", "score": 0.9996688280786786, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30911, "end": 30964, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120existing", "\u0120machine", "\u0120reading", "\u0120comprehension", "\u0120(", "M", "RC", ")", "\u0120datasets"], "seq_scores": [0.9967178702354431, 0.9919037222862244, 0.9993588328361511, 0.9996713399887085, 0.9994263648986816, 0.9912699460983276, 0.9996527433395386, 0.9995444416999817, 0.9996185302734375], "text": "existing machine reading comprehension (MRC) datasets", "score": 0.9974626435173882, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31001, "end": 31008, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120queries"], "seq_scores": [0.6945499181747437], "text": "queries", "score": 0.6945499181747437, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 31060, "end": 31073, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Our", "\u0120bas", "elines"], "seq_scores": [0.8244329690933228, 0.9217857718467712, 0.8553246855735779], "text": "Our baselines", "score": 0.867181142171224, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31103, "end": 31124, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120existing", "\u0120M", "RC", "\u0120datasets"], "seq_scores": [0.9975739121437073, 0.9935085773468018, 0.999679684638977, 0.9994747042655945], "text": "existing MRC datasets", "score": 0.9975592195987701, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 32538, "end": 32547, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Amb", "iver", "ts"], "seq_scores": [0.6722267866134644, 0.7243010401725769, 0.7455189228057861], "text": "Ambiverts", "score": 0.7140155831972758, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 32932, "end": 32936, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.962611973285675, 0.8783427476882935], "text": "ELMo", "score": 0.9204773604869843, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 32968, "end": 32973, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.981792151927948, 0.9641979336738586, 0.955522894859314], "text": "DocQA", "score": 0.9671709934870402, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 32977, "end": 32981, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.8754484057426453, 0.7398003339767456], "text": "ELMo", "score": 0.8076243698596954, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 33005, "end": 33010, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9864888787269592, 0.9750838279724121, 0.9708340167999268], "text": "DocQA", "score": 0.9774689078330994, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 33675, "end": 33680, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Doc", "Q", "A"], "seq_scores": [0.9982243180274963, 0.9978553652763367, 0.9980983138084412], "text": "DocQA", "score": 0.9980593323707581, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 33689, "end": 33693, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9976351261138916, 0.9984226226806641], "text": "ELMo", "score": 0.9980288743972778, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 33781, "end": 33785, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.998082160949707, 0.999002993106842], "text": "ELMo", "score": 0.9985425770282745, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 34008, "end": 34012, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9974598288536072, 0.9988786578178406], "text": "ELMo", "score": 0.9981692433357239, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 34026, "end": 34035, "seq_label": ["B-Task"], "seq_token": ["\u0120narrowing"], "seq_scores": [0.6259130239486694], "text": "narrowing", "score": 0.6259130239486694, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 34063, "end": 34069, "seq_label": ["B-MLModel", "I-Dataset", "I-MLModel"], "seq_token": ["\u0120Re", "Co", "RD"], "seq_scores": [0.5584851503372192, 0.5095264911651611, 0.6813858151435852], "text": "ReCoRD", "score": 0.5831324855486552, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 33810, "end": 33828, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120its", "\u0120language", "\u0120model"], "seq_scores": [0.9932135343551636, 0.9112474918365479, 0.9991307854652405], "text": "its language model", "score": 0.967863937218984, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 33866, "end": 33869, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.6487228274345398], "text": "the", "score": 0.6487228274345398, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 34041, "end": 34059, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120candid", "iate", "\u0120answers"], "seq_scores": [0.9927916526794434, 0.9934700727462769, 0.9894856214523315], "text": "candidiate answers", "score": 0.991915782292684, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 35122, "end": 35128, "seq_label": ["B-Method"], "seq_token": ["\u0120Amazon"], "seq_scores": [0.48367640376091003], "text": "Amazon", "score": 0.48367640376091003, "type": "ScholarlyEntity"}]}, "filename": "00018_1810_12885.json", "id": "00018_1810_12885"}