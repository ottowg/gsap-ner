{"text": "WikiHow: A Large Scale Text Summarization Dataset\n\nAbstract:\nSequence-to-sequence models have recently gained the state of the art performance in summarization. However, not too many large-scale high-quality datasets are available and almost all the available ones are mainly news articles with specific writing style. Moreover, abstractive human-style systems involving description of the content at a deeper level require data with higher levels of abstraction. In this paper, we present WikiHow, a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it.\n\n\n1 Introduction\nSummarization as the process of generating a shorter version of a piece of text while preserving important context information is one of the most challenging NLP tasks. Sequence-to-sequence neural networks have recently obtained significant performance improvements on summarization (Rush et al., 2015; Chopra et al., 2016). However, the existence of large-scale datasets is the key to success of these models. Moreover, the length of the articles and the diversity in their styles can create more complications.\nAlmost all existing summarization datasets such as DUC (Harman and Over, 2004), Gigaword (Napoles et al., 2012), New York Times (Sandhaus, 2008) and CNN/Daily Mail (Nallapati et al., 2016) consist of news articles. The news articles have their own specific styles and therefore the systems trained on only news may not be generalized well. On the other hand, the existing datasets may not be large enough (DUC) to train a sequence-to-sequence model, the summaries may be limited to only headlines (Gigaword), they may be more useful as an extractive summarization dataset (New York Times) and their abstraction level might be limited (CNN/Daily mail).\nTo overcome the issues of the existing datasets, we present a new large-scale dataset called Wiki-How using the online WikiHow 1 knowledge base. It contains articles about various topics written in different styles making them different form existing news datasets. Each article consists of multiple paragraphs and each paragraph starts with a sentence summarizing it. By merging the paragraphs to form the article and the paragraph outlines to form the summary, the resulting version of the dataset contains more than 200,000 longsequence pairs. We then present two features to show how abstractive our dataset is. Finally, we analyze the performance of some of the existing extractive and abstractive systems on WikiHow as benchmarks for further studies. The contribution of this work is three-fold:\n\u2022 We introduce a large-scale, diverse dataset with various writing styles, convenient for long-sequence text summarization.\n\u2022 We introduce level of abstractedness and compression ratio metrics to show how abstractive the new dataset is.\n\u2022 We evaluate the performance of the existing systems on WikiHow to create benchmarks and understand the challenges better.\n\n2 Existing Datasets\nThere are several datasets used to evaluate the summarization systems. We briefly describe the properties of these datasets as follows.\nThe Lead The most important information about an event Who? What? Where? When? Why? How?\nThe Body The crucial information expanding the topic Argument, Controversy, Story, Evidence,\n\nBackground details\nThe Tail Extra information Interesting, related items. Journalist Assessment\n\nDUC:\nThe Document Understanding Conference dataset (Harman and Over, 2004) contains 500 news articles and their summaries capped at 75 bytes. The summaries are written by human authors and there exist more than one summary per article which is its major advantage over other existing datasets. The DUC dataset cannot be used for training models with large number of parameters and therefore is used along with other datasets (Rush et al., 2015; Nallapati et al., 2017). Gigaword: Another collection of news articles used for summarization is Gigaword (Napoles et al., 2012). The original articles in the dataset do not have summaries paired with them. However, some prior work (Rush et al., 2015; Chopra et al., 2016) used a subset of this dataset and constructed pairs of summaries by using the first line of the article and its headline, making the dataset suitable for short text summarization tasks. New York Times: The New York Times (NYT) dataset (Sandhaus, 2008) is a large collection of articles published between 1996 and 2007. While this dataset has been mainly used for extractive systems (Hong and Nenkova, 2014; Durrett et al., 2016), Paulus et al. (2017) are the first to evaluate their abstractive system using NYT. CNN/Daily Mail: This dataset mainly used in recent summarization papers (Nallapati et al., 2016; See et al., 2017; Nallapati et al., 2017) consists of online CNN and Daily Mail news articles and was originally developed for question/answering systems. The highlights associated with each article are concatenated to form the summary. Two versions of this dataset depending on the preprocessing exist.  Nallapati et al. (2017) has used the entity anonymization to create the anonymized version of the dataset while See et al. (2017) NEWSROOM: This corpus (Grusky et al., 2018) is the most recent large-scale dataset introduced for text summarization. It consists of diverse summaries combining abstractive and extractive strategies yet it is another news dataset and the average length of summaries are limited to 26.7.\n\n3 WikiHow Dataset\nThe existing summarization datasets, consist of news articles. These articles are written by journalists and follow the journalistic style. The journalists usually follow the Inverted Pyramid style (Po\u00a8ttker, 2003) (depicted in Figure 1) to prioritize and structure a text by starting with mentioning the most important, interesting or attentiongrabbing elements of a story in the opening paragraphs and later adding details and any background information. This writing style might be the cause why lead-3 baselines (where the first three sentences are selected to form the summary) usually score higher compared to the existing summarization systems. We introduce a new dataset called WikiHow, obtained from WikiHow data dump. This dataset contains articles written by ordinary people, not journalists, describing the steps of doing a task throughout the text. Therefore, the Inverted Pyramid does not apply to it as all parts of the text can be of similar importance.\n\n3.1 WikiHow Knowledge Base\nThe WikiHow knowledge base contains online articles describing a procedural task about various topics (from arts and entertainment to computers and electronics) with multiple methods or steps and new articles are added to it regularly. Each article consists of a title starting with \"How to\" and a short description of the article. There are two types of articles: the first type of articles describe single-method tasks in different steps, while the second type of articles represent multiple steps of different methods for a task. Each step description starts with a bold line summarizing that step and is followed by a more detailed explanation. A truncated example of a WikiHow article and how the data pairs are constructed is shown in Figure 2.\nWait for a full load of clothing before running a washing machine. Washing machines take up a lot of water and electricity, so running a cycle for a couple of articles of clothing is inefficient. Hold off on laundry until you can fill the machine.\nTurn off the water when you're not using it. Avoid letting the water run while you're brushing your teeth or shaving. Keep your hoses and faucets turned off as much as possible. When you need them, use them sparingly.\nTake quicker showers to conserve water. One easy way to conserve water is to cut down on your shower time.\nPractice cutting your showers down to 10 minutes, then 7, then 5. Challenge yourself to take a shorter shower every day.\n\n3.2 Data Extraction and Dataset Construction\nWe made use of the python Scrapy 2 library to write a crawler to get the data from the Wiki-How website. The articles classified into 20 different categories, cover a wide range of topics. Our crawler was able to obtain 142, 783 unique articles (some containing more than one method) at the time of crawling (new articles are added regularly). To prepare the data for the summarization task, each method (if any) described in the article is considered as a separate article. To generate the reference summaries, bold lines representing the summary of the steps are extracted and concatenated. The remaining parts of the steps (the detailed descriptions) are also concatenated to form the source article. After this step, 230, 843 articles and reference summaries are generated. There are some articles with only the bold lines i.e. there is no more explanation for the steps, so they cannot be used for the summarization task. To filter out these articles, we used a size threshold so that pairs with summaries longer than the article size will be removed. The final dataset is made of 204, 004 articles and their summaries. The statistics of the dataset are shown in Table 1. The dataset is released to the public 3.\n\n4 WikiHow Properties\nThe large scale of the WikiHow dataset by having more than 230, 000 pairs, and its average article and summary lengths makes it a better choice compared to DUC and Gigaword corpus.\nWe also define two metrics to represent the abstraction level of WikiHow by comparing it with CNN/Daily mail known as one of the most abstractive and common datasets in recent summarization papers (Nallapati et al., 2016 (Nallapati et al., , 2017;; See et al., 2017; Paulus et al., 2017).\n\n4.1 Level of Abstractedness\nAbstractedness of the dataset is measured by calculating the unique n-grams in the reference summary which are not in the article. The comparison is shown in Figure 3. Except for common unigrams, bi-grams and trigrams between the articles, and the summaries, no other common n-grams exist in the WikiHow pairs. The higher level of abstractedness creates new challenges for the summarization systems as they have to be more creative in generating more novel summaries.\n\n4.2 Compression Ratio\nWe define compression ratio to characterize the summarization. We first calculate the average length of sentences for both the articles and the summaries. The compression ratio is then defined as the ratio between the average length of sentences and the average length of summaries. The higher the compression ratio, the more difficult the summarization task, as it needs to capture  higher levels of abstraction and semantics. Table 3 shows the results for WikiHow and CNN/Daily Mail. The higher compression ratio of WikiHow shows the need for higher levels of abstraction.\n\n5 Experiments\nWe evaluate the performance of the WikiHow dataset using existing extractive and abstractive baselines. The systems used and the results generated for WikiHow and CNN/Daily mail are described in the following sections.\n\n5.1 Evaluated Systems\nTextRank Extractive system: An extractive summarization system (Mihalcea and Tarau, 2004; Barrios et al., 2016) using a graph-based ranking method to select sentences from the article and form the summary.\nSequence-to-sequence model with attention: A baseline system applied by Chopra et al. (2016); Nallapati et al. (2016) to abstractive summarization task to generate summaries using the predefined vocabulary. This baseline is not able to handle Out of Vocabulary words (OOVs).\nPointer-generator abstractive system: A pointer-generator mechanism (See et al., 2017) allowing the model to freely switch between copying a word from the input sequence or generating a word form the predefined vocabulary. Pointer-generator with coverage abstractive system: The pointer-generator baseline with added coverage loss (See et al., 2017) to reduce the repetition in the final generated summary. Lead-3 baseline: A baseline selecting the first three sentences of the article to form the summary. This baseline cannot be directly used for the Wiki-How dataset as the first 3 sentences of each article only describe a small portion of the whole article. We created the Lead-3 baseline by extracting the first sentence of each paragraph and concatenated them to create the summary.\n\n5.2 Results\nTo study the performance of the evaluated systems, we used the Pyrouge package 4 to report the F1 score for ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) and the METEOR (Banerjee and Lavie, 2005) 5 both based on the exact matches and on inclusion of stem, paraphrasing and synonyms (s/p/s) to evaluate the methods . Table 2 represents the results of multiple baselines on both the CNN/Daily Mail (the well-known, most common abstractive summarization dataset) and also the proposed WikiHow dataset. As it can be seen, the summarization systems perform a lot better on CNN/Daily mail compared to the WikiHow dataset with lead-3 outperforming other baselines due to the news inverted pyramid writing style described earlier. On the other hand, the poor performance of lead-3 on WikiHow shows the different writing styles in its articles. Moreover, all baselines perform about 10 ROUGE scores better on the CNN/Daily mail compared to the WikiHow. This difference suggests new features and aspects inherent in the new dataset which can be used to further improve the summarization systems.\n\n6 Conclusion\nWe present WikiHow, a new large-scale summarization dataset consisting of diverse articles form WikiHow knowledge base. The WikiHow features discussed in the paper can create new challenges to the summarization systems. We hope that the new dataset can attract researchers attention as a choice to evaluate their systems.\n\nFootnotes:\n1: http://www.wikihow.com/\n2: https://scrapy.org/\n3: https://github.com/mahnazkoupaee/ WikiHow-Dataset\n4: pypi.python.org/pypi/pyrouge/0.1.3\n5: www.cs.cmu.edu/ \u02dcalavie/METEOR\n\nReferences:\n\n- Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evalu- ation measures for machine translation and/or sum- marization, pages 65-72.- Federico Barrios, Federico L\u00f3pez, Luis Argerich, and Rosa Wachenchauzer. 2016. Variations of the simi- larity function of textrank for automated summariza- tion. arXiv preprint arXiv:1602.03606.\n\n- Sumit Chopra, Michael Auli, Alexander M Rush, and SEAS Harvard. 2016. Abstractive sentence summa- rization with attentive recurrent neural networks. In HLT-NAACL, pages 93-98.\n\n- Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summariza- tion with compression and anaphoricity constraints. arXiv preprint arXiv:1603.08887.\n\n- Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Pa- pers), volume 1, pages 708-719.\n\n- Donna Harman and Paul Over. 2004. The effects of hu- man variation in duc summarization evaluation. Text Summarization Branches Out.\n\n- Kai Hong and Ani Nenkova. 2014. Improving the estimation of word importance for news multi- document summarization. In Proceedings of the 14th Conference of the European Chapter of the As- sociation for Computational Linguistics, pages 712- 721.\n\n- Chin-Yew Lin. 2004. Rouge: A package for auto- matic evaluation of summaries. In Text summariza- tion branches out: Proceedings of the ACL-04 work- shop, volume 8. Barcelona, Spain.\n\n- Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring- ing order into text. In Proceedings of the 2004 con- ference on empirical methods in natural language processing.\n\n- Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based se- quence model for extractive summarization of docu- ments. AAAI.\n\n- Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C \u00b8a glar Gulc \u00b8ehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence rnns and beyond. CoNLL 2016, page 280.\n\n- Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Pro- ceedings of the Joint Workshop on Automatic Knowl- edge Base Construction and Web-scale Knowledge Extraction, pages 95-100. Association for Compu- tational Linguistics.\n\n- Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive sum- marization. arXiv preprint arXiv:1705.04304.\n\n- Horst Po\u00a8ttker. 2003. News and its communicative quality: The inverted pyramidwhen and why did it appear? Journalism Studies, 4(4):501-511.\n\n- Alexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685.\n\n- Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.\n\n- Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer- generator networks. ACL.\n\n", "annotations": {"Abstract": [{"begin": 51, "end": 870, "idx": 0}], "Head": [{"begin": 873, "end": 887, "n": "1", "idx": 0}, {"begin": 3217, "end": 3236, "n": "2", "idx": 1}, {"begin": 3556, "end": 3574, "idx": 2}, {"begin": 3653, "end": 3657, "idx": 3}, {"begin": 5704, "end": 5721, "n": "3", "idx": 4}, {"begin": 6693, "end": 6719, "n": "3.1", "idx": 5}, {"begin": 8166, "end": 8210, "n": "3.2", "idx": 6}, {"begin": 9430, "end": 9450, "n": "4", "idx": 7}, {"begin": 9922, "end": 9949, "n": "4.1", "idx": 8}, {"begin": 10419, "end": 10440, "n": "4.2", "idx": 9}, {"begin": 11017, "end": 11030, "n": "5", "idx": 10}, {"begin": 11251, "end": 11272, "n": "5.1", "idx": 11}, {"begin": 12545, "end": 12556, "n": "5.2", "idx": 12}, {"begin": 13639, "end": 13651, "n": "6", "idx": 13}], "ReferenceToBib": [{"begin": 1171, "end": 1190, "target": "#b14", "idx": 0}, {"begin": 1191, "end": 1211, "target": "#b2", "idx": 1}, {"begin": 1456, "end": 1479, "target": "#b5", "idx": 2}, {"begin": 1490, "end": 1512, "target": "#b11", "idx": 3}, {"begin": 1529, "end": 1545, "target": "#b15", "idx": 4}, {"begin": 1565, "end": 1589, "target": "#b10", "idx": 5}, {"begin": 3704, "end": 3726, "target": "#b5", "idx": 6}, {"begin": 4078, "end": 4097, "target": "#b14", "idx": 7}, {"begin": 4098, "end": 4121, "target": "#b9", "idx": 8}, {"begin": 4204, "end": 4226, "target": "#b11", "idx": 9}, {"begin": 4330, "end": 4349, "target": "#b14", "idx": 10}, {"begin": 4350, "end": 4370, "target": "#b2", "idx": 11}, {"begin": 4606, "end": 4622, "target": "#b15", "idx": 12}, {"begin": 4753, "end": 4777, "target": "#b6", "idx": 13}, {"begin": 4778, "end": 4799, "target": "#b3", "idx": 14}, {"begin": 4801, "end": 4821, "target": "#b12", "idx": 15}, {"begin": 4956, "end": 4980, "target": "#b10", "idx": 16}, {"begin": 4981, "end": 4998, "target": "#b16", "idx": 17}, {"begin": 4999, "end": 5022, "target": "#b9", "idx": 18}, {"begin": 5286, "end": 5309, "target": "#b9", "idx": 19}, {"begin": 5398, "end": 5415, "target": "#b16", "idx": 20}, {"begin": 5438, "end": 5459, "target": "#b4", "idx": 21}, {"begin": 5920, "end": 5936, "target": "#b13", "idx": 22}, {"begin": 9829, "end": 9852, "target": "#b10", "idx": 23}, {"begin": 9853, "end": 9880, "target": "#b9", "idx": 24}, {"begin": 9881, "end": 9898, "target": "#b16", "idx": 25}, {"begin": 9899, "end": 9919, "target": "#b12", "idx": 26}, {"begin": 11336, "end": 11362, "target": "#b8", "idx": 27}, {"begin": 11363, "end": 11383, "target": "#b1", "idx": 28}, {"begin": 11551, "end": 11571, "target": "#b2", "idx": 29}, {"begin": 11573, "end": 11596, "target": "#b10", "idx": 30}, {"begin": 11822, "end": 11840, "target": "#b16", "idx": 31}, {"begin": 12085, "end": 12103, "target": "#b16", "idx": 32}, {"begin": 12694, "end": 12705, "target": "#b7", "idx": 33}], "ReferenceToFootnote": [{"begin": 2180, "end": 2181, "target": "#foot_0", "idx": 0}, {"begin": 8244, "end": 8245, "target": "#foot_1", "idx": 1}, {"begin": 9426, "end": 9427, "target": "#foot_2", "idx": 2}, {"begin": 12636, "end": 12637, "target": "#foot_3", "idx": 3}, {"begin": 12748, "end": 12749, "target": "#foot_4", "idx": 4}], "SectionFootnote": [{"begin": 13975, "end": 14160, "idx": 0}], "ReferenceString": [{"begin": 14177, "end": 14456, "id": "b0", "idx": 0}, {"begin": 14458, "end": 14652, "id": "b1", "idx": 1}, {"begin": 14656, "end": 14831, "id": "b2", "idx": 2}, {"begin": 14835, "end": 15021, "id": "b3", "idx": 3}, {"begin": 15025, "end": 15351, "id": "b4", "idx": 4}, {"begin": 15355, "end": 15487, "id": "b5", "idx": 5}, {"begin": 15491, "end": 15736, "id": "b6", "idx": 6}, {"begin": 15740, "end": 15921, "id": "b7", "idx": 7}, {"begin": 15925, "end": 16091, "id": "b8", "idx": 8}, {"begin": 16095, "end": 16261, "id": "b9", "idx": 9}, {"begin": 16265, "end": 16456, "id": "b10", "idx": 10}, {"begin": 16460, "end": 16721, "id": "b11", "idx": 11}, {"begin": 16725, "end": 16870, "id": "b12", "idx": 12}, {"begin": 16874, "end": 17013, "id": "b13", "idx": 13}, {"begin": 17017, "end": 17174, "id": "b14", "idx": 14}, {"begin": 17178, "end": 17291, "id": "b15", "idx": 15}, {"begin": 17295, "end": 17424, "id": "b16", "idx": 16}], "ReferenceToTable": [{"begin": 9385, "end": 9386, "target": "#tab_0", "idx": 0}, {"begin": 10875, "end": 10876, "target": "#tab_2", "idx": 1}, {"begin": 12874, "end": 12875, "target": "#tab_1", "idx": 2}], "Footnote": [{"begin": 13986, "end": 14012, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 14013, "end": 14035, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 14036, "end": 14088, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 14089, "end": 14126, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 14127, "end": 14160, "id": "foot_4", "n": "5", "idx": 4}], "Paragraph": [{"begin": 61, "end": 870, "idx": 0}, {"begin": 888, "end": 1400, "idx": 1}, {"begin": 1401, "end": 2052, "idx": 2}, {"begin": 2053, "end": 2854, "idx": 3}, {"begin": 2855, "end": 2978, "idx": 4}, {"begin": 2979, "end": 3091, "idx": 5}, {"begin": 3092, "end": 3215, "idx": 6}, {"begin": 3237, "end": 3372, "idx": 7}, {"begin": 3373, "end": 3461, "idx": 8}, {"begin": 3462, "end": 3554, "idx": 9}, {"begin": 3575, "end": 3651, "idx": 10}, {"begin": 3658, "end": 5702, "idx": 11}, {"begin": 5722, "end": 6691, "idx": 12}, {"begin": 6720, "end": 7470, "idx": 13}, {"begin": 7471, "end": 7718, "idx": 14}, {"begin": 7719, "end": 7936, "idx": 15}, {"begin": 7937, "end": 8043, "idx": 16}, {"begin": 8044, "end": 8164, "idx": 17}, {"begin": 8211, "end": 9428, "idx": 18}, {"begin": 9451, "end": 9631, "idx": 19}, {"begin": 9632, "end": 9920, "idx": 20}, {"begin": 9950, "end": 10417, "idx": 21}, {"begin": 10441, "end": 11015, "idx": 22}, {"begin": 11031, "end": 11249, "idx": 23}, {"begin": 11273, "end": 11478, "idx": 24}, {"begin": 11479, "end": 11753, "idx": 25}, {"begin": 11754, "end": 12543, "idx": 26}, {"begin": 12557, "end": 13637, "idx": 27}, {"begin": 13652, "end": 13973, "idx": 28}], "SectionHeader": [{"begin": 0, "end": 870, "idx": 0}], "SectionReference": [{"begin": 14162, "end": 17426, "idx": 0}], "Sentence": [{"begin": 61, "end": 160, "idx": 0}, {"begin": 161, "end": 318, "idx": 1}, {"begin": 319, "end": 463, "idx": 2}, {"begin": 464, "end": 647, "idx": 3}, {"begin": 648, "end": 735, "idx": 4}, {"begin": 736, "end": 870, "idx": 5}, {"begin": 888, "end": 1056, "idx": 6}, {"begin": 1057, "end": 1212, "idx": 7}, {"begin": 1213, "end": 1298, "idx": 8}, {"begin": 1299, "end": 1400, "idx": 9}, {"begin": 1401, "end": 1615, "idx": 10}, {"begin": 1616, "end": 1740, "idx": 11}, {"begin": 1741, "end": 2052, "idx": 12}, {"begin": 2053, "end": 2197, "idx": 13}, {"begin": 2198, "end": 2318, "idx": 14}, {"begin": 2319, "end": 2421, "idx": 15}, {"begin": 2422, "end": 2599, "idx": 16}, {"begin": 2600, "end": 2668, "idx": 17}, {"begin": 2669, "end": 2809, "idx": 18}, {"begin": 2810, "end": 2854, "idx": 19}, {"begin": 2855, "end": 2978, "idx": 20}, {"begin": 2979, "end": 3091, "idx": 21}, {"begin": 3092, "end": 3215, "idx": 22}, {"begin": 3237, "end": 3307, "idx": 23}, {"begin": 3308, "end": 3372, "idx": 24}, {"begin": 3373, "end": 3438, "idx": 25}, {"begin": 3439, "end": 3445, "idx": 26}, {"begin": 3446, "end": 3451, "idx": 27}, {"begin": 3452, "end": 3461, "idx": 28}, {"begin": 3462, "end": 3554, "idx": 29}, {"begin": 3575, "end": 3629, "idx": 30}, {"begin": 3630, "end": 3651, "idx": 31}, {"begin": 3658, "end": 3794, "idx": 32}, {"begin": 3795, "end": 3946, "idx": 33}, {"begin": 3947, "end": 4122, "idx": 34}, {"begin": 4123, "end": 4227, "idx": 35}, {"begin": 4228, "end": 4304, "idx": 36}, {"begin": 4305, "end": 4556, "idx": 37}, {"begin": 4557, "end": 4689, "idx": 38}, {"begin": 4690, "end": 4883, "idx": 39}, {"begin": 4884, "end": 5135, "idx": 40}, {"begin": 5136, "end": 5217, "idx": 41}, {"begin": 5218, "end": 5284, "idx": 42}, {"begin": 5285, "end": 5533, "idx": 43}, {"begin": 5534, "end": 5702, "idx": 44}, {"begin": 5722, "end": 5784, "idx": 45}, {"begin": 5785, "end": 5861, "idx": 46}, {"begin": 5862, "end": 6178, "idx": 47}, {"begin": 6179, "end": 6373, "idx": 48}, {"begin": 6374, "end": 6449, "idx": 49}, {"begin": 6450, "end": 6583, "idx": 50}, {"begin": 6584, "end": 6691, "idx": 51}, {"begin": 6720, "end": 6955, "idx": 52}, {"begin": 6956, "end": 7051, "idx": 53}, {"begin": 7052, "end": 7252, "idx": 54}, {"begin": 7253, "end": 7368, "idx": 55}, {"begin": 7369, "end": 7470, "idx": 56}, {"begin": 7471, "end": 7537, "idx": 57}, {"begin": 7538, "end": 7666, "idx": 58}, {"begin": 7667, "end": 7718, "idx": 59}, {"begin": 7719, "end": 7763, "idx": 60}, {"begin": 7764, "end": 7836, "idx": 61}, {"begin": 7837, "end": 7896, "idx": 62}, {"begin": 7897, "end": 7936, "idx": 63}, {"begin": 7937, "end": 7976, "idx": 64}, {"begin": 7977, "end": 8043, "idx": 65}, {"begin": 8044, "end": 8164, "idx": 66}, {"begin": 8211, "end": 8315, "idx": 67}, {"begin": 8316, "end": 8399, "idx": 68}, {"begin": 8400, "end": 8554, "idx": 69}, {"begin": 8555, "end": 8685, "idx": 70}, {"begin": 8686, "end": 8803, "idx": 71}, {"begin": 8804, "end": 8914, "idx": 72}, {"begin": 8915, "end": 8988, "idx": 73}, {"begin": 8989, "end": 9137, "idx": 74}, {"begin": 9138, "end": 9267, "idx": 75}, {"begin": 9268, "end": 9335, "idx": 76}, {"begin": 9336, "end": 9387, "idx": 77}, {"begin": 9388, "end": 9428, "idx": 78}, {"begin": 9451, "end": 9631, "idx": 79}, {"begin": 9632, "end": 9920, "idx": 80}, {"begin": 9950, "end": 10080, "idx": 81}, {"begin": 10081, "end": 10260, "idx": 82}, {"begin": 10261, "end": 10417, "idx": 83}, {"begin": 10441, "end": 10503, "idx": 84}, {"begin": 10504, "end": 10595, "idx": 85}, {"begin": 10596, "end": 10723, "idx": 86}, {"begin": 10724, "end": 10868, "idx": 87}, {"begin": 10869, "end": 10926, "idx": 88}, {"begin": 10927, "end": 11015, "idx": 89}, {"begin": 11031, "end": 11134, "idx": 90}, {"begin": 11135, "end": 11249, "idx": 91}, {"begin": 11273, "end": 11478, "idx": 92}, {"begin": 11479, "end": 11685, "idx": 93}, {"begin": 11686, "end": 11753, "idx": 94}, {"begin": 11754, "end": 11976, "idx": 95}, {"begin": 11977, "end": 12160, "idx": 96}, {"begin": 12161, "end": 12260, "idx": 97}, {"begin": 12261, "end": 12416, "idx": 98}, {"begin": 12417, "end": 12543, "idx": 99}, {"begin": 12557, "end": 12867, "idx": 100}, {"begin": 12868, "end": 13050, "idx": 101}, {"begin": 13051, "end": 13274, "idx": 102}, {"begin": 13275, "end": 13387, "idx": 103}, {"begin": 13388, "end": 13495, "idx": 104}, {"begin": 13496, "end": 13637, "idx": 105}, {"begin": 13652, "end": 13771, "idx": 106}, {"begin": 13772, "end": 13871, "idx": 107}, {"begin": 13872, "end": 13973, "idx": 108}], "ReferenceToFigure": [{"begin": 5957, "end": 5958, "target": "#fig_0", "idx": 0}, {"begin": 7468, "end": 7469, "target": "#fig_2", "idx": 1}, {"begin": 10115, "end": 10116, "target": "#fig_3", "idx": 2}], "Div": [{"begin": 61, "end": 870, "idx": 0}, {"begin": 873, "end": 3215, "idx": 1}, {"begin": 3217, "end": 3554, "idx": 2}, {"begin": 3556, "end": 3651, "idx": 3}, {"begin": 3653, "end": 5702, "idx": 4}, {"begin": 5704, "end": 6691, "idx": 5}, {"begin": 6693, "end": 8164, "idx": 6}, {"begin": 8166, "end": 9428, "idx": 7}, {"begin": 9430, "end": 9920, "idx": 8}, {"begin": 9922, "end": 10417, "idx": 9}, {"begin": 10419, "end": 11015, "idx": 10}, {"begin": 11017, "end": 11249, "idx": 11}, {"begin": 11251, "end": 12543, "idx": 12}, {"begin": 12545, "end": 13637, "idx": 13}, {"begin": 13639, "end": 13973, "idx": 14}], "SectionMain": [{"begin": 870, "end": 13973, "idx": 0}], "ScholarlyEntity": [{"label": "Task", "begin": 146, "end": 159, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.9971675276756287, 0.9958129525184631], "text": "summarization", "score": 0.9964902400970459, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 371, "end": 382, "seq_label": ["B-Task"], "seq_token": ["\u0120description"], "seq_scores": [0.5161160826683044], "text": "description", "score": 0.5161160826683044, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 490, "end": 497, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.8606823086738586, 0.8292875289916992], "text": "WikiHow", "score": 0.8449849188327789, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 791, "end": 798, "seq_label": ["I-Datasource", "I-Datasource", "B-Dataset", "I-Dataset"], "seq_token": ["\u0120knowledge", "\u0120base", "\u0120Wiki", "How"], "seq_scores": [0.39923611283302307, 0.43826988339424133, 0.894073486328125, 0.9213861227035522], "text": "WikiHow", "score": 0.6632414013147354, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 61, "end": 88, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["Sequ", "ence", "-", "to", "-", "sequence", "\u0120models"], "seq_scores": [0.9983890056610107, 0.9994899034500122, 0.9998233914375305, 0.999839186668396, 0.999855637550354, 0.9998481273651123, 0.9997748732566833], "text": "Sequence-to-sequence models", "score": 0.9995743036270142, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 183, "end": 216, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120high", "-", "quality", "\u0120datasets"], "seq_scores": [0.9976950287818909, 0.9984630346298218, 0.9991484880447388, 0.9974045157432556, 0.9996415376663208, 0.999724805355072, 0.9995961785316467], "text": "large-scale high-quality datasets", "score": 0.9988105126789638, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 246, "end": 264, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120available", "\u0120ones"], "seq_scores": [0.9007385969161987, 0.8251490592956543, 0.876598060131073], "text": "the available ones", "score": 0.8674952387809753, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 276, "end": 289, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.9915322661399841, 0.9908996820449829], "text": "news articles", "score": 0.9912159740924835, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 329, "end": 360, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120abstract", "ive", "\u0120human", "-", "style", "\u0120systems"], "seq_scores": [0.9798274040222168, 0.9816291332244873, 0.9881860017776489, 0.9921560287475586, 0.9953277111053467, 0.9924141764640808], "text": "abstractive human-style systems", "score": 0.9882567425568899, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 424, "end": 428, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.7129665613174438], "text": "data", "score": 0.7129665613174438, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 499, "end": 508, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9987329840660095, 0.9985572695732117], "text": "a dataset", "score": 0.9986451268196106, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 522, "end": 555, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120230", ",", "000", "\u0120article", "\u0120and", "\u0120summary", "\u0120pairs"], "seq_scores": [0.6172751188278198, 0.9986830353736877, 0.9982485771179199, 0.9980664849281311, 0.9988670349121094, 0.9995494484901428, 0.9986490607261658], "text": "230,000 article and summary pairs", "score": 0.9441912514822823, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 648, "end": 660, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120articles"], "seq_scores": [0.9814639687538147, 0.9664172530174255], "text": "The articles", "score": 0.9739406108856201, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 767, "end": 787, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120methods"], "seq_scores": [0.7922092080116272, 0.7256647348403931, 0.7381271719932556], "text": "the existing methods", "score": 0.7520003716150919, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 833, "end": 847, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120some", "\u0120bas", "elines"], "seq_scores": [0.593205451965332, 0.6190916299819946, 0.8049282431602478], "text": "some baselines", "score": 0.6724084417025248, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 888, "end": 901, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["Sum", "mar", "ization"], "seq_scores": [0.99498450756073, 0.9913334250450134, 0.9930332899093628], "text": "Summarization", "score": 0.993117074171702, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1157, "end": 1170, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "B-Task", "I-Task"], "seq_token": ["\u0120neural", "\u0120networks", "\u0120summar", "ization"], "seq_scores": [0.4441811144351959, 0.46682026982307434, 0.9970863461494446, 0.9968394041061401], "text": "summarization", "score": 0.7262317836284637, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1172, "end": 1189, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rush", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9987121820449829, 0.9965123534202576, 0.9966501593589783, 0.9961428046226501, 0.9963893890380859], "text": "Rush et al., 2015", "score": 0.9968813776969909, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1191, "end": 1210, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Chop", "ra", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9989374279975891, 0.9970389604568481, 0.9970270991325378, 0.9972265362739563, 0.9964994192123413, 0.9967447519302368], "text": "Chopra et al., 2016", "score": 0.9972456991672516, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1057, "end": 1093, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Sequence", "-", "to", "-", "sequence", "\u0120neural", "\u0120networks"], "seq_scores": [0.9951297044754028, 0.9987046718597412, 0.999170184135437, 0.9991679191589355, 0.9988719820976257, 0.9974365830421448, 0.9985058307647705], "text": "Sequence-to-sequence neural networks", "score": 0.998140982219151, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1239, "end": 1259, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120datasets"], "seq_scores": [0.9978766441345215, 0.9993175268173218, 0.999504804611206, 0.9992017149925232], "text": "large-scale datasets", "score": 0.9989751726388931, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1285, "end": 1297, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120models"], "seq_scores": [0.9899550676345825, 0.9964774250984192], "text": "these models", "score": 0.9932162463665009, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1323, "end": 1335, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120articles"], "seq_scores": [0.8094792366027832, 0.6164938807487488], "text": "the articles", "score": 0.712986558675766, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1421, "end": 1434, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.9132075905799866, 0.7898831963539124], "text": "summarization", "score": 0.8515453934669495, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1452, "end": 1455, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120D", "UC"], "seq_scores": [0.9992032647132874, 0.9990850687026978], "text": "DUC", "score": 0.9991441667079926, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1457, "end": 1478, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Har", "man", "\u0120and", "\u0120Over", ",", "\u01202004"], "seq_scores": [0.9986386895179749, 0.995564877986908, 0.9974188804626465, 0.9957579970359802, 0.9970207810401917, 0.9960744380950928], "text": "Harman and Over, 2004", "score": 0.9967459440231323, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1481, "end": 1489, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Gig", "aw", "ord"], "seq_scores": [0.9992165565490723, 0.9994606375694275, 0.999394416809082], "text": "Gigaword", "score": 0.9993572036425272, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1491, "end": 1511, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Nap", "oles", "\u0120et", "\u0120al", ".,", "\u01202012"], "seq_scores": [0.9993041753768921, 0.9976398944854736, 0.9975457787513733, 0.9979343414306641, 0.9975852966308594, 0.9968690276145935], "text": "Napoles et al., 2012", "score": 0.997813085714976, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1514, "end": 1528, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120New", "\u0120York", "\u0120Times"], "seq_scores": [0.9987906813621521, 0.9985425472259521, 0.9989171028137207], "text": "New York Times", "score": 0.998750110467275, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1530, "end": 1544, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sand", "haus", ",", "\u01202008"], "seq_scores": [0.9992002844810486, 0.9983661770820618, 0.9983586668968201, 0.9978371262550354], "text": "Sandhaus, 2008", "score": 0.9984405636787415, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1550, "end": 1564, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail"], "seq_scores": [0.9984418749809265, 0.999197781085968, 0.9991183876991272, 0.9989743232727051], "text": "CNN/Daily Mail", "score": 0.9989330917596817, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1566, "end": 1588, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.999444305896759, 0.9992918968200684, 0.9993757605552673, 0.9992812275886536, 0.9989196062088013, 0.998980700969696, 0.9987834095954895, 0.9987150430679321], "text": "Nallapati et al., 2016", "score": 0.9990989938378334, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1807, "end": 1810, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["D", "UC"], "seq_scores": [0.998965859413147, 0.9987097978591919], "text": "DUC", "score": 0.9988378286361694, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1815, "end": 1820, "seq_label": ["B-Method"], "seq_token": ["\u0120train"], "seq_scores": [0.5058083534240723], "text": "train", "score": 0.5058083534240723, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1899, "end": 1907, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["G", "ig", "aw", "ord"], "seq_scores": [0.9987753033638, 0.9987737536430359, 0.9993352293968201, 0.9992582201957703], "text": "Gigaword", "score": 0.9990356266498566, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1974, "end": 1988, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["New", "\u0120York", "\u0120Times"], "seq_scores": [0.9976831674575806, 0.9977017045021057, 0.9980685114860535], "text": "New York Times", "score": 0.9978177944819132, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2036, "end": 2050, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["CNN", "/", "Daily", "\u0120mail"], "seq_scores": [0.9979165196418762, 0.9989883303642273, 0.9987906813621521, 0.9984292387962341], "text": "CNN/Daily mail", "score": 0.9985311925411224, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1408, "end": 1443, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120existing", "\u0120summar", "ization", "\u0120datasets"], "seq_scores": [0.7276515960693359, 0.9914401769638062, 0.9981880784034729, 0.9985779523849487, 0.9991527795791626], "text": "all existing summarization datasets", "score": 0.9430021166801452, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1601, "end": 1614, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.9966306090354919, 0.9986522793769836], "text": "news articles", "score": 0.9976414442062378, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1616, "end": 1633, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120news", "\u0120articles"], "seq_scores": [0.9790815114974976, 0.9912914037704468, 0.9963729381561279], "text": "The news articles", "score": 0.9889152844746908, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1679, "end": 1690, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120systems"], "seq_scores": [0.9826489686965942, 0.9920431971549988], "text": "the systems", "score": 0.9873460829257965, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1707, "end": 1711, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120news"], "seq_scores": [0.9873513579368591], "text": "news", "score": 0.9873513579368591, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1760, "end": 1781, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120datasets"], "seq_scores": [0.998828113079071, 0.9993010759353638, 0.9994747042655945], "text": "the existing datasets", "score": 0.9992012977600098, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1821, "end": 1849, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120sequence", "-", "to", "-", "sequence", "\u0120model"], "seq_scores": [0.9962953925132751, 0.9974445104598999, 0.9997794032096863, 0.9998125433921814, 0.9997795224189758, 0.9997770190238953, 0.9995189905166626], "text": "a sequence-to-sequence model", "score": 0.9989153402192252, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1851, "end": 1864, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120summ", "aries"], "seq_scores": [0.9533413648605347, 0.9301832914352417, 0.874169647693634], "text": "the summaries", "score": 0.9192314346631368, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1888, "end": 1897, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120headlines"], "seq_scores": [0.649055540561676], "text": "headlines", "score": 0.649055540561676, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1937, "end": 1972, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120extract", "ive", "\u0120summar", "ization", "\u0120dataset"], "seq_scores": [0.9940096139907837, 0.9966843724250793, 0.9982460737228394, 0.9968357682228088, 0.9967862367630005, 0.9979935884475708], "text": "an extractive summarization dataset", "score": 0.996759275595347, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2146, "end": 2154, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "-", "How"], "seq_scores": [0.7368251085281372, 0.9312453866004944, 0.8360616564750671], "text": "Wiki-How", "score": 0.8347107172012329, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 2172, "end": 2196, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Method", "I-Method"], "seq_token": ["\u0120Wiki", "How", "\u01201", "\u0120knowledge", "\u0120base"], "seq_scores": [0.5469943284988403, 0.7082266211509705, 0.5428479313850403, 0.7141067385673523, 0.6703513264656067], "text": "WikiHow 1 knowledge base", "score": 0.636505389213562, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2425, "end": 2432, "seq_label": ["B-Method"], "seq_token": ["\u0120merging"], "seq_scores": [0.6598402261734009], "text": "merging", "score": 0.6598402261734009, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2729, "end": 2763, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120extract", "ive", "\u0120and", "\u0120abstract", "ive", "\u0120systems"], "seq_scores": [0.674555242061615, 0.9181215763092041, 0.9161208271980286, 0.9242276549339294, 0.948540210723877, 0.9126153588294983], "text": "extractive and abstractive systems", "score": 0.882363478342692, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 2767, "end": 2774, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.6457248330116272, 0.5084104537963867], "text": "WikiHow", "score": 0.577067643404007, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2079, "end": 2100, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120datasets"], "seq_scores": [0.9988262057304382, 0.9992470741271973, 0.9993359446525574], "text": "the existing datasets", "score": 0.9991364081700643, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2113, "end": 2138, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120large", "-", "scale", "\u0120dataset"], "seq_scores": [0.999054491519928, 0.9993075132369995, 0.9994433522224426, 0.9997164607048035, 0.9997686743736267, 0.9997357726097107], "text": "a new large-scale dataset", "score": 0.9995043774445852, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2210, "end": 2218, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120articles"], "seq_scores": [0.8433550596237183], "text": "articles", "score": 0.8433550596237183, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2295, "end": 2317, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120existing", "\u0120news", "\u0120datasets"], "seq_scores": [0.9966527819633484, 0.9811995029449463, 0.9994444251060486], "text": "existing news datasets", "score": 0.9924322366714478, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2516, "end": 2529, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120resulting"], "seq_scores": [0.8309118151664734, 0.5793612003326416], "text": "the resulting", "score": 0.7051365077495575, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2541, "end": 2552, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9979537725448608, 0.9991663694381714], "text": "the dataset", "score": 0.9985600709915161, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2572, "end": 2598, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120200", ",", "000", "\u0120long", "sequence", "\u0120pairs"], "seq_scores": [0.698219895362854, 0.9893065094947815, 0.9903448224067688, 0.9738508462905884, 0.9915763735771179, 0.9910476803779602], "text": "200,000 longsequence pairs", "score": 0.9390576879183451, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2653, "end": 2664, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120dataset"], "seq_scores": [0.9973360896110535, 0.9983981251716614], "text": "our dataset", "score": 0.9978671073913574, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2716, "end": 2763, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120extract", "ive", "\u0120and", "\u0120abstract", "ive", "\u0120systems"], "seq_scores": [0.937620222568512, 0.715533435344696, 0.9624699354171753, 0.9970730543136597, 0.9976182579994202, 0.9957115650177002, 0.9978673458099365, 0.9939690828323364], "text": "the existing extractive and abstractive systems", "score": 0.9497328624129295, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2945, "end": 2977, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120long", "-", "sequence", "\u0120text", "\u0120summar", "ization"], "seq_scores": [0.9915776252746582, 0.9982206225395203, 0.9980992674827576, 0.9973815083503723, 0.998042106628418, 0.9984472393989563], "text": "long-sequence text summarization", "score": 0.9969613949457804, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2870, "end": 2900, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "-", "scale", ",", "\u0120diverse", "\u0120dataset"], "seq_scores": [0.9990436434745789, 0.9996311664581299, 0.9997851252555847, 0.9998183846473694, 0.9996045231819153, 0.9997797608375549, 0.9998513460159302], "text": "a large-scale, diverse dataset", "score": 0.9996448499815804, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3072, "end": 3087, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120new", "\u0120dataset"], "seq_scores": [0.9970724582672119, 0.9948558807373047, 0.9991222023963928], "text": "the new dataset", "score": 0.9970168471336365, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3149, "end": 3156, "seq_label": ["B-Datasource", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.6458807587623596, 0.5174245238304138], "text": "WikiHow", "score": 0.5816526412963867, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3125, "end": 3145, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120systems"], "seq_scores": [0.8502607345581055, 0.7751625776290894, 0.8117178678512573], "text": "the existing systems", "score": 0.8123803933461508, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 3285, "end": 3306, "seq_label": ["B-Task", "I-Method", "I-Method"], "seq_token": ["\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.3399576246738434, 0.6301840543746948, 0.5722060203552246], "text": "summarization systems", "score": 0.5141158998012543, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3247, "end": 3263, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120datasets"], "seq_scores": [0.9989489912986755, 0.9995537400245667], "text": "several datasets", "score": 0.9992513656616211, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3281, "end": 3306, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.9980435371398926, 0.9982523322105408, 0.9997166991233826, 0.9994996786117554], "text": "the summarization systems", "score": 0.9988780617713928, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3346, "end": 3360, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9978602528572083, 0.9984513521194458], "text": "these datasets", "score": 0.998155802488327, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3662, "end": 3695, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Document", "\u0120Understanding", "\u0120Conference"], "seq_scores": [0.9937478303909302, 0.9956796765327454, 0.9930943250656128], "text": "Document Understanding Conference", "score": 0.9941739439964294, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3705, "end": 3726, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Har", "man", "\u0120and", "\u0120Over", ",", "\u01202004"], "seq_scores": [0.9988663196563721, 0.9980568289756775, 0.99886155128479, 0.9982224106788635, 0.998705267906189, 0.9982831478118896], "text": "Harman and Over, 2004", "score": 0.9984992543856303, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3951, "end": 3954, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120D", "UC"], "seq_scores": [0.9934978485107422, 0.9929250478744507], "text": "DUC", "score": 0.9932114481925964, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4079, "end": 4096, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rush", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9992164373397827, 0.9995012283325195, 0.9996020197868347, 0.9994520545005798, 0.9992325305938721], "text": "Rush et al., 2015", "score": 0.9994008541107178, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4098, "end": 4120, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9989901185035706, 0.9990221261978149, 0.9993637204170227, 0.9993379712104797, 0.999667763710022, 0.9997113347053528, 0.9996700286865234, 0.999541163444519], "text": "Nallapati et al., 2017", "score": 0.9994130283594131, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4123, "end": 4131, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Gig", "aw", "ord"], "seq_scores": [0.9892691969871521, 0.9908784031867981, 0.9890108704566956], "text": "Gigaword", "score": 0.9897194902102152, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4178, "end": 4191, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.9828505516052246, 0.9856500029563904], "text": "summarization", "score": 0.9842502772808075, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4195, "end": 4203, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Gig", "aw", "ord"], "seq_scores": [0.9827608466148376, 0.9865068197250366, 0.9807882308959961], "text": "Gigaword", "score": 0.9833519657452902, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4205, "end": 4225, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Nap", "oles", "\u0120et", "\u0120al", ".,", "\u01202012"], "seq_scores": [0.9994375109672546, 0.9992961883544922, 0.9994159936904907, 0.9995009899139404, 0.9994282126426697, 0.9991884827613831], "text": "Napoles et al., 2012", "score": 0.9993778963883718, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4331, "end": 4348, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rush", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9991074204444885, 0.9994363188743591, 0.9995579123497009, 0.9994099140167236, 0.9993292093276978], "text": "Rush et al., 2015", "score": 0.999368155002594, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4350, "end": 4369, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Chop", "ra", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9991734623908997, 0.9991706609725952, 0.9996116757392883, 0.9996992349624634, 0.9996036887168884, 0.9995974898338318], "text": "Chopra et al., 2016", "score": 0.9994760354359945, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4525, "end": 4549, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120short", "\u0120text", "\u0120summar", "ization"], "seq_scores": [0.9547209739685059, 0.9836642146110535, 0.9952599406242371, 0.9937475919723511], "text": "short text summarization", "score": 0.9818481802940369, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4557, "end": 4571, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120New", "\u0120York", "\u0120Times"], "seq_scores": [0.9946882724761963, 0.9717498421669006, 0.9868388772010803], "text": "New York Times", "score": 0.9844256639480591, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4577, "end": 4591, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120New", "\u0120York", "\u0120Times"], "seq_scores": [0.9900696873664856, 0.9296101927757263, 0.9649377465248108], "text": "New York Times", "score": 0.9615392088890076, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4593, "end": 4596, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["NY", "T"], "seq_scores": [0.9869188666343689, 0.976768970489502], "text": "NYT", "score": 0.9818439185619354, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4607, "end": 4621, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sand", "haus", ",", "\u01202008"], "seq_scores": [0.9991843104362488, 0.9992488026618958, 0.9993478655815125, 0.9991176724433899], "text": "Sandhaus, 2008", "score": 0.9992246627807617, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4734, "end": 4752, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120extract", "ive", "\u0120systems"], "seq_scores": [0.8839964270591736, 0.9655131697654724, 0.9539175629615784], "text": "extractive systems", "score": 0.9344757199287415, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4754, "end": 4776, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Hong", "\u0120and", "\u0120N", "en", "k", "ova", ",", "\u01202014"], "seq_scores": [0.9992715716362, 0.999239444732666, 0.9994164705276489, 0.9995142221450806, 0.9994667172431946, 0.9994706511497498, 0.9994028806686401, 0.9993579983711243], "text": "Hong and Nenkova, 2014", "score": 0.999392494559288, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4778, "end": 4798, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Dur", "rett", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9992496371269226, 0.9992209672927856, 0.9996143579483032, 0.9996817111968994, 0.9996447563171387, 0.9995662569999695], "text": "Durrett et al., 2016", "score": 0.9994962811470032, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4801, "end": 4821, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Paul", "us", "\u0120et", "\u0120al", ".", "\u0120(", "2017", ")"], "seq_scores": [0.9986407160758972, 0.999518632888794, 0.9988413453102112, 0.9985139966011047, 0.97315514087677, 0.9930533766746521, 0.9991771578788757, 0.9635648727416992], "text": "Paulus et al. (2017)", "score": 0.9905581548810005, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4879, "end": 4882, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Dataset"], "seq_token": ["\u0120abstract", "ive", "\u0120system", "\u0120NYT"], "seq_scores": [0.64284348487854, 0.9124945402145386, 0.9106125235557556, 0.9885032773017883], "text": "NYT", "score": 0.8636134564876556, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4884, "end": 4898, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail"], "seq_scores": [0.9762318730354309, 0.9886748194694519, 0.9407916069030762, 0.95191890001297], "text": "CNN/Daily Mail", "score": 0.9644042998552322, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4935, "end": 4948, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.7785636782646179, 0.6963440179824829], "text": "summarization", "score": 0.7374538481235504, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4957, "end": 4979, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.99943608045578, 0.9992228746414185, 0.9997016787528992, 0.9997039437294006, 0.999723494052887, 0.9997206330299377, 0.9996869564056396, 0.9995006322860718], "text": "Nallapati et al., 2016", "score": 0.9995870366692543, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4981, "end": 4997, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120See", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9982093572616577, 0.9996427297592163, 0.9997599720954895, 0.9997342228889465, 0.999658465385437], "text": "See et al., 2017", "score": 0.9994009494781494, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4999, "end": 5021, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9990712404251099, 0.9995115995407104, 0.9997270703315735, 0.9997326731681824, 0.9997435212135315, 0.9997734427452087, 0.9997479319572449, 0.9997264742851257], "text": "Nallapati et al., 2017", "score": 0.9996292442083359, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 5042, "end": 5060, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120CNN", "\u0120and", "\u0120Daily", "\u0120Mail"], "seq_scores": [0.6837841868400574, 0.8075878620147705, 0.5498430728912354, 0.9388091564178467], "text": "CNN and Daily Mail", "score": 0.7450060695409775, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5108, "end": 5134, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120question", "/", "ans", "w", "ering", "\u0120systems"], "seq_scores": [0.9209779500961304, 0.9719359278678894, 0.9769023656845093, 0.967969536781311, 0.9672150611877441, 0.9528360962867737], "text": "question/answering systems", "score": 0.9596394896507263, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5286, "end": 5301, "seq_label": ["I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["processing", "\u0120N", "all", "ap", "ati", "\u0120et", "\u0120al"], "seq_scores": [0.6602122783660889, 0.9880016446113586, 0.9988499879837036, 0.9993926286697388, 0.9991289973258972, 0.9990605711936951, 0.9987561702728271], "text": "Nallapati et al", "score": 0.9490574683461871, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5319, "end": 5343, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120(", "2017", ")", "\u0120the", "\u0120entity", "\u0120anonym", "ization"], "seq_scores": [0.9963900446891785, 0.9992191791534424, 0.9706412553787231, 0.6492428183555603, 0.505999743938446, 0.9761981964111328, 0.9594054818153381], "text": "the entity anonymization", "score": 0.8652995313916888, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5398, "end": 5415, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120See", "\u0120et", "\u0120al", ".", "\u0120(", "2017", ")"], "seq_scores": [0.9962623715400696, 0.9966346621513367, 0.9973371624946594, 0.9889101982116699, 0.9906066656112671, 0.9985733032226562, 0.81232088804245], "text": "See et al. (2017)", "score": 0.9686636073248727, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 5416, "end": 5424, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120NEW", "SR", "O", "OM"], "seq_scores": [0.9967727065086365, 0.9974061846733093, 0.9980720281600952, 0.9978929162025452], "text": "NEWSROOM", "score": 0.9975359588861465, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5439, "end": 5458, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gr", "usky", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9992621541023254, 0.9996397495269775, 0.9997549653053284, 0.9997932314872742, 0.9997170567512512, 0.9997114539146423], "text": "Grusky et al., 2018", "score": 0.9996464351812998, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5514, "end": 5532, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120text", "\u0120summar", "ization"], "seq_scores": [0.9532239437103271, 0.9917159676551819, 0.9868350625038147], "text": "text summarization", "score": 0.9772583246231079, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5577, "end": 5614, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120abstract", "ive", "\u0120and", "\u0120extract", "ive", "\u0120strategies"], "seq_scores": [0.9064860343933105, 0.9610884785652161, 0.9366304278373718, 0.9592226147651672, 0.9676585793495178, 0.8909273147583008], "text": "abstractive and extractive strategies", "score": 0.9370022416114807, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3737, "end": 3754, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120500", "\u0120news", "\u0120articles"], "seq_scores": [0.9970391988754272, 0.9959110021591187, 0.9974583983421326], "text": "500 news articles", "score": 0.9968028664588928, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3759, "end": 3764, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120their"], "seq_scores": [0.739448070526123], "text": "their", "score": 0.739448070526123, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3922, "end": 3945, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120existing", "\u0120datasets"], "seq_scores": [0.9936808347702026, 0.9869387149810791, 0.9991027116775513], "text": "other existing datasets", "score": 0.993240753809611, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3991, "end": 3997, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9540557861328125], "text": "models", "score": 0.9540557861328125, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4063, "end": 4077, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120datasets"], "seq_scores": [0.993802011013031, 0.9951174259185791], "text": "other datasets", "score": 0.994459718465805, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4133, "end": 4151, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Another", "\u0120collection"], "seq_scores": [0.7958176732063293, 0.9064789414405823], "text": "Another collection", "score": 0.8511483073234558, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4228, "end": 4249, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles", "\u0120The", "\u0120original", "\u0120articles"], "seq_scores": [0.8580682873725891, 0.9991362690925598, 0.9042770862579346, 0.8834503293037415, 0.8733890056610107], "text": "The original articles", "score": 0.9036641955375672, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4253, "end": 4264, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9766828417778015, 0.9626500010490417], "text": "the dataset", "score": 0.9696664214134216, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4376, "end": 4387, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120subset", "\u0120of"], "seq_scores": [0.9955433011054993, 0.9921199083328247, 0.5940366983413696], "text": "a subset of", "score": 0.8605666359265646, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4388, "end": 4400, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.7882084250450134, 0.9988011121749878], "text": "this dataset", "score": 0.8935047686100006, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4417, "end": 4435, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120pairs", "\u0120of", "\u0120summ", "aries"], "seq_scores": [0.8842068910598755, 0.6212172508239746, 0.5574182868003845, 0.7583504319190979], "text": "pairs of summaries", "score": 0.7052982151508331, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4500, "end": 4511, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9890298247337341, 0.9844628572463989], "text": "the dataset", "score": 0.9867463409900665, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4626, "end": 4656, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120collection", "\u0120of", "\u0120articles"], "seq_scores": [0.9984650611877441, 0.9994999170303345, 0.9987730383872986, 0.9991807341575623, 0.9995788931846619], "text": "a large collection of articles", "score": 0.9990995287895202, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4696, "end": 4708, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9809967279434204, 0.9745420813560486], "text": "this dataset", "score": 0.9777694046497345, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4734, "end": 4752, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120extract", "ive", "\u0120systems"], "seq_scores": [0.9941931366920471, 0.9942739605903625, 0.9857397675514221], "text": "extractive systems", "score": 0.991402288277944, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4848, "end": 4872, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120their", "\u0120abstract", "ive", "\u0120system"], "seq_scores": [0.9560084342956543, 0.9766518473625183, 0.9970165491104126, 0.9866107702255249], "text": "their abstractive system", "score": 0.9790719002485275, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4900, "end": 4912, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120This", "\u0120dataset"], "seq_scores": [0.9895747303962708, 0.9890790581703186], "text": "This dataset", "score": 0.9893268942832947, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5035, "end": 5074, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120online", "\u0120CNN", "\u0120and", "\u0120Daily", "\u0120Mail", "\u0120news", "\u0120articles"], "seq_scores": [0.9850003719329834, 0.9460743069648743, 0.998079776763916, 0.9983240962028503, 0.9987421631813049, 0.9946461319923401, 0.9982271790504456], "text": "online CNN and Daily Mail news articles", "score": 0.9884420037269592, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5108, "end": 5134, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120question", "/", "ans", "w", "ering", "\u0120systems"], "seq_scores": [0.9743385910987854, 0.9975927472114563, 0.9970683455467224, 0.995175838470459, 0.9967527985572815, 0.9879642724990845], "text": "question/answering systems", "score": 0.9914820988972982, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5218, "end": 5230, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Two", "\u0120versions"], "seq_scores": [0.8795379996299744, 0.756607711315155], "text": "Two versions", "score": 0.8180728554725647, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5234, "end": 5246, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9900814890861511, 0.9947868585586548], "text": "this dataset", "score": 0.992434173822403, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5354, "end": 5376, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120anonym", "ized", "\u0120version"], "seq_scores": [0.8365583419799805, 0.9292691349983215, 0.8810944557189941, 0.6006677746772766], "text": "the anonymized version", "score": 0.8118974268436432, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5380, "end": 5391, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9859349727630615, 0.9916151762008667], "text": "the dataset", "score": 0.9887750744819641, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5426, "end": 5437, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120This", "\u0120corpus"], "seq_scores": [0.8914288282394409, 0.5050379633903503], "text": "This corpus", "score": 0.6982333958148956, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5463, "end": 5498, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120most", "\u0120recent", "\u0120large", "-", "scale", "\u0120dataset"], "seq_scores": [0.992780864238739, 0.9907287359237671, 0.9987342953681946, 0.9987133741378784, 0.9995535016059875, 0.9995830655097961, 0.9995088577270508], "text": "the most recent large-scale dataset", "score": 0.9970860992159162, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5549, "end": 5566, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120diverse", "\u0120summ", "aries"], "seq_scores": [0.8943462371826172, 0.8383285403251648, 0.8521597981452942], "text": "diverse summaries", "score": 0.861611525217692, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5625, "end": 5645, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120another", "\u0120news", "\u0120dataset"], "seq_scores": [0.8473349213600159, 0.557303249835968, 0.9977692365646362], "text": "another news dataset", "score": 0.80080246925354, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5893, "end": 5919, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120In", "verted", "\u0120Pyramid", "\u0120style"], "seq_scores": [0.967573881149292, 0.936575710773468, 0.9972808361053467, 0.9966044425964355, 0.9412246346473694], "text": "the Inverted Pyramid style", "score": 0.9678519010543823, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5921, "end": 5935, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Po", "\u00c2\u00a8", "tt", "ker", ",", "\u01202003"], "seq_scores": [0.997356653213501, 0.9971879124641418, 0.9983927607536316, 0.9989736080169678, 0.9985883831977844, 0.9984962940216064], "text": "Po\u00a8ttker, 2003", "score": 0.9981659352779388, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6338, "end": 6350, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120existing"], "seq_scores": [0.5260064601898193, 0.5182211995124817], "text": "the existing", "score": 0.5221138298511505, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6351, "end": 6372, "seq_label": ["B-Task", "I-Method", "I-Method"], "seq_token": ["\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.3742068409919739, 0.7647514343261719, 0.7445101737976074], "text": "summarization systems", "score": 0.6278228163719177, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6408, "end": 6415, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.497719943523407, 0.7641586661338806], "text": "WikiHow", "score": 0.6309393048286438, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6431, "end": 6448, "seq_label": ["B-Datasource", "I-Datasource", "I-Method", "I-Method"], "seq_token": ["\u0120Wiki", "How", "\u0120data", "\u0120dump"], "seq_scores": [0.7839404344558716, 0.866264283657074, 0.6184123158454895, 0.6180517673492432], "text": "WikiHow data dump", "score": 0.7216672003269196, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6595, "end": 6615, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120In", "verted", "\u0120Pyramid"], "seq_scores": [0.9801434874534607, 0.9841104745864868, 0.9976575374603271, 0.9974914789199829], "text": "the Inverted Pyramid", "score": 0.9898507446050644, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5722, "end": 5757, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["The", "\u0120existing", "\u0120summar", "ization", "\u0120datasets"], "seq_scores": [0.9965972304344177, 0.9985098242759705, 0.9975548386573792, 0.9979776740074158, 0.9989297986030579], "text": "The existing summarization datasets", "score": 0.9979138731956482, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5770, "end": 5783, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.9929754734039307, 0.9964625239372253], "text": "news articles", "score": 0.994718998670578, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5785, "end": 5799, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120These", "\u0120articles"], "seq_scores": [0.5865683555603027, 0.562465250492096], "text": "These articles", "score": 0.5745168030261993, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6221, "end": 6237, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120lead", "-", "3", "\u0120bas", "elines"], "seq_scores": [0.9984238147735596, 0.9993376135826111, 0.9996548891067505, 0.9992032647132874, 0.9995290040969849], "text": "lead-3 baselines", "score": 0.9992297172546387, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6338, "end": 6372, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120existing", "\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.9934671521186829, 0.994536817073822, 0.995600700378418, 0.998979389667511, 0.998271107673645], "text": "the existing summarization systems", "score": 0.9961710333824157, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6387, "end": 6400, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120dataset"], "seq_scores": [0.9991658926010132, 0.9990146160125732, 0.9993398785591125], "text": "a new dataset", "score": 0.9991734623908997, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6450, "end": 6462, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120This", "\u0120dataset"], "seq_scores": [0.9933985471725464, 0.9970640540122986], "text": "This dataset", "score": 0.9952313005924225, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6472, "end": 6480, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120articles"], "seq_scores": [0.9121238589286804], "text": "articles", "score": 0.9121238589286804, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6724, "end": 6731, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.8759374022483826, 0.8730750679969788], "text": "WikiHow", "score": 0.8745062351226807, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7394, "end": 7401, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.816804051399231, 0.7942861914634705], "text": "WikiHow", "score": 0.8055451214313507, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6756, "end": 6762, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["\u0120knowledge", "\u0120base", "\u0120online"], "seq_scores": [0.5878326892852783, 0.5570359230041504, 0.5801618695259094], "text": "online", "score": 0.5750101606051127, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7418, "end": 7432, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data", "\u0120pairs"], "seq_scores": [0.7950014472007751, 0.903783917427063, 0.7955100536346436], "text": "the data pairs", "score": 0.8314318060874939, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8230, "end": 8236, "seq_label": ["B-Method"], "seq_token": ["\u0120python"], "seq_scores": [0.47578343749046326], "text": "python", "score": 0.47578343749046326, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8237, "end": 8245, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Sc", "rap", "y", "\u01202"], "seq_scores": [0.5849615335464478, 0.9755234122276306, 0.9653558135032654, 0.7019580602645874], "text": "Scrapy 2", "score": 0.8069497048854828, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8298, "end": 8306, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "-", "How"], "seq_scores": [0.970899224281311, 0.9157692790031433, 0.9371326565742493], "text": "Wiki-How", "score": 0.9412670532862345, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 8583, "end": 8596, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.808983564376831, 0.8917567729949951], "text": "summarization", "score": 0.8503701686859131, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 9118, "end": 9131, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.7493672966957092, 0.8851563334465027], "text": "summarization", "score": 0.817261815071106, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8280, "end": 8288, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9747694134712219, 0.9616318345069885], "text": "the data", "score": 0.9682006239891052, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8316, "end": 8328, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120articles"], "seq_scores": [0.9628851413726807, 0.9628234505653381], "text": "The articles", "score": 0.9628542959690094, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 8400, "end": 8411, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Our", "\u0120craw", "ler"], "seq_scores": [0.7897780537605286, 0.8956973552703857, 0.9256525635719299], "text": "Our crawler", "score": 0.8703759908676147, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8431, "end": 8455, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120142", ",", "\u01207", "83", "\u0120unique", "\u0120articles"], "seq_scores": [0.9967973828315735, 0.9967564940452576, 0.9991050362586975, 0.9993202686309814, 0.9990487694740295, 0.9993011951446533], "text": "142, 783 unique articles", "score": 0.9983881910641988, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8520, "end": 8532, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["new", "\u0120articles"], "seq_scores": [0.5691272020339966, 0.9495062828063965], "text": "new articles", "score": 0.7593167424201965, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8566, "end": 8574, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9957043528556824, 0.9918742775917053], "text": "the data", "score": 0.9937893152236938, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8932, "end": 8949, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120230", ",", "\u01208", "43", "\u0120articles"], "seq_scores": [0.9984217882156372, 0.9985712766647339, 0.9996538162231445, 0.9997519850730896, 0.9995414018630981], "text": "230, 843 articles", "score": 0.9991880536079407, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8954, "end": 8973, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120reference", "\u0120summ", "aries"], "seq_scores": [0.8821020722389221, 0.9079232811927795, 0.7849609851837158], "text": "reference summaries", "score": 0.8583287795384725, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9152, "end": 9166, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120articles"], "seq_scores": [0.9543349742889404, 0.9165325164794922], "text": "these articles", "score": 0.9354337453842163, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9268, "end": 9285, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120final", "\u0120dataset"], "seq_scores": [0.9966443777084351, 0.9975501894950867, 0.9990028738975525], "text": "The final dataset", "score": 0.9977324803670248, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9297, "end": 9318, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120204", ",", "\u012000", "4", "\u0120articles", "\u0120and"], "seq_scores": [0.9904714822769165, 0.9989402890205383, 0.9996991157531738, 0.999821126461029, 0.9997647404670715, 0.7039758563041687], "text": "204, 004 articles and", "score": 0.948778768380483, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9319, "end": 9334, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120their", "\u0120summ", "aries"], "seq_scores": [0.6990115642547607, 0.9887884259223938, 0.9852235317230225], "text": "their summaries", "score": 0.8910078406333923, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9354, "end": 9365, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9980714917182922, 0.9991143345832825], "text": "the dataset", "score": 0.9985929131507874, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9388, "end": 9399, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120dataset"], "seq_scores": [0.9973462820053101, 0.99713134765625], "text": "The dataset", "score": 0.99723881483078, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 9474, "end": 9481, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.6347385048866272, 0.6584576964378357], "text": "WikiHow", "score": 0.6465981006622314, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 9607, "end": 9610, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120D", "UC"], "seq_scores": [0.9960481524467468, 0.9900640845298767], "text": "DUC", "score": 0.9930561184883118, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 9615, "end": 9623, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Gig", "aw", "ord"], "seq_scores": [0.9967390894889832, 0.9942968487739563, 0.9931058287620544], "text": "Gigaword", "score": 0.9947139223416647, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9510, "end": 9524, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120230", ",", "\u0120000", "\u0120pairs"], "seq_scores": [0.7810757160186768, 0.990878164768219, 0.9915617108345032, 0.9934501051902771], "text": "230, 000 pairs", "score": 0.939241424202919, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 9697, "end": 9704, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.7974160313606262, 0.815336287021637], "text": "WikiHow", "score": 0.8063761591911316, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 9726, "end": 9740, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120mail"], "seq_scores": [0.9862251281738281, 0.9973282814025879, 0.9948709011077881, 0.9951953291893005], "text": "CNN/Daily mail", "score": 0.9934049099683762, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 9808, "end": 9821, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.9705528616905212, 0.8802387118339539], "text": "summarization", "score": 0.9253957867622375, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9830, "end": 9852, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.999402642250061, 0.9994919300079346, 0.9992315769195557, 0.9992278814315796, 0.9991797804832458, 0.9988334774971008, 0.9992128610610962, 0.9983751773834229], "text": "Nallapati et al., 2016", "score": 0.9991194158792496, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9854, "end": 9878, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["N", "all", "ap", "ati", "\u0120et", "\u0120al", ".,", "\u0120,", "\u01202017"], "seq_scores": [0.9994627833366394, 0.9995577931404114, 0.9996250867843628, 0.9996225833892822, 0.9994688630104065, 0.9993659853935242, 0.9993732571601868, 0.9990516304969788, 0.9987855553627014], "text": "Nallapati et al., , 2017", "score": 0.9993681708971659, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9881, "end": 9897, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120See", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9990761280059814, 0.9994444251060486, 0.99947589635849, 0.9994605183601379, 0.999426007270813], "text": "See et al., 2017", "score": 0.9993765950202942, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9899, "end": 9918, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Paul", "us", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9993439316749573, 0.9996429681777954, 0.9995967745780945, 0.9995360374450684, 0.9994441866874695, 0.9994706511497498], "text": "Paulus et al., 2017", "score": 0.9995057582855225, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 10246, "end": 10250, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wiki"], "seq_scores": [0.7576385140419006], "text": "Wiki", "score": 0.7576385140419006, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9968, "end": 9979, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9970203042030334, 0.9987871050834656], "text": "the dataset", "score": 0.9979037046432495, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10176, "end": 10179, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.6712176203727722], "text": "the", "score": 0.6712176203727722, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10242, "end": 10259, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120Wiki", "How", "\u0120pairs"], "seq_scores": [0.6169124245643616, 0.8407862186431885, 0.8627078533172607, 0.8435990214347839], "text": "the WikiHow pairs", "score": 0.7910013794898987, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10489, "end": 10502, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.978636622428894, 0.958240270614624], "text": "summarization", "score": 0.968438446521759, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10781, "end": 10794, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120summar", "ization"], "seq_scores": [0.9879933595657349, 0.9898973703384399], "text": "summarization", "score": 0.9889453649520874, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10899, "end": 10906, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.839942455291748, 0.8064555525779724], "text": "WikiHow", "score": 0.8231990039348602, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10911, "end": 10925, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail"], "seq_scores": [0.8722936511039734, 0.9190975427627563, 0.8328168988227844, 0.8412996530532837], "text": "CNN/Daily Mail", "score": 0.8663769364356995, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10959, "end": 10966, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.8477827906608582, 0.7436452507972717], "text": "WikiHow", "score": 0.7957140207290649, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10564, "end": 10576, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120articles"], "seq_scores": [0.9870908260345459, 0.9465749859809875], "text": "the articles", "score": 0.9668329060077667, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10581, "end": 10594, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120summ", "aries"], "seq_scores": [0.9779492616653442, 0.9365010261535645, 0.9069094061851501], "text": "the summaries", "score": 0.9404532313346863, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10677, "end": 10686, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120sentences"], "seq_scores": [0.9148714542388916], "text": "sentences", "score": 0.9148714542388916, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10713, "end": 10722, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120summ", "aries"], "seq_scores": [0.9003732204437256, 0.7864153385162354], "text": "summaries", "score": 0.8433942794799805, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11066, "end": 11073, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9960787892341614, 0.9965903759002686], "text": "WikiHow", "score": 0.996334582567215, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11182, "end": 11189, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9964953064918518, 0.9959562420845032], "text": "WikiHow", "score": 0.9962257742881775, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11194, "end": 11208, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120mail"], "seq_scores": [0.99434494972229, 0.9975243210792542, 0.9969148635864258, 0.9957084655761719], "text": "CNN/Daily mail", "score": 0.9961231499910355, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11088, "end": 11133, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120existing", "\u0120extract", "ive", "\u0120and", "\u0120abstract", "ive", "\u0120bas", "elines"], "seq_scores": [0.9713260531425476, 0.9732226729393005, 0.9990965127944946, 0.9985342025756836, 0.9977467656135559, 0.9992087483406067, 0.9980423450469971, 0.9983633160591125], "text": "existing extractive and abstractive baselines", "score": 0.9919425770640373, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11135, "end": 11146, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120systems"], "seq_scores": [0.9724136590957642, 0.9740695357322693], "text": "The systems", "score": 0.9732415974140167, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11273, "end": 11281, "seq_label": ["B-Method", "I-Method"], "seq_token": ["Text", "Rank"], "seq_scores": [0.9863676428794861, 0.9921269416809082], "text": "TextRank", "score": 0.9892472922801971, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11304, "end": 11328, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120extract", "ive", "\u0120summar", "ization"], "seq_scores": [0.788651168346405, 0.9479353427886963, 0.9563969373703003, 0.9423931241035461], "text": "extractive summarization", "score": 0.9088441431522369, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11337, "end": 11361, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["M", "ih", "al", "ce", "a", "\u0120and", "\u0120Tar", "au", ",", "\u01202004"], "seq_scores": [0.9961954355239868, 0.9920199513435364, 0.9886411428451538, 0.994392454624176, 0.9922892451286316, 0.9907666444778442, 0.9900079965591431, 0.9911190867424011, 0.9872742891311646, 0.9873598217964172], "text": "Mihalcea and Tarau, 2004", "score": 0.9910066068172455, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11363, "end": 11383, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bar", "ri", "os", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9968552589416504, 0.9952669143676758, 0.9946566820144653, 0.9932993650436401, 0.9934093952178955, 0.9920944571495056, 0.9931957721710205], "text": "Barrios et al., 2016", "score": 0.9941111207008362, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11393, "end": 11412, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120graph", "-", "based", "\u0120ranking"], "seq_scores": [0.5582637786865234, 0.7902299761772156, 0.8087287545204163, 0.7314436435699463], "text": "graph-based ranking", "score": 0.7221665382385254, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11282, "end": 11299, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Extract", "ive", "\u0120system"], "seq_scores": [0.9857959747314453, 0.9959204196929932, 0.9959838390350342], "text": "Extractive system", "score": 0.9925667444864908, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11301, "end": 11335, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120An", "\u0120extract", "ive", "\u0120summar", "ization", "\u0120system"], "seq_scores": [0.9975335597991943, 0.9947195053100586, 0.9991592168807983, 0.9979524612426758, 0.9980080723762512, 0.9980989098548889], "text": "An extractive summarization system", "score": 0.9975786209106445, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11479, "end": 11483, "seq_label": ["B-ModelArchitecture"], "seq_token": ["Sequ"], "seq_scores": [0.5327911376953125], "text": "Sequ", "score": 0.5327911376953125, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11511, "end": 11520, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120attention"], "seq_scores": [0.6014704704284668], "text": "attention", "score": 0.6014704704284668, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11551, "end": 11572, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Chop", "ra", "\u0120et", "\u0120al", ".", "\u0120(", "2016", ");"], "seq_scores": [0.9986465573310852, 0.9995881915092468, 0.9998194575309753, 0.9998031258583069, 0.746052622795105, 0.9996192455291748, 0.9994826316833496, 0.995954155921936], "text": "Chopra et al. (2016);", "score": 0.9673707485198975, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11573, "end": 11596, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120N", "all", "ap", "ati", "\u0120et", "\u0120al", ".", "\u0120(", "2016", ")"], "seq_scores": [0.9950749278068542, 0.9978398084640503, 0.9995985627174377, 0.9997751116752625, 0.9997945427894592, 0.9998040795326233, 0.7460525631904602, 0.999629020690918, 0.9996009469032288, 0.9993532299995422], "text": "Nallapati et al. (2016)", "score": 0.9736522793769836, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 11600, "end": 11625, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120abstract", "ive", "\u0120summar", "ization"], "seq_scores": [0.9982490539550781, 0.998980700969696, 0.9993882179260254, 0.9994421601295471], "text": "abstractive summarization", "score": 0.9990150332450867, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11479, "end": 11505, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["Sequ", "ence", "-", "to", "-", "sequence", "\u0120model"], "seq_scores": [0.9966787099838257, 0.9997116923332214, 0.9998083710670471, 0.9998593330383301, 0.9998422861099243, 0.9998492002487183, 0.9997302889823914], "text": "Sequence-to-sequence model", "score": 0.9993542688233512, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11522, "end": 11539, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120A", "\u0120baseline", "\u0120system"], "seq_scores": [0.9990068078041077, 0.9995679259300232, 0.999138593673706], "text": "A baseline system", "score": 0.9992377758026123, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11686, "end": 11699, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120baseline"], "seq_scores": [0.9948962330818176, 0.9986202716827393], "text": "This baseline", "score": 0.9967582523822784, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11754, "end": 11771, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["Po", "inter", "-", "gener", "ator"], "seq_scores": [0.9402651786804199, 0.9640063643455505, 0.9663677215576172, 0.9498860239982605, 0.9529235363006592], "text": "Pointer-generator", "score": 0.9546897649765015, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11794, "end": 11811, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120pointer", "-", "gener", "ator"], "seq_scores": [0.8226243257522583, 0.9404790997505188, 0.9124091863632202, 0.904503345489502], "text": "pointer-generator", "score": 0.8950039893388748, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11823, "end": 11839, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["See", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9958307147026062, 0.9980484247207642, 0.9984878301620483, 0.9986020922660828, 0.9985194802284241], "text": "See et al., 2017", "score": 0.9978977084159851, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11977, "end": 11994, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120Po", "inter", "-", "gener", "ator"], "seq_scores": [0.9349966049194336, 0.9670239686965942, 0.973660409450531, 0.9652098417282104, 0.9656825065612793], "text": "Pointer-generator", "score": 0.9613146662712098, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 12033, "end": 12050, "seq_label": ["I-Method", "B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120coverage", "\u0120pointer", "-", "gener", "ator"], "seq_scores": [0.7123013138771057, 0.9100476503372192, 0.9515402913093567, 0.9506411552429199, 0.9389288425445557], "text": "pointer-generator", "score": 0.8926918506622314, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12071, "end": 12084, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120coverage", "\u0120loss"], "seq_scores": [0.5774218440055847, 0.6135528683662415], "text": "coverage loss", "score": 0.5954873561859131, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12086, "end": 12102, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["See", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9929799437522888, 0.9975530505180359, 0.9984999895095825, 0.9987577199935913, 0.9986655712127686], "text": "See et al., 2017", "score": 0.9972912549972535, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12307, "end": 12315, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "-", "How"], "seq_scores": [0.5452117323875427, 0.7499552369117737, 0.6822677254676819], "text": "Wiki-How", "score": 0.6591448982556661, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11754, "end": 11771, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["Po", "inter", "-", "gener", "ator"], "seq_scores": [0.9352449178695679, 0.9483240246772766, 0.9381455779075623, 0.9291972517967224, 0.8912408947944641], "text": "Pointer-generator", "score": 0.9284305334091186, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11850, "end": 11859, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9958592057228088, 0.9961613416671753], "text": "the model", "score": 0.9960102736949921, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11977, "end": 11994, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Po", "inter", "-", "gener", "ator"], "seq_scores": [0.9302188158035278, 0.9815918207168579, 0.9759519696235657, 0.9741942286491394, 0.9732312560081482], "text": "Pointer-generator", "score": 0.9670376181602478, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12029, "end": 12059, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120pointer", "-", "gener", "ator", "\u0120baseline"], "seq_scores": [0.9877102375030518, 0.9960657954216003, 0.997268557548523, 0.997986912727356, 0.9982435703277588, 0.9968646168708801], "text": "The pointer-generator baseline", "score": 0.9956899483998617, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12161, "end": 12176, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Lead", "-", "3", "\u0120baseline"], "seq_scores": [0.9907688498497009, 0.9897484183311462, 0.9959810972213745, 0.9926770925521851], "text": "Lead-3 baseline", "score": 0.9922938644886017, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12178, "end": 12188, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120A", "\u0120baseline"], "seq_scores": [0.9979413151741028, 0.9968166947364807], "text": "A baseline", "score": 0.9973790049552917, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12261, "end": 12274, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120baseline"], "seq_scores": [0.9947223663330078, 0.9903312921524048], "text": "This baseline", "score": 0.9925268292427063, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12428, "end": 12447, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u01203", "\u0120the", "\u0120Lead", "-", "3", "\u0120baseline"], "seq_scores": [0.524950385093689, 0.9772467613220215, 0.991503119468689, 0.9978094696998596, 0.9983622431755066, 0.9978370070457458], "text": "the Lead-3 baseline", "score": 0.9146181643009186, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12620, "end": 12635, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Py", "rou", "ge", "\u0120package"], "seq_scores": [0.9134487509727478, 0.9979140162467957, 0.9971004128456116, 0.6563286185264587], "text": "Pyrouge package", "score": 0.8911979496479034, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12665, "end": 12672, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120R", "OU", "GE", "-", "1"], "seq_scores": [0.9084125757217407, 0.9059641361236572, 0.9026190042495728, 0.8638614416122437, 0.8116123080253601], "text": "ROUGE-1", "score": 0.8784938931465149, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12674, "end": 12681, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120R", "OU", "GE", "-", "2"], "seq_scores": [0.8046888709068298, 0.8672896027565002, 0.8798093199729919, 0.7691680788993835, 0.7189391851425171], "text": "ROUGE-2", "score": 0.8079790115356446, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12686, "end": 12693, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120R", "OU", "GE", "-", "L"], "seq_scores": [0.8169229030609131, 0.8629488348960876, 0.8745974898338318, 0.7841585874557495, 0.7719694972038269], "text": "ROUGE-L", "score": 0.8221194624900818, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12695, "end": 12704, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Lin", ",", "\u01202004"], "seq_scores": [0.9933392405509949, 0.9637491106987, 0.9444242119789124], "text": "Lin, 2004", "score": 0.9671708544095358, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12714, "end": 12720, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120MET", "E", "OR"], "seq_scores": [0.9268356561660767, 0.9901608228683472, 0.9836242198944092], "text": "METEOR", "score": 0.966873566309611, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12722, "end": 12746, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ban", "er", "jee", "\u0120and", "\u0120Lav", "ie", ",", "\u01202005"], "seq_scores": [0.9901022911071777, 0.9776855111122131, 0.9796736240386963, 0.9805124998092651, 0.9783661365509033, 0.9861143827438354, 0.985548198223114, 0.9806453585624695], "text": "Banerjee and Lavie, 2005", "score": 0.9823310002684593, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12933, "end": 12947, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120Mail"], "seq_scores": [0.991738498210907, 0.9977172613143921, 0.9971827268600464, 0.9975035786628723], "text": "CNN/Daily Mail", "score": 0.9960355162620544, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13034, "end": 13041, "seq_label": ["I-Task", "I-Task", "B-Dataset", "I-Dataset"], "seq_token": ["\u0120summar", "ization", "\u0120Wiki", "How"], "seq_scores": [0.5908843278884888, 0.8145560622215271, 0.9403356313705444, 0.9471494555473328], "text": "WikiHow", "score": 0.8232313692569733, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13120, "end": 13134, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120mail"], "seq_scores": [0.9923540353775024, 0.9968143105506897, 0.9970313310623169, 0.9953209757804871], "text": "CNN/Daily mail", "score": 0.995380163192749, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13151, "end": 13158, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9137869477272034, 0.9338995814323425], "text": "WikiHow", "score": 0.923843264579773, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 13172, "end": 13178, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120lead", "-", "3"], "seq_scores": [0.9832279682159424, 0.9754126071929932, 0.9865115880966187], "text": "lead-3", "score": 0.9817173878351847, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 13318, "end": 13324, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120lead", "-", "3"], "seq_scores": [0.9791219830513, 0.9718685746192932, 0.983908474445343], "text": "lead-3", "score": 0.9782996773719788, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13328, "end": 13335, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9185769557952881, 0.9243887662887573], "text": "WikiHow", "score": 0.9214828610420227, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13456, "end": 13470, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CNN", "/", "Daily", "\u0120mail"], "seq_scores": [0.9903351068496704, 0.997062623500824, 0.9968084692955017, 0.9954847097396851], "text": "CNN/Daily mail", "score": 0.9949227273464203, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13487, "end": 13494, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9243978261947632, 0.9476648569107056], "text": "WikiHow", "score": 0.9360313415527344, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12585, "end": 12606, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120evaluated", "\u0120systems"], "seq_scores": [0.9964260458946228, 0.9978969097137451, 0.9969502091407776], "text": "the evaluated systems", "score": 0.9970910549163818, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12902, "end": 12920, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120multiple", "\u0120bas", "elines"], "seq_scores": [0.9966919422149658, 0.9984568357467651, 0.9983749389648438], "text": "multiple baselines", "score": 0.9978412389755249, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12949, "end": 13010, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["the", "\u0120well", "-", "known", ",", "\u0120most", "\u0120common", "\u0120abstract", "ive", "\u0120summar", "ization", "\u0120dataset"], "seq_scores": [0.9842371344566345, 0.9857354760169983, 0.9982101917266846, 0.9866917133331299, 0.9894281029701233, 0.9968649744987488, 0.9984387755393982, 0.999049961566925, 0.9990686774253845, 0.9989084005355835, 0.9988855719566345, 0.9987742304801941], "text": "the well-known, most common abstractive summarization dataset", "score": 0.9945244342088699, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13070, "end": 13095, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.9977129697799683, 0.9966993927955627, 0.999494194984436, 0.9988609552383423], "text": "the summarization systems", "score": 0.9981918781995773, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13193, "end": 13208, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["-", "\u0120other", "\u0120bas", "elines"], "seq_scores": [0.5842294096946716, 0.9867540001869202, 0.9797585010528564, 0.9919480681419373], "text": "other baselines", "score": 0.8856724947690964, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13398, "end": 13411, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120bas", "elines"], "seq_scores": [0.9947405457496643, 0.9910716414451599, 0.9949369430541992], "text": "all baselines", "score": 0.9935830434163412, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13558, "end": 13573, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120new", "\u0120dataset"], "seq_scores": [0.998734176158905, 0.9982724189758301, 0.9984163045883179], "text": "the new dataset", "score": 0.9984742999076843, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13611, "end": 13636, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.9863226413726807, 0.9880465269088745, 0.9985571503639221, 0.9959467053413391], "text": "the summarization systems", "score": 0.9922182559967041, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13663, "end": 13670, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9974852800369263, 0.9967654943466187], "text": "WikiHow", "score": 0.9971253871917725, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13690, "end": 13696, "seq_label": ["B-Task"], "seq_token": ["\u0120summar"], "seq_scores": [0.9709340333938599], "text": "summar", "score": 0.9709340333938599, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 13748, "end": 13755, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.9382840991020203, 0.9699762463569641], "text": "WikiHow", "score": 0.9541301727294922, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13776, "end": 13783, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "How"], "seq_scores": [0.97743159532547, 0.965003252029419], "text": "WikiHow", "score": 0.9712174236774445, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13849, "end": 13870, "seq_label": ["B-Task", "I-Method", "I-Method"], "seq_token": ["\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.5225805044174194, 0.7223793268203735, 0.702298641204834], "text": "summarization systems", "score": 0.6490861574808756, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13672, "end": 13711, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120large", "-", "scale", "\u0120summar", "ization", "\u0120dataset"], "seq_scores": [0.9990578293800354, 0.9994988441467285, 0.9990053772926331, 0.9996706247329712, 0.9997524619102478, 0.9996336698532104, 0.999536395072937, 0.9997256398200989], "text": "a new large-scale summarization dataset", "score": 0.9994851052761078, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13726, "end": 13742, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120diverse", "\u0120articles"], "seq_scores": [0.982132613658905, 0.9760184288024902], "text": "diverse articles", "score": 0.9790755212306976, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13845, "end": 13870, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120summar", "ization", "\u0120systems"], "seq_scores": [0.9730193614959717, 0.9713943600654602, 0.998498797416687, 0.9959432482719421], "text": "the summarization systems", "score": 0.9847139418125153, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13885, "end": 13900, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120new", "\u0120dataset"], "seq_scores": [0.999237060546875, 0.9994150400161743, 0.9997630715370178], "text": "the new dataset", "score": 0.9994717240333557, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13959, "end": 13972, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120their", "\u0120systems"], "seq_scores": [0.9115310907363892, 0.9347330927848816], "text": "their systems", "score": 0.9231320917606354, "type": "ScholarlyEntity"}]}, "filename": "00009_1810_09305.json", "id": "00009_1810_09305"}