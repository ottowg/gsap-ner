{"text": "A Simple Method for Commonsense Reasoning\n\nAbstract:\nCommonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset [1]. In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.)\n\n\n1 Introduction\nAlthough deep neural networks have achieved remarkable successes (e.g., [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]), their dependence on supervised learning has been challenged as a significant weakness. This dependence prevents deep neural networks from being applied to problems where labeled data is scarce. An example of such problems is common sense reasoning, such as the Winograd Schema Challenge [1], where the labeled set is typically very small, on the order of hundreds of examples. Below is an example question from this dataset:\n\u2022 The trophy doesn't fit in the suitcase because it is too big. What is too big?\nAnswer 0: the trophy. Answer 1: the suitcase Although it is straightforward for us to choose the answer to be \"the trophy\" according to our common sense, answering this type of question is a great challenge for machines because there is no training data, or very little of it.\nIn this paper, we present a surprisingly simple method for common sense reasoning with Winograd schema multiple choice questions. Key to our method is th e use of language models (LMs), trained on a large amount of unlabeled data, to score multiple choice questions posed by the challenge and similar datasets. More concretely, in the above example, we will first substitute the pronoun (\"it\") with the candidates (\"the trophy\" and \"the suitcase\"), and then use LMs to compute the probability of the two resulting sentences (\"The trophy doesn't fit in the suitcase because the trophy is too big.\" and \"The trophy doesn't fit in the suitcase because the suitcase is too big.\"). The substitution that results in a more probable sentence will be the correct answer.\nOn both Pronoun Disambiguation and Winograd Schema challenges, our method outperforms previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 70.0% accuracy, which is better than the state-of-art accuracy of 66.7%. On a Winograd Schema dataset, WSC-273, our method achieves 63.7% accuracy, 11% above that of the current state-of-art result (52.8%) 2 A unique feature of Winograd Schema questions is the presence of a special word that decides the correct reference choice. In the above example, \"big\" is this special word. When \"big\" is replaced by \"small\", the correct answer switches to \"the suitcase\". Although detecting this feature is not part of the challenge, further analysis shows that our system successfully discovers this special word to make its decisions in many cases, indicating a good grasp of commonsense knowledge.\n\n2 Related Work\nUnsupervised learning has been used to discover simple commonsense relationships. For example, Mikolov et al. [16, 17] show that by learning to predict adjacent words in a sentence, word vectors can be used to answer analogy questions such as: Man:King::Woman:?. Our work uses a similar intuition that language modeling can naturally capture common sense knowledge. The difference is that Winograd Schema questions require more contextual information, hence our use of LMs instead of just word vectors.\nNeural LMs have also been applied successfully to improve downstream applications [18, 19, 20, 21].\nIn [18, 19, 20, 21], researchers have shown that pre-trained LMs can be used as feature representations for a sentence, or a paragraph to improve NLP applications such as document classification, machine translation, question answering, etc. The combined evidence suggests that LMs trained on a massive amount of unlabeled data can capture many aspects of natural language and the world's knowledge, especially commonsense information.\nPrevious attempts on solving the Winograd Schema Challenge usually involve heavy utilization of annotated knowledge bases, rule-based reasoning, or hand-crafted features [22, 23, 24]. In particular, Rahman and Ng [25] employ human annotators to build more supervised training data. Their model utilizes nearly 70K hand-crafted features, including querying data from Google Search API. Sharma et al. [26] rely on a semantic parser to understand the question, query texts through Google Search, and reason on the graph produced by the parser. Similarly, Sch\u00fcller [24] formalizes the knowledgegraph data structure and a reasoning process based on cognitive linguistics theories. Bailey et al. [23] introduces a framework for reasoning, using expensive annotated knowledge bases as axioms.\nThe current best approach makes use of the skip-gram model to learn word representations [27]. The model incorporates several knowledge bases to regularize its training process, resulting in Knowledge Enhanced Embeddings (KEE). A semantic similarity scorer and a deep neural network classifier are then combined on top of KEE to predict the answers. The final system, therefore, includes both supervised and unsupervised models, besides three different knowledge bases. In contrast, our unsupervised method is simpler while having significantly higher accuracy. Unsupervised training is done on text corpora which can be cheaply curated.\nUsing language models in reading comprehension tests also produced many great successes. Namely Chu et al. [28] used bi-directional RNNs to predict the last word of a passage in the LAMBADA challenge. Similarly, LMs are also used to produce features for a classifier in the Store Close Test 2017, giving best accuracy against other methods [29]. In a broader context, LMs are used to produce good word embeddings, significantly improved a wide variety of downstream tasks, including the general problem of question answering [20, 30].\n\n3 Methods\nWe first substitute the pronoun in the original sentence with each of the candidate choices. The problem of coreference resolution then reduces to identifying which substitution results in a more probable sentence. By reframing the problem this way, language modeling becomes a natural solution Figure 1 : Overview of our method and analysis. We consider the test \"The trophy doesn't fit in the suitcase because it is too big.\" Our method first substitutes two candidate references trophy and suitcase into the pronoun position. We then use an LM to score the resulting two substitutions. By looking at probability ratio at every word position, we are able to detect \"big\" as the main contributor to trophy being the chosen answer. When \"big\" is switched to \"small\", the answer changes to suitcase. This switching behaviour is an important feature characterizing the Winograd Schema Challenge.\nby its definition. Namely, LMs are trained on text corpora, which encodes human knowledge in the form of natural language. During inference, LMs are able to assign probability to any given text based on what they have learned from training data. An overview of our method is shown in Figure 1.\nSuppose the sentence S of n consecutive words has its pronoun to be resolved specified at the k th position: S = {w 1 , .., w k\u22121 , w k \u2261 p, w k+1 , .., w n }. We make use of a trained language model P \u03b8 (w t |w 1 , w 2 , .., w t\u22121 ), which defines the probability of word w t conditioned on the previous words w 1 , ..., w t\u22121 . The substitution of a candidate reference c in to the pronoun position k results in a new sentence S w k \u2190c (we use notation w k \u2190 c to mean that word w k is substituted by candidate c). We consider two different ways of scoring the substitution:\u2022 Score f ull (w k \u2190 c) = P \u03b8 (w 1 , w 2 , ..., w k\u22121 , c, w k+1 , ..., w n )\nwhich scores how probable the resulting full sentence is, and\u2022 Score partial (w k \u2190 c) = P \u03b8 (w k+1 , ..., w n |w 1 , ..., w k\u22121 , c)\nwhich scores how probable the part of the resulting sentence following c is, given its antecedent.\nIn other words, it only scores a part S w k \u2190c conditioned on the rest of the substituted sentence. An example of these two scores is shown in Table 1. In our experiments, we find that partial scoring strategy is generally better than the naive full scoring strategy.\nTable 1 : Example of full and partial scoring for the test \"The trophy doesn't fit in the suitcase because it is too big.\" with two reference choices \"the suitcase\" and \"the trophy\". c = the suitcase Score f ull (w k \u2190 \"the suitcase\") = P (The trophy doesn't fit in the suitcase because the suitcase is too big) Score partial (w k \u2190 \"the suitcase\") = P (is too big| The trophy doesn't fit in the suitcase because the suitcase) c = the trophy Score f ull (w k \u2190 \"the trophy\") = P (The trophy doesn't fit in the suitcase because the trophy is too big) Score partial (w k \u2190 \"the trophy\") = P (is too big| The trophy doesn't fit in suitcase because the trophy)\n\n4 Experimental settings\nIn this section we describe tests for commonsense reasoning and the LMs used to solve these tasks. We also detail training text corpora used in our experiments.\nEvaluation on Commonsense Reasoning Tests. We conduct experiments to evaluate our methods on two tasks: Pronoun Disambiguation Problems and Winograd Schema Challenge. These two tasks have been proposed as potential alternatives to the Turing Test, specifically targeting its potential weaknesses and inadequacy [1].\nOn the former task, we use the original set of 60 questions (PDP-60) as the main benchmark 3. Later analysis augments this test with 62 questions from the development set to avoid bias presented in the original smaller set. 4 The second task (WSC-273) is qualitatively much more difficult 5. Its recent best reported result is only 3% of accuracy above random guess [27]. This task consists of 273 questions and is designed to work against techniques such as traditional linguistic restrictions, common heuristics or simple statistical test over text corpora (\"Google-proof \") [1].\nRecurrent language models. We consider two types of recurrent LMs, one processes word inputs and the other processes character inputs. Their output layer, however, is constructed to only produce word outputs, allowing both types of input processing to join in ensembles. Namely, our LMs predict a distribution over a large vocabulary (800K words) at each time step, using a softmax layer. Following [31], we employ importance sampling at the softmax layer with 8,192 negative samples for each mini-batch to significantly speed up training. We use two layers of LSTM [32] with 8,192 hidden units and a projection layer to a smaller dimensionality at output gates for faster processing.\nFor models that process words, we use a big embedding look up matrix with vocabulary size 800K and embedding size 1,024. For character-level input, we use a vocabulary size of 256 characters and embedding size 16. Characters in the same word are concatenated and used as input at a single time step. The resulting character embedding is processed using eight convolutions before going into the LSTM layers. More details about our LMs can be found in Appendix A.\nTraining text corpora. We perform experiments on several different text copora to examine the effect of training data type on test accuracy. Namely, we consider LM-1-Billion, CommonCrawl 6, SQuAD and Gutenberg Books. For SQuAD, we collect context passages from the Stanford Question-Answering Dataset [33] to form its training and validation sets accordingly.\n\n5 Main results\nOur experiments start with testing LMs trained on all text corpora with PDP-60 and WSC-273. Next, we show that it is possible to customize training data to obtain even better results.\n\n5.1 The first challenge in 2016: PDP-60\nWe first examine unsupervised single-model resolvers on PDP-60 by training one character-level and one word-level LM on the Gutenberg corpus. In Table 2, these two resolvers outperform previous results by a large margin. For this task, we found full scoring gives better results than partial scoring. In Section 6.2, we provide evidences that this is an atypical case due to the very small size of PDP-60. Next, we allow systems to take in necessary components to maximize their test performance. This includes making use of supervised training data that maps commonsense reasoning questions to their correct answer. Here we simply train another three variants of LMs on LM-1-Billion, CommonCrawl, and SQuAD and ensemble all of them. As reported in Table 3, this ensemble of five unsupervised models outperform the best system in the 2016 competition (58.3%) by a large margin. Specifically, we achieve 70.0% accuracy, better than the more recent reported results from Quan Liu et al (66.7%) [27], who makes use of three knowledge bases and a supervised deep neural network.\n\n5.2 Winograd Schema Challenge\nOn the harder task WSC-273, our single-model resolvers also outperform the current state-of-the-art by a large margin, as shown in This task is more difficult than PDP-60. First, the overall performance of all competing systems are much lower than that of PDP-60. Second, incorporating supervised learning and expensive annotated knowledge bases to USSM provides insignificant gain this time (+3%), comparing to the large gain on PDP-60 (+19%). 7\n\n5.3 Customized training data for Winograd Schema Challenge\nAs previous systems collect relevant data from knowledge bases after observing questions during evaluation [25, 26], we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of F 1 (n) scores when counting overlapping n-grams:Similarity_Score document = 4 n=1 nF 1 (n) 4 n=1 n\n7 Our results so far have been with recurrent language models. As a comparison, we also trained a subwordlevel Transformer [37] LM on Wikipedia texts and obtain competitive performance (58.3% on PDP-60 and 54.1% on WSC-273).\n\nNumber of documents\nOne day when John and I had been out on somebusiness of our master 's , and were returning gently on a , straight road , at some distance we saw a boy trying to leap a pony over a gate ; the pony would not take the leap ,and the boy cut him with the whip , but he only turned off on one side . He whipped him again , but the pony turned off on the other side . Then the boy got off and gave him a hard thrashing , and knocked him about the head ... Right: An excerpt from the document whose score is 0.083 (highest ranking). In comparison, a perfect score is of 1.0. Documents in this corpus contain long series of events with complex references from several pronouns. The top 0.1% of highest ranked documents is chosen as our new training corpus. Details of the ranking is shown in Figure 2. This procedure resulted in nearly 1,000,000 documents, with the highest ranking document having a score of 8 \u00d7 10 \u22122 , still relatively small to a perfect score of 1.0. We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events.\nWe train four different LMs on STORIES and add them to the previous ensemble of 10 LMs, resulting in a gain of 2% accuracy in the final system as shown in Table 5. Remarkably, single models trained on this corpus are already extremely strong, with a word-level LM achieving 62.6% accuracy, even better than the ensemble of 10 models previously trained on 4 other text corpora (61.5%). We introduce a method to potentially detect keywords at which our proposed resolvers make decision between the two candidates c correct and c incorrect . Namely, we look at the following ratio:q t = P \u03b8 (w t |w 1 , w 2 , ..., w t\u22121 ; w k \u2190 c correct ) P \u03b8 (w t |w 1 , w 2 , ..., w t\u22121 ; w k \u2190 c incorrect )\nWhere 1 \u2264 t \u2264 n for full scoring, and k + 1 \u2264 t \u2264 n for partial scoring. It follows that the choice between c correct or c incorrect is made by the value of Q = t q t being bigger than 1.0 or not. By looking at the value of each individual q t , it is possible to retrieve words with the largest values of q t and hence most responsible for the final value of Q.\nWe visualize the probability ratios q t to have more insights into the decisions of our resolvers. Figure 3 displays a sample of incorrect decisions made by full scoring and is corrected by partial scoring. Interestingly, we found q t with large values coincides with the special keyword of each Winograd Schema in several cases. Intuitively, this means the LMs assigned very low probability for the keyword after observing the wrong substitution. It follows that we can predict the keyword in each the Winograd Schema question by selecting top word positions with the highest value of q t . Figure 3 : A sample of questions from WSC-273 predicted incorrectly by full scoring, but corrected by partial scoring. Here we mark the correct prediction by an asterisk and display the normalized probability ratio qt by coloring its corresponding word. It can be seen that the wrong predictions are made mainly due to q t at the pronoun position, where the LM has not observed the full sentence.\nPartial scoring shifts the attention to later words and places highest q values on the special keywords, marked by a squared bracket. These keywords characterizes the Winograd Schema Challenge, as they uniquely decide the correct answer. In the last question, since the special keyword appear before the pronoun, our resolver instead chose \"upset\", as a reasonable switch word could be \"annoying\". For questions with keyword appearing before the reference, we detect them by backward-scoring models. Namely, we ensemble 6 LMs, each trained on one text corpora with word order reversed. This ensemble also outperforms the previous best system on WSC-273 with a remarkable accuracy of 58.2%. Overall, we are able to discover a significant amount of special keywords (115 out of 178 correctly answered questions) as shown in Table 6. This strongly indicates a correct understanding of the context and a good grasp of commonsense knowledge in the resolver's decision process.\n6.2 Partial scoring is better than full scoring.\nIn this set of experiments, we look at wrong predictions from a word-level LM. With full scoring strategy, we observe that q t at the pronoun position is most responsible for a very large percentage of incorrect decisions as shown in Figfure 3 and Table 7. For example, with the test \"The trophy cannot fit in the suitcase because it is too big.\", the system might return c incorrect =\"suitcase\" simply because c correct = \"trophy\" is a very rare word in its training corpus and therefore, is assigned a very low probability, overpowering subsequent q t values. Following this reasoning, we apply a simple fix to full scoring by normalizing its score with the unigram count of c: Score f ull normalized = Score f ull /Count(c). Partial scoring, on the other hand, disregards c altogether. As shown in Figure 4, this normalization fixes full scoring in 9 out of 10 tested LMs on PDP-122. On WSC-273, the result is very decisive as partial scoring strongly outperforms the other two scoring in all cases. Since PDP-122 is a larger superset of PDP-60, we attribute the different behaviour observed on PDP-60 as an atypical case due to its very small size. Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. We next rank the text corpora based on ensemble performance for more reliable results. Namely, we compare the previous ensemble of 10 models against the same set of models trained on each single text corpus. This time, the original ensemble trained on a diverse set of text corpora outperforms all other single-corpus ensembles including STORIES. This highlights the important role of diversity in training data for commonsense reasoning accuracy of the final system.\n\n7 Conclusion\nWe introduce a simple unsupervised method for Commonsense Reasoning tasks. Key to our proposal are large language models, trained on a number of massive and diverse text corpora. The resulting systems outperform previous best systems on both Pronoun Disambiguation Problems and Winograd Schema Challenge. Remarkably on the later benchmark, we are able to achieve 63.7% accuracy, comparing to 52.8% accuracy of the previous state-of-the-art, who utilizes supervised learning and expensively annotated knowledge bases. We analyze our system's answers and observe that it discovers key features of the question that decides the correct answer, indicating good understanding of the context and commonsense knowledge. We also demonstrated that ensembles of models benefit the most when trained on a diverse set of text corpora.\nWe anticipate that this simple technique will be a strong building block for future systems that utilize reasoning ability on commonsense knowledge.\n\nB Data contamination in CommonCrawl\nUsing the similarity scoring technique in section 5.3, we observe a large amount of low quality training text on the lower end of the ranking. Namely, these are documents whose content are mostly unintelligible or unrecognized by our vocabulary. Training LMs for commonsense reasoning tasks on full CommonCrawl, therefore, might not be ideal. On the other hand, we detected and removed a portion of PDP-122 questions presented as an extremely high ranked document.\n\nFootnotes:\n2: Code to reproduce these results are available at https://github.com/tensorflow/models/tree/ master/research/lm_commonsense.\n3: https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/PDPChallenge2016.xml\n4: http://commonsensereasoning.org/disambiguation.html\n5: https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml\n6: We evaluate all models trained on CommonCrawl after approximately 10-billion words are consumed.\n\nReferences:\n\n- Hector J Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In AAAI spring symposium: Logical formalizations of commonsense reasoning, 2011.- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\n- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human- level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701-1708, 2014.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni- tion. Advances in Neural Information Processing Systems, 2015.\n\n- Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.\n\n- Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.\n\n- Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.\n\n- Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256, 2016.\n\n- Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Katya Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. arXiv preprint arXiv:1712.01769, 2017.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.\n\n- Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014.\n\n- Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\n- Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving human parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567, 2018.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\n- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119, 2013.\n\n- Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3079-3087, 2015.\n\n- Prajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to sequence learning. In Conference on Empirical Methods in Natural Language Processing, 2017.\n\n- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018.\n\n- Jeremy Howard and Sebastian Ruder. Fine-tuned language models for text classification. arXiv preprint arXiv:1801.06146, 2018.\n\n- Haoruo Peng, Daniel Khashabi, and Dan Roth. Solving hard coreference problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 809-819, 2015.\n\n- Dan Bailey, Amelia Harrison, Yuliya Lierler, Vladimir Lifschitz, and Julian Michael. The winograd schema challenge and reasoning about correlation. In In Working Notes of the Symposium on Logical Formalizations of Commonsense Reasoning, 2015.\n\n- Peter Sch\u00fcller. Tackling winograd schemas by formalizing relevance theory in knowledge graphs. In Fourteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2014.\n\n- Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the winograd schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 777-789. Association for Computational Linguistics, 2012.\n\n- Arpit Sharma, Nguyen Ha Vo, Somak Aditya, and Chitta Baral. Towards addressing the winograd schema challenge-building and using a semantic parser and a knowledge hunting module. In IJCAI, pages 1319-1325, 2015.\n\n- Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. Combing context and commonsense knowledge through neural networks for solving winograd schema problems. CoRR, abs/1611.04146, 2016.\n\n- Zewei Chu, Hai Wang, Kevin Gimpel, and David A. McAllester. Broad context language modeling as reading comprehension. CoRR, abs/1610.08431, 2016.\n\n- Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46-51, 2017.\n\n- Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.\n\n- Rafal J\u00f3zefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. CoRR, abs/1602.02410, 2016.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\n- Quan Liu, Hui Jiang, Zhen-Hua Ling, Si Wei, and Yu Hu. Probabilistic reasoning via deep learning: Neural association models. CoRR, abs/1603.07704, 2016.\n\n- George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41, 1995.\n\n- Hugo Liu and Push Singh. Conceptnet-a practical commonsense reasoning tool-kit. BT technology journal, 22(4):211-226, 2004.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010, 2017.\n\n- John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011. Experiment LM variant / training corpus Single models on PDP-60 Word-LM 1/Gutenberg and Char-LM 1/Gutenberg Ensemble on PDP-60 Two single models on PDP-60 + Word-LM 2/SQuAD + Char-LM 2/LM1B + Char-LM 3/CommonCrawl Ensemble of 10 models Ensemble on PDP-60 + on WSC-273 Word-LM 1/Gutenberg (different random seed) + Word-LM 1/LM1B + Char-LM 4/Gutenberg + Char-LM 4/SQuAD + Char-LM 4/CommonCrawl Ensemble of 14 models Ensemble of 10 models on WSC-273 + on WSC-273 Word-LM 1/STORIES + Char-LM 2/STORIES + Word-LM 3/STORIES + Word-LM 4/STORIES Ensemble of 6 Word-LM 1/Gutenberg + Word-LM 1/STORIES + backward-scoring models Char-LM 4/CommonCrawl + Char-LM 4/SQuAD + on WSC-273 Word-LM 4/LM1B + Char-LM 2/STORIES +\n\n", "annotations": {"Abstract": [{"begin": 43, "end": 1254, "idx": 0}], "Head": [{"begin": 1257, "end": 1271, "n": "1", "idx": 0}, {"begin": 3917, "end": 3931, "n": "2", "idx": 1}, {"begin": 6931, "end": 6940, "n": "3", "idx": 2}, {"begin": 9942, "end": 9965, "n": "4", "idx": 3}, {"begin": 12533, "end": 12547, "n": "5", "idx": 4}, {"begin": 12733, "end": 12772, "n": "5.1", "idx": 5}, {"begin": 13849, "end": 13878, "n": "5.2", "idx": 6}, {"begin": 14327, "end": 14385, "n": "5.3", "idx": 7}, {"begin": 15255, "end": 15274, "idx": 8}, {"begin": 21168, "end": 21180, "n": "7", "idx": 9}, {"begin": 22154, "end": 22189, "idx": 10}], "ReferenceToBib": [{"begin": 213, "end": 216, "target": "#b0", "idx": 0}, {"begin": 1344, "end": 1347, "target": "#b1", "idx": 1}, {"begin": 1348, "end": 1350, "target": "#b2", "idx": 2}, {"begin": 1351, "end": 1353, "target": "#b3", "idx": 3}, {"begin": 1354, "end": 1356, "target": "#b4", "idx": 4}, {"begin": 1357, "end": 1359, "target": "#b5", "idx": 5}, {"begin": 1360, "end": 1362, "target": "#b6", "idx": 6}, {"begin": 1363, "end": 1365, "target": "#b7", "idx": 7}, {"begin": 1366, "end": 1368, "target": "#b8", "idx": 8}, {"begin": 1369, "end": 1372, "target": "#b9", "idx": 9}, {"begin": 1373, "end": 1376, "target": "#b10", "idx": 10}, {"begin": 1377, "end": 1380, "target": "#b11", "idx": 11}, {"begin": 1381, "end": 1384, "target": "#b12", "idx": 12}, {"begin": 1385, "end": 1388, "target": "#b13", "idx": 13}, {"begin": 1389, "end": 1392, "target": "#b14", "idx": 14}, {"begin": 1682, "end": 1685, "target": "#b0", "idx": 15}, {"begin": 4042, "end": 4046, "target": "#b15", "idx": 16}, {"begin": 4047, "end": 4050, "target": "#b16", "idx": 17}, {"begin": 4517, "end": 4521, "target": "#b17", "idx": 18}, {"begin": 4522, "end": 4525, "target": "#b18", "idx": 19}, {"begin": 4526, "end": 4529, "target": "#b19", "idx": 20}, {"begin": 4530, "end": 4533, "target": "#b20", "idx": 21}, {"begin": 4538, "end": 4542, "target": "#b17", "idx": 22}, {"begin": 4543, "end": 4546, "target": "#b18", "idx": 23}, {"begin": 4547, "end": 4550, "target": "#b19", "idx": 24}, {"begin": 4551, "end": 4554, "target": "#b20", "idx": 25}, {"begin": 5141, "end": 5145, "target": "#b21", "idx": 26}, {"begin": 5146, "end": 5149, "target": "#b22", "idx": 27}, {"begin": 5150, "end": 5153, "target": "#b23", "idx": 28}, {"begin": 5184, "end": 5188, "target": "#b24", "idx": 29}, {"begin": 5370, "end": 5374, "target": "#b25", "idx": 30}, {"begin": 5532, "end": 5536, "target": "#b23", "idx": 31}, {"begin": 5661, "end": 5665, "target": "#b22", "idx": 32}, {"begin": 5846, "end": 5850, "target": "#b26", "idx": 33}, {"begin": 6502, "end": 6506, "target": "#b27", "idx": 34}, {"begin": 6735, "end": 6739, "target": "#b28", "idx": 35}, {"begin": 6920, "end": 6924, "target": "#b19", "idx": 36}, {"begin": 6925, "end": 6928, "target": "#b29", "idx": 37}, {"begin": 10438, "end": 10441, "target": "#b0", "idx": 38}, {"begin": 10809, "end": 10813, "target": "#b26", "idx": 39}, {"begin": 11020, "end": 11023, "target": "#b0", "idx": 40}, {"begin": 11424, "end": 11428, "target": "#b30", "idx": 41}, {"begin": 11591, "end": 11595, "target": "#b31", "idx": 42}, {"begin": 12473, "end": 12477, "target": "#b32", "idx": 43}, {"begin": 13765, "end": 13769, "target": "#b26", "idx": 44}, {"begin": 14493, "end": 14497, "target": "#b24", "idx": 45}, {"begin": 14498, "end": 14501, "target": "#b25", "idx": 46}, {"begin": 15152, "end": 15156, "target": "#b36", "idx": 47}], "ReferenceToFootnote": [{"begin": 3430, "end": 3431, "target": "#foot_0", "idx": 0}, {"begin": 10534, "end": 10535, "target": "#foot_1", "idx": 1}, {"begin": 10667, "end": 10668, "target": "#foot_2", "idx": 2}, {"begin": 10732, "end": 10733, "target": "#foot_3", "idx": 3}, {"begin": 12359, "end": 12360, "target": "#foot_4", "idx": 4}], "SectionFootnote": [{"begin": 22656, "end": 23106, "idx": 0}], "ReferenceString": [{"begin": 23123, "end": 23290, "id": "b0", "idx": 0}, {"begin": 23292, "end": 23473, "id": "b1", "idx": 1}, {"begin": 23477, "end": 23720, "id": "b2", "idx": 2}, {"begin": 23724, "end": 23887, "id": "b3", "idx": 3}, {"begin": 23891, "end": 24167, "id": "b4", "idx": 4}, {"begin": 24171, "end": 24413, "id": "b5", "idx": 5}, {"begin": 24417, "end": 24627, "id": "b6", "idx": 6}, {"begin": 24631, "end": 24951, "id": "b7", "idx": 7}, {"begin": 24955, "end": 25197, "id": "b8", "idx": 8}, {"begin": 25201, "end": 25415, "id": "b9", "idx": 9}, {"begin": 25419, "end": 25686, "id": "b10", "idx": 10}, {"begin": 25690, "end": 25875, "id": "b11", "idx": 11}, {"begin": 25879, "end": 26053, "id": "b12", "idx": 12}, {"begin": 26057, "end": 26340, "id": "b13", "idx": 13}, {"begin": 26344, "end": 26616, "id": "b14", "idx": 14}, {"begin": 26620, "end": 26776, "id": "b15", "idx": 15}, {"begin": 26780, "end": 27005, "id": "b16", "idx": 16}, {"begin": 27009, "end": 27148, "id": "b17", "idx": 17}, {"begin": 27152, "end": 27333, "id": "b18", "idx": 18}, {"begin": 27337, "end": 27626, "id": "b19", "idx": 19}, {"begin": 27630, "end": 27755, "id": "b20", "idx": 20}, {"begin": 27759, "end": 28005, "id": "b21", "idx": 21}, {"begin": 28009, "end": 28251, "id": "b22", "idx": 22}, {"begin": 28255, "end": 28455, "id": "b23", "idx": 23}, {"begin": 28459, "end": 28769, "id": "b24", "idx": 24}, {"begin": 28773, "end": 28983, "id": "b25", "idx": 25}, {"begin": 28987, "end": 29186, "id": "b26", "idx": 26}, {"begin": 29190, "end": 29335, "id": "b27", "idx": 27}, {"begin": 29339, "end": 29596, "id": "b28", "idx": 28}, {"begin": 29600, "end": 29825, "id": "b29", "idx": 29}, {"begin": 29829, "end": 29977, "id": "b30", "idx": 30}, {"begin": 29981, "end": 30086, "id": "b31", "idx": 31}, {"begin": 30090, "end": 30256, "id": "b32", "idx": 32}, {"begin": 30260, "end": 30412, "id": "b33", "idx": 33}, {"begin": 30416, "end": 30520, "id": "b34", "idx": 34}, {"begin": 30524, "end": 30647, "id": "b35", "idx": 35}, {"begin": 30651, "end": 30879, "id": "b36", "idx": 36}, {"begin": 30883, "end": 31774, "id": "b37", "idx": 37}], "ReferenceToTable": [{"begin": 9165, "end": 9166, "idx": 0}, {"begin": 9290, "end": 9291, "idx": 1}, {"begin": 12924, "end": 12925, "target": "#tab_0", "idx": 2}, {"begin": 13528, "end": 13529, "target": "#tab_1", "idx": 3}, {"begin": 16528, "end": 16529, "target": "#tab_4", "idx": 4}, {"begin": 19239, "end": 19240, "target": "#tab_5", "idx": 5}, {"begin": 19686, "end": 19687, "target": "#tab_6", "idx": 6}], "Footnote": [{"begin": 22667, "end": 22793, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 22794, "end": 22874, "id": "foot_1", "n": "3", "idx": 1}, {"begin": 22875, "end": 22929, "id": "foot_2", "n": "4", "idx": 2}, {"begin": 22930, "end": 23006, "id": "foot_3", "n": "5", "idx": 3}, {"begin": 23007, "end": 23106, "id": "foot_4", "n": "6", "idx": 4}], "Paragraph": [{"begin": 53, "end": 1254, "idx": 0}, {"begin": 1272, "end": 1819, "idx": 1}, {"begin": 1820, "end": 1900, "idx": 2}, {"begin": 1901, "end": 2177, "idx": 3}, {"begin": 2178, "end": 2940, "idx": 4}, {"begin": 2941, "end": 3915, "idx": 5}, {"begin": 3932, "end": 4434, "idx": 6}, {"begin": 4435, "end": 4534, "idx": 7}, {"begin": 4535, "end": 4970, "idx": 8}, {"begin": 4971, "end": 5756, "idx": 9}, {"begin": 5757, "end": 6394, "idx": 10}, {"begin": 6395, "end": 6929, "idx": 11}, {"begin": 6941, "end": 7834, "idx": 12}, {"begin": 7835, "end": 8128, "idx": 13}, {"begin": 8129, "end": 8705, "idx": 14}, {"begin": 8783, "end": 8844, "idx": 15}, {"begin": 8917, "end": 9015, "idx": 16}, {"begin": 9016, "end": 9283, "idx": 17}, {"begin": 9284, "end": 9940, "idx": 18}, {"begin": 9966, "end": 10126, "idx": 19}, {"begin": 10127, "end": 10442, "idx": 20}, {"begin": 10443, "end": 11024, "idx": 21}, {"begin": 11025, "end": 11709, "idx": 22}, {"begin": 11710, "end": 12171, "idx": 23}, {"begin": 12172, "end": 12531, "idx": 24}, {"begin": 12548, "end": 12731, "idx": 25}, {"begin": 12773, "end": 13847, "idx": 26}, {"begin": 13879, "end": 14325, "idx": 27}, {"begin": 14386, "end": 14978, "idx": 28}, {"begin": 15029, "end": 15253, "idx": 29}, {"begin": 15275, "end": 16366, "idx": 30}, {"begin": 16367, "end": 16945, "idx": 31}, {"begin": 17059, "end": 17421, "idx": 32}, {"begin": 17422, "end": 18410, "idx": 33}, {"begin": 18411, "end": 19382, "idx": 34}, {"begin": 19383, "end": 19431, "idx": 35}, {"begin": 19432, "end": 21166, "idx": 36}, {"begin": 21181, "end": 22003, "idx": 37}, {"begin": 22004, "end": 22152, "idx": 38}, {"begin": 22190, "end": 22654, "idx": 39}], "SectionHeader": [{"begin": 0, "end": 1254, "idx": 0}], "SectionReference": [{"begin": 23108, "end": 31776, "idx": 0}], "Sentence": [{"begin": 53, "end": 122, "idx": 0}, {"begin": 123, "end": 217, "idx": 1}, {"begin": 218, "end": 336, "idx": 2}, {"begin": 337, "end": 504, "idx": 3}, {"begin": 505, "end": 721, "idx": 4}, {"begin": 722, "end": 986, "idx": 5}, {"begin": 987, "end": 1169, "idx": 6}, {"begin": 1170, "end": 1254, "idx": 7}, {"begin": 1272, "end": 1481, "idx": 8}, {"begin": 1482, "end": 1588, "idx": 9}, {"begin": 1589, "end": 1771, "idx": 10}, {"begin": 1772, "end": 1819, "idx": 11}, {"begin": 1820, "end": 1883, "idx": 12}, {"begin": 1884, "end": 1900, "idx": 13}, {"begin": 1901, "end": 1922, "idx": 14}, {"begin": 1923, "end": 2177, "idx": 15}, {"begin": 2178, "end": 2307, "idx": 16}, {"begin": 2308, "end": 2488, "idx": 17}, {"begin": 2489, "end": 2774, "idx": 18}, {"begin": 2775, "end": 2854, "idx": 19}, {"begin": 2855, "end": 2940, "idx": 20}, {"begin": 2941, "end": 3158, "idx": 21}, {"begin": 3159, "end": 3296, "idx": 22}, {"begin": 3297, "end": 3554, "idx": 23}, {"begin": 3555, "end": 3604, "idx": 24}, {"begin": 3605, "end": 3686, "idx": 25}, {"begin": 3687, "end": 3915, "idx": 26}, {"begin": 3932, "end": 4013, "idx": 27}, {"begin": 4014, "end": 4194, "idx": 28}, {"begin": 4195, "end": 4297, "idx": 29}, {"begin": 4298, "end": 4434, "idx": 30}, {"begin": 4435, "end": 4534, "idx": 31}, {"begin": 4535, "end": 4776, "idx": 32}, {"begin": 4777, "end": 4970, "idx": 33}, {"begin": 4971, "end": 5154, "idx": 34}, {"begin": 5155, "end": 5252, "idx": 35}, {"begin": 5253, "end": 5355, "idx": 36}, {"begin": 5356, "end": 5511, "idx": 37}, {"begin": 5512, "end": 5646, "idx": 38}, {"begin": 5647, "end": 5756, "idx": 39}, {"begin": 5757, "end": 5851, "idx": 40}, {"begin": 5852, "end": 5984, "idx": 41}, {"begin": 5985, "end": 6106, "idx": 42}, {"begin": 6107, "end": 6226, "idx": 43}, {"begin": 6227, "end": 6318, "idx": 44}, {"begin": 6319, "end": 6394, "idx": 45}, {"begin": 6395, "end": 6483, "idx": 46}, {"begin": 6484, "end": 6595, "idx": 47}, {"begin": 6596, "end": 6740, "idx": 48}, {"begin": 6741, "end": 6929, "idx": 49}, {"begin": 6941, "end": 7033, "idx": 50}, {"begin": 7034, "end": 7155, "idx": 51}, {"begin": 7156, "end": 7283, "idx": 52}, {"begin": 7284, "end": 7368, "idx": 53}, {"begin": 7369, "end": 7469, "idx": 54}, {"begin": 7470, "end": 7529, "idx": 55}, {"begin": 7530, "end": 7672, "idx": 56}, {"begin": 7673, "end": 7739, "idx": 57}, {"begin": 7740, "end": 7834, "idx": 58}, {"begin": 7835, "end": 7853, "idx": 59}, {"begin": 7854, "end": 7957, "idx": 60}, {"begin": 7958, "end": 8080, "idx": 61}, {"begin": 8081, "end": 8128, "idx": 62}, {"begin": 8129, "end": 8288, "idx": 63}, {"begin": 8289, "end": 8458, "idx": 64}, {"begin": 8459, "end": 8645, "idx": 65}, {"begin": 8646, "end": 8705, "idx": 66}, {"begin": 8783, "end": 8844, "idx": 67}, {"begin": 8917, "end": 9015, "idx": 68}, {"begin": 9016, "end": 9115, "idx": 69}, {"begin": 9116, "end": 9167, "idx": 70}, {"begin": 9168, "end": 9283, "idx": 71}, {"begin": 9284, "end": 9406, "idx": 72}, {"begin": 9407, "end": 9466, "idx": 73}, {"begin": 9467, "end": 9940, "idx": 74}, {"begin": 9966, "end": 10064, "idx": 75}, {"begin": 10065, "end": 10126, "idx": 76}, {"begin": 10127, "end": 10169, "idx": 77}, {"begin": 10170, "end": 10293, "idx": 78}, {"begin": 10294, "end": 10442, "idx": 79}, {"begin": 10443, "end": 10536, "idx": 80}, {"begin": 10537, "end": 10734, "idx": 81}, {"begin": 10735, "end": 10814, "idx": 82}, {"begin": 10815, "end": 11024, "idx": 83}, {"begin": 11025, "end": 11051, "idx": 84}, {"begin": 11052, "end": 11159, "idx": 85}, {"begin": 11160, "end": 11295, "idx": 86}, {"begin": 11296, "end": 11413, "idx": 87}, {"begin": 11414, "end": 11564, "idx": 88}, {"begin": 11565, "end": 11709, "idx": 89}, {"begin": 11710, "end": 11830, "idx": 90}, {"begin": 11831, "end": 11923, "idx": 91}, {"begin": 11924, "end": 12009, "idx": 92}, {"begin": 12010, "end": 12116, "idx": 93}, {"begin": 12117, "end": 12171, "idx": 94}, {"begin": 12172, "end": 12194, "idx": 95}, {"begin": 12195, "end": 12312, "idx": 96}, {"begin": 12313, "end": 12388, "idx": 97}, {"begin": 12389, "end": 12531, "idx": 98}, {"begin": 12548, "end": 12639, "idx": 99}, {"begin": 12640, "end": 12731, "idx": 100}, {"begin": 12773, "end": 12914, "idx": 101}, {"begin": 12915, "end": 12993, "idx": 102}, {"begin": 12994, "end": 13073, "idx": 103}, {"begin": 13074, "end": 13178, "idx": 104}, {"begin": 13179, "end": 13269, "idx": 105}, {"begin": 13270, "end": 13389, "idx": 106}, {"begin": 13390, "end": 13506, "idx": 107}, {"begin": 13507, "end": 13650, "idx": 108}, {"begin": 13651, "end": 13847, "idx": 109}, {"begin": 13879, "end": 14050, "idx": 110}, {"begin": 14051, "end": 14142, "idx": 111}, {"begin": 14143, "end": 14325, "idx": 112}, {"begin": 14386, "end": 14537, "idx": 113}, {"begin": 14538, "end": 14630, "idx": 114}, {"begin": 14631, "end": 14754, "idx": 115}, {"begin": 14755, "end": 14879, "idx": 116}, {"begin": 14880, "end": 14978, "idx": 117}, {"begin": 15029, "end": 15091, "idx": 118}, {"begin": 15092, "end": 15253, "idx": 119}, {"begin": 15275, "end": 15568, "idx": 120}, {"begin": 15569, "end": 15635, "idx": 121}, {"begin": 15636, "end": 15799, "idx": 122}, {"begin": 15800, "end": 15841, "idx": 123}, {"begin": 15842, "end": 15943, "idx": 124}, {"begin": 15944, "end": 16022, "idx": 125}, {"begin": 16023, "end": 16067, "idx": 126}, {"begin": 16068, "end": 16236, "idx": 127}, {"begin": 16237, "end": 16366, "idx": 128}, {"begin": 16367, "end": 16751, "idx": 129}, {"begin": 16752, "end": 16905, "idx": 130}, {"begin": 16906, "end": 16945, "idx": 131}, {"begin": 17059, "end": 17131, "idx": 132}, {"begin": 17132, "end": 17255, "idx": 133}, {"begin": 17256, "end": 17421, "idx": 134}, {"begin": 17422, "end": 17520, "idx": 135}, {"begin": 17521, "end": 17628, "idx": 136}, {"begin": 17629, "end": 17751, "idx": 137}, {"begin": 17752, "end": 17869, "idx": 138}, {"begin": 17870, "end": 18013, "idx": 139}, {"begin": 18014, "end": 18132, "idx": 140}, {"begin": 18133, "end": 18267, "idx": 141}, {"begin": 18268, "end": 18410, "idx": 142}, {"begin": 18411, "end": 18544, "idx": 143}, {"begin": 18545, "end": 18648, "idx": 144}, {"begin": 18649, "end": 18808, "idx": 145}, {"begin": 18809, "end": 18910, "idx": 146}, {"begin": 18911, "end": 18996, "idx": 147}, {"begin": 18997, "end": 19100, "idx": 148}, {"begin": 19101, "end": 19241, "idx": 149}, {"begin": 19242, "end": 19382, "idx": 150}, {"begin": 19383, "end": 19431, "idx": 151}, {"begin": 19432, "end": 19510, "idx": 152}, {"begin": 19511, "end": 19688, "idx": 153}, {"begin": 19689, "end": 19993, "idx": 154}, {"begin": 19994, "end": 20159, "idx": 155}, {"begin": 20160, "end": 20220, "idx": 156}, {"begin": 20221, "end": 20318, "idx": 157}, {"begin": 20319, "end": 20434, "idx": 158}, {"begin": 20435, "end": 20584, "idx": 159}, {"begin": 20585, "end": 20698, "idx": 160}, {"begin": 20699, "end": 20785, "idx": 161}, {"begin": 20786, "end": 20906, "idx": 162}, {"begin": 20907, "end": 21045, "idx": 163}, {"begin": 21046, "end": 21166, "idx": 164}, {"begin": 21181, "end": 21255, "idx": 165}, {"begin": 21256, "end": 21359, "idx": 166}, {"begin": 21360, "end": 21485, "idx": 167}, {"begin": 21486, "end": 21697, "idx": 168}, {"begin": 21698, "end": 21893, "idx": 169}, {"begin": 21894, "end": 22003, "idx": 170}, {"begin": 22004, "end": 22152, "idx": 171}, {"begin": 22190, "end": 22332, "idx": 172}, {"begin": 22333, "end": 22435, "idx": 173}, {"begin": 22436, "end": 22532, "idx": 174}, {"begin": 22533, "end": 22654, "idx": 175}], "ReferenceToFigure": [{"begin": 7243, "end": 7244, "idx": 0}, {"begin": 8126, "end": 8127, "idx": 1}, {"begin": 16065, "end": 16066, "target": "#fig_1", "idx": 2}, {"begin": 17528, "end": 17529, "idx": 3}, {"begin": 18021, "end": 18022, "idx": 4}, {"begin": 20240, "end": 20241, "target": "#fig_2", "idx": 5}, {"begin": 20592, "end": 20593, "target": "#fig_3", "idx": 6}], "Div": [{"begin": 53, "end": 1254, "idx": 0}, {"begin": 1257, "end": 3915, "idx": 1}, {"begin": 3917, "end": 6929, "idx": 2}, {"begin": 6931, "end": 9940, "idx": 3}, {"begin": 9942, "end": 12531, "idx": 4}, {"begin": 12533, "end": 12731, "idx": 5}, {"begin": 12733, "end": 13847, "idx": 6}, {"begin": 13849, "end": 14325, "idx": 7}, {"begin": 14327, "end": 15253, "idx": 8}, {"begin": 15255, "end": 21166, "idx": 9}, {"begin": 21168, "end": 22152, "idx": 10}, {"begin": 22154, "end": 22654, "idx": 11}], "SectionMain": [{"begin": 1254, "end": 22654, "idx": 0}], "ScholarlyEntity": [{"label": "Task", "begin": 53, "end": 74, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["Comm", "onsense", "\u0120reasoning"], "seq_scores": [0.9776265621185303, 0.9900704026222229, 0.9910778999328613], "text": "Commonsense reasoning", "score": 0.9862582882245382, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 107, "end": 121, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120deep", "\u0120learning"], "seq_scores": [0.8446321487426758, 0.8241828680038452], "text": " deep learning", "score": 0.8344075083732605, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 188, "end": 204, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.5703590512275696, 0.5625509023666382, 0.5567956566810608, 0.6740150451660156, 0.5215583443641663], "text": " Winograd Schema", "score": 0.5770557999610901, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 214, "end": 215, "seq_label": ["B-ReferenceLink"], "seq_token": ["1"], "seq_scores": [0.9890871047973633], "text": "1", "score": 0.9890871047973633, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 263, "end": 285, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.985561192035675, 0.9902877807617188, 0.9884049892425537], "text": " commonsense reasoning", "score": 0.9880846540133158, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 313, "end": 335, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120un", "super", "vised", "\u0120learning"], "seq_scores": [0.9772921800613403, 0.9899405837059021, 0.9891703724861145, 0.9885978698730469], "text": " unsupervised learning", "score": 0.986250251531601, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 475, "end": 497, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9782117605209351, 0.987591028213501, 0.9854556918144226], "text": " commonsense reasoning", "score": 0.9837528268496195, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 512, "end": 535, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation"], "seq_scores": [0.9698019027709961, 0.9877400994300842, 0.9889696836471558, 0.9904112815856934, 0.9910399317741394, 0.9909963607788086, 0.9902631044387817], "text": " Pronoun Disambiguation", "score": 0.9870317663465228, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 539, "end": 555, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.9316743016242981, 0.9451554417610168, 0.9445555210113525, 0.9693407416343689, 0.965976893901825], "text": " Winograd Schema", "score": 0.9513405799865723, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 811, "end": 824, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["-", "engine", "ered", "\u0120LM", "-", "1", "-", "B", "illion"], "seq_scores": [0.5228700637817383, 0.5196400880813599, 0.5052706599235535, 0.9766735434532166, 0.9929673671722412, 0.9929112792015076, 0.9930130839347839, 0.9917392730712891, 0.991477906703949], "text": " LM-1-Billion", "score": 0.8318403628137376, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 825, "end": 837, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "C", "rawl"], "seq_scores": [0.9470572471618652, 0.9730676412582397, 0.9799572825431824], "text": " CommonCrawl", "score": 0.9666940569877625, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 838, "end": 844, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9824079871177673, 0.9878867864608765, 0.9907189011573792], "text": " SQuAD", "score": 0.987004558245341, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 845, "end": 861, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Gutenberg", "\u0120Books"], "seq_scores": [0.7649708986282349, 0.9383338689804077], "text": " Gutenberg Books", "score": 0.8516523838043213, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 158, "end": 174, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120neural", "\u0120networks"], "seq_scores": [0.8833380341529846, 0.8597046732902527], "text": " neural networks", "score": 0.8715213537216187, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 290, "end": 306, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120neural", "\u0120networks"], "seq_scores": [0.8921017646789551, 0.8459864854812622], "text": " neural networks", "score": 0.8690441250801086, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 368, "end": 384, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9923081398010254, 0.9937082529067993], "text": " language models", "score": 0.9930081963539124, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 396, "end": 430, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120massive", "\u0120amount", "\u0120of", "\u0120unl", "abled", "\u0120data"], "seq_scores": [0.9844560623168945, 0.9897924661636353, 0.9902281761169434, 0.9867072105407715, 0.9745882153511047, 0.9975401163101196, 0.9972010850906372], "text": " a massive amount of unlabled data", "score": 0.9886447616985866, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 440, "end": 466, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120multiple", "\u0120choice", "\u0120questions"], "seq_scores": [0.8887261748313904, 0.8811600804328918, 0.8335887789726257], "text": " multiple choice questions", "score": 0.8678250114123026, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 567, "end": 578, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120models"], "seq_scores": [0.9963948130607605, 0.9952367544174194], "text": " our models", "score": 0.99581578373909, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 589, "end": 623, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120previous", "\u0120state", "-", "of", "-", "the", "-", "art", "\u0120methods"], "seq_scores": [0.8493296504020691, 0.9230769872665405, 0.9676952958106995, 0.9695143103599548, 0.9777503609657288, 0.974900484085083, 0.9734398126602173, 0.9646102786064148, 0.9368709921836853], "text": " previous state-of-the-art methods", "score": 0.948576463593377, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 730, "end": 768, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120an", "\u0120array", "\u0120of", "\u0120large", "\u0120R", "NN", "\u0120language", "\u0120models"], "seq_scores": [0.9773067831993103, 0.9591372013092041, 0.9461338520050049, 0.8167343139648438, 0.991730272769928, 0.9986252784729004, 0.9970335960388184, 0.9986407160758972], "text": " an array of large RNN language models", "score": 0.9606677517294884, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 866, "end": 886, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120customized", "\u0120corpus"], "seq_scores": [0.986967146396637, 0.9846587181091309, 0.9878930449485779], "text": " a customized corpus", "score": 0.9865063031514486, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 927, "end": 941, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.9575886726379395, 0.9858814477920532], "text": " training data", "score": 0.9717350602149963, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1019, "end": 1030, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120system"], "seq_scores": [0.9737368822097778, 0.9655126929283142], "text": " our system", "score": 0.969624787569046, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1345, "end": 1346, "seq_label": ["B-ReferenceLink"], "seq_token": ["2"], "seq_scores": [0.9949426054954529], "text": "2", "score": 0.9949426054954529, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1347, "end": 1349, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01203"], "seq_scores": [0.9935861229896545], "text": " 3", "score": 0.9935861229896545, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1350, "end": 1352, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01204"], "seq_scores": [0.9899134635925293], "text": " 4", "score": 0.9899134635925293, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1353, "end": 1355, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01205"], "seq_scores": [0.9888230562210083], "text": " 5", "score": 0.9888230562210083, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1356, "end": 1358, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01206"], "seq_scores": [0.9851517081260681], "text": " 6", "score": 0.9851517081260681, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1359, "end": 1361, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01207"], "seq_scores": [0.985293984413147], "text": " 7", "score": 0.985293984413147, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1362, "end": 1364, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01208"], "seq_scores": [0.9811170697212219], "text": " 8", "score": 0.9811170697212219, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1365, "end": 1367, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01209"], "seq_scores": [0.9706371426582336], "text": " 9", "score": 0.9706371426582336, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1368, "end": 1371, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012010"], "seq_scores": [0.9755527973175049], "text": " 10", "score": 0.9755527973175049, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1372, "end": 1375, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012011"], "seq_scores": [0.9700090289115906], "text": " 11", "score": 0.9700090289115906, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1376, "end": 1379, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012012"], "seq_scores": [0.9713371992111206], "text": " 12", "score": 0.9713371992111206, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1380, "end": 1383, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012013"], "seq_scores": [0.9752355813980103], "text": " 13", "score": 0.9752355813980103, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1384, "end": 1387, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012014"], "seq_scores": [0.9765754342079163], "text": " 14", "score": 0.9765754342079163, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1388, "end": 1391, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012015"], "seq_scores": [0.987239420413971], "text": " 15", "score": 0.987239420413971, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1414, "end": 1434, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120supervised", "\u0120learning"], "seq_scores": [0.9323763847351074, 0.9377844929695129], "text": " supervised learning", "score": 0.9350804388523102, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1619, "end": 1642, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120common", "\u0120sense", "\u0120reasoning"], "seq_scores": [0.9868148565292358, 0.9923539161682129, 0.9906657934188843], "text": " common sense reasoning", "score": 0.989944855372111, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1655, "end": 1681, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.9394580721855164, 0.9189992547035217, 0.8922525644302368, 0.8274431824684143, 0.7938936352729797, 0.8567206859588623], "text": " Winograd Schema Challenge", "score": 0.8714612325032552, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1683, "end": 1684, "seq_label": ["B-ReferenceLink"], "seq_token": ["1"], "seq_scores": [0.9931380152702332], "text": "1", "score": 0.9931380152702332, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1280, "end": 1301, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120deep", "\u0120neural", "\u0120networks"], "seq_scores": [0.7129458785057068, 0.6621558666229248, 0.616124153137207], "text": " deep neural networks", "score": 0.6637419660886129, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1506, "end": 1511, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120deep"], "seq_scores": [0.5922324657440186], "text": " deep", "score": 0.5922324657440186, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1564, "end": 1577, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120labeled", "\u0120data"], "seq_scores": [0.9776001572608948, 0.9857784509658813], "text": " labeled data", "score": 0.9816893041133881, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1692, "end": 1708, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120labeled", "\u0120set"], "seq_scores": [0.9905078411102295, 0.9926511645317078, 0.9872691631317139], "text": " the labeled set", "score": 0.9901427229245504, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1749, "end": 1770, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120hundreds", "\u0120of", "\u0120examples"], "seq_scores": [0.7569363713264465, 0.8699920773506165, 0.8231181502342224], "text": " hundreds of examples", "score": 0.8166821996370951, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1805, "end": 1818, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9873287677764893, 0.9957148432731628], "text": " this dataset", "score": 0.991521805524826, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2140, "end": 2154, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.7023247480392456, 0.9822126626968384], "text": " training data", "score": 0.842268705368042, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2236, "end": 2259, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120common", "\u0120sense", "\u0120reasoning"], "seq_scores": [0.9499426484107971, 0.9583410024642944, 0.948867917060852], "text": " common sense reasoning", "score": 0.9523838559786478, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2264, "end": 2306, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Task", "I-Method"], "seq_token": ["\u0120Win", "og", "rad", "\u0120schema", "\u0120multiple", "\u0120choice", "\u0120questions"], "seq_scores": [0.7409582138061523, 0.8717177510261536, 0.8656715750694275, 0.878337025642395, 0.5681872963905334, 0.37453871965408325, 0.5726196765899658], "text": " Winograd schema multiple choice questions", "score": 0.6960043225969587, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2264, "end": 2306, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Win", "og", "rad", "\u0120schema", "\u0120multiple", "\u0120choice", "\u0120questions"], "seq_scores": [0.9640598893165588, 0.9351526498794556, 0.9686144590377808, 0.9756823778152466, 0.9858209490776062, 0.9886199235916138, 0.9921162724494934], "text": " Winograd schema multiple choice questions", "score": 0.9728666458811078, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2340, "end": 2356, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9857228398323059, 0.9945830702781677], "text": " language models", "score": 0.9901529550552368, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2358, "end": 2361, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["L", "Ms"], "seq_scores": [0.973703920841217, 0.977187991142273], "text": "LMs", "score": 0.975445955991745, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2374, "end": 2407, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120amount", "\u0120of", "\u0120unl", "abel", "ed", "\u0120data"], "seq_scores": [0.9608001708984375, 0.962464451789856, 0.9677709937095642, 0.9502700567245483, 0.9316821694374084, 0.9977002739906311, 0.9975831508636475, 0.997448742389679], "text": " a large amount of unlabeled data", "score": 0.9707150012254715, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2417, "end": 2443, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120multiple", "\u0120choice", "\u0120questions"], "seq_scores": [0.9824581742286682, 0.9885110855102539, 0.9917029738426208], "text": " multiple choice questions", "score": 0.9875574111938477, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2452, "end": 2487, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120challenge", "\u0120and", "\u0120similar", "\u0120datasets"], "seq_scores": [0.9845414161682129, 0.988072395324707, 0.9913498163223267, 0.9490301012992859, 0.996894121170044], "text": " the challenge and similar datasets", "score": 0.9819775700569153, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2639, "end": 2643, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9760298728942871, 0.9819236397743225], "text": " LMs", "score": 0.9789767563343048, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2948, "end": 2971, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation"], "seq_scores": [0.9757011532783508, 0.9906542897224426, 0.9899283051490784, 0.9918069243431091, 0.9914692044258118, 0.9912200570106506, 0.9906241297721863], "text": " Pronoun Disambiguation", "score": 0.9887720091002328, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2975, "end": 2991, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.9728971123695374, 0.9824752807617188, 0.9814514517784119, 0.9845679998397827, 0.983340322971344], "text": " Winograd Schema", "score": 0.9809464335441589, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3132, "end": 3148, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["ated", "\u0120hand", "-", "engine", "ered"], "seq_scores": [0.6485822200775146, 0.508408784866333, 0.5436132550239563, 0.5471187233924866, 0.5261394381523132], "text": " hand-engineered", "score": 0.5547724843025208, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 3163, "end": 3186, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation"], "seq_scores": [0.9594592452049255, 0.9852035045623779, 0.9882832765579224, 0.9901703000068665, 0.9909530878067017, 0.9915790557861328, 0.990371584892273], "text": " Pronoun Disambiguation", "score": 0.9851457221167428, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3195, "end": 3202, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9740687608718872, 0.9773306846618652, 0.9765596985816956, 0.9775717854499817], "text": " PDP-60", "score": 0.9763827323913574, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 3301, "end": 3317, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.9634097218513489, 0.9716705679893494, 0.9700106978416443, 0.9756490588188171, 0.9716057777404785], "text": " Winograd Schema", "score": 0.9704691648483277, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3326, "end": 3334, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9798907041549683, 0.9844695329666138, 0.979972779750824, 0.9817264676094055], "text": " WSC-273", "score": 0.9815148711204529, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 3451, "end": 3467, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.8871766328811646, 0.9397662281990051, 0.937676191329956, 0.9444822669029236, 0.924638569355011], "text": " Winograd Schema", "score": 0.9267479777336121, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3003, "end": 3014, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120method"], "seq_scores": [0.9047642946243286, 0.8404227495193481], "text": " our method", "score": 0.8725935220718384, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3026, "end": 3060, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120previous", "\u0120state", "-", "of", "-", "the", "-", "art", "\u0120methods"], "seq_scores": [0.769308865070343, 0.8213045001029968, 0.8939736485481262, 0.8989704847335815, 0.9275913238525391, 0.9169998168945312, 0.9218201637268066, 0.8930061459541321, 0.7349687218666077], "text": " previous state-of-the-art methods", "score": 0.8642159634166293, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3161, "end": 3194, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation", "\u0120dataset"], "seq_scores": [0.9205358028411865, 0.9497388005256653, 0.9665725827217102, 0.9721305966377258, 0.9710921049118042, 0.9784769415855408, 0.9755603075027466, 0.9761914014816284, 0.963944137096405], "text": " a Pronoun Disambiguation dataset", "score": 0.9638047417004904, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3203, "end": 3214, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120method"], "seq_scores": [0.8865557312965393, 0.7996403574943542], "text": " our method", "score": 0.8430980443954468, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3299, "end": 3325, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120dataset"], "seq_scores": [0.8672658205032349, 0.8927085995674133, 0.8985699415206909, 0.9369422793388367, 0.9565436840057373, 0.9637578129768372, 0.9714611768722534], "text": " a Winograd Schema dataset", "score": 0.9267499021121434, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3335, "end": 3346, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120method"], "seq_scores": [0.8550848364830017, 0.7795989513397217], "text": " our method", "score": 0.8173418939113617, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3451, "end": 3477, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120questions"], "seq_scores": [0.9136133790016174, 0.8249160051345825, 0.9034774899482727, 0.9589628577232361, 0.9471230506896973, 0.9619606733322144], "text": " Winograd Schema questions", "score": 0.91834224263827, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3776, "end": 3787, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120system"], "seq_scores": [0.9060751795768738, 0.8711714148521423], "text": " our system", "score": 0.8886232972145081, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3932, "end": 3953, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Un", "super", "vised", "\u0120learning"], "seq_scores": [0.9871482253074646, 0.9952841401100159, 0.9955279231071472, 0.994061291217804], "text": "Unsupervised learning", "score": 0.9930053949356079, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4026, "end": 4041, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mik", "ol", "ov", "\u0120et", "\u0120al", "."], "seq_scores": [0.8953330516815186, 0.9175025820732117, 0.9364922642707825, 0.9027280211448669, 0.9337162971496582, 0.8820604085922241], "text": " Mikolov et al.", "score": 0.911305437485377, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4043, "end": 4046, "seq_label": ["B-ReferenceLink", "I-ReferenceLink"], "seq_token": ["16", ","], "seq_scores": [0.987612783908844, 0.5620630979537964], "text": "16,", "score": 0.7748379409313202, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4046, "end": 4049, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012017"], "seq_scores": [0.9861654043197632], "text": " 17", "score": 0.9861654043197632, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4113, "end": 4126, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120word", "\u0120vectors"], "seq_scores": [0.8090777397155762, 0.8226715922355652], "text": " word vectors", "score": 0.8158746659755707, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4233, "end": 4251, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.8862959742546082, 0.9259073138237], "text": " language modeling", "score": 0.906101644039154, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4320, "end": 4346, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120questions"], "seq_scores": [0.6039527654647827, 0.5555177330970764, 0.5669316649436951, 0.5923930406570435, 0.4816160798072815, 0.49172717332839966], "text": " Winograd Schema questions", "score": 0.5486897428830465, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4420, "end": 4433, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120word", "\u0120vectors"], "seq_scores": [0.7168262600898743, 0.6930984258651733], "text": " word vectors", "score": 0.7049623429775238, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4233, "end": 4242, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120language"], "seq_scores": [0.6016678810119629], "text": " language", "score": 0.6016678810119629, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4400, "end": 4404, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9772497415542603, 0.9713647365570068], "text": " LMs", "score": 0.9743072390556335, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4518, "end": 4520, "seq_label": ["I-MLModel", "I-MLModel", "I-MLModel", "B-ReferenceLink"], "seq_token": ["ural", "\u0120L", "Ms", "18"], "seq_scores": [0.5456765294075012, 0.573890745639801, 0.5634514093399048, 0.9949669241905212], "text": "18", "score": 0.6694964021444321, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4521, "end": 4524, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012019"], "seq_scores": [0.9927143454551697], "text": " 19", "score": 0.9927143454551697, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4525, "end": 4528, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012020"], "seq_scores": [0.9945443868637085], "text": " 20", "score": 0.9945443868637085, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4529, "end": 4532, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012021"], "seq_scores": [0.9933858513832092], "text": " 21", "score": 0.9933858513832092, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4435, "end": 4437, "seq_label": ["B-MLModelGeneric"], "seq_token": ["Ne"], "seq_scores": [0.5113514065742493], "text": "Ne", "score": 0.5113514065742493, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4539, "end": 4541, "seq_label": ["B-ReferenceLink"], "seq_token": ["18"], "seq_scores": [0.9952623844146729], "text": "18", "score": 0.9952623844146729, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4542, "end": 4545, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012019"], "seq_scores": [0.9933682084083557], "text": " 19", "score": 0.9933682084083557, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4546, "end": 4549, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012020"], "seq_scores": [0.9942571520805359], "text": " 20", "score": 0.9942571520805359, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4550, "end": 4553, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012021"], "seq_scores": [0.9931488633155823], "text": " 21", "score": 0.9931488633155823, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4614, "end": 4622, "seq_label": ["B-Method"], "seq_token": ["\u0120feature"], "seq_scores": [0.5047731399536133], "text": " feature", "score": 0.5047731399536133, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4705, "end": 4729, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120document", "\u0120classification"], "seq_scores": [0.9899824261665344, 0.9940699338912964], "text": " document classification", "score": 0.9920261800289154, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4730, "end": 4750, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120translation"], "seq_scores": [0.9895871877670288, 0.9897788166999817], "text": " machine translation", "score": 0.9896830022335052, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4751, "end": 4770, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120question", "\u0120answering"], "seq_scores": [0.9879186749458313, 0.989343523979187], "text": " question answering", "score": 0.9886310994625092, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4583, "end": 4599, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120pre", "-", "trained", "\u0120L", "Ms"], "seq_scores": [0.9836944937705994, 0.9809550642967224, 0.9845308065414429, 0.9714378714561462, 0.994390070438385], "text": " pre-trained LMs", "score": 0.9830016613006591, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4812, "end": 4816, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9889503121376038, 0.9880344271659851], "text": " LMs", "score": 0.9884923696517944, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4827, "end": 4862, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120massive", "\u0120amount", "\u0120of", "\u0120unl", "abel", "ed", "\u0120data"], "seq_scores": [0.9858046174049377, 0.991736114025116, 0.9929681420326233, 0.9847936034202576, 0.9766775369644165, 0.998175859451294, 0.9980542659759521, 0.9978905320167542], "text": " a massive amount of unlabeled data", "score": 0.9907625839114189, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5003, "end": 5029, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.9490578174591064, 0.9787319898605347, 0.9807927012443542, 0.9762951135635376, 0.9791048169136047, 0.7455535531044006], "text": " Winograd Schema Challenge", "score": 0.9349226653575897, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5066, "end": 5086, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120annot", "ated", "\u0120knowledge"], "seq_scores": [0.5459536910057068, 0.6054241061210632, 0.5939667820930481], "text": " annotated knowledge", "score": 0.5817815264066061, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5093, "end": 5114, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120rule", "-", "based", "\u0120reasoning"], "seq_scores": [0.9340575337409973, 0.933247983455658, 0.9346551895141602, 0.9278148412704468], "text": " rule-based reasoning", "score": 0.9324438869953156, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5118, "end": 5140, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120hand", "-", "crafted", "\u0120features"], "seq_scores": [0.5264542102813721, 0.549547016620636, 0.5783224105834961, 0.5313569903373718], "text": " hand-crafted features", "score": 0.546420156955719, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5142, "end": 5144, "seq_label": ["B-ReferenceLink"], "seq_token": ["22"], "seq_scores": [0.9946803450584412], "text": "22", "score": 0.9946803450584412, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5145, "end": 5148, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012023"], "seq_scores": [0.9935965538024902], "text": " 23", "score": 0.9935965538024902, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5149, "end": 5152, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012024"], "seq_scores": [0.9916940331459045], "text": " 24", "score": 0.9916940331459045, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5169, "end": 5183, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Rahman", "\u0120and", "\u0120Ng"], "seq_scores": [0.8906411528587341, 0.7980109453201294, 0.9343889951705933], "text": " Rahman and Ng", "score": 0.8743470311164856, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5185, "end": 5187, "seq_label": ["B-ReferenceLink"], "seq_token": ["25"], "seq_scores": [0.983627438545227], "text": "25", "score": 0.983627438545227, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5195, "end": 5212, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120annot", "ators"], "seq_scores": [0.8931952118873596, 0.8996250033378601, 0.8948519825935364], "text": " human annotators", "score": 0.8958907326062521, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 5336, "end": 5354, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Google", "\u0120Search", "\u0120API"], "seq_scores": [0.8459129929542542, 0.7667617797851562, 0.5565927624702454], "text": " Google Search API", "score": 0.7230891784032186, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5355, "end": 5368, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sharma", "\u0120et", "\u0120al"], "seq_scores": [0.8660633563995361, 0.6617822051048279, 0.779507040977478], "text": " Sharma et al", "score": 0.769117534160614, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5371, "end": 5373, "seq_label": ["B-ReferenceLink"], "seq_token": ["26"], "seq_scores": [0.9899472594261169], "text": "26", "score": 0.9899472594261169, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5384, "end": 5400, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120semantic", "\u0120parser"], "seq_scores": [0.3778500556945801, 0.7090499997138977], "text": " semantic parser", "score": 0.5434500277042389, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 5448, "end": 5462, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Google", "\u0120Search"], "seq_scores": [0.8534786701202393, 0.7695509195327759], "text": " Google Search", "score": 0.8115147948265076, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5522, "end": 5531, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sch", "\u00c3\u00bc", "ller"], "seq_scores": [0.7649819254875183, 0.8508404493331909, 0.8597235679626465], "text": " Sch\u00fcller", "score": 0.8251819809277853, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5533, "end": 5535, "seq_label": ["B-ReferenceLink"], "seq_token": ["24"], "seq_scores": [0.9842989444732666], "text": "24", "score": 0.9842989444732666, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5614, "end": 5636, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120cognitive", "\u0120lingu", "istics"], "seq_scores": [0.5157873630523682, 0.6597459316253662, 0.6706488728523254], "text": " cognitive linguistics", "score": 0.6153940558433533, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5646, "end": 5659, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bailey", "\u0120et", "\u0120al"], "seq_scores": [0.8674559593200684, 0.6922085285186768, 0.8185205459594727], "text": " Bailey et al", "score": 0.7927283445994059, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5662, "end": 5664, "seq_label": ["B-ReferenceLink"], "seq_token": ["23"], "seq_scores": [0.9878690838813782], "text": "23", "score": 0.9878690838813782, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5066, "end": 5092, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120annot", "ated", "\u0120knowledge", "\u0120bases"], "seq_scores": [0.7022777795791626, 0.6868100166320801, 0.7013922333717346, 0.7333604693412781], "text": " annotated knowledge bases", "score": 0.7059601247310638, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5221, "end": 5251, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120more", "\u0120supervised", "\u0120training", "\u0120data"], "seq_scores": [0.9524242281913757, 0.9061219692230225, 0.9968585968017578, 0.9960216879844666], "text": " more supervised training data", "score": 0.9628566205501556, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5252, "end": 5264, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Their", "\u0120model"], "seq_scores": [0.9887380599975586, 0.9864889979362488], "text": " Their model", "score": 0.9876135289669037, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5326, "end": 5331, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.7261441349983215], "text": " data", "score": 0.7261441349983215, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5382, "end": 5400, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120semantic", "\u0120parser"], "seq_scores": [0.7451575994491577, 0.6194054484367371, 0.6433776617050171], "text": " a semantic parser", "score": 0.6693135698636373, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5428, "end": 5440, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120query", "\u0120texts"], "seq_scores": [0.5316158533096313, 0.7092618942260742], "text": " query texts", "score": 0.6204388737678528, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5709, "end": 5745, "seq_label": ["I-MLModelGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120parser", "\u0120expensive", "\u0120annot", "ated", "\u0120knowledge", "\u0120bases"], "seq_scores": [0.5122891664505005, 0.5706942677497864, 0.5204595923423767, 0.7630107998847961, 0.7358987331390381, 0.7228854894638062], "text": " expensive annotated knowledge bases", "score": 0.637539674838384, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 5799, "end": 5809, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120skip", "-", "gram"], "seq_scores": [0.6806580424308777, 0.6478428244590759, 0.6024013757705688], "text": " skip-gram", "score": 0.6436340808868408, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5824, "end": 5845, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120word", "\u0120representations"], "seq_scores": [0.8375507593154907, 0.9159775972366333], "text": " word representations", "score": 0.876764178276062, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5847, "end": 5849, "seq_label": ["B-ReferenceLink"], "seq_token": ["27"], "seq_scores": [0.9948427081108093], "text": "27", "score": 0.9948427081108093, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5947, "end": 5977, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Knowledge", "\u0120Enhanced", "\u0120Emb", "edd", "ings"], "seq_scores": [0.968663215637207, 0.9869785308837891, 0.9905396699905396, 0.9873656034469604, 0.9844595193862915], "text": " Knowledge Enhanced Embeddings", "score": 0.9836013078689575, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5979, "end": 5982, "seq_label": ["B-Method", "I-Method"], "seq_token": ["K", "EE"], "seq_scores": [0.9288822412490845, 0.9700848460197449], "text": "KEE", "score": 0.9494835436344147, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5986, "end": 6006, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120semantic", "\u0120similarity"], "seq_scores": [0.3558242619037628, 0.633981466293335], "text": " semantic similarity", "score": 0.4949028640985489, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6078, "end": 6082, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120K", "EE"], "seq_scores": [0.896508514881134, 0.9739840626716614], "text": " KEE", "score": 0.9352462887763977, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6243, "end": 6256, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120un", "super", "vised"], "seq_scores": [0.37036973237991333, 0.7616007328033447, 0.776228129863739], "text": " unsupervised", "score": 0.636066198348999, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6318, "end": 6340, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Un", "super", "vised", "\u0120training"], "seq_scores": [0.958442747592926, 0.9798387289047241, 0.9812594056129456, 0.9730456471443176], "text": " Unsupervised training", "score": 0.9731466323137283, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5757, "end": 5782, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["The", "\u0120current", "\u0120best", "\u0120approach"], "seq_scores": [0.8803837299346924, 0.8844610452651978, 0.8980887532234192, 0.8409803509712219], "text": "The current best approach", "score": 0.8759784698486328, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5795, "end": 5815, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120skip", "-", "gram", "\u0120model"], "seq_scores": [0.8617901802062988, 0.9100353717803955, 0.9189172387123108, 0.8944275379180908, 0.8451921343803406], "text": " the skip-gram model", "score": 0.8860724925994873, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5851, "end": 5861, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120model"], "seq_scores": [0.9900934100151062, 0.9887381792068481], "text": " The model", "score": 0.9894157946109772, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6093, "end": 6097, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.5707126259803772], "text": " the", "score": 0.5707126259803772, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6106, "end": 6123, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120final", "\u0120system"], "seq_scores": [0.9630372524261475, 0.9627209305763245, 0.9560472369194031], "text": " The final system", "score": 0.960601806640625, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6149, "end": 6184, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120supervised", "\u0120and", "\u0120un", "super", "vised", "\u0120models"], "seq_scores": [0.6269899010658264, 0.9954277276992798, 0.9900457262992859, 0.9981549382209778, 0.9978163242340088, 0.9972103238105774], "text": " supervised and unsupervised models", "score": 0.934274156888326, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6239, "end": 6263, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120un", "super", "vised", "\u0120method"], "seq_scores": [0.8324693441390991, 0.8732696175575256, 0.8848631381988525, 0.8671295046806335, 0.8884941935539246], "text": " our unsupervised method", "score": 0.8692451596260071, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6351, "end": 6364, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9946770668029785, 0.9967303276062012, 0.9903766512870789], "text": " text corpora", "score": 0.9939280152320862, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6419, "end": 6441, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120reading", "\u0120comprehension"], "seq_scores": [0.959717869758606, 0.9437726140022278], "text": " reading comprehension", "score": 0.9517452418804169, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6503, "end": 6505, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-ReferenceLink"], "seq_token": ["\u0120et", "\u0120al", ".", "28"], "seq_scores": [0.6467086672782898, 0.7979137301445007, 0.6706675291061401, 0.9890768527984619], "text": "28", "score": 0.7760916948318481, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6511, "end": 6531, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120bi", "-", "direction", "al", "\u0120R", "NN", "s"], "seq_scores": [0.8658384084701538, 0.9394840598106384, 0.941297709941864, 0.9373337030410767, 0.9190449714660645, 0.9189426302909851, 0.8825291395187378], "text": " bi-directional RNNs", "score": 0.9149243746485028, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6534, "end": 6556, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120predict", "\u0120the", "\u0120last", "\u0120word"], "seq_scores": [0.510674774646759, 0.5864932537078857, 0.7310236096382141, 0.7396169304847717], "text": " predict the last word", "score": 0.6419521421194077, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6576, "end": 6584, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "B", "ADA"], "seq_scores": [0.8859001398086548, 0.8797702789306641, 0.8251522779464722, 0.8023676872253418], "text": " LAMBADA", "score": 0.8482975959777832, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6668, "end": 6690, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Store", "\u0120Close", "\u0120Test", "\u01202017"], "seq_scores": [0.5736079216003418, 0.6716743111610413, 0.4510501027107239, 0.4476254880428314], "text": " Store Close Test 2017", "score": 0.5359894558787346, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6736, "end": 6738, "seq_label": ["B-ReferenceLink"], "seq_token": ["29"], "seq_scores": [0.9955539107322693], "text": "29", "score": 0.9955539107322693, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6786, "end": 6807, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120good", "\u0120word", "\u0120embed", "d", "ings"], "seq_scores": [0.6146057844161987, 0.43962016701698303, 0.9321821331977844, 0.9175330400466919, 0.906987726688385], "text": " good word embeddings", "score": 0.7621857702732087, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6900, "end": 6919, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120question", "\u0120answering"], "seq_scores": [0.9789494276046753, 0.9848129153251648], "text": " question answering", "score": 0.98188117146492, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6921, "end": 6923, "seq_label": ["B-ReferenceLink"], "seq_token": ["20"], "seq_scores": [0.996523916721344], "text": "20", "score": 0.996523916721344, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6924, "end": 6927, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012030"], "seq_scores": [0.9943405389785767], "text": " 30", "score": 0.9943405389785767, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6400, "end": 6416, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9854295253753662, 0.9919328093528748], "text": " language models", "score": 0.9886811673641205, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6511, "end": 6531, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120bi", "-", "direction", "al", "\u0120R", "NN", "s"], "seq_scores": [0.9644914865493774, 0.9824526309967041, 0.9843564629554749, 0.9845045804977417, 0.9814607501029968, 0.9739412069320679, 0.9506179094314575], "text": " bi-directional RNNs", "score": 0.9745464324951172, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6606, "end": 6610, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9744060635566711, 0.9666783809661865], "text": " LMs", "score": 0.9705422222614288, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6648, "end": 6661, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120class", "ifier"], "seq_scores": [0.9785940647125244, 0.9762933254241943, 0.9882879853248596], "text": " a classifier", "score": 0.9810584584871928, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6762, "end": 6766, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9660125970840454, 0.973785936832428], "text": " LMs", "score": 0.9698992669582367, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7048, "end": 7071, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120core", "ference", "\u0120resolution"], "seq_scores": [0.9629637002944946, 0.9649964570999146, 0.9736762046813965], "text": " coreference resolution", "score": 0.9672121206919352, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7190, "end": 7208, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.9206897020339966, 0.9637473225593567], "text": " language modeling", "score": 0.9422185122966766, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7807, "end": 7823, "seq_label": ["B-Dataset", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma"], "seq_scores": [0.6387965679168701, 0.5462993383407593, 0.588202953338623, 0.6371303796768188, 0.612234354019165], "text": " Winograd Schema", "score": 0.6045327186584473, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7368, "end": 7379, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Our", "\u0120method"], "seq_scores": [0.716049075126648, 0.6633181571960449], "text": " Our method", "score": 0.6896836161613464, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7481, "end": 7487, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120an", "\u0120LM"], "seq_scores": [0.9172798991203308, 0.9043017029762268], "text": " an LM", "score": 0.9107908010482788, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7861, "end": 7865, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9868038296699524, 0.977196216583252], "text": " LMs", "score": 0.9820000231266022, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7880, "end": 7893, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9944571256637573, 0.9984729886054993, 0.9932553768157959], "text": " text corpora", "score": 0.9953951636950175, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7939, "end": 7947, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120natural"], "seq_scores": [0.5398951172828674], "text": " natural", "score": 0.5398951172828674, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7975, "end": 7979, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.984255850315094, 0.9788363575935364], "text": " LMs", "score": 0.9815461039543152, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8065, "end": 8079, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.9933071732521057, 0.995276689529419], "text": " training data", "score": 0.9942919313907623, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 8303, "end": 8328, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120trained", "\u0120language", "\u0120model"], "seq_scores": [0.9929426908493042, 0.9952114224433899, 0.993863582611084, 0.9954190254211426], "text": " a trained language model", "score": 0.9943591803312302, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9200, "end": 9225, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring", "\u0120strategy"], "seq_scores": [0.712481677532196, 0.7755401134490967, 0.5759066343307495], "text": " partial scoring strategy", "score": 0.6879761417706808, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9250, "end": 9282, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120naive", "\u0120full", "\u0120scoring", "\u0120strategy"], "seq_scores": [0.558125376701355, 0.68329918384552, 0.8765364289283752, 0.8264951705932617, 0.7321888208389282], "text": " the naive full scoring strategy", "score": 0.7353289961814881, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10003, "end": 10025, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9856148958206177, 0.9934418201446533, 0.9904423356056213], "text": " commonsense reasoning", "score": 0.9898330171902975, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 10029, "end": 10037, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120L", "Ms"], "seq_scores": [0.9446555972099304, 0.9517714381217957, 0.9283463358879089], "text": " the LMs", "score": 0.9415911237398783, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10079, "end": 10101, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.991520345211029, 0.9971756935119629, 0.9976518750190735, 0.9921002984046936], "text": " training text corpora", "score": 0.9946120530366898, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10140, "end": 10162, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Commons", "ense", "\u0120Reason", "ing"], "seq_scores": [0.9689062237739563, 0.9864454865455627, 0.9848390817642212, 0.9844820499420166], "text": " Commonsense Reasoning", "score": 0.9811682105064392, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10230, "end": 10262, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation", "\u0120Problems"], "seq_scores": [0.9728807806968689, 0.988248348236084, 0.9917499423027039, 0.9928268790245056, 0.9931627511978149, 0.9933282136917114, 0.9919628500938416, 0.9157605171203613], "text": " Pronoun Disambiguation Problems", "score": 0.9799900352954865, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10266, "end": 10292, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.9661498665809631, 0.9863878488540649, 0.9903770089149475, 0.9909979104995728, 0.9904038906097412, 0.9898960590362549], "text": " Winograd Schema Challenge", "score": 0.9857020974159241, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10361, "end": 10373, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Turing", "\u0120Test"], "seq_scores": [0.5940278768539429, 0.4395847022533417], "text": " Turing Test", "score": 0.5168062895536423, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 10439, "end": 10440, "seq_label": ["B-ReferenceLink"], "seq_token": ["1"], "seq_scores": [0.984157919883728], "text": "1", "score": 0.984157919883728, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10504, "end": 10510, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["PD", "P", "-", "60"], "seq_scores": [0.9846581220626831, 0.9922695159912109, 0.9859089255332947, 0.9885110855102539], "text": "PDP-60", "score": 0.9878369122743607, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10686, "end": 10693, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["W", "SC", "-", "273"], "seq_scores": [0.9811914563179016, 0.9857606291770935, 0.9859719276428223, 0.987146258354187], "text": "WSC-273", "score": 0.9850175678730011, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 10810, "end": 10812, "seq_label": ["B-ReferenceLink"], "seq_token": ["27"], "seq_scores": [0.9928761720657349], "text": "27", "score": 0.9928761720657349, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11004, "end": 11016, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["Google", "-", "proof"], "seq_scores": [0.927058219909668, 0.9475019574165344, 0.9516614079475403], "text": "Google-proof", "score": 0.9420738617579142, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11021, "end": 11022, "seq_label": ["B-ReferenceLink"], "seq_token": ["1"], "seq_scores": [0.9925464987754822], "text": "1", "score": 0.9925464987754822, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10469, "end": 10502, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120original", "\u0120set", "\u0120of", "\u012060", "\u0120questions"], "seq_scores": [0.9907826781272888, 0.9937637448310852, 0.9962640404701233, 0.9497963190078735, 0.9276747107505798, 0.9963710308074951], "text": " the original set of 60 questions", "score": 0.975775420665741, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10575, "end": 10588, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012062", "\u0120questions"], "seq_scores": [0.9909353852272034, 0.9973793029785156], "text": " 62 questions", "score": 0.9941573441028595, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10593, "end": 10613, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120development", "\u0120set"], "seq_scores": [0.8958038091659546, 0.9981322884559631, 0.9967605471611023], "text": " the development set", "score": 0.9635655482610067, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10640, "end": 10665, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120original", "\u0120smaller", "\u0120set"], "seq_scores": [0.9939557909965515, 0.9962173104286194, 0.9980091452598572, 0.9972314238548279], "text": " the original smaller set", "score": 0.996353417634964, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10836, "end": 10850, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120273", "\u0120questions"], "seq_scores": [0.9815888404846191, 0.9970757961273193], "text": " 273 questions", "score": 0.9893323183059692, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10988, "end": 11001, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9837215542793274, 0.9929466843605042, 0.9683076739311218], "text": " text corpora", "score": 0.9816586375236511, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11398, "end": 11403, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120soft"], "seq_scores": [0.4453592598438263], "text": " soft", "score": 0.4453592598438263, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11425, "end": 11427, "seq_label": ["B-ReferenceLink"], "seq_token": ["31"], "seq_scores": [0.9950657486915588], "text": "31", "score": 0.9950657486915588, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11439, "end": 11459, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120importance", "\u0120sampling"], "seq_scores": [0.9579799771308899, 0.96455317735672], "text": " importance sampling", "score": 0.9612665772438049, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11585, "end": 11590, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120L", "ST", "M"], "seq_scores": [0.8866395950317383, 0.9489632248878479, 0.9469968676567078], "text": " LSTM", "score": 0.927533229192098, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11592, "end": 11594, "seq_label": ["B-ReferenceLink"], "seq_token": ["32"], "seq_scores": [0.9945091605186462], "text": "32", "score": 0.9945091605186462, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11025, "end": 11050, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["Rec", "urrent", "\u0120language", "\u0120models"], "seq_scores": [0.9887771010398865, 0.9953382015228271, 0.9978848099708557, 0.9976720213890076], "text": "Recurrent language models", "score": 0.9949180334806442, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11063, "end": 11090, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120types", "\u0120of", "\u0120recurrent", "\u0120L", "Ms"], "seq_scores": [0.921405553817749, 0.8969424962997437, 0.9117202162742615, 0.6712784171104431, 0.9957069754600525, 0.9960305094718933], "text": " two types of recurrent LMs", "score": 0.8988473614056905, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11091, "end": 11095, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120one"], "seq_scores": [0.6522378325462341], "text": " one", "score": 0.6522378325462341, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11105, "end": 11117, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120word", "\u0120inputs"], "seq_scores": [0.896504819393158, 0.7506219744682312], "text": " word inputs", "score": 0.8235633969306946, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11121, "end": 11131, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120other"], "seq_scores": [0.8482667803764343, 0.8272643685340881], "text": " the other", "score": 0.8377655744552612, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11141, "end": 11158, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120character", "\u0120inputs"], "seq_scores": [0.8304721117019653, 0.7295528650283813], "text": " character inputs", "score": 0.7800124883651733, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11284, "end": 11294, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120en", "semb", "les"], "seq_scores": [0.9110139012336731, 0.9011085629463196, 0.8704128265380859], "text": " ensembles", "score": 0.8941784302393595, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11303, "end": 11311, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120L", "Ms"], "seq_scores": [0.985806405544281, 0.9884968996047974, 0.994289755821228], "text": " our LMs", "score": 0.9895310203234354, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11339, "end": 11358, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120vocabulary"], "seq_scores": [0.9196020364761353, 0.9136921167373657, 0.858988344669342], "text": " a large vocabulary", "score": 0.897427499294281, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11360, "end": 11370, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["800", "K", "\u0120words"], "seq_scores": [0.8720592260360718, 0.889762818813324, 0.8635967373847961], "text": "800K words", "score": 0.875139594078064, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11485, "end": 11508, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01208", ",", "192", "\u0120negative", "\u0120samples"], "seq_scores": [0.9405021667480469, 0.9553626775741577, 0.9801506996154785, 0.9877179861068726, 0.9836926460266113], "text": " 8,192 negative samples", "score": 0.9694852352142334, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 12103, "end": 12108, "seq_label": ["I-Method", "I-Method", "I-Method", "B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120big", "\u0120embed", "ding", "\u0120L", "ST", "M"], "seq_scores": [0.4882718026638031, 0.5513327121734619, 0.5206987857818604, 0.8514152765274048, 0.9561558961868286, 0.9589221477508545], "text": " LSTM", "score": 0.7211327701807022, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 11713, "end": 11720, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9016972184181213], "text": " models", "score": 0.9016972184181213, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12135, "end": 12143, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120L", "Ms"], "seq_scores": [0.9447370171546936, 0.9430424571037292, 0.9758235812187195], "text": " our LMs", "score": 0.9545343518257141, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12332, "end": 12345, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120LM", "-", "1", "-", "B", "illion"], "seq_scores": [0.9867413640022278, 0.9953561425209045, 0.9950645565986633, 0.9948246479034424, 0.9938592314720154, 0.9942376613616943], "text": " LM-1-Billion", "score": 0.9933472673098246, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12346, "end": 12360, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "C", "rawl", "\u01206"], "seq_scores": [0.9823223948478699, 0.9927623867988586, 0.9937673807144165, 0.9844617247581482], "text": " CommonCrawl 6", "score": 0.9883284717798233, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12361, "end": 12367, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9904609322547913, 0.9943382143974304, 0.9951561689376831], "text": " SQuAD", "score": 0.9933184385299683, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12371, "end": 12387, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Gutenberg", "\u0120Books"], "seq_scores": [0.9054433107376099, 0.9819802045822144], "text": " Gutenberg Books", "score": 0.9437117576599121, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12392, "end": 12398, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9925230145454407, 0.9952512979507446, 0.9957167506217957], "text": " SQuAD", "score": 0.994497021039327, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12436, "end": 12472, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Stanford", "\u0120Question", "-", "An", "sw", "ering", "\u0120Dat", "as", "et"], "seq_scores": [0.8790498971939087, 0.9641368985176086, 0.9724430441856384, 0.9723330736160278, 0.9736931324005127, 0.9708431363105774, 0.9144462943077087, 0.9276230335235596, 0.952168881893158], "text": " Stanford Question-Answering Dataset", "score": 0.9474152657720778, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12474, "end": 12476, "seq_label": ["B-ReferenceLink"], "seq_token": ["33"], "seq_scores": [0.9940866231918335], "text": "33", "score": 0.9940866231918335, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12172, "end": 12193, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["Training", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9843527674674988, 0.9977425336837769, 0.9979409575462341, 0.9895184636116028], "text": "Training text corpora", "score": 0.9923886805772781, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12220, "end": 12250, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120different", "\u0120text", "\u0120cop", "ora"], "seq_scores": [0.9855390787124634, 0.9770021438598633, 0.9963850975036621, 0.9978169202804565, 0.9961115717887878], "text": " several different text copora", "score": 0.9905709624290466, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12275, "end": 12289, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.8444995880126953, 0.7344970703125], "text": " training data", "score": 0.7894983291625977, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12410, "end": 12427, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120context", "\u0120passages"], "seq_scores": [0.9229442477226257, 0.9580190181732178], "text": " context passages", "score": 0.9404816329479218, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12485, "end": 12518, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["as", "et", "\u0120its", "\u0120training", "\u0120and", "\u0120validation", "\u0120sets"], "seq_scores": [0.5058790445327759, 0.5103241801261902, 0.9799329042434692, 0.6884134411811829, 0.9858618974685669, 0.9772976636886597, 0.9955761432647705], "text": " its training and validation sets", "score": 0.8061836106436593, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12619, "end": 12626, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.7490297555923462, 0.6835868954658508, 0.5964475870132446, 0.6014188528060913], "text": " PDP-60", "score": 0.6576207727193832, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12630, "end": 12638, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9909589290618896, 0.9940154552459717, 0.9906248450279236, 0.9895437955856323], "text": " WSC-273", "score": 0.9912857562303543, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12582, "end": 12586, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9875512719154358, 0.9858506321907043], "text": " LMs", "score": 0.9867009520530701, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12597, "end": 12614, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9807265400886536, 0.9758554697036743, 0.9962000250816345, 0.9860602617263794], "text": " all text corpora", "score": 0.9847105741500854, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12686, "end": 12700, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.9853934645652771, 0.9856280088424683], "text": " training data", "score": 0.9855107367038727, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12828, "end": 12835, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9915165901184082, 0.9939866065979004, 0.9939583539962769, 0.9950407147407532], "text": " PDP-60", "score": 0.9936255663633347, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12838, "end": 12847, "seq_label": ["B-Method"], "seq_token": ["\u0120training"], "seq_scores": [0.5197374820709229], "text": " training", "score": 0.5197374820709229, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12896, "end": 12906, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Gutenberg"], "seq_scores": [0.5185561180114746], "text": " Gutenberg", "score": 0.5185561180114746, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 13017, "end": 13030, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.935786247253418, 0.9746651649475098], "text": " full scoring", "score": 0.9552257061004639, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 13056, "end": 13072, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring"], "seq_scores": [0.9586158990859985, 0.9818673133850098], "text": " partial scoring", "score": 0.9702416062355042, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13170, "end": 13177, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9915696978569031, 0.9939715266227722, 0.9941450953483582, 0.9953593611717224], "text": " PDP-60", "score": 0.993761420249939, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13332, "end": 13354, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.956749677658081, 0.9793769121170044, 0.9711073637008667], "text": " commonsense reasoning", "score": 0.969077984491984, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13443, "end": 13456, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120LM", "-", "1", "-", "B", "illion"], "seq_scores": [0.9804307818412781, 0.993839681148529, 0.994797945022583, 0.9949783682823181, 0.9940711855888367, 0.9945695996284485], "text": " LM-1-Billion", "score": 0.9921145935853323, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13457, "end": 13469, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "C", "rawl"], "seq_scores": [0.9672176241874695, 0.9853770136833191, 0.9893432855606079], "text": " CommonCrawl", "score": 0.9806459744771322, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13474, "end": 13480, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120S", "Qu", "AD"], "seq_scores": [0.9876313805580139, 0.9919054508209229, 0.9941449761390686], "text": " SQuAD", "score": 0.9912272691726685, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13766, "end": 13768, "seq_label": ["B-ReferenceLink"], "seq_token": ["27"], "seq_scores": [0.990265429019928], "text": "27", "score": 0.990265429019928, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12789, "end": 12825, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120un", "super", "vised", "\u0120single", "-", "model", "\u0120res", "ol", "vers"], "seq_scores": [0.9937427639961243, 0.9967619180679321, 0.9982482194900513, 0.9971380233764648, 0.9987844824790955, 0.9991243481636047, 0.9980892539024353, 0.998659610748291, 0.9980792999267578], "text": " unsupervised single-model resolvers", "score": 0.9976253244611952, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12847, "end": 12889, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120one", "\u0120character", "-", "level", "\u0120and", "\u0120one", "\u0120word", "-", "level", "\u0120LM"], "seq_scores": [0.9823387265205383, 0.9615703225135803, 0.9978399276733398, 0.9982661604881287, 0.9963571429252625, 0.9852897524833679, 0.9939392805099487, 0.9984706044197083, 0.9982655644416809, 0.9972579479217529], "text": " one character-level and one word-level LM", "score": 0.9909595429897309, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12926, "end": 12946, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120two", "\u0120res", "ol", "vers"], "seq_scores": [0.9775136709213257, 0.9601196050643921, 0.9949729442596436, 0.9978742599487305, 0.9976333379745483], "text": " these two resolvers", "score": 0.985622763633728, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13193, "end": 13201, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120systems"], "seq_scores": [0.9578180909156799], "text": " systems", "score": 0.9578180909156799, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13297, "end": 13322, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120supervised", "\u0120training", "\u0120data"], "seq_scores": [0.9810412526130676, 0.991776704788208, 0.9919762015342712], "text": " supervised training data", "score": 0.9882647196451823, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13332, "end": 13364, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning", "\u0120questions"], "seq_scores": [0.6254177093505859, 0.5964110493659973, 0.625771164894104, 0.683684229850769], "text": " commonsense reasoning questions", "score": 0.6328210383653641, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13410, "end": 13418, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120another"], "seq_scores": [0.8375250101089478], "text": " another", "score": 0.8375250101089478, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13418, "end": 13440, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120three", "\u0120variants", "\u0120of", "\u0120L", "Ms"], "seq_scores": [0.7463688254356384, 0.993002712726593, 0.9953407049179077, 0.9886600971221924, 0.9981162548065186], "text": " three variants of LMs", "score": 0.9442977190017701, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13493, "end": 13505, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120of", "\u0120them"], "seq_scores": [0.6731707453727722, 0.8244224190711975, 0.8799136877059937], "text": " all of them", "score": 0.7925022840499878, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13530, "end": 13572, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120this", "\u0120ensemble", "\u0120of", "\u0120five", "\u0120un", "super", "vised", "\u0120models"], "seq_scores": [0.9912341237068176, 0.9946268200874329, 0.9951251149177551, 0.9655918478965759, 0.9943441152572632, 0.9988049268722534, 0.9987924098968506, 0.9987850785255432], "text": " this ensemble of five unsupervised models", "score": 0.9921630546450615, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13583, "end": 13599, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120best", "\u0120system"], "seq_scores": [0.9536396265029907, 0.9752593040466309, 0.9644212126731873], "text": " the best system", "score": 0.9644400477409363, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13787, "end": 13809, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120three", "\u0120knowledge", "\u0120bases"], "seq_scores": [0.5718958973884583, 0.5997883677482605, 0.48004379868507385], "text": " three knowledge bases", "score": 0.5505760212739309, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13813, "end": 13846, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120supervised", "\u0120deep", "\u0120neural", "\u0120network"], "seq_scores": [0.9693030118942261, 0.9648391604423523, 0.9709742665290833, 0.9567815661430359, 0.953212320804596], "text": " a supervised deep neural network", "score": 0.9630220651626586, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13897, "end": 13905, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9908615350723267, 0.9926924109458923, 0.9901526570320129, 0.9885936975479126], "text": " WSC-273", "score": 0.9905750751495361, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 14042, "end": 14049, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.785974383354187, 0.8202420473098755, 0.8120999932289124, 0.8250498175621033], "text": " PDP-60", "score": 0.8108415603637695, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14134, "end": 14141, "seq_label": ["B-MLModel", "I-Dataset", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.48443371057510376, 0.47955751419067383, 0.5954298973083496, 0.5123550295829773], "text": " PDP-60", "score": 0.5179440379142761, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14164, "end": 14184, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120supervised", "\u0120learning"], "seq_scores": [0.9566338062286377, 0.9696793556213379], "text": " supervised learning", "score": 0.9631565809249878, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14227, "end": 14232, "seq_label": ["I-Method", "B-MLModel", "I-MLModel"], "seq_token": ["ated", "\u0120US", "SM"], "seq_scores": [0.6012476682662964, 0.9832942485809326, 0.9851747155189514], "text": " USSM", "score": 0.8565722107887268, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14308, "end": 14315, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.5489625930786133, 0.45961037278175354, 0.5753499269485474, 0.5166374444961548], "text": " PDP-60", "score": 0.5251400843262672, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13906, "end": 13933, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120single", "-", "model", "\u0120res", "ol", "vers"], "seq_scores": [0.9908891320228577, 0.9895552396774292, 0.9981788396835327, 0.9989036321640015, 0.9982068538665771, 0.9978675842285156, 0.9958305954933167], "text": " our single-model resolvers", "score": 0.9956331253051758, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13949, "end": 13978, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120current", "\u0120state", "-", "of", "-", "the", "-", "art"], "seq_scores": [0.8016218543052673, 0.9130996465682983, 0.9613422155380249, 0.9709209203720093, 0.9763227701187134, 0.9817107319831848, 0.9827038645744324, 0.9796205163002014, 0.9749456644058228], "text": " the current state-of-the-art", "score": 0.949143131573995, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14084, "end": 14106, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120competing", "\u0120systems"], "seq_scores": [0.6584004163742065, 0.8324325680732727, 0.9651172161102295], "text": " all competing systems", "score": 0.8186500668525696, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14188, "end": 14224, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120expensive", "\u0120annot", "ated", "\u0120knowledge", "\u0120bases"], "seq_scores": [0.7861989736557007, 0.786271333694458, 0.9519069790840149, 0.9604001045227051, 0.9594213962554932], "text": " expensive annotated knowledge bases", "score": 0.8888397574424743, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14494, "end": 14496, "seq_label": ["B-ReferenceLink"], "seq_token": ["25"], "seq_scores": [0.9961275458335876], "text": "25", "score": 0.9961275458335876, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14497, "end": 14500, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012026"], "seq_scores": [0.9954442977905273], "text": " 26", "score": 0.9954442977905273, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14601, "end": 14623, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9864934682846069, 0.9935187697410583, 0.9913250207901001], "text": " commonsense reasoning", "score": 0.9904457529385885, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 14801, "end": 14813, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "C", "rawl"], "seq_scores": [0.8657411932945251, 0.8512611389160156, 0.8740363121032715], "text": " CommonCrawl", "score": 0.8636795481046041, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14388, "end": 14405, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120previous", "\u0120systems"], "seq_scores": [0.9578620195388794, 0.9405909776687622], "text": " previous systems", "score": 0.9492264986038208, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14413, "end": 14427, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120relevant", "\u0120data"], "seq_scores": [0.9884699583053589, 0.9840670824050903], "text": " relevant data", "score": 0.9862685203552246, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14432, "end": 14448, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120knowledge", "\u0120bases"], "seq_scores": [0.6216312050819397, 0.516261875629425], "text": " knowledge bases", "score": 0.5689465403556824, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14464, "end": 14474, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9342865943908691], "text": " questions", "score": 0.9342865943908691, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14554, "end": 14579, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120customized", "\u0120text", "\u0120corpus"], "seq_scores": [0.9956669807434082, 0.9964346885681152, 0.9971357583999634, 0.9968282580375671], "text": " a customized text corpus", "score": 0.9965164214372635, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14588, "end": 14598, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9616805911064148], "text": " questions", "score": 0.9616805911064148, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14681, "end": 14693, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120answers"], "seq_scores": [0.9126459360122681, 0.9364035129547119], "text": " the answers", "score": 0.92452472448349, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14739, "end": 14753, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120res", "ol", "vers"], "seq_scores": [0.9596390724182129, 0.9551976919174194, 0.9873391389846802, 0.986895740032196], "text": " our resolvers", "score": 0.9722679108381271, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14782, "end": 14792, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.9327418208122253], "text": " documents", "score": 0.9327418208122253, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14864, "end": 14878, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120questions"], "seq_scores": [0.8974236249923706, 0.9288400411605835], "text": " the questions", "score": 0.913131833076477, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 15064, "end": 15074, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120recurrent"], "seq_scores": [0.4987354874610901], "text": " recurrent", "score": 0.4987354874610901, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 15126, "end": 15151, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120sub", "word", "level", "\u0120Trans", "former"], "seq_scores": [0.850541889667511, 0.9525981545448303, 0.9422876238822937, 0.9694826006889343, 0.9648691415786743], "text": " subwordlevel Transformer", "score": 0.9359558820724487, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 15153, "end": 15155, "seq_label": ["B-ReferenceLink"], "seq_token": ["37"], "seq_scores": [0.9894890189170837], "text": "37", "score": 0.9894890189170837, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 15162, "end": 15172, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wikipedia"], "seq_scores": [0.9145115613937378], "text": " Wikipedia", "score": 0.9145115613937378, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15223, "end": 15230, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9884067177772522, 0.9925576448440552, 0.9886004328727722, 0.9897069931030273], "text": " PDP-60", "score": 0.9898179471492767, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15243, "end": 15251, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9871600866317749, 0.9920631051063538, 0.9879095554351807, 0.9891006350517273], "text": " WSC-273", "score": 0.9890583455562592, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15064, "end": 15090, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120recurrent", "\u0120language", "\u0120models"], "seq_scores": [0.9923147559165955, 0.9976044297218323, 0.997280478477478], "text": " recurrent language models", "score": 0.9957332213719686, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15124, "end": 15151, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120sub", "word", "level", "\u0120Trans", "former"], "seq_scores": [0.9383828639984131, 0.9396989345550537, 0.9289302229881287, 0.9533106088638306, 0.9418103694915771, 0.9265009164810181], "text": " a subwordlevel Transformer", "score": 0.9381056527296702, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15162, "end": 15178, "seq_label": ["I-MLModelGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120LM", "\u0120Wikipedia", "\u0120texts"], "seq_scores": [0.7374987602233887, 0.9859075546264648, 0.995144248008728], "text": " Wikipedia texts", "score": 0.9061835209528605, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 16257, "end": 16265, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120STOR", "IES"], "seq_scores": [0.9842365980148315, 0.9842997789382935], "text": " STORIES", "score": 0.9842681884765625, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15841, "end": 15851, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120Documents"], "seq_scores": [0.6199257373809814], "text": " Documents", "score": 0.6199257373809814, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15854, "end": 15866, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120corpus"], "seq_scores": [0.94265216588974, 0.9909347891807556], "text": " this corpus", "score": 0.9667934775352478, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15997, "end": 16021, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["%", "\u0120of", "\u0120highest", "\u0120documents", "\u0120our", "\u0120new", "\u0120training", "\u0120corpus"], "seq_scores": [0.597197949886322, 0.5964487791061401, 0.5285742282867432, 0.5293911695480347, 0.9668479561805725, 0.9213212728500366, 0.992382824420929, 0.9974859952926636], "text": " our new training corpus", "score": 0.7662062719464302, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16094, "end": 16121, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120nearly", "\u01201", ",", "000", ",", "000", "\u0120documents"], "seq_scores": [0.6619418263435364, 0.5737600922584534, 0.9738274216651917, 0.9781374931335449, 0.9740414023399353, 0.9629925489425659, 0.9140712022781372], "text": " nearly 1,000,000 documents", "score": 0.8626817124230521, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16244, "end": 16257, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9551912546157837, 0.9742410182952881], "text": " this dataset", "score": 0.9647161364555359, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 16397, "end": 16405, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120STOR", "IES"], "seq_scores": [0.9825366139411926, 0.98553466796875], "text": " STORIES", "score": 0.9840356409549713, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16375, "end": 16394, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120four", "\u0120different", "\u0120L", "Ms"], "seq_scores": [0.983511209487915, 0.9855940937995911, 0.9840635061264038, 0.9957892298698425], "text": " four different LMs", "score": 0.9872395098209381, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16421, "end": 16453, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120previous", "\u0120ensemble", "\u0120of", "\u012010", "\u0120L", "Ms"], "seq_scores": [0.9919865131378174, 0.9945112466812134, 0.9950762391090393, 0.9890919923782349, 0.9667235016822815, 0.9954348206520081, 0.9984568357467651], "text": " the previous ensemble of 10 LMs", "score": 0.9901830213410514, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16492, "end": 16509, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120final", "\u0120system"], "seq_scores": [0.9817655086517334, 0.9753943681716919, 0.9786560535430908], "text": " the final system", "score": 0.978605310122172, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16542, "end": 16556, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120single", "\u0120models"], "seq_scores": [0.9958512783050537, 0.9970121383666992], "text": " single models", "score": 0.9964317083358765, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16567, "end": 16579, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120corpus"], "seq_scores": [0.973892331123352, 0.9810670614242554], "text": " this corpus", "score": 0.9774796962738037, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16614, "end": 16630, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120word", "-", "level", "\u0120LM"], "seq_scores": [0.9943363070487976, 0.9913797378540039, 0.9988219141960144, 0.998786985874176, 0.9985665678977966], "text": " a word-level LM", "score": 0.9963783025741577, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16673, "end": 16699, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120ensemble", "\u0120of", "\u012010", "\u0120models"], "seq_scores": [0.9947212934494019, 0.9966331124305725, 0.9937955141067505, 0.9854275584220886, 0.9985194802284241], "text": " the ensemble of 10 models", "score": 0.9938193917274475, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16721, "end": 16742, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01204", "\u0120other", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9777235388755798, 0.9654070138931274, 0.9940726161003113, 0.9982202649116516, 0.9938206076622009], "text": " 4 other text corpora", "score": 0.9858488082885742, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16813, "end": 16836, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120proposed", "\u0120res", "ol", "vers"], "seq_scores": [0.9774004817008972, 0.9859244227409363, 0.9935784339904785, 0.9962312579154968, 0.9929454922676086], "text": " our proposed resolvers", "score": 0.9892160177230835, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17078, "end": 17091, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.7864624857902527, 0.8669451475143433], "text": " full scoring", "score": 0.826703816652298, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17114, "end": 17130, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring"], "seq_scores": [0.8304570913314819, 0.8594065308570862], "text": " partial scoring", "score": 0.8449318110942841, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17578, "end": 17591, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.9380362033843994, 0.9388958215713501], "text": " full scoring", "score": 0.9384660124778748, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17611, "end": 17627, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring"], "seq_scores": [0.9493736624717712, 0.9530447125434875], "text": " partial scoring", "score": 0.9512091875076294, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 17717, "end": 17726, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Win", "og", "rad"], "seq_scores": [0.8291606903076172, 0.6555978059768677, 0.5965576171875], "text": " Winograd", "score": 0.693772037823995, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 17924, "end": 17933, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Win", "og", "rad"], "seq_scores": [0.788644552230835, 0.5429917573928833, 0.49581801891326904], "text": " Winograd", "score": 0.6091514428456625, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18051, "end": 18059, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9727186560630798, 0.9827604293823242, 0.9760564565658569, 0.9764987826347351], "text": " WSC-273", "score": 0.977008581161499, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18084, "end": 18097, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.9465253949165344, 0.9553536772727966], "text": " full scoring", "score": 0.9509395360946655, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18115, "end": 18131, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring"], "seq_scores": [0.9620863795280457, 0.9683719277381897], "text": " partial scoring", "score": 0.9652291536331177, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17505, "end": 17519, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120res", "ol", "vers"], "seq_scores": [0.9821838140487671, 0.9745057821273804, 0.9821394085884094, 0.9767904281616211], "text": " our resolvers", "score": 0.9789048582315445, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17775, "end": 17783, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120L", "Ms"], "seq_scores": [0.9730948805809021, 0.9891497492790222, 0.9893209934234619], "text": " the LMs", "score": 0.9838552077611288, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18024, "end": 18046, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120A", "\u0120sample", "\u0120of", "\u0120questions"], "seq_scores": [0.7625045776367188, 0.7644686698913574, 0.9081658124923706, 0.9480162262916565], "text": " A sample of questions", "score": 0.8457888215780258, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18367, "end": 18374, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120LM"], "seq_scores": [0.9832083582878113, 0.9851776957511902], "text": " the LM", "score": 0.9841930270195007, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18411, "end": 18426, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["Part", "ial", "\u0120scoring"], "seq_scores": [0.9821574687957764, 0.9911483526229858, 0.9888312816619873], "text": "Partial scoring", "score": 0.9873790343602499, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18577, "end": 18603, "seq_label": ["B-Dataset", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.6981302499771118, 0.5537620782852173, 0.6316698789596558, 0.6656942963600159, 0.6945739984512329, 0.4469413459300995], "text": " Winograd Schema Challenge", "score": 0.6151286413272222, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19055, "end": 19063, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9418063759803772, 0.9416901469230652, 0.9231822490692139, 0.939487099647522], "text": " WSC-273", "score": 0.9365414679050446, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19349, "end": 19357, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120res"], "seq_scores": [0.5607990026473999, 0.596184492111206], "text": " the res", "score": 0.578491747379303, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18812, "end": 18822, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9553654193878174], "text": " questions", "score": 0.9553654193878174, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18885, "end": 18909, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120backward", "-", "scoring", "\u0120models"], "seq_scores": [0.9897056818008423, 0.9937424063682556, 0.996574878692627, 0.9950534701347351], "text": " backward-scoring models", "score": 0.993769109249115, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18930, "end": 18936, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u01206", "\u0120L", "Ms"], "seq_scores": [0.9731467962265015, 0.9262843728065491, 0.9955399632453918], "text": " 6 LMs", "score": 0.9649903774261475, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18953, "end": 18970, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120one", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9160493016242981, 0.9517717957496643, 0.9912019968032837, 0.9682149887084961], "text": " one text corpora", "score": 0.9568095207214355, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18996, "end": 19010, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120ensemble"], "seq_scores": [0.9870733022689819, 0.994239091873169], "text": " This ensemble", "score": 0.9906561970710754, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19027, "end": 19052, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120previous", "\u0120best", "\u0120system"], "seq_scores": [0.9827150702476501, 0.9920961856842041, 0.9934914708137512, 0.989250898361206], "text": " the previous best system", "score": 0.9893884062767029, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19176, "end": 19219, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["115", "\u0120out", "\u0120of", "\u0120178", "\u0120correctly", "\u0120answered", "\u0120questions"], "seq_scores": [0.9278772473335266, 0.6787525415420532, 0.7856011986732483, 0.6793158054351807, 0.9844604134559631, 0.9782771468162537, 0.9853553771972656], "text": "115 out of 178 correctly answered questions", "score": 0.8599485329219273, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19386, "end": 19402, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Partial", "\u0120scoring"], "seq_scores": [0.7591720819473267, 0.79636150598526], "text": " Partial scoring", "score": 0.7777667939662933, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19417, "end": 19430, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.829888105392456, 0.8038375377655029], "text": " full scoring", "score": 0.8168628215789795, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19515, "end": 19528, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.8533940315246582, 0.8692390322685242], "text": " full scoring", "score": 0.8613165318965912, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20044, "end": 20057, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120scoring"], "seq_scores": [0.8274285793304443, 0.8735530972480774], "text": " full scoring", "score": 0.8504908382892609, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20159, "end": 20175, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Partial", "\u0120scoring"], "seq_scores": [0.8959089517593384, 0.92035311460495], "text": " Partial scoring", "score": 0.9081310331821442, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20267, "end": 20280, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method"], "seq_token": ["\u0120normal", "ization", "\u0120full", "\u0120scoring"], "seq_scores": [0.3505997359752655, 0.7877524495124817, 0.7997605204582214, 0.8839805722236633], "text": " full scoring", "score": 0.705523319542408, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20309, "end": 20317, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "122"], "seq_scores": [0.9901931881904602, 0.993939995765686, 0.9935321807861328, 0.9941844344139099], "text": " PDP-122", "score": 0.9929624497890472, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20321, "end": 20329, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "SC", "-", "273"], "seq_scores": [0.9902708530426025, 0.9941317439079285, 0.9928739070892334, 0.9931992888450623], "text": " WSC-273", "score": 0.9926189482212067, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 20361, "end": 20377, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120partial", "\u0120scoring"], "seq_scores": [0.9094802141189575, 0.9429436922073364], "text": " partial scoring", "score": 0.926211953163147, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20440, "end": 20448, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "122"], "seq_scores": [0.9894089698791504, 0.9939980506896973, 0.9933986663818359, 0.9938045740127563], "text": " PDP-122", "score": 0.99265256524086, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20472, "end": 20479, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9887182712554932, 0.9932637214660645, 0.992891788482666, 0.9935531616210938], "text": " PDP-60", "score": 0.9921067357063293, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20529, "end": 20536, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "60"], "seq_scores": [0.9893208742141724, 0.9938116073608398, 0.9932899475097656, 0.9941049218177795], "text": " PDP-60", "score": 0.9926318377256393, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20620, "end": 20628, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120STOR", "IES"], "seq_scores": [0.903495728969574, 0.9635149836540222], "text": " STORIES", "score": 0.9335053563117981, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21036, "end": 21044, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120STOR", "IES"], "seq_scores": [0.934856653213501, 0.9792701005935669], "text": " STORIES", "score": 0.9570633769035339, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21114, "end": 21136, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9649704694747925, 0.9682241678237915, 0.9638054370880127], "text": " commonsense reasoning", "score": 0.9656666914621989, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19493, "end": 19509, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120word", "-", "level", "\u0120LM"], "seq_scores": [0.9755730032920837, 0.972539484500885, 0.9929577112197876, 0.9916988611221313, 0.9904910326004028], "text": " a word-level LM", "score": 0.9846520185470581, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19779, "end": 19790, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120system"], "seq_scores": [0.9003687500953674, 0.9116910696029663], "text": " the system", "score": 0.9060299098491669, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19886, "end": 19906, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120its", "\u0120training", "\u0120corpus"], "seq_scores": [0.8776746392250061, 0.8575499653816223, 0.974267303943634], "text": " its training corpus", "score": 0.9031639695167542, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20283, "end": 20285, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u01209"], "seq_scores": [0.8614559173583984], "text": " 9", "score": 0.8614559173583984, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20451, "end": 20469, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012010", "\u0120tested", "\u0120L", "Ms", "\u0120a", "\u0120larger", "\u0120supers", "et"], "seq_scores": [0.7061335444450378, 0.9627542495727539, 0.9662343263626099, 0.9941218495368958, 0.5969998240470886, 0.7491400837898254, 0.7820627689361572, 0.7008094191551208], "text": " a larger superset", "score": 0.8072820082306862, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20711, "end": 20728, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9790645241737366, 0.9880088567733765, 0.9908492565155029, 0.9584036469459534], "text": " the text corpora", "score": 0.9790815711021423, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20804, "end": 20839, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120previous", "\u0120ensemble", "\u0120of", "\u012010", "\u0120models"], "seq_scores": [0.993476927280426, 0.9955787062644958, 0.9964737296104431, 0.9901493787765503, 0.9827100038528442, 0.9972298741340637], "text": " the previous ensemble of 10 models", "score": 0.9926031033198038, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20847, "end": 20870, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120same", "\u0120set", "\u0120of", "\u0120models"], "seq_scores": [0.9729457497596741, 0.9855772852897644, 0.9868293404579163, 0.9902286529541016, 0.9935064315795898], "text": " the same set of models", "score": 0.9858174920082092, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20881, "end": 20905, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120single", "\u0120text", "\u0120corpus"], "seq_scores": [0.9647013545036316, 0.9272527694702148, 0.9906193017959595, 0.993777871131897], "text": " each single text corpus", "score": 0.9690878242254257, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20917, "end": 20939, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120original", "\u0120ensemble"], "seq_scores": [0.991594135761261, 0.9951040744781494, 0.9960026144981384], "text": " the original ensemble", "score": 0.9942336082458496, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20950, "end": 20980, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120diverse", "\u0120set", "\u0120of", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9895128607749939, 0.9944307208061218, 0.9967052340507507, 0.9938866496086121, 0.9950600266456604, 0.9987882971763611, 0.9953839182853699], "text": " a diverse set of text corpora", "score": 0.9948239581925529, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20992, "end": 21026, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120other", "\u0120single", "-", "cor", "p", "us", "\u0120en", "semb", "les"], "seq_scores": [0.8922533392906189, 0.9229925870895386, 0.9686440825462341, 0.9932449460029602, 0.9933023452758789, 0.992218017578125, 0.9928172826766968, 0.9950043559074402, 0.9939217567443848, 0.9940081834793091], "text": " all other single-corpus ensembles", "score": 0.9738406896591186, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21096, "end": 21110, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data"], "seq_scores": [0.9658272862434387, 0.9804632067680359], "text": " training data", "score": 0.9731452465057373, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21148, "end": 21165, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120final", "\u0120system"], "seq_scores": [0.9693936109542847, 0.9756981730461121, 0.9650474190711975], "text": " the final system", "score": 0.9700464010238647, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21226, "end": 21248, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120simple", "\u0120un", "super", "vised", "\u0120method", "\u0120Commons", "ense", "\u0120Reason", "ing"], "seq_scores": [0.5451048016548157, 0.5827720165252686, 0.7678834795951843, 0.8150765299797058, 0.5790512561798096, 0.9858807325363159, 0.994175374507904, 0.9916274547576904, 0.9906873106956482], "text": " Commonsense Reasoning", "score": 0.8058065507147048, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21422, "end": 21445, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120P", "ron", "oun", "\u0120Dis", "amb", "ig", "uation"], "seq_scores": [0.9433399438858032, 0.9824716448783875, 0.9909312725067139, 0.9920068383216858, 0.9926456212997437, 0.9924176931381226, 0.9900049567222595], "text": " Pronoun Disambiguation", "score": 0.983402567250388, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21458, "end": 21484, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120Win", "og", "rad", "\u0120Sche", "ma", "\u0120Challenge"], "seq_scores": [0.5076568126678467, 0.6881055235862732, 0.7443753480911255, 0.8938959240913391, 0.898270308971405, 0.7661819458007812], "text": " Winograd Schema Challenge", "score": 0.7497476438681284, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21634, "end": 21654, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120supervised", "\u0120learning"], "seq_scores": [0.9502450227737427, 0.9673612117767334], "text": " supervised learning", "score": 0.958803117275238, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21279, "end": 21301, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "\u0120language", "\u0120models"], "seq_scores": [0.9940963983535767, 0.9967952370643616, 0.9978050589561462], "text": " large language models", "score": 0.9962322314580282, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21313, "end": 21358, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120number", "\u0120of", "\u0120massive", "\u0120and", "\u0120diverse", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9613568782806396, 0.94881272315979, 0.9359596967697144, 0.9392773509025574, 0.9932559728622437, 0.9930417537689209, 0.9959774613380432, 0.9986708164215088, 0.9966300129890442], "text": " a number of massive and diverse text corpora", "score": 0.9736647407213846, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21359, "end": 21381, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120resulting", "\u0120systems"], "seq_scores": [0.9812349677085876, 0.98592209815979, 0.983854353427887], "text": " The resulting systems", "score": 0.9836704730987549, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21392, "end": 21414, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120previous", "\u0120best", "\u0120systems"], "seq_scores": [0.8215428590774536, 0.8587618470191956, 0.811190128326416], "text": " previous best systems", "score": 0.8304982781410217, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21499, "end": 21509, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120later"], "seq_scores": [0.5628495216369629, 0.49791789054870605], "text": " the later", "score": 0.5303837060928345, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21590, "end": 21620, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120previous", "\u0120state", "-", "of", "-", "the", "-", "art"], "seq_scores": [0.592221736907959, 0.6518104076385498, 0.6327518224716187, 0.6097187995910645, 0.5356140732765198, 0.5886699557304382, 0.542702317237854, 0.5951202511787415, 0.5134235620498657], "text": " the previous state-of-the-art", "score": 0.5846703251202902, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21708, "end": 21719, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120system"], "seq_scores": [0.9039055705070496, 0.9244334697723389], "text": " our system", "score": 0.9141695201396942, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21919, "end": 21939, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120en", "semb", "les", "\u0120of", "\u0120models"], "seq_scores": [0.9826042056083679, 0.9936290383338928, 0.9936017394065857, 0.994337260723114, 0.9948792457580566], "text": " ensembles of models", "score": 0.9918102979660034, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21972, "end": 22002, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120diverse", "\u0120set", "\u0120of", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.988292396068573, 0.9947174191474915, 0.9964981079101562, 0.9950904846191406, 0.9920154809951782, 0.998722493648529, 0.9974440336227417], "text": " a diverse set of text corpora", "score": 0.9946829165731158, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22080, "end": 22095, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120future", "\u0120systems"], "seq_scores": [0.7579984068870544, 0.6836007237434387], "text": " future systems", "score": 0.7207995653152466, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22129, "end": 22151, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120commons", "ense", "\u0120knowledge"], "seq_scores": [0.8612127900123596, 0.8505102396011353, 0.8355681896209717], "text": " commonsense knowledge", "score": 0.8490970730781555, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22195, "end": 22218, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120similarity", "\u0120scoring"], "seq_scores": [0.5052465200424194, 0.42228734493255615, 0.8719395995140076], "text": " the similarity scoring", "score": 0.5998244881629944, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 22452, "end": 22474, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120commons", "ense", "\u0120reasoning"], "seq_scores": [0.9808562397956848, 0.9892487525939941, 0.9860906600952148], "text": " commonsense reasoning", "score": 0.985398550828298, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22488, "end": 22500, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "C", "rawl"], "seq_scores": [0.6699360013008118, 0.6222426295280457, 0.705073893070221], "text": " CommonCrawl", "score": 0.6657508412996928, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22588, "end": 22596, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "DP", "-", "122"], "seq_scores": [0.7853305339813232, 0.8636730313301086, 0.7818734049797058, 0.8946605920791626], "text": " PDP-122", "score": 0.8313843905925751, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22255, "end": 22257, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.6532192826271057], "text": " a", "score": 0.6532192826271057, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22340, "end": 22346, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["\u0120low", "\u0120quality", "\u0120training", "\u0120text", "\u0120these"], "seq_scores": [0.8774992227554321, 0.9881114959716797, 0.9907976388931274, 0.9910596013069153, 0.6778276562690735], "text": " these", "score": 0.9050591230392456, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22350, "end": 22360, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.5054728984832764], "text": " documents", "score": 0.5054728984832764, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22444, "end": 22448, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120L", "Ms"], "seq_scores": [0.9745707511901855, 0.9832741618156433], "text": " LMs", "score": 0.9789224565029144, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22483, "end": 22500, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120full", "\u0120Common", "C", "rawl"], "seq_scores": [0.6361382007598877, 0.818873405456543, 0.6963075399398804, 0.7283238172531128], "text": " full CommonCrawl", "score": 0.719910740852356, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22588, "end": 22606, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120P", "DP", "-", "122", "\u0120questions"], "seq_scores": [0.5939635634422302, 0.9846493601799011, 0.986226499080658, 0.9868708848953247, 0.9777650237083435], "text": " PDP-122 questions", "score": 0.9058950662612915, "type": "ScholarlyEntity"}]}, "filename": "00005_1806_02847.json", "id": "00005_1806_02847"}