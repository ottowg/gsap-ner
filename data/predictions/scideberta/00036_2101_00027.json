{"text": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling\n\nAbstract:\nRecent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present the Pile: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets-both existing and newly constructed-many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. 1\n\n\n1 Introduction\nRecent breakthroughs in general-purpose language modeling have demonstrated the effectiveness of training massive models on large text corpora for downstream applications (Radford et al., 2019; Shoeybi et al., 2019; Raffel et al., 2019; Rosset, 2019; Brown et al., 2020; Lepikhin et al., 2020). As the field continues to scale up language model training, the demand for high-quality massive text data will continue to grow (Kaplan et al., 2020).\nThe growing need for data in language modeling has caused most existing large-scale language models to turn to the Common Crawl for most or all of their data (Brown et al., 2020; Raffel et al., 2019). While training on the Common Crawl has been effective, recent work has shown that dataset di-1 https://pile.eleuther.ai/ versity leads to better downstream generalization capability (Rosset, 2019). Additionally, large-scale language models have been shown to effectively acquire knowledge in a novel domain with only relatively small amounts of training data from that domain (Rosset, 2019; Brown et al., 2020; Carlini et al., 2020). These results suggest that by mixing together a large number of smaller, high quality, diverse datasets, we can improve the general cross-domain knowledge and downstream generalization capabilities of the model compared to models trained on only a handful of data sources.\nTo address this need, we introduce the Pile: a 825.18 GiB English text dataset designed for training large scale language models. The Pile is composed of 22 diverse and high-quality datasets, including both established natural language processing datasets and several newly introduced ones. In addition to its utility in training large language models, the Pile can also serve as a broad-coverage benchmark for cross-domain knowledge and generalization ability of language models. We introduce new datasets derived from the following sources: PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH ExPorter. We also introduce OpenWebText2 and BookCorpus2, which are extensions of the original OpenWebText (Gokaslan and Cohen, 2019) and BookCorpus (Zhu et al., 2015; Kobayashi, 2018) datasets, respectively.\nIn addition, we incorporate several existing highquality datasets: Books3 (Presser, 2020), Project Gutenberg (PG-19) (Rae et al., 2019), Open-Subtitles (Tiedemann, 2016), English Wikipedia, DM Mathematics (Saxton et al., 2019), EuroParl (Koehn, 2005), and the Enron Emails corpus (Klimt and Yang, 2004). To supplement these, we also in-1 arXiv:2101.00027v1 [cs.CL] 31 Dec 2020 Through our analyses, we confirm that the Pile is significantly distinct from pure Common Crawl data. Additionally, our evaluations show that the existing GPT-2 and GPT-3 models perform poorly on many components of the Pile, and that models trained on the Pile significantly outperform both raw and filtered Common Crawl models. To complement the performance evaluations, we also perform an exploratory analysis of the text within the Pile to provide a detailed picture of the data. We hope that our extensive documentation of the construction and characteristics of the Pile will help researchers make informed decisions about potential downstream applications.\nFinally, we make publicly available the preprocessing code for the constituent datasets of the Pile and the code for constructing alternative versions 2. In the interest of reproducibility, we also document all processing performed on each dataset (and the Pile as a whole) in as much detail as possible. For further details about the processing of each dataset, see Section 2 and Appendix C.\n\n1.1 Contributions\nThe core contributions of this paper are:\n1. The introduction of a 825.18 GiB englishlanguage dataset for language modeling combining 22 diverse sources.\n2. The introduction of 14 new language modeling datasets, which we expect to be of independent interest to researchers.\n3. Evaluations demonstrating significant improvements across many domains by GPT-2sized models trained on this new dataset, compared to training on CC-100 and raw Common Crawl.\n4. The investigation and documentation of this dataset, which we hope will better inform researchers about how to use it as well as motivate them to undertake similar investigations of their own data.\n\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets, as shown in Table 1. Following Brown et al. (2020), we increase the weights of higher quality components, with certain high-quality datasets such as Wikipedia being seen up to 3 times (\"epochs\") for each full epoch over the Pile. Detailed information about the construction of each dataset is available in Appendix C.\n\n2.1 Pile-CC\nCommon Crawl is a collection of website crawls from 2008 onwards, including raw web pages, metadata and text extractions. Due to the raw nature of the dataset, Common Crawl has the advantage of including text from diverse domains, but at the cost of varying quality data. Due to this, use of Common Crawl typically necessitates well-designed extraction and filtering. Our Common Crawl-based dataset, Pile-CC, uses jus-Text (Endr\u00e9dy and Nov\u00e1k, 2013) on Web Archive files (raw HTTP responses including page HTML) for extraction, which yields higher quality output than directly using the WET files (extracted plaintext).\n\n2.2 PubMed Central\nPubMed Central (PMC) is a subset of the PubMed online repository for biomedical articles run by the United States of America's National Center for Biotechnology Information (NCBI), providing open, full-text access to nearly five million publications. Most publications indexed by PMC are recent, and their inclusion is mandated for all NIH funded research starting from 2008 by the NIH Public Access Policy. We included PMC in the hopes that it will benefit potential downstream applications to the medical domain.\n\n2.3 Books3\nBooks3 is a dataset of books derived from a copy of the contents of the Bibliotik private tracker made available by Shawn Presser (Presser, 2020). Bibliotik consists of a mix of fiction and nonfiction books and is almost an order of magnitude larger than our next largest book dataset .We included Bibliotik because books are invaluable for long-range context modeling research and coherent storytelling.\n\n2.4 OpenWebText2\nOpenWebText2 (OWT2) is a generalized web scrape dataset inspired by WebText (Radford et al., 2019) and OpenWebTextCorpus (Gokaslan and Cohen, 2019). Similar to the original WebText, we use net upvotes on Reddit submissions as a proxy for outgoing link quality. OpenWebText2 includes more recent content from Reddit submissions up until 2020, content from multiple languages, document metadata, multiple dataset versions, and open source replication code. We included OWT2 as a high quality general purpose dataset.\n\n2.5 ArXiv\nArXiv is a preprint server for research papers that has operated since 1991. As shown in fig. 10, arXiv papers are predominantly in the fields of Math, Computer Science, and Physics. We included arXiv in the hopes that it will be a source of high quality text and math knowledge, and benefit potential downstream applications to research in these areas. ArXiv papers are written in LaTeX, a common typesetting language for mathematics, computer science, physics, and some adjacent fields. Training a language model to be able to generate papers written in LaTeX could be a huge boon to the research community.\n\n2.6 GitHub\nGitHub is a large corpus of open-source code repositories. Motivated by the ability of GPT-3 (Brown et al., 2020) to generate plausible code completions despite its training data not containing any explicitly gathered code datasets, we included GitHub in the hopes that it would enable better downstream performance on code-related tasks.\n\n2.7 FreeLaw\nThe Free Law Project is a US-registered non-profit that provides access to and analytical tools for academic studies in the legal realm. CourtListener, 3 part of the Free Law Project, provides bulk downloads for millions of legal opinions from federal and state courts. While the full dataset provides multiple modalities of legal proceedings, including dockets, bibliographic information on judges, 3 https://www.courtlistener.com/ and other metadata, we focused specifically on court opinions due to an abundance of full-text entries. This data is entirely within the public domain.\n\n2.8 Stack Exchange\nThe Stack Exchange Data Dump 4 contains an anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers. It is one of the largest publicly available repositories of question-answer pairs, and covers a wide range of subjects-from programming, to gardening, to Buddhism. We included Stack Exchange in the hopes that it will improve the question answering capabilities of downstream models on diverse domains.\n\n2.9 USPTO Backgrounds\nUSPTO Backgrounds is a dataset of background sections from patents granted by the United States Patent and Trademark Office, derived from its published bulk archives 5. A typical patent background lays out the general context of the invention, gives an overview of the technical field, and sets up the framing of the problem space. We included USPTO Backgrounds because it contains a large volume of technical writing on applied subjects, aimed at a non-technical audience.\n\n2.10 Wikipedia (English)\nWikipedia is a standard source of high-quality text for language modeling. In addition to being a source of high quality, clean English text, it is also valuable as it is written in expository prose, and spans many domains.\n\n2.11 PubMed Abstracts\nPubMed Abstracts consists of the abstracts from 30 million publications in PubMed, the online repository for biomedical articles run by the National Library of Medicine. While the PMC (see Section 2.2) provides full-text access, the subset of coverage is significantly limited and biased towards recent publications. PubMed also incorporates MED-LINE, which expands the coverage of biomedical abstracts from 1946 to present day.\n\n2.12 Project Gutenberg\nProject Gutenberg is a dataset of classic Western literature. The specific Project Gutenberg derived dataset we used, PG-19, consists of Project Gutenberg books from before 1919 (Rae et al., 2019), which represent distinct styles from the more modern Books3 and BookCorpus. Additionally, the PG-19 dataset is already being used for long-distance context modeling.\n\n2.13 OpenSubtitles\nThe OpenSubtitles dataset is an English language dataset of subtitles from movies and television shows gathered by Tiedemann (2016). Subtitles provide an important source of natural dialog, as well as an understanding of fictional formats other than prose, which may prove useful for creative writing generation tasks such as screenwriting, speechwriting, and interactive storytelling.\n\n2.14 DeepMind Mathematics\nThe DeepMind Mathematics dataset consists of a collection of mathematical problems from topics such as algebra, arithmetic, calculus, number theory, and probability, formatted as natural language prompts (Saxton et al., 2019). One major weakness of large language models has been performance on mathematical tasks (Brown et al., 2020), which may be due in part to a lack of math problems in the training set. By explicitly including a dataset of mathematical problems, we hope to improve the mathematical ability of language models trained on the Pile.\n\n2.15 BookCorpus2\nBookCorpus2 is an expanded version of the original BookCorpus (Zhu et al., 2015), a widely used language modeling corpus consisting of books written by \"as of yet unpublished authors.\" BookCorpus is therefore unlikely to have significant overlap with Project Gutenberg and Books3, which consist of published books. BookCorpus is also commonly used as dataset for training language models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019).\n\n2.16 Ubuntu IRC\nThe Ubuntu IRC dataset is derived from the publicly available chatlogs 6 of all Ubuntu-related channels on the Freenode IRC chat server. Chatlog data provides an opportunity to model real-time human interactions, which feature a level of spontaneity not typically found in other modes of social media.\n\n2.17 EuroParl\nEuroParl (Koehn, 2005) is a multilingual parallel corpus originally introduced for machine translation but which has also seen use in several other fields of NLP (Groves and Way, 2006; Van Halteren, 2008; Ciobanu et al., 2017). We use the most current version at time of writing, which consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012.\n\n2.18 YouTube Subtitles\nThe YouTube Subtitles dataset is a parallel corpus of text gathered from human generated closedcaptions on YouTube. In addition to providing multilingual data, Youtube Subtitles is also a source of educational content, popular culture, and natural dialog.\n\n2.19 PhilPapers\nThe PhilPapers 7 dataset consists of open-access philosophy publications from an international database maintained by the Center for Digital Philosophy at the University of Western Ontario. We included PhilPapers because it spans a wide body of abstract, conceptual discourse, and its articles contain high quality academic writing.\n\n2.20 NIH Grant Abstracts: ExPORTER\nThe NIH Grant abstracts provides a bulk-data repository for awarded applications through the Ex-PORTER 8 service covering the fiscal years 1985present. We included the dataset because it contains examples of high-quality scientific writing.\n\n2.21 Hacker News\nHacker News 9 is a link aggregator operated by Y Combinator, a startup incubator and investment fund. Users submit articles defined as \"anything that gratifies one's intellectual curiosity,\" but submitted articles tend to focus on topics in computer science and entrepreneurship. Users can comment on submitted stories, resulting in comment trees discussing and critiquing submitted stories. We scrape, parse, and include these comment trees since we believe they provide high quality dialogue and debate on niche topics.\n\n2.22 Enron Emails\nThe Enron Emails dataset (Klimt and Yang, 2004) is a valuable corpus commonly used for research about the usage patterns of email. We included Enron Emails to aid in understanding the modality of email communications, which is typically not found in any of our other datasets.\n3 Benchmarking Language Models with the Pile\nWhile the Pile was conceived as a training dataset for large-scale language models, its coverage of multiple disparate domains makes it also suitable as an evaluation dataset. In this section, we describe how the Pile can be used as a broad-coverage dataset for benchmarking language models.\n\n3.1 Benchmarking Guidelines\nThe Pile is provided as train, validation, and testing splits. The validation and testing components each contain 0.1% of the data, sampled uniformly at random. While this is a far smaller percentage than most datasets, the sheer size of the dataset results in over 1 GiB of validation and testing data each. We highlight that while we have made efforts to deduplicate documents within the Pile (See: Section D.2), it is still possible that some documents are duplicated across the train/validation/test splits.\nOur preferred metric is bits per UTF-8 encoded byte (BPB). Bits per byte is preferred over bits per character or perplexity when using Pile as a metric due to its invariance to different tokenization schemes and the ambiguity of measuring characters in Unicode. To compute bits per byte from a given negative log likelihood loss , we computeBPB = (L T /L B ) log 2 (e ) = (L T /L B ) / ln(2)\n, where L T is the length of the dataset in tokens and L B is the length of the dataset in UTF-8 encoded bytes. We find that L T /L B is 0.29335 GPT-2tokens/byte across the Pile; dataset-specific values of L T /L B can be found in Table 7.\n\n3.2 Test Perplexity with GPT-2 and GPT-3\nWe compute the test perplexity of the constituent datasets of the Pile using GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), shown in Figure 2. We use all available versions of GPT-2, and all four versions of GPT-3 available via the OpenAI API. Because of the cost associated with using the OpenAI API, we evaluate on one-tenth of the respective test sets for most of the constituent datasets. We report the perplexity converted to bits per UTF-8 encoded byte (BPB). Importantly, we compute perplexity by evaluating each document independently within each dataset, as opposed to concatenating all documents as is common practice for computing perplexity on large corpora.\nFull details of the perplexity computation can be found in Appendix E.2.\nUnsurprisingly, larger language models generally attain lower perplexity compared to smaller models.\nRecent work has shown an increased focus on the empirical scaling laws of language models (Kaplan et al., 2020; Henighan et al., 2020). As such, we investigate the scaling law for the GPT-2 and GPT-3 families of models on perplexity evaluation on the Pile. The scaling law relation for the GPT-3 family of models is shown in Figure 2. 10 The line of best fit shown in the figure has a coefficient of -0.1674 and an intercept of 2.5516. Interestingly, while GPT-2 and GPT-3 were not trained on the Pile, there still appears to be a clear scaling law without diminishing returns. We hypothesize that this is due to the inherent generalization capability of these models. We leave a more rigorous analysis of zero-shot scaling laws to future work.\n\n3.3 Relative Componentwise GPT-3 Pile Performance\nDetermining which components GPT-3 underperforms on provides information about which Pile components are most dissimilar to the distribution of text (web pages and books) that GPT-3 was trained on. These components would thus make especially good candidates for supplementing GPT-3 training data. These results are also valuable for determining which types of datasets to emphasize for future iterations of the Pile.\nDue to the difference in entropy of different datasets, directly comparing perplexity of GPT-3 on different Pile components is not an accurate indication of relative performance. Ideally we would train a GPT-3 model from scratch on the Pile and compare the difference in loss per dataset with that of the original GPT-3. Because of resource constraints, we instead use a GPT-2 model trained from scratch on the Pile (see Section 4) to construct a proxy measure. To construct our proxy, we first measure the improvement from the GPT-2-Pile model to GPT-3 on each component. Then, we normalize our results by setting the change on OpenWebText2 to be zero. This computation is shown in the equation below:\u2206 set = L GPT3 set \u2212 L GPT3 owt2 \u2212 L GPT2Pile set \u2212 L GPT2Pile owt2\nSince GPT2-Pile was trained on both OWT2 and the dataset we are evaluating, we expect the second term in \u2206 set to reflect the difference in the intrinsic difficulty of the two datasets. Thus the total value of \u2206 set reflects how much harder the dataset we are evaluating was for GPT-3 than OWT2, minus the relative difficulty of the two tasks. As GPT-3 was trained on data very similar to OWT2, this gives us a proxy for how much better GPT-3 would do if it were trained on the Pile.\nThe results are shown in Figure 3. As a sanity check, we observe that datasets that are contained in, or are extremely similar to, GPT-3's training set (Books3, Wikipedia (en), Pile-CC and Project Gutenberg) score close to zero on our metric.\nGPT-3 appears to perform poorly on datasets pertaining to research or academic writing like PubMed Central, PubMed Abstracts, and ArXiv; domain-specific datasets like FreeLaw, Hack-erNews, and USPTO Backgrounds; and on datasets containing predominantly text distinct from natural language, like GitHub and DM Mathematics.\nIn addition, the majority of datasets see less of an improvement than OpenWebText2. As such, we expect a GPT-3 sized model trained on Pile to perform significantly better on research related tasks, software tasks, and symbol manipulation tasks than the base model. Additionally, this experiment provides evidence that the majority of Pile components are not redundant with the predominantly web-based GPT-3 training data.\nWe note that this metric is only a proxy for similarity, and that it could be confounded by dataset specific scaling effects. Although our results largely accord with expectations, there are some puzzling results, like the datasets on which GPT-3 outperformed GPT-2 Pile. We hypothesize that GPT-3 learns to be so good at these datasets that training on them explicitly does not notably benefit the model's performance. We leave a more rigorous analysis of these effects for future work.\n\n4 Evaluation\nTo confirm the effectiveness of the Pile for improving language modeling quality, we train architecturally-identical 1.3 billion parameter models based on those in Brown et al. (2020) on different datasets and evaluate on the WikiText and LAMBADA tasks as benchmarks of language modeling ability. We also report results on the Pile as a measure of more cross-domain generalization.\n\n4.1 Methodology\nTo ensure a fair comparison across datasets of different sizes, we decontaminate any instances of the evaluation sets using the same 13-gram overlap filtering as in Brown et al. (2020) and downsample to 40GB to control for dataset size. As we control for dataset size, we emphasize that our evaluation is generous to CC-100 (en), which is about 1/3 the size of the Pile in reality.\nWe compare the following datasets: the Pile, the En- et al., 2019; Conneau et al., 2020), and a sample of raw CC WET files filtered for English-only.\n\n4.2 Results\nOn traditional language modeling benchmarks, the Pile improves significantly on WikiText and shows negligible changes in LAMBADA. However, models trained on Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, as shown in We hypothesize that this is due to the perplexity based filtering used in CC-100, where a language model is trained on Wikipedia and all data with a perplexity too high or too low is discarded. This effectively discards any data too similar to or too different from Wikipedia, which severely limits the diversity of the collected data. This result suggests that future work using Common Crawl should take caution with filtering to preserve its diversity.\n\n5 Structural Statistics\nIn this section, we cover the Structural Statistics of the dataset, which provide more coarse-grained and statistical information about the Pile. In Sec-   tion 6, we provide a closer investigation and documentation of the textual content within the Pile datasets.\n\n5.1 Document Lengths and Tokenization\nEach dataset consists of a large number of documents. We analyze the distribution of document lengths, as well as the number of bytes-per-token using the GPT-2 tokenizer in order to put our ablations in context.\nWhile the majority of documents in the Pile are short, there is a long tail of very long documents (Figure 5).\n\n5.2 Language and Dialects\nWhile only 13% of the world's population speaks English, the vast majority of NLP research is done on English. For the Pile, we took a similar approach to the dataset used by Brown et al. (2020) and focused predominantly on English, while also not explicitly filtering out other languages when collecting our own data. When evaluating a multilingual dataset, our main criteria for inclusion was whether the English component of the dataset merited inclusion alone. We plan to create a fully multi-   lingual expansion of the Pile as future work.\nUsing fasttext (Su\u00e1rez et al., 2019a), we determine that the Pile is 97.4% English. We note that due to issues with language identification, particularly with rare languages Caswell et al. (2020), this methodology provides only a rough estimate for English content and no reliable conclusions for low-resource languages can be drawn.\n\n6 Investigating and Documenting the Datasets\nAs the scale of machine learning research has grown, scrutiny has been placed on the ever larger datasets that models are trained on (Prabhu and Birhane, 2020; Biderman and Scheirer, 2020) While this issue has been raised within AI ethics and bias research (Hovy and Spruit, 2016; Hutchinson et al., 2020; Blodgett et al., 2020), it has not been a focal point of concern within the language modeling community. Despite the proliferation of work exploring and documenting issues with datasets (Gebru et al., 2018; Bender and Friedman, 2018; Jo and Gebru, 2020), no dataset intended to train massive language models has been seriously documented by its creators 12. Therefore, our analyses serve two goals: to address ethical concerns about the Pile, and to promote and normalize the practice of engaging with the AI ethics literature.\nNatural language processing technologies are widely applicable and can be used in extremely different contexts. What is and is not appropriate data to train on can therefore vary wildly with the application context. In our view, the best approach is to document rather than eliminate potentially concerning aspects of datasets 13, particularly since the purpose of the Pile is to train general-purpose language models. The primary goal of our documentation, therefore, is to empower NLP researchers to make informed decisions.\n\n6.1 Documenting Methods\nTo document the Pile, we chose to implement two frameworks that have been proposed by methodologists and ethics researchers. The first, the datasheets methodology (Gebru et al., 2018), is a general purpose methodology that is recommended by several methodologists (Raji and Yang, 2019; Biderman and Scheirer, 2020) and appears to be used more frequently by practitioners than alterna-tives (Seck et al., 2018; Costa-juss\u00e0 et al., 2020; Thieme et al., 2020). The second, the data statements methodology (Bender and Friedman, 2018), was proposed specifically for natural language processing and has been well received by the NLP community. Our datasheet and data statement will be featured in the GitHub repository where the code for the Pile is stored and will also be available as separate documents on arXiv (Biderman et al., 2021; Biderman, 2021).\nIn addition to the datasheet and data statement, there is additional information that may be helpful to people training language models that these documents do not cover. In the rest of this section we investigate and document in greater detail some of this additional contextual information.\n\n6.2 Topical Distribution\nIn order to better understand the specific subject matter covered by the Pile, we performed a topic modeling analysis on its components. Using Gensim (Rehurek et al., 2011), we trained 16-topic Latent Dirichlet Allocation (Blei et al., 2003) models on each component of the validation set of the Pile concurrently, in an online fashion (Hoffman et al., 2010). We filtered the Pile for English only for this analysis. Afterwards, we computed the perplexity of the Common Crawl-derived (Pile-CC) topic model on the document sets of the other components. In this way, we provide a rough measure of the degree to which parts of the Pile contain topics not well covered within Common Crawl.\nIn Figure 7, these cross-component perplexities are shown, with a vertical line indicating the perplexity of the Pile-CC topic model evaluated on the documents of OpenWebText2. This component was chosen as a baseline of comparison for similar reasons as in the previous evaluation: it is derived in a similar manner (filtered crawls of the open web) as the Common Crawl, and thus is expected to contain a similar distribution of topics. Although Pile-CC is somewhat diverse in its content, several of the Pile's other components deviate from it strongly in their topical focus, as evidenced by higher perplexity on Github, PhilPapers, and EuroParl.\nWe also documented the topical clusters inferred from our LDA models for each component, which we provide in Appendix C. As expected, though the larger CC-derived component itself represents a diversity of content-including politics, education, sports and entertainment-the content clusters it misses become apparent when compared qualitatively to other components of the Pile. Notably, the data modes covering programming, logic, physics, and legal knowledge appear largely absent.\n\n6.3 Pejorative Content\nDue to the wide diversity in origins, it is possible for the Pile to contain pejorative, sexually explicit, or otherwise objectionable content. As this content may not be desirable for some use cases, we break down profanity on a per-dataset level.\nWe used the profanity-checker Python package (Zhou, 2019). This package includes a \"toxicity model\" trained on multiple profanity lists as well as the Wikidetox Toxic Comment Dataset (Wulczyn et al., 2016) and classifies a given string as being profane or not profane.\nWe considered only the English sentences in each dataset using the same language classifier from Section 3.7.\nWe did this since profanity-checker is built for English and other languages may improperly impact the results. For instance, the German nominative/accusative feminine/plural definite article \"die\" is flagged as being profane regardless of context. We split each sentence into words and computed the percentage of words that are flagged as profane for each component of the Pile. We emphasize that this methodology is only a proxy for profanity, given the complexity of determining whether a given word or phrase is profane in context.\nAs shown in Figure 8, the Pile as a whole appears less profane than Pile-CC. Further, the majority of Pile components appear less profane than Pile-CC as well.\nWe also broke each dataset down on a sentence level, to allow profanity-checker to check entire sentences. Splitting datasets by sentence allows for additional context to be considered when determining whether content is pejorative. Our results are shown in Figure 12.\n\n6.4 Bias and Sentiment Co-occurrence\nAs language models may pick up unexpected biases from the training data, we performed a preliminary analysis of the different components that make up the Pile. Because models with different characteristics may be trained on the Pile, we aimed to document the biases of the data and not a specific model. We primarily focus on co-occurrence tests, where we analyzed what words occur in the same sentence as other specific words. Using this information, we can estimate what words strongly bias towards a category word, as well as calculate the general sentiment of surrounding words.\nWe focused our analysis on gender, religion, and race. Our goal is to provide users of this dataset with preliminary guidance on how the different components are biased so that they can make decisions on which components to train on.\nAll tables and figures in this section can be found in the Appendix.\n\n6.4.1 Gender\nWe computed gender associations by computing cooccurrences for binary pronouns. For each word, we computed the difference in the rate it co-occurs with \"he\" and \"she\" 14 and weighed it by the square root of its frequency. We report the top 15 most biased adjectives or adverbs (Loper and Bird, 2002) for each in Table 10. We see that words like \"military\", \"criminal\", and \"offensive\" strongly bias towards men, while \"little\", \"married\", \"sexual\", and \"happy\" bias towards women.\nIn addition, we computed the average sentiment (Baccianella et al., 2010) of words cooccurring with the gendered pronouns across each dataset in Figure 13. Generally, we find no significant sentiment bias towards men or women. This, of course, does not mean that the dataset is free of gender bias (as our co-occurrence tests show).\n\n6.4.2 Religion\nWe computed a similar co-occurrence analysis for religion, which can be found in Table 11. Like gender, we find that these co-occurrences reflect how these terms are used in pockets of online discourse. For example, \"radical\" co-occurs with \"muslim\" at a high rate, while \"rational\" often co-occurs with \"atheist\". This analysis also demonstrates some of the limitations of a purely co-occurrence based analysis. For example, \"religious\" often co-occurs with \"atheist\", which likely reflects the type of conversations in which the word \"atheist\" is likely to occur as opposed to a descriptor of \"atheist\". In addition, we computed the average sentiment of co-occurrences across each of the constituent datasets in Figure 14. Over the entire dataset, we find that \"Buddhist\" has the highest sentiment, followed by \"Hindu\", \"Christian\", \"Atheist\", and \"Muslim\". Notably, \"Jew\" is the lowest, perhaps reflecting its historical use as a pejorative.\n\n6.4.3 Race\nFinally, we ran the same analysis for racial groups. Here, as identifiers like \"black\" or \"white\" often do not indicate race, we instead compute co-occurences with phrases like \"black man\" or \"white woman\".\nWe show the top 15 most biased words for each demographic in Table 12. Once again, we found that the co-occurrences reflect the context in which these terms are used. For example, the 4 most biased words for \"black\" are \"unarmed\", \"civil\", \"criminal\", and \"scary\".\nSimilar to above, we compute the average sentiment of co-occurring words. We report the average sentiment numbers in Table 13. We find that \"hispanic/latino\" narrowly edges out \"asian\" for the highest sentiment, followed by \"white\". On the other hand, \"black\" had the lowest sentiment, at -0.15.\nWe note that for all demographics, the average sentiment is negative. We hypothesize that this is due to the specific context for which the phrases we use to compute co-occurrences appear. For example, it is often quite common for news articles to describe suspects as an \"asian man\".\n\n6.5 Author Consent and Public Data\nAnother issue with the use of texts in natural language processing research is consent. Although one is typically not legally obligated to receive the permission of an author to train a NLP algorithm on their work 15 , many consider doing so a moral obli-gation or a good measure to guard against misuse (Obar, 2020; Prabhu and Birhane, 2020). On the other hand, there is significant disagreement surrounding the ethics of repurposing data protected by terms of service in research contexts (Vitak et al., 2016; Fiesler et al., 2020), particularly given the power asymmetries inherent in digital platforms, which often close off independent researchers from investigating public data while simultaneously compelling users to consent to its private use (Halavais, 2019).\nWhile much of the Pile's data comes from sources that have expressly consented to its wider dissemination and use in research, researchers often fail to clearly document where their data came from and under what terms its use was consented to. In light of this, we felt it appropriate to release the Pile with transparency around how the authors of its data have indicated that that data can be used.\nTo provide needed nuance to our discussion of consent, we identified three tiers of availability for public use. Public data is data which is freely and readily available on the internet. This primarily excludes data which is pay-walled (regardless of how easy that paywall is to bypass) and data which cannot be easily obtained but can be obtained, e.g. through a torrent or on the dark web. Terms of Service (ToS) compliant data is data which is obtained and used in a fashion that is known to be consistent with the terms of service of the data host. Data with authorial consent is data for which the original authors of the work consented to the use of their data, or where a reasonable person could not assume that their data would not be used for purposes such as research. ToS compliant data and authorial consented data differ in two main ways: It is important to keep in mind that people typically do not read Terms of Service, and additionally that being ToS-compliant does not entail authorial consent. We adopted a strict model of consent, where ambiguous or unknown consent is treated as nonconsensual.\nTable 5 summarizes our understanding of the status of each of the datasets within the Pile. Datasets marked with a are compliant in the relevant respects, though a couple datasets are worth remarking on in particular. Book3 and OpenSubtitles are being used in a fashion that is consistent with the terms of service of the data host. However, this is somewhat misleading in that the data host is not authorized to post the data online by the parties that own it. The Enron Emails dataset was not collected with the permission of the authors, but was collected by the U.S. government as part of a criminal investigation. While the people whose emails are in the Enron dataset are aware of this fact, they were not given the ability to consent to its inclusion in any way.\nThere are five datasets included in the Pile that were not collected and distributed in a ToS compliant fashion and for which the authors had no ability to consent to their data being used. Each of these datasets are widely used, both in the NLP literature and the world at large. With the exception of the YouTube Subtitles dataset, each of these datasets were published by researchers and are passed around freely on the internet. The YouTube Subtitles dataset was created by us for this project, using a very popular unofficial API that is both widely used and easily obtainable on Pip, Conda, and GitHub, among other places. Given the processing applied and the difficulty of identifying particular files in the Pile, we feel that our use of these datasets does not constitute significantly increased harm beyond that which has already been done by the widespread publication of these datasets.\n\n7 Implications and Broader Impacts\nThe Pile represents yet another stepping stone along the path of scaling models and datasets to ever larger sizes and capabilities. There are many serious concerns about how the emergence of progressively stronger AI systems will influence the wider world (Brundage et al., 2018; Amodei et al., 2016; Bostrom and Yudkowsky, 2014; Bostrom, 2014; Critch and Krueger, 2020), and we believe that they merit serious thought. In this section we discuss the legal ramifications of the Pile, and then consider the impact of the Pile to AI alignment from two angles: accelerating AI timelines and the dangers posed by unaligned language models.\n\n7.1 Legality of Content\nWhile the machine learning community has begun to discuss the issue of the legality of training models on copyright data, there is little acknowledgment of the fact that the processing and distribution of data owned by others may also be a violation of copyright law. As a step in that direc- tion, we discuss the reasons we believe that our use of copyright data is in compliance with US copyright law. 16 Under pre (1984) (and affirmed in subsequent rulings such as aff (2013); Google (2015)), noncommercial, not-for-profit use of copyright media is preemptively fair use. Additionally, our use is transformative, in the sense that the original form of the data is ineffective for our purposes and our form of the data is ineffective for the purposes of the original documents. Although we use the full text of copyright works, this is not necessarily disqualifying when the full work is necessary (ful, 2003). In our case, the long-term dependencies in natural language require that the full text be used in order to produce the best results (Dai et al., 2019; Rae et al., 2019; Henighan et al., 2020; Liu et al., 2018).\nCopyright law varies by country, and there may be 16 This discussion does not, and is not intended to, constitute legal advice; rather, it is a general discussion of law. Only your attorney can provide assurances that the information contained herein is applicable or appropriate to a particular situation. If in doubt, it is always advisable to speak to an intellectual property attorney.\nadditional restrictions on some of these works in particular jurisdictions. To enable easier compliance with local laws, the Pile reproduction code is available and can be used to exclude certain components of the Pile which are inappropriate for the user. Unfortunately, we do not have the metadata necessary to determine exactly which texts are copyrighted, and so this can only be undertaken at the component level. Thus, this should be be taken to be a heuristic rather than a precise determination.\n\n7.2 Acceleration of AI Timelines\nThere is serious concern that AI systems may soon be meaningfully more capable than humans in all relevant economic tasks (Grace et al., 2018; Yudkowsky, 2013). Relatedly, there are serious unresolved questions surrounding how to properly align such powerful AI systems with human interests (Bostrom and Yudkowsky, 2014; Russell, 2019; Bostrom, 2014; Amodei et al., 2016) and generally avoid morally catastrophic outcomes (Sotala and Gloor, 2017; Shulman and Bostrom, 2020). As such, it has been argued that accelerating the development of such powerful AI systems may be undesirable before these concerns have been more adequately addressed (Bostrom, 2014).\nThere are several pragmatic responses to this view:\n1. Due to human competition, curiosity, and cultural diversity, halting technological development is incredibly difficult, if not impossible.\n(Russell, 2019) (Critch and Krueger, 2020)\n\n2. AI development is experimental in nature:\nThe alignment problem can only be solved through development, testing and (hopefully non-existential) failure.\n3. High powered language models, along with their more general successors, must be capable of viewing morally problematic content without adopting it in their output. We elaborate on this in the following section.\nWith this in mind, we accept the reality that the Pile could potentially accelerate AI timelines. However, we hope our efforts to establish best practices, such as thoroughly documenting the contents of our data, will help encourage diligence for downstream researchers on alignment problems.\n\n7.3 Negative LM Output\nThere has been much discussion about the possible negative effects of powerful language models in the world (Brown et al., 2020; Brundage et al., 2018). Some of these possible problems, such as the ability to mass produce low quality content for the purpose of Search Engine Optimization, are inherent problems to the way online content is distributed, and cannot be stopped by those developing language models alone. Directly solving these problems would require sweeping changes to the architecture of the Internet, such as vastly expanded Public Key Infrastructure and distributed authentication of identity (Ferguson and Schneier, 2003).\nAnother concern is that training such models on huge datasets will almost inevitably require them to have undesirable content in their training sets, such as that promoting hateful stereotypes (Christian, 2020). Having models output undesirable content is, by definition, undesirable, but we believe that attacking this problem from the training set side is unproductive and ultimately leads us away from optimal solutions. If a person reads a racist piece of content, they do not then immediately adopt its racist views-they may be capable of doing so, but can decide not to. This capacity to understand undesirable content and then decide to ignore it is an essential future research direction. Not only would this allow models to use \"dirtier\" data with less concern, but also to use their gained knowledge to better understand what not to do. We recognize that, despite recent progress in human-guided learning (Stiennon et al., 2020), the technology is not yet at this stage, and have thus made a number of editorial decisions as described in this paper. However, this approach seems essential to the future of these models and AI more broadly, and more research is needed.\n\n8 Related Work\nSelf-supervised training of natural language processing models on large, unlabeled text corpora, has seen widespread adoption in the field. Word representation models such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) were trained on datasets such as Wikipedia, Gigaword (Graff et al., 2003), or a non-public Google News corpus. More recently, language models (Radford et al., 2018 (Radford et al., , 2019;; Brown et al., 2020; Rosset, 2019; Shoeybi et al., 2019) and masked language models (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2019), 2019). C4 is comparably-sized to the Pile, while mC4 and CC-100 are larger, multilingual datasets. However, C4/mC4 require immense computational resources to preprocess the data, with its maintainers even recommending the use of a distributed cloud service, 17 setting a high bar of entry to using these datasets. CC-100 is directly downloadable and pre-cleaned; however, its English portion is much smaller than the Pile. Importantly, these three datasets are all derived entirely from Common Crawl-as discussed above, the current best practice in training largescale language models involve using both large web scrapes and more targeted, higher-quality datasets, which the Pile directly addresses. Leo Gao led the project, implemented the main Pile codebase, contributed to the model training code, performed the evaluations and the language analysis, interpreted the perplexity analysis results, implemented the processing to create the final data, and processed Pile-CC, PubMed Central, ArXiv, and Ubuntu IRC. Stella Biderman led the data analysis, the broader impact analysis, and the data documentation, and coordinated the project. She also wrote the analysis of structural statistics, authorial consent, and copyright law. Sid Black implemented the model training and evaluation code and processed YouTube Subtitles, Stack Exchange, and GitHub. Laurence Golding implemented deduplication, performed the n-gram analysis, and processed OpenWebText2. Travis Hoppe processed FreeLaw, Pubmed Abstracts, ExPorter, and PhilPapers. Charles Foster performed the topic modeling analysis, contributed to the discussion of authorial consent, and processed USPTO Backgrounds. Jason Phang implemented and performed the GPT-2/3 perplexity analysis and advised the project. Horace He performed the bias and sentiment analysis. Anish Thite implemented and performed the profanity analysis and processed Hacker News. Noa Nabeshima processed GitHub. Shawn Presser processed BookCorpus2. Connor Leahy wrote the alignment implication analysis and the model training code.\n\nB Excluded Datasets\nIn the course of building the Pile, we considered including and ultimately decided to not use several datasets. We excluded several datasets on the grounds that they were too small to be worth spending time on or because the English component of the data did not merit inclusion on its own. However we also decided to exclude several data sets for other reasons, which we document here for transparency:\n1. US Congressional Record. The official record of the United States Congress (1800 -today) records important points of debate at the highest levels of American government. It reflects the opinions and biases of the political class over the past 200 years, including segregationism and xenophobia. In particular, we found a large quantity of extremely racist content that we did not feel appropriate for a dataset intended for general-purpose language modeling.\n2. Fanfiction. Hundreds of GiB of fanfiction has been written and put online, primarily on the websites www.fanfiction.net and www.https://archiveofourown. org/. This represents a significant untapped resource for language modeling as it is almost exclusively short-form fiction, a writing style that is not represented in most language modeling datasets. We ultimately decided to exclude fanfiction on logistical grounds: we found other sources of data that were easier to obtain.\n3. Literotica. Literotica is a website where users can upload short-form erotic fiction. We had originally planned on including it in the Pile and even went as far as scraping and processing it. However we decided to not include it for several reasons. Firstly, once we decided to exclude fanfiction, Literotica represented our sole source of short-form fiction, which would likely lead to undesirable biases in the trained model. Secondly, Literotica would require significantly more investigation, assessment, and care than we spent on the other datasets. Thirdly, Literotica contains a significant amount of stereotyping, including racial fetishes. While Literotica is likely usable for some tasks, we are not comfortable including it in the Pile.\n\nC Dataset Details\nThis section contains additional information about each dataset listed in Section 2, including how it was obtained, how it was processed, and any other details relevant for replication. The intent of this section is to provide as much detail as possible, so that Pile can be replicated in the future if necessary, and so that any future processing of these and similar datasets can use or improve on our methods. As such, all code created for processing has been made publicly available under permissive open source licenses and is referenced in footnotes where applicable.\n\nC.1 Pile-CC\nWe extract Common Crawl using jusText (Endr\u00e9dy and Nov\u00e1k, 2013) We do not believe that document level filtering is sufficient for WET files because many of the issues with WET files stem from intra-document boilerplate. We also find many of the heuristics used in Raffel et al. ( 2019), such as the removal of all lines without terminal punctuation, the word \"javascript\", and 3-sentence deduplication to be too aggressive.\n\nC.1.2 Extraction\nIn addition to jusText, we also considered Trafilatura, Newspaper, Goose3, and DragNet. While we were originally intending on creating an extraction benchmark, this proved infeasible given our available resources, and we chose jusText based on visual inspection of the output. In inspection, we noticed that jusText has the characteristic that it discards more data than many other extractors, which is not a major drawback given the large volume of CC data available. This was as expected, given jusText's intended application for text corpora creation. In contrast, trafilatura is, for instance, better at preserving the structure of the website faithfully, often correctly extracting elements such as tables, but it kept too much unnecessary boilerplate. Had we used trafilatura, we would have required an additional intra-page filtering step to remove boilerplate from the page.\n\nC.1.3 Languages\nWhile jusText does technically support several other languages, the quality on those languages is worse than on English as many constants in the algorithm are specifically tuned for English. Additionally, jusText is completely unable to handle languages such as Chinese and Japanese, which do not use spaces to delimit words.\nDue to the difficulty of maintaining an acceptable level of extraction quality across all languages, we decided to restrict the scope of the CC dataset to only English and leave a high-quality, fully multilingual, WARC-based CC-based dataset to future work. To filter for only English, we use the py-cld2 library and only attempt to extract text from documents where English is the most common language.\nWe use pycld2 instead of fasttext because it is capable of classifying the language from the HTML directly, and since jusText requires knowledge of the language of the webpage before extraction. Additionally, pycld2 was significantly faster than jus-Text, and by only processing with jusText documents classified as English by pycld2, we reduced the required computation by approximately half.\nExtracting text from websites for language modeling, especially for multilingual corpora, is highly nontrivial, and we leave the refinement of such extraction to future work.\n\nC.1.4 Filtering\nTo filter CC for quality, we follow Brown et al. (2020) 2020), with \u03b1 = 3. Our choice of \u03b1 targets the filtering ratio necessary to filter our subset of CC to the size we needed. The impact of \u03b1 on the filtering ratio is shown in Table 6.\n\nC.2 Pubmed Central\nWe use pandoc 1. 19.2.4 (MacFarlane, 2006 19.2.4 (MacFarlane, -2020) ) to convert the JATS format data provided by PMC to markdown. Afterwards, we remove any line beginning with :::, which is used by pandoc to indicate html classes in markdown.\n\nC.3 Books3\nNo additional details.\n\nC.4 OpenWebText2\nTo produce the dataset, URLs and their associated metadata were first extracted from all Reddit submissions up to April 2020. URLs were deduplicated, with each unique URL featuring a list of associated submissions metadata, and an aggregate score. URLs with an aggregate score of less then 3 were removed. The links were then scraped and processed with Newspaper scraper. Deduplication was performed at the document level using in memory MinHashLSH through the DataSketch library.\nBoth\n\nC.4.1 Extractor Choice\nWe chose to use Newspaper instead of jusText for OpenWebText2 for consistency with OpenWeb-TextCorpus. Additionally, by using multiple different html extractors for different components of the Pile, we reduce the potential impact of systematic biases from any one extractor negatively impacting the dataset.\n\nC.5 ArXiv\nWe downloaded the T E X sources of all papers on arXiv up to the July 2020 dump (the last file included in our data is arXiv_src_2007_068.tar) via arXiv's S3 Bulk Source File Access 18, and used pandoc 1.19.2.4 to convert these source files to Markdown, discarding any papers which had errors during the conversion process. This yielded a total of 1,264,405 papers.\nWe remove any line beginning with :::, which is used by pandoc to indicate html classes in markdown.\n\nC.6 GitHub\nWe separate the data gathering process into two steps:\n1. Gathering a list of the desired repositories and their metadata\n2. Extracting all text data useful for language modeling from each repository\nFor the first step, mirroring the approach of the WebText dataset, we use GitHub 'stars' as a proxy for quality, and choose to gather only repositories with more than 100 stars. For practical reasons, we also limit the list of repositories gathered to repositories with less than 1GB of files. Since Github's API limits the number of search results to 1000, in order to comprehensively gather all repositories we need to create many small queries that each return fewer than 1000 results in such a way that every repository of interest will be returned by at least one of our queries. To achieve this, we bound our initial search by size to return only repositories between a lower bound of 0 and 5 bytes. At the time of writing, this returns 965 results. For the next step, we set our lower bound one above our previous upper bound, and decide on a new upper bound that should also return fewer than 1000 results by   This tends not to overshoot, because Github repositories follow a power distribution with respect to size, but if it does, we simply use the amount of repositories our new query returned in order to construct a new upper bound estimate.\nUsing the gathered list of repositories, we clone each one, extract any text-based files, and discard the rest. Because some repositories took an impractical amount of time to clone and/or extract, we set a hard time limit of 300 seconds for both the git cloning and text extraction steps. As such, some larger repositories may only be partially extracted. We also impose a file size limit of 100kB on extracted files, as we found that the majority of files over that size were typically very repetitive autogenerated source files or data files, and that setting this file size limit was an effective cleaning step to limit the data to code.\nBecause we wanted to limit the size of the overall Pile, we randomly sampled 95.0 GiB of the 630.64 GiB of Github data we collected in total and leave quality filtering to future work. However, we believe code generation will be an increasingly important component of language models as they continue to scale up and increase in their ability to generalize. As such, we hope to extend this dataset in future work.\n\nC.7 FreeLaw\nWe download the court opinions data in bulk from CourtListener, 19 and extract the raw text using BeautifulSoup.\n\nC.8 Stack Exchange\nTo construct the dataset, we download and parse every Stack Exchange database dump to plaintext files. We opt to extract the top three answers with at least three upvotes, discarding all other responses. We only include the plain text question and response and do not incorporate any metadata. Motivated by large-scale language models' few-shot ability (Brown et al., 2020), we provide context by prepending all questions and answers with Q:\\n\\n and A:\\n\\n respectively.\nThe resulting dataset contains a total of 15,622,475 documents across a total of 365 Stack Exchanges and Meta-Stack Exchanges, the bulk of which is from StackOverflow.\n\nC.9 USPTO Backgrounds\nThe United States Patent and Trademark Office (USPTO) has published bulk archives of the full text of all patents granted in the US from 1976 to September 2020. From these archives, we extract the Background sections, along with key grantspecific metadata, such as the inventor, assignee, and classification information.\nThe file format used for storing bulk text US patents has changed over time. Prior to 2002, all of the datasets are in a specialized format called APS (Automated Patent System). Since 2002, the data is XML encoded. Partially as a function of this change, the location of the \"Background\" section has also shifted. Our converter accounts for these structural shifts and extracts the raw text from each patent's Background.\n\nC.10 PubMed Abstracts\nAbout one-third of the articles in the dataset were missing or contained a malformed title or abstract and were excluded. Additionally, PubMed Central (see Section 2.2) contains full-text resources to many recent publications; any publications which already appear in PMC are excluded from this set. To process the data, we concatenated the title and abstract and removed any copyright information. The remaining dataset contains 15,518,009 titles and abstracts.\n\nC.11 Project Gutenberg\nNo additional details.\n\nC.12 OpenSubtitles\nTo create the text dataset, we simply extract the subtitle text from each XML file in the English language dataset provided by Tiedemann (2016), discarding any provided metadata.\n\nC.13 Wikipedia (English)\nWe use the wikipedia/20200301.en dataset from TensorFlow Datasets. 20 We prepend the title to the body of each article, separated by two newlines.\n\nC.14 DeepMind Mathematics\nWe include instances from the Easy, Medium, and Hard components of DeepMind Mathematics, breaking each curriculum item (such as algebra__polynomial_roots) into 8 KiB chunks.\n20 https://www.tensorflow.org/datasets/ catalog/wikipedia#wikipedia20200301en\n\nC.15 Ubuntu IRC\nWe processed all logs from July 5, 2004 through September 1, 2020.\nTo process the data, all system messages, such as joins, disconnects, nick changes, etc. were discarded, but actions (i.e using /me) were kept. Timestamps were removed, and all logs for the same channel in a given week were concatenated into a single document, with each the logs for each day prepended with the date if that day's log is non-empty.\n\nC.16 BookCorpus2\nThe original BookCorpus consists of 11,038 books. However, due to issues with availability of the original BookCorpus, as well as the possibility of collecting a larger version, we decided to collect our own version of BookCorpus using a similar methodology as Kobayashi (2018). Our version of BookCorpus contains 17,868 books instead.\nWe create and use a modified version of the epubto-text converter in Kobayashi ( 2018) that:\n\u2022 Correctly preserves the document structure across chapters, matching the table of contents very closely;\n\u2022 Correctly renders tables of data, whereas by default html2txt produces poor-quality results for tables,\n\u2022 Correctly preserves code structure, so that source code is visually coherent,\n\u2022 Converts numbered lists from \"1\\.\" to \"1.\"\n\u2022 Runs the full text through ftfy.fix_text() (Speer, 2019), replacing Unicode apostrophes with ascii apostrophes and expanding Unicode ellipses to \"...\" (three separate ascii characters).\n\nC.17 EuroParl\nWe download the data in bulk from 21. We remove all basic tag information and only retain the name of each document as a title. For example, <SPEAKER ID=77 LANGUAGE=\"NL\" NAME=\"Pronk\"> becomes Pronk, and then extract the body of each document, discarding those that are shorter than 200 characters.\n\nC.18 HackerNews\nWe first use the Hackernews BigQuery dataset to obtain a list of all story ids in our date range. For the Pile we use the first Hacker News post (1) to post number 24531712. This corresponds to a date range of approximately 10/09/2006 to 09/20/2020. We use the BigQuery dataset to gather story ids for efficiency purposes. However, the BigQuery dataset was lacking some information for stories, so we used the official Hacker News API for story and comment text retrieval.\nHacker News displays and stores comments in a tree-like manner, with children comments replying to parent comments. However, most language models require input data to be in a sequential form. Considering each path through the comment tree as a sequence could be detrimental, since there will be a large amount of near-duplicate comment sequences. In addition, only taking one path through the comment tree for each story leaves out a large portion of the comment data. Therefore, we parsed comments in a hybrid form. For every top-level comment (comments that have no parent comment), we create a sequence of comments by traversing down the comment tree from the top-level comment. We choose the next comment by taking the child comment with the highest number of children comments (a cheap attempt at taking a long path through the comment tree, note that it does not take the longest possible path).\nWe consider all stories that have at least one comment and are not flagged by the moderators for potential conduct violations. Since comments are stored in HTML, we use the html2text package to extract the text from the post.\nWe order each document by listing the title, url, sub-title, and author at the top. Top-level comments are delimited by \"\\n----\\n\" and sub-comment chains are delimited by \"\\n~~~\\n\". We include author and extracted text for each comment.\n\nC.19 YouTube Subtitles\nWe construct the dataset in three stages:\n1. We build a large list of search terms by prompting a GPT-3 model with a manually selected list of queries, manually filtering the responses, and repeating this process iteratively until a suitable size is reached. The list of terms is centred around, but not limited to, educational topics.\n2. We use requests-html to gather a list of 1000 Youtube video IDs for each search term, and deduplicate the resulting video ids across search terms.\n3. We use YoutubeTranscriptApi 22 to gather all human generated closed captions for every available language for each video.\nTo align each language in parallel, we split the captions for each language into parallel minute-long sections by timestamp, and arrange each language in a random order within these sections, appending the language as a header to each minute-long section to provide context. If only a single language is available, the output is just the subtitles, with no header appended.\nIn total, subtitles for 173,651 videos were gathered.\n\nC.20 PhilPapers\nThe PhilPapers (PP) are indexed using OAI-MPH, the Open Archives Initiative Protocol for Metadata Harvesting. As such, the first step to collect the data is to get the XML for all links. This was done using pyoaiharvester. 23 rom that, each publication is downloaded. Some entries do not exist, or have been removed by the authors. Papers with text are extracted using pdfbox, and papers with non-machine readable text are ignored. Non-English language publications are kept, and the metadata reflects the language reported by the OAI-MPH XML. The text is filtered with pdf_filter.py from PDFextract, and we discard any papers with less than 1000 characters. 24\n\nC.21 NIH Grant abstracts: ExPORTER\nThe NIH provides a bulk-data repository for awarded applications through the ExPORTER service covering the fiscal years 1985-present. These data come from the NIH, but also other other Health and Human Services agencies (ACF, AHRQ, CDC, HRSA, FDA), and the VA. Additionally, the NIH provides a legacy data format named CRISP for awarded applications during the fiscal years 1970-2009.\nWe merged both the ExPORTER and CRISP data to form a consolidated dataset of awarded applications. Entries were deduplicated based off their application ID, and excluded if their abstract text was missing or too short. Small grants, especially administrative ones, consisted solely of short boilerplate. For this reason, we further deduplicated on abstract text. All grants types were considered, including new applications (Application Type Code 1) and renewals (Application Type Code 2) as the text differed enough to provide novel input. The text was then minimally parsed to remove administrative boilerplate, (ex. most old awards contain some variation of \"description: (provided by applicant)\"). In total, there were 939,668 grant application abstracts added.\n\nC.22 Enron Emails\nTo extract the data, we used the mailparser package 25 to extract the body of each email as a document.\n\nD General Data Processing\nThis section discusses any processes applied across multiple datasets.\nTo combine the constituent datasets, we iterate until the size of the output dataset is the desired size, drawing documents from datasets at random, weighted by the number of documents in each dataset times the number of epochs desired on that dataset. Because the number of documents involved is high, by the law of large numbers, the number of copies of each dataset present in the Pile is approximately equal to its epoch count.\nShuffling a dataset posed a major problem due to our limited memory and computational budget. We follow Hardin (2018), a method descended from Rao (1961), and interleave our output to produce 30 output piles.\nWe hold out approximately 10GiB of data from the Pile, of which 2GiB are used to create the validation and test splits, and the remainder is held in reserve. From the training set, we remove any 25 https://github.com/SpamScope/ mail-parser elements that are also present verbatim in any of the held out data, to prevent leakage.\n\nD.1 Weights\nSimilar to Brown et al. (2020), we increase the weight of certain components such that the number of epochs elapsed on data we consider high quality is greater than one. Our choice of weights was primarily informed by the source of the data and the size of the dataset; we attempted to upweight academic texts the most, which we felt provided the highest quality data, as well as smaller sets, such that they would have a more pronounced impact on the data. We strictly disallowed any data more than 3 epochs and avoided having any data with more than 2 epochs.\n\nD.2 Deduplication\nDue to memory constraints we did not perform Pile wide de-duplication. Instead, de-duplication was performed at the document level within Open-WebText2 and Pile-CC as those sets were the most likely to contain duplicate documents.\nThe same technique was used for both OpenWeb-Text2 and Common Crawl-MinHashLSH with the Python Datasketch library. 26 We used 10 hash functions for each Minhash and an approximate Jaccard similarity of 0.5. This produced a duplicate rate of 28% in OpenWebText2 and 26% for Common Crawl.\nThe main challenge here was computational, leading us on a journey through the various LSH persistence options. A simple quadratic Minhash comparison of all documents would have taken several hundred thousand years, motivating the use of LSH. Initially, we did not have sufficient RAM for inmemory LSH and chose to use the Cassandra backend when de-duplicating OpenWebText2. This was reasonably fast, but the same method resulted in a corrupted database about 3 4 of the way through processing Common Crawl. After the Cassandra corruption, we briefly tested the experimental Mongo implementation; however this was quite slow due to the nature of Mongo itself. In the end, we ran in-memory LSH on a machine with enough RAM for Common Crawl, taking several days.\n\nD.3 Downstream Validation Leakage\nTo avoid leakage of data from downstream evaluations, recent work (Radford et al., 2019; Brown et al., 2020; Shoeybi et al., 2019) has removed any data in the training set that may overlap with the evaluation metrics. We decided not to perform any such removal, because it is impossible to anticipate all potential downstream evaluation metrics, and so any particular selection of metrics would inevitably either become obsolete as the choice of benchmarks in the field changes, or potentially hinder the development of new benchmarks for models trained on Pile.\nFor models trained on Pile and evaluated on metrics other than Pile's own validation and test sets, we encourage authors to remove overlaps between Pile and the validation data of these additional downstream evaluations. We do not anticipate that such leakage removal will hurt model performance, as the validation sets of most benchmarks are very small in relation to the size of the Pile, and so choosing to evaluate on more metrics will not be a disadvantage for any model.\n\nE Investigating data E.1 13-Gram Analysis\nAs part of our exploratory analysis, we calculated the counts of all 13-grams across Common Crawl. We chose n = 13 due to its use in prior work (Brown et al., 2020). There were a total of 40,216,231,078 different 13-grams in this dataset. The 1000 most common range from 11 million occurrences down to 20k.\nThe most frequently occurring 13-grams were character repetitions used for styling such as \"----\", \" * * * * \", \"! ! ! !\", at 11 million, 5.8 million and 1.1 million respectively. Other characters used in this manner include the following: \"# . > ?\". In the 264k count range, we see repetitions of badly formatted HTML escape characters \"; &nbsp\", \"; amp\". Boilerplate from standard forum software appears around the 180k occurrences range, such as the following: \"select the forum that you want to visit from the selection below\".\nOverall, a large amount of common HTML and CSS is included in the top 1000, along with boilerplate text from Amazon Affiliate Advertising,\n\nComponent\nTokens TripAdvisor, SimplyHired, Associated Press, Post-Media, The FCC etc. PHP error messages and password login prompts also made an appearance.\nIt may be of interest to fans of Portal that repetitions of \"the cake is a lie .\" achieved a high count.\n\nE.2 Benchmark Perplexity Computation\nTo compute the perplexity for a given dataset, we tokenize each document separately, divide the document into segments of up to the maximum sequence length of the model (1024 tokens for GPT-2, 2048 for GPT-3), and predict the logits of the each segment. The inputs to the model are the immediate prior tokens the e.g. for scoring tokens 1 to 1024, we provide tokens 0 to 1023 at the input context. The respective language model implementations handle the causal attention masking. This ensures that every token in the dataset is scored exactly once. This also means that some tokens will have more input context than others. We then aggregate over the whole dataset and compute the final per-plexity score. The perplexity for the whole Pile is computed by aggregating over the constituent datasets (i.e. weighted by dataset size, not a simple average of dataset perplexities). Both GPT-2 and GPT-3 share the same tokenizer and vocabulary, making the perplexity scores directly comparable. We use the Hugging Face (Wolf et al., 2020) implementation of GPT-2, and the OpenAI API for GPT-3. The davinci model in the OpenAI API is presumed to correspond to a 175B parameter version of GPT-3.\nIn Table 8 we show the test set perplexities (i.e. not normalized by UTF-8 length, as in Table 2). Because of the costs associated with using the OpenAI API, we compute test perplexities on only one-tenth of the test set in Tables 8 and Table 2. Specifically, we randomly sample one-tenth of the documents of each dataset except for three: Ubuntu IRC, Book-Corpus2, and PhilPapers. In Table 9, we show test perplexity computed on the full test set on all GPT-2 models.\n\nE.3 Pejorative Content\nInitially we decided on separating pejorative content into 4 groups: sex-related terminology, slurs, neither of these categories, and both of these categories. We adapted a public \"naughty words\" list and broke them into these categories with the intern of looking at the proportion of each category in each dataset. However, this provided many issues.\nFirst, any blacklist of words would be hard-pressed to catch all the instances of pejorative content, since purposeful misspellings of words could evade the censor and still have the intended effect. Furthermore, words and their intents are always evolving, therefore any list created would likely be always outdated. Another issue pertains to sorting the words into the categories. Words are highly dependent on their context, so a word would change categories with different contexts.\n\nF Data Samples\nThe following consists of two random, noncherrypicked 512-byte samples from each constituent dataset of the Pile, sampled from the validation split.\n\nF.1 Pile-CC\npot trending topics and the coverage around them. First up, there's a bit of a visual redesign. Previously, clicking on a trending topic would highlight a story from one publication, and you'd have to scroll down past a live video section to view related stories. Facebook is replacing that system with a simple carousel, which does a better job of showing you different coverage options. To be clear, the change doesn't affect how stories are sourced, according to Facebook. It's still the same algorithm pickin e public safety. He said the bridge saves commuters two or three minutes when trains pass -and those minutes could be vital.\n\"Two to three minutes may not mean much if you're just driving home from work, but if you're the one waiting for an ambulance to get to your home, if you're the one waiting for a fire truck to get to your home, if you're the one waiting for a police car to get to your home, those two to three minutes could mean the difference between life or death,\" Sharp said. \"That's what this pro\n\nF.2 PubMed Central\nIntroduction ============\nTotal knee arthroplasty (TKA) is a promising treatment for endstage osteoarthritis (OA) of the knee for alleviating pain and restoring the function of the knee. Some of the cases with bilateral TKA are symptomatic, necessitating revision arthroplasty in both the knees. A bilateral revision TKA can be done ei ent\\'s ability to make judgements and decisions about their work experiences and learning that will position them as future critical thinkers, life longer enquirers and learners.\n\nConclusion {#jmrs290-sec-0014} ==========\nIdentification of the core capabilities that our stakeholder community rate highly has proved informative in assisting us to describe a \"work ready *plus\"* medical imaging graduate for the New Zealand context. The results have provided data to the curriculum development team allowing them\n\nF.3 Books3\ncept of _for\u00e7age_ , 'a forcing of language enacted by the advent of an \"other\" language that is at once immanent and created', 44as Badiou puts it: this opens up vistas of a truly syntactic analysis of the poem, in which, again, Badiou would be close to his philosophical other, Deleuze, who, as we just saw, defines style through a-grammaticality and who tries to define what he calls an 'intensive line of syntax'.45\nNevertheless, the insistence on syntax as guarantee involves a _seventh paradox_ , the parad rnment, before the Second World War there were 5,300 communities and two million burakumin. The BLL thinks there must be at least three million burakumin living in Japan today.\nWe visited a hall in Osaka where a taiko drum group, made up exclusively of young burakumin, were about to start their weekly rehearsal. The small gymnasium was filled with taiko drums of all sizes.\nThe smallest was about the size of a snare drum, the largest about the size of a compact car. The Japanese drum group Kodo have made th\n\nF.4 OpenWebText2\nprime minister to repatriate all the police sent to Catalonia before the referendum.\nWhat were the results?\nWith nearly all votes counted, the pro-independence parties Together for Catalonia (JxCat), Republican Left of Catalonia (ERC) and Popular Unity (CUP) were on course to win a total of 70 seats in total, giving them a majority in the new parliament.\nCitizens (Cs) had 25.3% of the vote, winning 37 seats in the 135-seat chamber.\nIts leader told the BBC her party had been \"victorious\". Ms In\u00e9s Arrima so accommodate Stablecoins.\nWhile some analysts opined that Stablecoins are created to bring growth into the crypto space, they are becoming a solid way to reduce crypto volatility due to the fact that their value are pegged to fiat currency. lose\"> <at-form state=\"vm.form\" autocomplete=\"off\" id=\"external_test_form\"> <at-input-group col=\"12\" tab=\"20\" state=\"vm.form.inputs\" form-id=\"external_test\"></at-input-group> <at-action-group col=\"12\" pos=\"right\"> <at-action-button variant=\"tertiary\" ng-click=\"vm.onClose()\" > {{::vm.strings.get('CLOSE')}} </at-action-button> <at-action-button variant=\"primary\" n F.7 FreeLaw ssible, and further, that the weight of the evidence, the credibility of the witnesses and the persuasive effect of the testimony is for the sole determination of the trier of fact. This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did *487 under the previous statutory requirement found in 12 V.S.A. \u00a7 2385. In essense, the defendants urge that this Court should reconsider the case of Green Mountain Marble Co. v. Highway Board, supra, and follow the Federal practice of looking to the evide ng to the fact that the relevant Arkansas statutes and rules provide for criminal sanctions against school officials who fail to enforce the immunization requirements, the Morningstar and Lake Hamilton School Districts characterized themselves as disinterested bystanders caught in the crossfire between the Schoolchildren and the Officials. See Ark. Code Ann. \u00a7 6-18-702(c)(2)(B) (2000) (\"Any school official, parent, or guardian violating the regulations shall be subject to the penalties imposed herein.\"); Id  In 1840, he married Charlotte Elizabeth Grove Annesley, daughter of Arthur Grove Annesley and Elizabeth n\u00e9e Mahon, and they had at least one child: John Loftus Bland . After Charlotte's death in 18 heart of the University campus, a meeting-place for all academic disciplines, improving its opportunities to co-operate across traditional academic boundaries. It also gives USBE-students an opportunity to take an active part of student environment created for the 37 000 students at Ume\u00e5 University.\n\nOrganization\nUme\u00e5 School of Business, Economics and Statistics has three departments:\nthe Department of Business Administration, the Department of Economics and the Department of Statistics.\n\nUSBE Career Cent\nF.10 USPTO Backgrounds nductivity types), it is necessary that at least some process is steps differentiate between p-type and n-type transistors. Separate implant steps, for example, are needed to define n-well and p-well structures and to dope the source/drain regions of n-channel and p-channel transistors. Whenever possible, however, it is generally desirable to use a single process step to define transistor features regardless of the transistor type. Single process steps imply a single mask step, which is always desirable to enser further comprising a means of identifying the user by voice recognition. Also, an object is dispenser further comprising a means of identifying a supervisor by voice recognition. Furthermore, an object is a dispenser further comprising a means of customizing the plurality of aural messages for instructing the user during each of the plurality of washing steps. An object of the invention is a dispenser for metering a liquid cleanser to a user and prompting the user in compliance with a recommended wash F.11 PubMed Abstracts ent (REM) latency were found to be significantly worse in Group 1 as compared with Group 2. Cognitive and executive parameters were significantly impaired in Group 1. Shorter total sleep time, poorer sleep efficiency, and prolonged sleep latencies were observed to be associated with poor memory and executive function in patients with refractory epilepsy. Our study strongly suggests that sleep disturbances, mainly shorter total sleep time, poor sleep efficiency, and prolonged sleep latencies, are associated neurons in vesicular GABA transporter (VGAT)-venus transgenic mouse. Inhibitory neurons play important roles in a number of brain functions. They are composed of GABAergic neurons and glycinergic neurons, and vesicular GABA transporter (VGAT) is specifically expressed in these neurons. Since the inhibitory neurons are scattered around in the CNS, it is difficult to identify these cells in living brain preparations. The glutamate decarboxylase (GAD) 67-GFP knock-in mouse has been widely used for the identif F.12 Gutenberg (PG-19) s he met with as a novelist, he was anxious to prosecute his original profession of medicine; and having procured from a foreign university the degree of M.D., he commenced to practise physic in Chelsea, but without success. He wrote, however, an essay \"On the External Use of Water,\" in which he seems to have partly anticipated the method of the cold-water cure. In 1753 he published his \"Adventures of Count Fathom;\" and, two years later, encouraged by a liberal subscription, he issued a translation of \"Don Yearn; Its Advertising brought us such Renown, We jumped Three Hundred Thousand, on that Turn!\" XXXVI I think the man exaggerated some His increased Circulation,-but, I vum! If I could get Two Thousand for one Tale, I'd write him Something that would simply Hum! XXXVII For I remember, shopping by the way, I saw a Novel writ by Bertha Clay; And there was scrawled across its Title-Page, \"This is the Stuff that Sells-so People say!\" XXXVIII Listen-a moment listen!-Of the same Wood-pulp on wh F.13 OpenSubtitles ad for you.\" \" Too bad for me?\" \"How about too bad for you?\" \"Oh no!\" \"Luckily I keep a spare.\" \"Look everyone!\" \"My winky was a key!\" \"Oh dear, bloody Dutchman.\" \"Foxxy, I'm coming!\" \"Don't do anything stupid or the shooting begins.\" \"Austin, take Ducky I'll stay here and be your backup.\" \"Ducky, what do we do?\" \"I'm not really a \"hands-onevil-genius\".\" \"Think you were always the smart one.\" \"I could re-write the output capacity to the tractorbeam from one of the conduit boxes up there.\" \"Come on, let's .\" \"this calls for... four people.\" \"Yes!\" \"we got it.\" \"guys.\" \"We got it.\" \" Got what?\" \" Our sub.\" \" Did he say sub?\" \" Mm-hmm.\" \"Only private sub on the Florida coast rated for 300 fathoms.\" \"Sub as in submarine?\" \"Following up on your haloclines.\" \"so we're going to have to drive all night... if we're going to be there by morning.\" \"Anybody have trouble sleeping in a car?\" \"whoa.\" \"Wait a minute.\" \"What happened to the nice offices in Canaveral City?\" \"Mr. Benirall expects you to take 'em.\" \"we just go F.14 DM Mathematics ~~s gupta Thanks for the heads up! --a3_nm How exactly is this service better than, say, a simple text file on my own machine with a daily reminder set up through some other means?\nWhy would I want to use some third-party website for somethi or Amazon EC2 and Amazon SQS. The bandwidth tier in which you will be charged each month will be calculated based on your use of each of these services separately, and could therefore vary across services ). The child now begins to understand that she is a subject that can be observed by others, just like she can observe the behavior of others, and she can begin to consider others' perspectives on herself. It is at this point that the child begins to fully apprecia d an entire chapter detailing the remarkable achievements of Ashkenazi Jews and hold them up as exhibit A in the argument that human evolution has been, in Wade's words, recent, copious, and regional. The example of Ashkenazi evolution is supposed to show the absurdity of the view, held by authors like Jared Diamond and Stephen Jay Gould, that human evolution either stopped one hundred thousand years ago or that natural selection has somehow continued to sculpt the bodies but not the brains of different gro\n\nF.21 NIH ExPorter\nrapies that can inhibit the EMT, but few assays for EMT inhibitors in high throughput screens (HTS) have developed. A change in fibroblast growth factor receptor 2 (FGFR2) splicing occurs during the EMT and using an innovative luciferase-based splicing reporter assay we previously carried out a genome-wide high throughput cDNA expression screen for regulators of this splicing switch. This screen identified the epithelial cell type specific splicing regulators ESRP1 and ESRP2 demonstrating the feasibility of l and behavioral research projects utilizing primates residing in a seminatural habitat. This population has the most extensive computerized demographic and genetics database available to researchers anywhere in the world. The population management program for CS has been designed to optimize the health and well-being of the monkeys, to enhance the value of the colony for research. In addition, the goal is to provide healthy animals to the scientific community for biomedical research, including AIDS and Sl\n\nF.22 Enron Emails\nwant to make sure that my vacation time gets paid at 100% before I go down to the 90% level.\n\nFootnotes:\n2: https://github.com/EleutherAI/ the-pile\n4: https://archive.org/details/ stackexchange\n5: https://bulkdata.uspto.gov/\n6: https://irclogs.ubuntu.com/\n7: https://philpapers.org/\n8: https://exporter.nih.gov/\n9: https://news.ycombinator.com\n10: While the sizes of GPT-3 models on the OpenAI API have not been publicized, we assume here that ada, babbage, curie and davinci models correspond to 2.7B, 6.7B, 13B and 175B parameter models respectively.\n11: The data was obtained from http://data.statmt. org/cc-100/.\n12: Brown et al. (2020)  discusses ethical issues surrounding their model, but do not discuss those surrounding the training dataset itself.\n13: That said, we did exclude several datasets, see Appendix B for details.\n14: We chose to only study male and female pronouns as a simplifying assumption. Studying \"they\" would require us to isolate its usage as a singular noun.\n15: Laws vary by country. For a discussion of US law, see Section 7.1\n17: https://www.tensorflow.org/datasets/ catalog/c4\n18: https://arxiv.org/help/bulk_data_s3\n19: https://www.courtlistener.com/api/ bulk-info/\n21: http://www.statmt.org/europarl/\n22: https://github.com/jdepoix/ youtube-transcript-api\n23: https://github.com/vphill/ pyoaiharvester/\n24: https://github.com/sdtblck/PDFextract\n26: https://github.com/ekzhu/datasketch\n\nReferences:\n\n- Sony corp. of america v. universal city studios, inc. 2003. Kelly v. arriba soft corp.- Righthaven llc v. hoehn.\n\n- Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. 2016. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.\n\n- Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- tiani. 2010. Sentiwordnet 3.0: An enhanced lexical re- source for sentiment analysis and opinion mining. In LREC. European Language Resources Association.\n\n- Emily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Lin- guistics, 6:587-604.\n\n- Stella Biderman. 2021. Data statement for the Pile. arXiv preprint arXiv.\n\n- Stella Biderman, Kieran Bicheno, and Leo Gao. 2021. Datasheet for the Pile. arXiv preprint arXiv. Stella Biderman and Walter J. Scheirer. 2020. Pitfalls in machine learning research: Reexamining the devel- opment cycle. NeurIPS \"I Can't Believe It's Not Bet- ter!\" Workshop.\n\n- David M Blei, Andrew Y Ng, and Michael I Jordan.\n\n- Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993-1022.\n\n- Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of \"bias\" in NLP. arXiv preprint arXiv:2005.14050.\n\n- Nick Bostrom. 2014. Superintelligence: Paths, Dan- gers, Strategies. Oxford University Press, Inc. Nick Bostrom and Eliezer Yudkowsky. 2014. The ethics of artificial intelligence. The Cambridge hand- book of artificial intelligence, 1:316-334.\n\n- Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. 2018. The malicious use of artificial intelligence: Fore- casting, prevention, and mitigation. arXiv preprint arXiv:1802.07228. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlings- son, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models. Isaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. 2020. Language id in the wild: Unex- pected challenges on the path to a thousand-language web text corpus. arXiv preprint arXiv:2010.14571. Brian Christian. 2020. The Alignment Problem: Ma- chine Learning and Human Values. WW Norton & Company.\n\n- Alina Maria Ciobanu, Liviu P Dinu, and Andrea Sgarro. 2017. Towards a map of the syntactic similarity of lan- guages. In International Conference on Computational Linguistics and Intelligent Text Processing, pages 576- 590. Springer.\n\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross- lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, pages 8440-8451, Online. As- sociation for Computational Linguistics.\n\n- Marta R Costa-juss\u00e0, Roger Creus, Oriol Domingo, Albert Dom\u00ednguez, Miquel Escobar, Cayetana L\u00f3pez, Marina Garcia, and Margarita Geleta. 2020. Mt- adapted datasheets for datasets: Template and reposi- tory. arXiv preprint arXiv:2005.13156. Andrew Critch and David Krueger. 2020. AI Re- search Considerations for Human Existential Safety (ARCHES). Preprint at acritch.com/arches. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a fixed-length context. arXiv preprint arXiv:1901.02860.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics.\n\n- Istv\u00e1n Endr\u00e9dy and Attila Nov\u00e1k. 2013. More effec- tive boilerplate removal -the GoldMiner algorithm. In Polibits. Niels Ferguson and Bruce Schneier. 2003. Practical Cryptography. John Wiley & Sons. Casey Fiesler, Nathan Beard, and Brian C Keegan.\n\n- No robots, spiders, or scrapers: Legal and ethical regulation of data collection methods in social media terms of service. In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 187-196.\n\n- Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. 2018. Datasheets for datasets. arXiv preprint arXiv:1803.09010. Aaron Gokaslan and Vanya Cohen. 2019. Openweb- text corpus. http://Skylion007.github.io/ OpenWebTextCorpus. Authors Guild v. Google. 2015. . Docket No. 13-4829- cv, 804:202.\n\n- Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. 2018. When will AI exceed human performance? evidence from AI experts. Jour- nal of Artificial Intelligence Research, 62:729-754.\n\n- David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Con- sortium, Philadelphia, 4(1):34.\n\n- Declan Groves and Andy Way. 2006. Hybridity in mt: Experiments on the Europarl corpus. In Proceeedings of the 11th Annual conference of the European Associ- ation for Machine Translation (EAMT 2006).\n\n- Alexander Halavais. 2019. Overcoming terms of ser- vice: a proposal for ethical distributed research. Infor- mation, Communication & Society, 22(11):1567-1581.\n\n- Chris Hardin. 2018. How to shuffle a big dataset. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scal- ing laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701.\n\n- Matthew Hoffman, Francis Bach, and David Blei.\n\n- Online learning for latent dirichlet allocation. advances in neural information processing systems, 23:856-864.\n\n- Dirk Hovy and Shannon L Spruit. 2016. The social im- pact of natural language processing. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pages 591-598.\n\n- Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen De- nuyl. 2020. Social biases in NLP models as bar- riers for persons with disabilities. arXiv preprint arXiv:2005.00813. Eun Seo Jo and Timnit Gebru. 2020. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Con- ference on Fairness, Accountability, and Transparency, pages 306-316.\n\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Bryan Klimt and Yiming Yang. 2004. The Enron cor- pus: A new dataset for email classification research. In European Conference on Machine Learning, pages 217-226. Springer. Sosuke Kobayashi. 2018. Homemade bookcor- pus. https://github.com/BIGBALLON/ cifar-10-cnn.\n\n- Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, vol- ume 5, pages 79-86. Citeseer. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional com- putation and automatic sharding. arXiv preprint arXiv:2006.16668.\n\n- Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692. Edward Loper and Steven Bird. 2002. NLTK: The Nat- ural Language Toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computa- tional Linguistics, pages 62-69. Somerset, NJ: Associa- tion for Computational Linguistics. http://arXiv. org/abs/cs/0205028.\n\n- John MacFarlane. 2006-2020. Pandoc: a universal document converter.\n\n- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Ad- vances in Neural Information Processing Systems, vol- ume 26, pages 3111-3119. Curran Associates, Inc.\n\n- Jonathan A Obar. 2020. Sunlight alone is not a disin- fectant: Consent and the futility of opening big data black boxes (without assistance). Big Data & Society, 7(1):2053951720935615.\n\n- Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word repre- d a suitable substitute for the advice of a qualified health care professional. Do not disregard or avoid professional medical advice due to content published within Cureus.\n\n", "annotations": {"Abstract": [{"begin": 66, "end": 1056, "idx": 0}], "Head": [{"begin": 1059, "end": 1073, "n": "1", "idx": 0}, {"begin": 4779, "end": 4796, "n": "1.1", "idx": 1}, {"begin": 5450, "end": 5469, "n": "2", "idx": 2}, {"begin": 5842, "end": 5853, "n": "2.1", "idx": 3}, {"begin": 6474, "end": 6492, "n": "2.2", "idx": 4}, {"begin": 7009, "end": 7019, "n": "2.3", "idx": 5}, {"begin": 7426, "end": 7442, "n": "2.4", "idx": 6}, {"begin": 7959, "end": 7968, "n": "2.5", "idx": 7}, {"begin": 8580, "end": 8590, "n": "2.6", "idx": 8}, {"begin": 8931, "end": 8942, "n": "2.7", "idx": 9}, {"begin": 9529, "end": 9547, "n": "2.8", "idx": 10}, {"begin": 10061, "end": 10082, "n": "2.9", "idx": 11}, {"begin": 10558, "end": 10582, "n": "2.10", "idx": 12}, {"begin": 10808, "end": 10829, "n": "2.11", "idx": 13}, {"begin": 11260, "end": 11282, "n": "2.12", "idx": 14}, {"begin": 11648, "end": 11666, "n": "2.13", "idx": 15}, {"begin": 12054, "end": 12079, "n": "2.14", "idx": 16}, {"begin": 12634, "end": 12650, "n": "2.15", "idx": 17}, {"begin": 13103, "end": 13118, "n": "2.16", "idx": 18}, {"begin": 13422, "end": 13435, "n": "2.17", "idx": 19}, {"begin": 13825, "end": 13847, "n": "2.18", "idx": 20}, {"begin": 14105, "end": 14120, "n": "2.19", "idx": 21}, {"begin": 14455, "end": 14489, "n": "2.20", "idx": 22}, {"begin": 14732, "end": 14748, "n": "2.21", "idx": 23}, {"begin": 15272, "end": 15289, "n": "2.22", "idx": 24}, {"begin": 15905, "end": 15932, "n": "3.1", "idx": 25}, {"begin": 17078, "end": 17118, "n": "3.2", "idx": 26}, {"begin": 18724, "end": 18773, "n": "3.3", "idx": 27}, {"begin": 21921, "end": 21933, "n": "4", "idx": 28}, {"begin": 22317, "end": 22332, "n": "4.1", "idx": 29}, {"begin": 22866, "end": 22877, "n": "4.2", "idx": 30}, {"begin": 23589, "end": 23612, "n": "5", "idx": 31}, {"begin": 23879, "end": 23916, "n": "5.1", "idx": 32}, {"begin": 24241, "end": 24266, "n": "5.2", "idx": 33}, {"begin": 25148, "end": 25192, "n": "6", "idx": 34}, {"begin": 26555, "end": 26578, "n": "6.1", "idx": 35}, {"begin": 27723, "end": 27747, "n": "6.2", "idx": 36}, {"begin": 29567, "end": 29589, "n": "6.3", "idx": 37}, {"begin": 31184, "end": 31220, "n": "6.4", "idx": 38}, {"begin": 32108, "end": 32120, "n": "6.4.1", "idx": 39}, {"begin": 32936, "end": 32950, "n": "6.4.2", "idx": 40}, {"begin": 33897, "end": 33907, "n": "6.4.3", "idx": 41}, {"begin": 34962, "end": 34996, "n": "6.5", "idx": 42}, {"begin": 38954, "end": 38988, "n": "7", "idx": 43}, {"begin": 39626, "end": 39649, "n": "7.1", "idx": 44}, {"begin": 41669, "end": 41701, "n": "7.2", "idx": 45}, {"begin": 42599, "end": 42643, "n": "2.", "idx": 46}, {"begin": 43263, "end": 43285, "n": "7.3", "idx": 47}, {"begin": 45108, "end": 45122, "n": "8", "idx": 48}, {"begin": 47762, "end": 47781, "idx": 49}, {"begin": 49882, "end": 49899, "idx": 50}, {"begin": 50475, "end": 50486, "idx": 51}, {"begin": 50912, "end": 50928, "idx": 52}, {"begin": 51813, "end": 51828, "idx": 53}, {"begin": 53129, "end": 53144, "idx": 54}, {"begin": 53385, "end": 53403, "idx": 55}, {"begin": 53650, "end": 53660, "idx": 56}, {"begin": 53685, "end": 53701, "idx": 57}, {"begin": 54189, "end": 54211, "idx": 58}, {"begin": 54521, "end": 54530, "idx": 59}, {"begin": 54999, "end": 55009, "idx": 60}, {"begin": 57423, "end": 57434, "idx": 61}, {"begin": 57549, "end": 57567, "idx": 62}, {"begin": 58208, "end": 58229, "idx": 63}, {"begin": 58974, "end": 58995, "idx": 64}, {"begin": 59460, "end": 59482, "idx": 65}, {"begin": 59507, "end": 59525, "idx": 66}, {"begin": 59706, "end": 59730, "idx": 67}, {"begin": 59879, "end": 59904, "idx": 68}, {"begin": 60158, "end": 60173, "idx": 69}, {"begin": 60591, "end": 60607, "idx": 70}, {"begin": 61564, "end": 61577, "idx": 71}, {"begin": 61877, "end": 61892, "idx": 72}, {"begin": 63733, "end": 63755, "idx": 73}, {"begin": 64796, "end": 64811, "idx": 74}, {"begin": 65475, "end": 65509, "idx": 75}, {"begin": 66662, "end": 66679, "idx": 76}, {"begin": 66785, "end": 66810, "idx": 77}, {"begin": 67853, "end": 67864, "idx": 78}, {"begin": 68428, "end": 68445, "idx": 79}, {"begin": 69726, "end": 69759, "idx": 80}, {"begin": 70801, "end": 70842, "idx": 81}, {"begin": 71822, "end": 71831, "idx": 82}, {"begin": 72085, "end": 72121, "idx": 83}, {"begin": 73780, "end": 73802, "idx": 84}, {"begin": 74644, "end": 74658, "idx": 85}, {"begin": 74809, "end": 74820, "idx": 86}, {"begin": 75846, "end": 75864, "idx": 87}, {"begin": 76381, "end": 76422, "idx": 88}, {"begin": 76714, "end": 76724, "idx": 89}, {"begin": 77750, "end": 77766, "idx": 90}, {"begin": 80422, "end": 80434, "idx": 91}, {"begin": 80614, "end": 80630, "idx": 92}, {"begin": 86042, "end": 86059, "idx": 93}, {"begin": 87086, "end": 87103, "idx": 94}], "ReferenceToBib": [{"begin": 1245, "end": 1267, "idx": 0}, {"begin": 1268, "end": 1289, "idx": 1}, {"begin": 1290, "end": 1310, "idx": 2}, {"begin": 1311, "end": 1324, "idx": 3}, {"begin": 1325, "end": 1344, "idx": 4}, {"begin": 1345, "end": 1367, "idx": 5}, {"begin": 1497, "end": 1518, "target": "#b28", "idx": 6}, {"begin": 1678, "end": 1698, "idx": 7}, {"begin": 1699, "end": 1719, "idx": 8}, {"begin": 1903, "end": 1917, "idx": 9}, {"begin": 2097, "end": 2111, "idx": 10}, {"begin": 2112, "end": 2131, "idx": 11}, {"begin": 2132, "end": 2153, "target": "#b11", "idx": 12}, {"begin": 3285, "end": 3303, "idx": 13}, {"begin": 3304, "end": 3320, "idx": 14}, {"begin": 3419, "end": 3434, "idx": 15}, {"begin": 3462, "end": 3480, "idx": 16}, {"begin": 3482, "end": 3514, "idx": 17}, {"begin": 3538, "end": 3571, "idx": 18}, {"begin": 3582, "end": 3595, "target": "#b29", "idx": 19}, {"begin": 3625, "end": 3647, "target": "#b28", "idx": 20}, {"begin": 5567, "end": 5573, "target": "#b17", "idx": 21}, {"begin": 6277, "end": 6302, "target": "#b16", "idx": 22}, {"begin": 7150, "end": 7165, "idx": 23}, {"begin": 7511, "end": 7541, "idx": 24}, {"begin": 7564, "end": 7590, "idx": 25}, {"begin": 8678, "end": 8704, "idx": 26}, {"begin": 11461, "end": 11479, "idx": 27}, {"begin": 12284, "end": 12305, "idx": 28}, {"begin": 12394, "end": 12414, "idx": 29}, {"begin": 12713, "end": 12731, "idx": 30}, {"begin": 13039, "end": 13061, "target": "#b28", "idx": 31}, {"begin": 13062, "end": 13082, "target": "#b15", "idx": 32}, {"begin": 13083, "end": 13100, "target": "#b31", "idx": 33}, {"begin": 13445, "end": 13458, "target": "#b29", "idx": 34}, {"begin": 13598, "end": 13620, "target": "#b21", "idx": 35}, {"begin": 13621, "end": 13640, "idx": 36}, {"begin": 13641, "end": 13662, "target": "#b12", "idx": 37}, {"begin": 15315, "end": 15336, "target": "#b28", "idx": 38}, {"begin": 17196, "end": 17224, "idx": 39}, {"begin": 17229, "end": 17255, "idx": 40}, {"begin": 18068, "end": 18089, "target": "#b28", "idx": 41}, {"begin": 18090, "end": 18112, "idx": 42}, {"begin": 22111, "end": 22117, "target": "#b17", "idx": 43}, {"begin": 22511, "end": 22517, "target": "#b17", "idx": 44}, {"begin": 22768, "end": 22781, "idx": 45}, {"begin": 22782, "end": 22803, "target": "#b13", "idx": 46}, {"begin": 24455, "end": 24461, "target": "#b17", "idx": 47}, {"begin": 24828, "end": 24850, "idx": 48}, {"begin": 25002, "end": 25008, "target": "#b17", "idx": 49}, {"begin": 25326, "end": 25352, "idx": 50}, {"begin": 25353, "end": 25381, "target": "#b6", "idx": 51}, {"begin": 25450, "end": 25473, "target": "#b26", "idx": 52}, {"begin": 25474, "end": 25498, "target": "#b27", "idx": 53}, {"begin": 25499, "end": 25521, "target": "#b9", "idx": 54}, {"begin": 25685, "end": 25704, "target": "#b18", "idx": 55}, {"begin": 26742, "end": 26762, "target": "#b18", "idx": 56}, {"begin": 26969, "end": 26988, "idx": 57}, {"begin": 26989, "end": 27014, "target": "#b14", "idx": 58}, {"begin": 27015, "end": 27035, "idx": 59}, {"begin": 27388, "end": 27411, "target": "#b6", "idx": 60}, {"begin": 27412, "end": 27427, "target": "#b5", "idx": 61}, {"begin": 27898, "end": 27920, "idx": 62}, {"begin": 27970, "end": 27989, "idx": 63}, {"begin": 28084, "end": 28106, "idx": 64}, {"begin": 29884, "end": 29896, "idx": 65}, {"begin": 30022, "end": 30044, "idx": 66}, {"begin": 32398, "end": 32420, "target": "#b31", "idx": 67}, {"begin": 32649, "end": 32675, "target": "#b3", "idx": 68}, {"begin": 35301, "end": 35313, "target": "#b34", "idx": 69}, {"begin": 35314, "end": 35339, "idx": 70}, {"begin": 35488, "end": 35508, "idx": 71}, {"begin": 35509, "end": 35530, "idx": 72}, {"begin": 35749, "end": 35765, "target": "#b22", "idx": 73}, {"begin": 39245, "end": 39268, "idx": 74}, {"begin": 39269, "end": 39289, "target": "#b2", "idx": 75}, {"begin": 39290, "end": 39318, "idx": 76}, {"begin": 39319, "end": 39333, "target": "#b10", "idx": 77}, {"begin": 39334, "end": 39359, "idx": 78}, {"begin": 40067, "end": 40073, "target": "#b0", "idx": 79}, {"begin": 40122, "end": 40128, "target": "#b1", "idx": 80}, {"begin": 40550, "end": 40561, "idx": 81}, {"begin": 40695, "end": 40713, "target": "#b14", "idx": 82}, {"begin": 40714, "end": 40731, "idx": 83}, {"begin": 40732, "end": 40754, "idx": 84}, {"begin": 40755, "end": 40772, "target": "#b30", "idx": 85}, {"begin": 41824, "end": 41844, "target": "#b19", "idx": 86}, {"begin": 41845, "end": 41861, "idx": 87}, {"begin": 41993, "end": 42022, "idx": 88}, {"begin": 42023, "end": 42037, "idx": 89}, {"begin": 42038, "end": 42052, "target": "#b10", "idx": 90}, {"begin": 42053, "end": 42073, "target": "#b2", "idx": 91}, {"begin": 42124, "end": 42148, "idx": 92}, {"begin": 42149, "end": 42175, "idx": 93}, {"begin": 42344, "end": 42359, "target": "#b10", "idx": 94}, {"begin": 43394, "end": 43414, "idx": 95}, {"begin": 43415, "end": 43437, "idx": 96}, {"begin": 43897, "end": 43926, "idx": 97}, {"begin": 44121, "end": 44138, "idx": 98}, {"begin": 44843, "end": 44866, "idx": 99}, {"begin": 45304, "end": 45329, "target": "#b35", "idx": 100}, {"begin": 45343, "end": 45365, "target": "#b33", "idx": 101}, {"begin": 45419, "end": 45439, "target": "#b20", "idx": 102}, {"begin": 45508, "end": 45529, "target": "#b28", "idx": 103}, {"begin": 45530, "end": 45555, "idx": 104}, {"begin": 45556, "end": 45575, "idx": 105}, {"begin": 45576, "end": 45589, "idx": 106}, {"begin": 45590, "end": 45611, "idx": 107}, {"begin": 45639, "end": 45660, "target": "#b15", "idx": 108}, {"begin": 45661, "end": 45678, "target": "#b31", "idx": 109}, {"begin": 45679, "end": 45699, "idx": 110}, {"begin": 45699, "end": 45706, "idx": 111}, {"begin": 50525, "end": 50550, "target": "#b16", "idx": 112}, {"begin": 53194, "end": 53200, "target": "#b17", "idx": 113}, {"begin": 53421, "end": 53445, "idx": 114}, {"begin": 53446, "end": 53474, "idx": 115}, {"begin": 57921, "end": 57941, "idx": 116}, {"begin": 67418, "end": 67431, "target": "#b23", "idx": 117}, {"begin": 67889, "end": 67895, "target": "#b17", "idx": 118}, {"begin": 69826, "end": 69848, "idx": 119}, {"begin": 69849, "end": 69868, "idx": 120}, {"begin": 69869, "end": 69890, "idx": 121}, {"begin": 70987, "end": 71007, "idx": 122}, {"begin": 73135, "end": 73154, "idx": 123}], "ReferenceToFootnote": [{"begin": 4536, "end": 4537, "target": "#foot_0", "idx": 0}, {"begin": 9577, "end": 9578, "target": "#foot_1", "idx": 1}, {"begin": 10249, "end": 10250, "target": "#foot_2", "idx": 2}, {"begin": 13190, "end": 13191, "target": "#foot_3", "idx": 3}, {"begin": 14136, "end": 14137, "target": "#foot_4", "idx": 4}, {"begin": 14593, "end": 14594, "target": "#foot_5", "idx": 5}, {"begin": 18313, "end": 18315, "target": "#foot_7", "idx": 6}, {"begin": 25853, "end": 25855, "target": "#foot_9", "idx": 7}, {"begin": 26354, "end": 26356, "target": "#foot_10", "idx": 8}, {"begin": 32288, "end": 32290, "target": "#foot_11", "idx": 9}, {"begin": 54713, "end": 54715, "target": "#foot_14", "idx": 10}, {"begin": 57499, "end": 57501, "target": "#foot_15", "idx": 11}, {"begin": 61612, "end": 61614, "target": "#foot_16", "idx": 12}, {"begin": 64273, "end": 64275, "target": "#foot_17", "idx": 13}, {"begin": 65035, "end": 65037, "target": "#foot_18", "idx": 14}, {"begin": 65471, "end": 65473, "target": "#foot_19", "idx": 15}, {"begin": 68792, "end": 68794, "target": "#foot_20", "idx": 16}], "SectionFootnote": [{"begin": 87198, "end": 88524, "idx": 0}], "ReferenceString": [{"begin": 88541, "end": 88627, "id": "b0", "idx": 0}, {"begin": 88629, "end": 88653, "id": "b1", "idx": 1}, {"begin": 88657, "end": 88817, "id": "b2", "idx": 2}, {"begin": 88821, "end": 89030, "id": "b3", "idx": 3}, {"begin": 89034, "end": 89258, "id": "b4", "idx": 4}, {"begin": 89262, "end": 89335, "id": "b5", "idx": 5}, {"begin": 89339, "end": 89613, "id": "b6", "idx": 6}, {"begin": 89617, "end": 89665, "id": "b7", "idx": 7}, {"begin": 89669, "end": 89752, "id": "b8", "idx": 8}, {"begin": 89756, "end": 89928, "id": "b9", "idx": 9}, {"begin": 89932, "end": 90175, "id": "b10", "idx": 10}, {"begin": 90179, "end": 91253, "id": "b11", "idx": 11}, {"begin": 91257, "end": 91490, "id": "b12", "idx": 12}, {"begin": 91494, "end": 91895, "id": "b13", "idx": 13}, {"begin": 91899, "end": 92482, "id": "b14", "idx": 14}, {"begin": 92486, "end": 92865, "id": "b15", "idx": 15}, {"begin": 92869, "end": 93116, "id": "b16", "idx": 16}, {"begin": 93120, "end": 93345, "id": "b17", "idx": 17}, {"begin": 93349, "end": 93712, "id": "b18", "idx": 18}, {"begin": 93716, "end": 93918, "id": "b19", "idx": 19}, {"begin": 93922, "end": 94051, "id": "b20", "idx": 20}, {"begin": 94055, "end": 94254, "id": "b21", "idx": 21}, {"begin": 94258, "end": 94417, "id": "b22", "idx": 22}, {"begin": 94421, "end": 94710, "id": "b23", "idx": 23}, {"begin": 94714, "end": 94760, "id": "b24", "idx": 24}, {"begin": 94764, "end": 94875, "id": "b25", "idx": 25}, {"begin": 94879, "end": 95102, "id": "b26", "idx": 26}, {"begin": 95106, "end": 95545, "id": "b27", "idx": 27}, {"begin": 95549, "end": 96034, "id": "b28", "idx": 28}, {"begin": 96038, "end": 96428, "id": "b29", "idx": 29}, {"begin": 96432, "end": 96625, "id": "b30", "idx": 30}, {"begin": 96629, "end": 97188, "id": "b31", "idx": 31}, {"begin": 97192, "end": 97259, "id": "b32", "idx": 32}, {"begin": 97263, "end": 97529, "id": "b33", "idx": 33}, {"begin": 97533, "end": 97717, "id": "b34", "idx": 34}, {"begin": 97721, "end": 98002, "id": "b35", "idx": 35}], "ReferenceToTable": [{"begin": 5541, "end": 5542, "target": "#tab_0", "idx": 0}, {"begin": 17074, "end": 17075, "target": "#tab_14", "idx": 1}, {"begin": 32439, "end": 32441, "target": "#tab_0", "idx": 2}, {"begin": 33038, "end": 33040, "target": "#tab_0", "idx": 3}, {"begin": 34182, "end": 34184, "target": "#tab_0", "idx": 4}, {"begin": 34503, "end": 34505, "target": "#tab_4", "idx": 5}, {"begin": 37290, "end": 37291, "target": "#tab_8", "idx": 6}, {"begin": 53381, "end": 53382, "idx": 7}, {"begin": 73319, "end": 73320, "target": "#tab_18", "idx": 8}, {"begin": 73405, "end": 73406, "idx": 9}, {"begin": 73541, "end": 73554, "target": "#tab_18", "idx": 10}, {"begin": 73701, "end": 73702, "idx": 11}], "Footnote": [{"begin": 87209, "end": 87251, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 87252, "end": 87297, "id": "foot_1", "n": "4", "idx": 1}, {"begin": 87298, "end": 87328, "id": "foot_2", "n": "5", "idx": 2}, {"begin": 87329, "end": 87359, "id": "foot_3", "n": "6", "idx": 3}, {"begin": 87360, "end": 87386, "id": "foot_4", "n": "7", "idx": 4}, {"begin": 87387, "end": 87415, "id": "foot_5", "n": "8", "idx": 5}, {"begin": 87416, "end": 87447, "id": "foot_6", "n": "9", "idx": 6}, {"begin": 87448, "end": 87656, "id": "foot_7", "n": "10", "idx": 7}, {"begin": 87657, "end": 87720, "id": "foot_8", "n": "11", "idx": 8}, {"begin": 87721, "end": 87861, "id": "foot_9", "n": "12", "idx": 9}, {"begin": 87862, "end": 87937, "id": "foot_10", "n": "13", "idx": 10}, {"begin": 87938, "end": 88092, "id": "foot_11", "n": "14", "idx": 11}, {"begin": 88093, "end": 88162, "id": "foot_12", "n": "15", "idx": 12}, {"begin": 88163, "end": 88214, "id": "foot_13", "n": "17", "idx": 13}, {"begin": 88215, "end": 88254, "id": "foot_14", "n": "18", "idx": 14}, {"begin": 88255, "end": 88304, "id": "foot_15", "n": "19", "idx": 15}, {"begin": 88305, "end": 88340, "id": "foot_16", "n": "21", "idx": 16}, {"begin": 88341, "end": 88395, "id": "foot_17", "n": "22", "idx": 17}, {"begin": 88396, "end": 88442, "id": "foot_18", "n": "23", "idx": 18}, {"begin": 88443, "end": 88484, "id": "foot_19", "n": "24", "idx": 19}, {"begin": 88485, "end": 88524, "id": "foot_20", "n": "26", "idx": 20}], "ReferenceToFormula": [{"begin": 50767, "end": 50771, "idx": 0}, {"begin": 53201, "end": 53205, "idx": 1}, {"begin": 61025, "end": 61029, "idx": 2}], "Paragraph": [{"begin": 76, "end": 1056, "idx": 0}, {"begin": 1074, "end": 1519, "idx": 1}, {"begin": 1520, "end": 2427, "idx": 2}, {"begin": 2428, "end": 3344, "idx": 3}, {"begin": 3345, "end": 4384, "idx": 4}, {"begin": 4385, "end": 4777, "idx": 5}, {"begin": 4797, "end": 4838, "idx": 6}, {"begin": 4839, "end": 4950, "idx": 7}, {"begin": 4951, "end": 5070, "idx": 8}, {"begin": 5071, "end": 5247, "idx": 9}, {"begin": 5248, "end": 5448, "idx": 10}, {"begin": 5470, "end": 5840, "idx": 11}, {"begin": 5854, "end": 6472, "idx": 12}, {"begin": 6493, "end": 7007, "idx": 13}, {"begin": 7020, "end": 7424, "idx": 14}, {"begin": 7443, "end": 7957, "idx": 15}, {"begin": 7969, "end": 8578, "idx": 16}, {"begin": 8591, "end": 8929, "idx": 17}, {"begin": 8943, "end": 9527, "idx": 18}, {"begin": 9548, "end": 10059, "idx": 19}, {"begin": 10083, "end": 10556, "idx": 20}, {"begin": 10583, "end": 10806, "idx": 21}, {"begin": 10830, "end": 11258, "idx": 22}, {"begin": 11283, "end": 11646, "idx": 23}, {"begin": 11667, "end": 12052, "idx": 24}, {"begin": 12080, "end": 12632, "idx": 25}, {"begin": 12651, "end": 13101, "idx": 26}, {"begin": 13119, "end": 13420, "idx": 27}, {"begin": 13436, "end": 13823, "idx": 28}, {"begin": 13848, "end": 14103, "idx": 29}, {"begin": 14121, "end": 14453, "idx": 30}, {"begin": 14490, "end": 14730, "idx": 31}, {"begin": 14749, "end": 15270, "idx": 32}, {"begin": 15290, "end": 15566, "idx": 33}, {"begin": 15567, "end": 15611, "idx": 34}, {"begin": 15612, "end": 15903, "idx": 35}, {"begin": 15933, "end": 16444, "idx": 36}, {"begin": 16445, "end": 16786, "idx": 37}, {"begin": 16837, "end": 17076, "idx": 38}, {"begin": 17119, "end": 17803, "idx": 39}, {"begin": 17804, "end": 17876, "idx": 40}, {"begin": 17877, "end": 17977, "idx": 41}, {"begin": 17978, "end": 18722, "idx": 42}, {"begin": 18774, "end": 19190, "idx": 43}, {"begin": 19191, "end": 19893, "idx": 44}, {"begin": 19961, "end": 20444, "idx": 45}, {"begin": 20445, "end": 20687, "idx": 46}, {"begin": 20688, "end": 21009, "idx": 47}, {"begin": 21010, "end": 21431, "idx": 48}, {"begin": 21432, "end": 21919, "idx": 49}, {"begin": 21934, "end": 22315, "idx": 50}, {"begin": 22333, "end": 22714, "idx": 51}, {"begin": 22715, "end": 22864, "idx": 52}, {"begin": 22878, "end": 23587, "idx": 53}, {"begin": 23613, "end": 23877, "idx": 54}, {"begin": 23917, "end": 24128, "idx": 55}, {"begin": 24129, "end": 24239, "idx": 56}, {"begin": 24267, "end": 24812, "idx": 57}, {"begin": 24813, "end": 25146, "idx": 58}, {"begin": 25193, "end": 26026, "idx": 59}, {"begin": 26027, "end": 26553, "idx": 60}, {"begin": 26579, "end": 27428, "idx": 61}, {"begin": 27429, "end": 27721, "idx": 62}, {"begin": 27748, "end": 28433, "idx": 63}, {"begin": 28434, "end": 29082, "idx": 64}, {"begin": 29083, "end": 29565, "idx": 65}, {"begin": 29590, "end": 29838, "idx": 66}, {"begin": 29839, "end": 30107, "idx": 67}, {"begin": 30108, "end": 30217, "idx": 68}, {"begin": 30218, "end": 30753, "idx": 69}, {"begin": 30754, "end": 30913, "idx": 70}, {"begin": 30914, "end": 31182, "idx": 71}, {"begin": 31221, "end": 31803, "idx": 72}, {"begin": 31804, "end": 32037, "idx": 73}, {"begin": 32038, "end": 32106, "idx": 74}, {"begin": 32121, "end": 32601, "idx": 75}, {"begin": 32602, "end": 32934, "idx": 76}, {"begin": 32951, "end": 33895, "idx": 77}, {"begin": 33908, "end": 34114, "idx": 78}, {"begin": 34115, "end": 34379, "idx": 79}, {"begin": 34380, "end": 34675, "idx": 80}, {"begin": 34676, "end": 34960, "idx": 81}, {"begin": 34997, "end": 35766, "idx": 82}, {"begin": 35767, "end": 36167, "idx": 83}, {"begin": 36168, "end": 37283, "idx": 84}, {"begin": 37284, "end": 38053, "idx": 85}, {"begin": 38054, "end": 38952, "idx": 86}, {"begin": 38989, "end": 39624, "idx": 87}, {"begin": 39650, "end": 40773, "idx": 88}, {"begin": 40774, "end": 41163, "idx": 89}, {"begin": 41164, "end": 41667, "idx": 90}, {"begin": 41702, "end": 42360, "idx": 91}, {"begin": 42361, "end": 42412, "idx": 92}, {"begin": 42413, "end": 42554, "idx": 93}, {"begin": 42555, "end": 42597, "idx": 94}, {"begin": 42644, "end": 42754, "idx": 95}, {"begin": 42755, "end": 42968, "idx": 96}, {"begin": 42969, "end": 43261, "idx": 97}, {"begin": 43286, "end": 43927, "idx": 98}, {"begin": 43928, "end": 45106, "idx": 99}, {"begin": 45123, "end": 47760, "idx": 100}, {"begin": 47782, "end": 48185, "idx": 101}, {"begin": 48186, "end": 48647, "idx": 102}, {"begin": 48648, "end": 49129, "idx": 103}, {"begin": 49130, "end": 49880, "idx": 104}, {"begin": 49900, "end": 50473, "idx": 105}, {"begin": 50487, "end": 50910, "idx": 106}, {"begin": 50929, "end": 51811, "idx": 107}, {"begin": 51829, "end": 52154, "idx": 108}, {"begin": 52155, "end": 52558, "idx": 109}, {"begin": 52559, "end": 52952, "idx": 110}, {"begin": 52953, "end": 53127, "idx": 111}, {"begin": 53145, "end": 53383, "idx": 112}, {"begin": 53404, "end": 53648, "idx": 113}, {"begin": 53661, "end": 53683, "idx": 114}, {"begin": 53702, "end": 54182, "idx": 115}, {"begin": 54183, "end": 54187, "idx": 116}, {"begin": 54212, "end": 54519, "idx": 117}, {"begin": 54531, "end": 54896, "idx": 118}, {"begin": 54897, "end": 54997, "idx": 119}, {"begin": 55010, "end": 55064, "idx": 120}, {"begin": 55065, "end": 55131, "idx": 121}, {"begin": 55132, "end": 55209, "idx": 122}, {"begin": 55210, "end": 56365, "idx": 123}, {"begin": 56366, "end": 57007, "idx": 124}, {"begin": 57008, "end": 57421, "idx": 125}, {"begin": 57435, "end": 57547, "idx": 126}, {"begin": 57568, "end": 58038, "idx": 127}, {"begin": 58039, "end": 58206, "idx": 128}, {"begin": 58230, "end": 58550, "idx": 129}, {"begin": 58551, "end": 58972, "idx": 130}, {"begin": 58996, "end": 59458, "idx": 131}, {"begin": 59483, "end": 59505, "idx": 132}, {"begin": 59526, "end": 59704, "idx": 133}, {"begin": 59731, "end": 59877, "idx": 134}, {"begin": 59905, "end": 60078, "idx": 135}, {"begin": 60079, "end": 60156, "idx": 136}, {"begin": 60174, "end": 60240, "idx": 137}, {"begin": 60241, "end": 60589, "idx": 138}, {"begin": 60608, "end": 60943, "idx": 139}, {"begin": 60944, "end": 61036, "idx": 140}, {"begin": 61037, "end": 61143, "idx": 141}, {"begin": 61144, "end": 61249, "idx": 142}, {"begin": 61250, "end": 61329, "idx": 143}, {"begin": 61330, "end": 61374, "idx": 144}, {"begin": 61375, "end": 61562, "idx": 145}, {"begin": 61578, "end": 61875, "idx": 146}, {"begin": 61893, "end": 62365, "idx": 147}, {"begin": 62366, "end": 63268, "idx": 148}, {"begin": 63269, "end": 63494, "idx": 149}, {"begin": 63495, "end": 63731, "idx": 150}, {"begin": 63756, "end": 63797, "idx": 151}, {"begin": 63798, "end": 64091, "idx": 152}, {"begin": 64092, "end": 64241, "idx": 153}, {"begin": 64242, "end": 64366, "idx": 154}, {"begin": 64367, "end": 64740, "idx": 155}, {"begin": 64741, "end": 64794, "idx": 156}, {"begin": 64812, "end": 65473, "idx": 157}, {"begin": 65510, "end": 65894, "idx": 158}, {"begin": 65895, "end": 66660, "idx": 159}, {"begin": 66680, "end": 66783, "idx": 160}, {"begin": 66811, "end": 66881, "idx": 161}, {"begin": 66882, "end": 67313, "idx": 162}, {"begin": 67314, "end": 67522, "idx": 163}, {"begin": 67523, "end": 67851, "idx": 164}, {"begin": 67865, "end": 68426, "idx": 165}, {"begin": 68446, "end": 68676, "idx": 166}, {"begin": 68677, "end": 68963, "idx": 167}, {"begin": 68964, "end": 69724, "idx": 168}, {"begin": 69760, "end": 70322, "idx": 169}, {"begin": 70323, "end": 70799, "idx": 170}, {"begin": 70843, "end": 71149, "idx": 171}, {"begin": 71150, "end": 71681, "idx": 172}, {"begin": 71682, "end": 71820, "idx": 173}, {"begin": 71832, "end": 71978, "idx": 174}, {"begin": 71979, "end": 72083, "idx": 175}, {"begin": 72122, "end": 73309, "idx": 176}, {"begin": 73310, "end": 73778, "idx": 177}, {"begin": 73803, "end": 74155, "idx": 178}, {"begin": 74156, "end": 74642, "idx": 179}, {"begin": 74659, "end": 74807, "idx": 180}, {"begin": 74821, "end": 75458, "idx": 181}, {"begin": 75459, "end": 75844, "idx": 182}, {"begin": 75865, "end": 75890, "idx": 183}, {"begin": 75891, "end": 76379, "idx": 184}, {"begin": 76423, "end": 76712, "idx": 185}, {"begin": 76725, "end": 77143, "idx": 186}, {"begin": 77144, "end": 77413, "idx": 187}, {"begin": 77414, "end": 77612, "idx": 188}, {"begin": 77613, "end": 77748, "idx": 189}, {"begin": 77767, "end": 77851, "idx": 190}, {"begin": 77852, "end": 77874, "idx": 191}, {"begin": 77875, "end": 78123, "idx": 192}, {"begin": 78124, "end": 78202, "idx": 193}, {"begin": 78203, "end": 78302, "idx": 194}, {"begin": 78303, "end": 80420, "idx": 195}, {"begin": 80435, "end": 80507, "idx": 196}, {"begin": 80508, "end": 80612, "idx": 197}, {"begin": 80631, "end": 84996, "idx": 198}, {"begin": 84997, "end": 86040, "idx": 199}, {"begin": 86060, "end": 87084, "idx": 200}, {"begin": 87104, "end": 87196, "idx": 201}], "SectionHeader": [{"begin": 0, "end": 1056, "idx": 0}], "SectionReference": [{"begin": 88526, "end": 98004, "idx": 0}], "Sentence": [{"begin": 76, "end": 260, "idx": 0}, {"begin": 261, "end": 381, "idx": 1}, {"begin": 382, "end": 538, "idx": 2}, {"begin": 539, "end": 697, "idx": 3}, {"begin": 698, "end": 872, "idx": 4}, {"begin": 873, "end": 992, "idx": 5}, {"begin": 993, "end": 1056, "idx": 6}, {"begin": 1074, "end": 1368, "idx": 7}, {"begin": 1369, "end": 1519, "idx": 8}, {"begin": 1520, "end": 1720, "idx": 9}, {"begin": 1721, "end": 1841, "idx": 10}, {"begin": 1842, "end": 1918, "idx": 11}, {"begin": 1919, "end": 2154, "idx": 12}, {"begin": 2155, "end": 2427, "idx": 13}, {"begin": 2428, "end": 2481, "idx": 14}, {"begin": 2482, "end": 2557, "idx": 15}, {"begin": 2558, "end": 2718, "idx": 16}, {"begin": 2719, "end": 2908, "idx": 17}, {"begin": 2909, "end": 3145, "idx": 18}, {"begin": 3146, "end": 3344, "idx": 19}, {"begin": 3345, "end": 3648, "idx": 20}, {"begin": 3649, "end": 3701, "idx": 21}, {"begin": 3702, "end": 3823, "idx": 22}, {"begin": 3824, "end": 4050, "idx": 23}, {"begin": 4051, "end": 4204, "idx": 24}, {"begin": 4205, "end": 4384, "idx": 25}, {"begin": 4385, "end": 4538, "idx": 26}, {"begin": 4539, "end": 4689, "idx": 27}, {"begin": 4690, "end": 4777, "idx": 28}, {"begin": 4797, "end": 4838, "idx": 29}, {"begin": 4839, "end": 4841, "idx": 30}, {"begin": 4842, "end": 4870, "idx": 31}, {"begin": 4871, "end": 4950, "idx": 32}, {"begin": 4951, "end": 5070, "idx": 33}, {"begin": 5071, "end": 5247, "idx": 34}, {"begin": 5248, "end": 5448, "idx": 35}, {"begin": 5470, "end": 5543, "idx": 36}, {"begin": 5544, "end": 5752, "idx": 37}, {"begin": 5753, "end": 5840, "idx": 38}, {"begin": 5854, "end": 5975, "idx": 39}, {"begin": 5976, "end": 6125, "idx": 40}, {"begin": 6126, "end": 6221, "idx": 41}, {"begin": 6222, "end": 6472, "idx": 42}, {"begin": 6493, "end": 6743, "idx": 43}, {"begin": 6744, "end": 6900, "idx": 44}, {"begin": 6901, "end": 7007, "idx": 45}, {"begin": 7020, "end": 7166, "idx": 46}, {"begin": 7167, "end": 7424, "idx": 47}, {"begin": 7443, "end": 7591, "idx": 48}, {"begin": 7592, "end": 7703, "idx": 49}, {"begin": 7704, "end": 7897, "idx": 50}, {"begin": 7898, "end": 7957, "idx": 51}, {"begin": 7969, "end": 8045, "idx": 52}, {"begin": 8046, "end": 8151, "idx": 53}, {"begin": 8152, "end": 8322, "idx": 54}, {"begin": 8323, "end": 8457, "idx": 55}, {"begin": 8458, "end": 8578, "idx": 56}, {"begin": 8591, "end": 8649, "idx": 57}, {"begin": 8650, "end": 8929, "idx": 58}, {"begin": 8943, "end": 9079, "idx": 59}, {"begin": 9080, "end": 9212, "idx": 60}, {"begin": 9213, "end": 9375, "idx": 61}, {"begin": 9376, "end": 9479, "idx": 62}, {"begin": 9480, "end": 9527, "idx": 63}, {"begin": 9548, "end": 9757, "idx": 64}, {"begin": 9758, "end": 9921, "idx": 65}, {"begin": 9922, "end": 10059, "idx": 66}, {"begin": 10083, "end": 10251, "idx": 67}, {"begin": 10252, "end": 10414, "idx": 68}, {"begin": 10415, "end": 10556, "idx": 69}, {"begin": 10583, "end": 10657, "idx": 70}, {"begin": 10658, "end": 10806, "idx": 71}, {"begin": 10830, "end": 10999, "idx": 72}, {"begin": 11000, "end": 11146, "idx": 73}, {"begin": 11147, "end": 11258, "idx": 74}, {"begin": 11283, "end": 11344, "idx": 75}, {"begin": 11345, "end": 11556, "idx": 76}, {"begin": 11557, "end": 11646, "idx": 77}, {"begin": 11667, "end": 11799, "idx": 78}, {"begin": 11800, "end": 12052, "idx": 79}, {"begin": 12080, "end": 12306, "idx": 80}, {"begin": 12307, "end": 12488, "idx": 81}, {"begin": 12489, "end": 12632, "idx": 82}, {"begin": 12651, "end": 12835, "idx": 83}, {"begin": 12836, "end": 12965, "idx": 84}, {"begin": 12966, "end": 13101, "idx": 85}, {"begin": 13119, "end": 13255, "idx": 86}, {"begin": 13256, "end": 13420, "idx": 87}, {"begin": 13436, "end": 13663, "idx": 88}, {"begin": 13664, "end": 13823, "idx": 89}, {"begin": 13848, "end": 13963, "idx": 90}, {"begin": 13964, "end": 14103, "idx": 91}, {"begin": 14121, "end": 14310, "idx": 92}, {"begin": 14311, "end": 14453, "idx": 93}, {"begin": 14490, "end": 14641, "idx": 94}, {"begin": 14642, "end": 14730, "idx": 95}, {"begin": 14749, "end": 14850, "idx": 96}, {"begin": 14851, "end": 15028, "idx": 97}, {"begin": 15029, "end": 15140, "idx": 98}, {"begin": 15141, "end": 15270, "idx": 99}, {"begin": 15290, "end": 15420, "idx": 100}, {"begin": 15421, "end": 15566, "idx": 101}, {"begin": 15567, "end": 15611, "idx": 102}, {"begin": 15612, "end": 15787, "idx": 103}, {"begin": 15788, "end": 15903, "idx": 104}, {"begin": 15933, "end": 15995, "idx": 105}, {"begin": 15996, "end": 16093, "idx": 106}, {"begin": 16094, "end": 16241, "idx": 107}, {"begin": 16242, "end": 16444, "idx": 108}, {"begin": 16445, "end": 16503, "idx": 109}, {"begin": 16504, "end": 16706, "idx": 110}, {"begin": 16707, "end": 16786, "idx": 111}, {"begin": 16837, "end": 16948, "idx": 112}, {"begin": 16949, "end": 17076, "idx": 113}, {"begin": 17119, "end": 17376, "idx": 114}, {"begin": 17377, "end": 17525, "idx": 115}, {"begin": 17526, "end": 17598, "idx": 116}, {"begin": 17599, "end": 17803, "idx": 117}, {"begin": 17804, "end": 17876, "idx": 118}, {"begin": 17877, "end": 17977, "idx": 119}, {"begin": 17978, "end": 18113, "idx": 120}, {"begin": 18114, "end": 18234, "idx": 121}, {"begin": 18235, "end": 18413, "idx": 122}, {"begin": 18414, "end": 18555, "idx": 123}, {"begin": 18556, "end": 18646, "idx": 124}, {"begin": 18647, "end": 18722, "idx": 125}, {"begin": 18774, "end": 18971, "idx": 126}, {"begin": 18972, "end": 19070, "idx": 127}, {"begin": 19071, "end": 19190, "idx": 128}, {"begin": 19191, "end": 19369, "idx": 129}, {"begin": 19370, "end": 19511, "idx": 130}, {"begin": 19512, "end": 19652, "idx": 131}, {"begin": 19653, "end": 19763, "idx": 132}, {"begin": 19764, "end": 19844, "idx": 133}, {"begin": 19845, "end": 19893, "idx": 134}, {"begin": 19961, "end": 20146, "idx": 135}, {"begin": 20147, "end": 20304, "idx": 136}, {"begin": 20305, "end": 20444, "idx": 137}, {"begin": 20445, "end": 20479, "idx": 138}, {"begin": 20480, "end": 20687, "idx": 139}, {"begin": 20688, "end": 21009, "idx": 140}, {"begin": 21010, "end": 21093, "idx": 141}, {"begin": 21094, "end": 21274, "idx": 142}, {"begin": 21275, "end": 21431, "idx": 143}, {"begin": 21432, "end": 21557, "idx": 144}, {"begin": 21558, "end": 21703, "idx": 145}, {"begin": 21704, "end": 21851, "idx": 146}, {"begin": 21852, "end": 21919, "idx": 147}, {"begin": 21934, "end": 22230, "idx": 148}, {"begin": 22231, "end": 22315, "idx": 149}, {"begin": 22333, "end": 22569, "idx": 150}, {"begin": 22570, "end": 22714, "idx": 151}, {"begin": 22715, "end": 22864, "idx": 152}, {"begin": 22878, "end": 23007, "idx": 153}, {"begin": 23008, "end": 23326, "idx": 154}, {"begin": 23327, "end": 23468, "idx": 155}, {"begin": 23469, "end": 23587, "idx": 156}, {"begin": 23613, "end": 23758, "idx": 157}, {"begin": 23759, "end": 23877, "idx": 158}, {"begin": 23917, "end": 23970, "idx": 159}, {"begin": 23971, "end": 24128, "idx": 160}, {"begin": 24129, "end": 24239, "idx": 161}, {"begin": 24267, "end": 24377, "idx": 162}, {"begin": 24378, "end": 24585, "idx": 163}, {"begin": 24586, "end": 24731, "idx": 164}, {"begin": 24732, "end": 24812, "idx": 165}, {"begin": 24813, "end": 24896, "idx": 166}, {"begin": 24897, "end": 25146, "idx": 167}, {"begin": 25193, "end": 25603, "idx": 168}, {"begin": 25604, "end": 25856, "idx": 169}, {"begin": 25857, "end": 26026, "idx": 170}, {"begin": 26027, "end": 26138, "idx": 171}, {"begin": 26139, "end": 26242, "idx": 172}, {"begin": 26243, "end": 26445, "idx": 173}, {"begin": 26446, "end": 26553, "idx": 174}, {"begin": 26579, "end": 26703, "idx": 175}, {"begin": 26704, "end": 27036, "idx": 176}, {"begin": 27037, "end": 27216, "idx": 177}, {"begin": 27217, "end": 27428, "idx": 178}, {"begin": 27429, "end": 27599, "idx": 179}, {"begin": 27600, "end": 27721, "idx": 180}, {"begin": 27748, "end": 27884, "idx": 181}, {"begin": 27885, "end": 28107, "idx": 182}, {"begin": 28108, "end": 28164, "idx": 183}, {"begin": 28165, "end": 28299, "idx": 184}, {"begin": 28300, "end": 28433, "idx": 185}, {"begin": 28434, "end": 28610, "idx": 186}, {"begin": 28611, "end": 28870, "idx": 187}, {"begin": 28871, "end": 29082, "idx": 188}, {"begin": 29083, "end": 29460, "idx": 189}, {"begin": 29461, "end": 29565, "idx": 190}, {"begin": 29590, "end": 29733, "idx": 191}, {"begin": 29734, "end": 29838, "idx": 192}, {"begin": 29839, "end": 29897, "idx": 193}, {"begin": 29898, "end": 30107, "idx": 194}, {"begin": 30108, "end": 30217, "idx": 195}, {"begin": 30218, "end": 30329, "idx": 196}, {"begin": 30330, "end": 30466, "idx": 197}, {"begin": 30467, "end": 30597, "idx": 198}, {"begin": 30598, "end": 30753, "idx": 199}, {"begin": 30754, "end": 30830, "idx": 200}, {"begin": 30831, "end": 30913, "idx": 201}, {"begin": 30914, "end": 31020, "idx": 202}, {"begin": 31021, "end": 31146, "idx": 203}, {"begin": 31147, "end": 31182, "idx": 204}, {"begin": 31221, "end": 31380, "idx": 205}, {"begin": 31381, "end": 31524, "idx": 206}, {"begin": 31525, "end": 31648, "idx": 207}, {"begin": 31649, "end": 31803, "idx": 208}, {"begin": 31804, "end": 31858, "idx": 209}, {"begin": 31859, "end": 32037, "idx": 210}, {"begin": 32038, "end": 32106, "idx": 211}, {"begin": 32121, "end": 32200, "idx": 212}, {"begin": 32201, "end": 32342, "idx": 213}, {"begin": 32343, "end": 32442, "idx": 214}, {"begin": 32443, "end": 32601, "idx": 215}, {"begin": 32602, "end": 32757, "idx": 216}, {"begin": 32758, "end": 32828, "idx": 217}, {"begin": 32829, "end": 32934, "idx": 218}, {"begin": 32951, "end": 33041, "idx": 219}, {"begin": 33042, "end": 33153, "idx": 220}, {"begin": 33154, "end": 33265, "idx": 221}, {"begin": 33266, "end": 33363, "idx": 222}, {"begin": 33364, "end": 33556, "idx": 223}, {"begin": 33557, "end": 33675, "idx": 224}, {"begin": 33676, "end": 33810, "idx": 225}, {"begin": 33811, "end": 33895, "idx": 226}, {"begin": 33908, "end": 33960, "idx": 227}, {"begin": 33961, "end": 34114, "idx": 228}, {"begin": 34115, "end": 34185, "idx": 229}, {"begin": 34186, "end": 34281, "idx": 230}, {"begin": 34282, "end": 34379, "idx": 231}, {"begin": 34380, "end": 34453, "idx": 232}, {"begin": 34454, "end": 34506, "idx": 233}, {"begin": 34507, "end": 34612, "idx": 234}, {"begin": 34613, "end": 34675, "idx": 235}, {"begin": 34676, "end": 34745, "idx": 236}, {"begin": 34746, "end": 34864, "idx": 237}, {"begin": 34865, "end": 34960, "idx": 238}, {"begin": 34997, "end": 35084, "idx": 239}, {"begin": 35085, "end": 35340, "idx": 240}, {"begin": 35341, "end": 35766, "idx": 241}, {"begin": 35767, "end": 36010, "idx": 242}, {"begin": 36011, "end": 36167, "idx": 243}, {"begin": 36168, "end": 36280, "idx": 244}, {"begin": 36281, "end": 36355, "idx": 245}, {"begin": 36356, "end": 36560, "idx": 246}, {"begin": 36561, "end": 36721, "idx": 247}, {"begin": 36722, "end": 36947, "idx": 248}, {"begin": 36948, "end": 37181, "idx": 249}, {"begin": 37182, "end": 37283, "idx": 250}, {"begin": 37284, "end": 37375, "idx": 251}, {"begin": 37376, "end": 37501, "idx": 252}, {"begin": 37502, "end": 37616, "idx": 253}, {"begin": 37617, "end": 37745, "idx": 254}, {"begin": 37746, "end": 37902, "idx": 255}, {"begin": 37903, "end": 38053, "idx": 256}, {"begin": 38054, "end": 38243, "idx": 257}, {"begin": 38244, "end": 38334, "idx": 258}, {"begin": 38335, "end": 38486, "idx": 259}, {"begin": 38487, "end": 38682, "idx": 260}, {"begin": 38683, "end": 38952, "idx": 261}, {"begin": 38989, "end": 39120, "idx": 262}, {"begin": 39121, "end": 39408, "idx": 263}, {"begin": 39409, "end": 39624, "idx": 264}, {"begin": 39650, "end": 39917, "idx": 265}, {"begin": 39918, "end": 40056, "idx": 266}, {"begin": 40057, "end": 40224, "idx": 267}, {"begin": 40225, "end": 40429, "idx": 268}, {"begin": 40430, "end": 40562, "idx": 269}, {"begin": 40563, "end": 40773, "idx": 270}, {"begin": 40774, "end": 40944, "idx": 271}, {"begin": 40945, "end": 41080, "idx": 272}, {"begin": 41081, "end": 41163, "idx": 273}, {"begin": 41164, "end": 41239, "idx": 274}, {"begin": 41240, "end": 41420, "idx": 275}, {"begin": 41421, "end": 41582, "idx": 276}, {"begin": 41583, "end": 41667, "idx": 277}, {"begin": 41702, "end": 41862, "idx": 278}, {"begin": 41863, "end": 42176, "idx": 279}, {"begin": 42177, "end": 42360, "idx": 280}, {"begin": 42361, "end": 42412, "idx": 281}, {"begin": 42413, "end": 42554, "idx": 282}, {"begin": 42555, "end": 42597, "idx": 283}, {"begin": 42644, "end": 42754, "idx": 284}, {"begin": 42755, "end": 42921, "idx": 285}, {"begin": 42922, "end": 42968, "idx": 286}, {"begin": 42969, "end": 43066, "idx": 287}, {"begin": 43067, "end": 43261, "idx": 288}, {"begin": 43286, "end": 43438, "idx": 289}, {"begin": 43439, "end": 43703, "idx": 290}, {"begin": 43704, "end": 43927, "idx": 291}, {"begin": 43928, "end": 44139, "idx": 292}, {"begin": 44140, "end": 44351, "idx": 293}, {"begin": 44352, "end": 44504, "idx": 294}, {"begin": 44505, "end": 44624, "idx": 295}, {"begin": 44625, "end": 44774, "idx": 296}, {"begin": 44775, "end": 44987, "idx": 297}, {"begin": 44988, "end": 45106, "idx": 298}, {"begin": 45123, "end": 45262, "idx": 299}, {"begin": 45263, "end": 45476, "idx": 300}, {"begin": 45477, "end": 45707, "idx": 301}, {"begin": 45708, "end": 45799, "idx": 302}, {"begin": 45800, "end": 46014, "idx": 303}, {"begin": 46015, "end": 46123, "idx": 304}, {"begin": 46124, "end": 46401, "idx": 305}, {"begin": 46402, "end": 46715, "idx": 306}, {"begin": 46716, "end": 46840, "idx": 307}, {"begin": 46841, "end": 46932, "idx": 308}, {"begin": 46933, "end": 47054, "idx": 309}, {"begin": 47055, "end": 47157, "idx": 310}, {"begin": 47158, "end": 47233, "idx": 311}, {"begin": 47234, "end": 47372, "idx": 312}, {"begin": 47373, "end": 47467, "idx": 313}, {"begin": 47468, "end": 47520, "idx": 314}, {"begin": 47521, "end": 47608, "idx": 315}, {"begin": 47609, "end": 47640, "idx": 316}, {"begin": 47641, "end": 47677, "idx": 317}, {"begin": 47678, "end": 47760, "idx": 318}, {"begin": 47782, "end": 47893, "idx": 319}, {"begin": 47894, "end": 48072, "idx": 320}, {"begin": 48073, "end": 48185, "idx": 321}, {"begin": 48186, "end": 48213, "idx": 322}, {"begin": 48214, "end": 48358, "idx": 323}, {"begin": 48359, "end": 48483, "idx": 324}, {"begin": 48484, "end": 48647, "idx": 325}, {"begin": 48648, "end": 48662, "idx": 326}, {"begin": 48663, "end": 48770, "idx": 327}, {"begin": 48771, "end": 48803, "idx": 328}, {"begin": 48804, "end": 48809, "idx": 329}, {"begin": 48810, "end": 49003, "idx": 330}, {"begin": 49004, "end": 49129, "idx": 331}, {"begin": 49130, "end": 49144, "idx": 332}, {"begin": 49145, "end": 49218, "idx": 333}, {"begin": 49219, "end": 49324, "idx": 334}, {"begin": 49325, "end": 49382, "idx": 335}, {"begin": 49383, "end": 49560, "idx": 336}, {"begin": 49561, "end": 49687, "idx": 337}, {"begin": 49688, "end": 49781, "idx": 338}, {"begin": 49782, "end": 49880, "idx": 339}, {"begin": 49900, "end": 50085, "idx": 340}, {"begin": 50086, "end": 50312, "idx": 341}, {"begin": 50313, "end": 50473, "idx": 342}, {"begin": 50487, "end": 50706, "idx": 343}, {"begin": 50707, "end": 50910, "idx": 344}, {"begin": 50929, "end": 51016, "idx": 345}, {"begin": 51017, "end": 51205, "idx": 346}, {"begin": 51206, "end": 51397, "idx": 347}, {"begin": 51398, "end": 51483, "idx": 348}, {"begin": 51484, "end": 51686, "idx": 349}, {"begin": 51687, "end": 51811, "idx": 350}, {"begin": 51829, "end": 52019, "idx": 351}, {"begin": 52020, "end": 52154, "idx": 352}, {"begin": 52155, "end": 52412, "idx": 353}, {"begin": 52413, "end": 52558, "idx": 354}, {"begin": 52559, "end": 52753, "idx": 355}, {"begin": 52754, "end": 52952, "idx": 356}, {"begin": 52953, "end": 53127, "idx": 357}, {"begin": 53145, "end": 53219, "idx": 358}, {"begin": 53220, "end": 53323, "idx": 359}, {"begin": 53324, "end": 53383, "idx": 360}, {"begin": 53404, "end": 53535, "idx": 361}, {"begin": 53536, "end": 53648, "idx": 362}, {"begin": 53661, "end": 53683, "idx": 363}, {"begin": 53702, "end": 53827, "idx": 364}, {"begin": 53828, "end": 53949, "idx": 365}, {"begin": 53950, "end": 54007, "idx": 366}, {"begin": 54008, "end": 54073, "idx": 367}, {"begin": 54074, "end": 54182, "idx": 368}, {"begin": 54183, "end": 54187, "idx": 369}, {"begin": 54212, "end": 54314, "idx": 370}, {"begin": 54315, "end": 54519, "idx": 371}, {"begin": 54531, "end": 54673, "idx": 372}, {"begin": 54674, "end": 54854, "idx": 373}, {"begin": 54855, "end": 54896, "idx": 374}, {"begin": 54897, "end": 54997, "idx": 375}, {"begin": 55010, "end": 55064, "idx": 376}, {"begin": 55065, "end": 55131, "idx": 377}, {"begin": 55132, "end": 55209, "idx": 378}, {"begin": 55210, "end": 55387, "idx": 379}, {"begin": 55388, "end": 55503, "idx": 380}, {"begin": 55504, "end": 55794, "idx": 381}, {"begin": 55795, "end": 55915, "idx": 382}, {"begin": 55916, "end": 55965, "idx": 383}, {"begin": 55966, "end": 56365, "idx": 384}, {"begin": 56366, "end": 56477, "idx": 385}, {"begin": 56478, "end": 56655, "idx": 386}, {"begin": 56656, "end": 56722, "idx": 387}, {"begin": 56723, "end": 57007, "idx": 388}, {"begin": 57008, "end": 57192, "idx": 389}, {"begin": 57193, "end": 57365, "idx": 390}, {"begin": 57366, "end": 57421, "idx": 391}, {"begin": 57435, "end": 57547, "idx": 392}, {"begin": 57568, "end": 57670, "idx": 393}, {"begin": 57671, "end": 57771, "idx": 394}, {"begin": 57772, "end": 57861, "idx": 395}, {"begin": 57862, "end": 58038, "idx": 396}, {"begin": 58039, "end": 58206, "idx": 397}, {"begin": 58230, "end": 58390, "idx": 398}, {"begin": 58391, "end": 58550, "idx": 399}, {"begin": 58551, "end": 58627, "idx": 400}, {"begin": 58628, "end": 58728, "idx": 401}, {"begin": 58729, "end": 58765, "idx": 402}, {"begin": 58766, "end": 58864, "idx": 403}, {"begin": 58865, "end": 58972, "idx": 404}, {"begin": 58996, "end": 59117, "idx": 405}, {"begin": 59118, "end": 59295, "idx": 406}, {"begin": 59296, "end": 59394, "idx": 407}, {"begin": 59395, "end": 59458, "idx": 408}, {"begin": 59483, "end": 59505, "idx": 409}, {"begin": 59526, "end": 59704, "idx": 410}, {"begin": 59731, "end": 59763, "idx": 411}, {"begin": 59764, "end": 59800, "idx": 412}, {"begin": 59801, "end": 59877, "idx": 413}, {"begin": 59905, "end": 60078, "idx": 414}, {"begin": 60079, "end": 60118, "idx": 415}, {"begin": 60119, "end": 60156, "idx": 416}, {"begin": 60174, "end": 60240, "idx": 417}, {"begin": 60241, "end": 60384, "idx": 418}, {"begin": 60385, "end": 60589, "idx": 419}, {"begin": 60608, "end": 60657, "idx": 420}, {"begin": 60658, "end": 60886, "idx": 421}, {"begin": 60887, "end": 60943, "idx": 422}, {"begin": 60944, "end": 61036, "idx": 423}, {"begin": 61037, "end": 61143, "idx": 424}, {"begin": 61144, "end": 61249, "idx": 425}, {"begin": 61250, "end": 61329, "idx": 426}, {"begin": 61330, "end": 61374, "idx": 427}, {"begin": 61375, "end": 61419, "idx": 428}, {"begin": 61420, "end": 61562, "idx": 429}, {"begin": 61578, "end": 61615, "idx": 430}, {"begin": 61616, "end": 61705, "idx": 431}, {"begin": 61706, "end": 61875, "idx": 432}, {"begin": 61893, "end": 61990, "idx": 433}, {"begin": 61991, "end": 62066, "idx": 434}, {"begin": 62067, "end": 62142, "idx": 435}, {"begin": 62143, "end": 62215, "idx": 436}, {"begin": 62216, "end": 62365, "idx": 437}, {"begin": 62366, "end": 62481, "idx": 438}, {"begin": 62482, "end": 62558, "idx": 439}, {"begin": 62559, "end": 62713, "idx": 440}, {"begin": 62714, "end": 62835, "idx": 441}, {"begin": 62836, "end": 62883, "idx": 442}, {"begin": 62884, "end": 63048, "idx": 443}, {"begin": 63049, "end": 63268, "idx": 444}, {"begin": 63269, "end": 63395, "idx": 445}, {"begin": 63396, "end": 63494, "idx": 446}, {"begin": 63495, "end": 63578, "idx": 447}, {"begin": 63579, "end": 63676, "idx": 448}, {"begin": 63677, "end": 63731, "idx": 449}, {"begin": 63756, "end": 63797, "idx": 450}, {"begin": 63798, "end": 64014, "idx": 451}, {"begin": 64015, "end": 64091, "idx": 452}, {"begin": 64092, "end": 64241, "idx": 453}, {"begin": 64242, "end": 64366, "idx": 454}, {"begin": 64367, "end": 64641, "idx": 455}, {"begin": 64642, "end": 64740, "idx": 456}, {"begin": 64741, "end": 64794, "idx": 457}, {"begin": 64812, "end": 64921, "idx": 458}, {"begin": 64922, "end": 64998, "idx": 459}, {"begin": 64999, "end": 65037, "idx": 460}, {"begin": 65038, "end": 65079, "idx": 461}, {"begin": 65080, "end": 65143, "idx": 462}, {"begin": 65144, "end": 65243, "idx": 463}, {"begin": 65244, "end": 65355, "idx": 464}, {"begin": 65356, "end": 65473, "idx": 465}, {"begin": 65510, "end": 65643, "idx": 466}, {"begin": 65644, "end": 65770, "idx": 467}, {"begin": 65771, "end": 65894, "idx": 468}, {"begin": 65895, "end": 65993, "idx": 469}, {"begin": 65994, "end": 66113, "idx": 470}, {"begin": 66114, "end": 66198, "idx": 471}, {"begin": 66199, "end": 66257, "idx": 472}, {"begin": 66258, "end": 66435, "idx": 473}, {"begin": 66436, "end": 66513, "idx": 474}, {"begin": 66514, "end": 66596, "idx": 475}, {"begin": 66597, "end": 66660, "idx": 476}, {"begin": 66680, "end": 66783, "idx": 477}, {"begin": 66811, "end": 66881, "idx": 478}, {"begin": 66882, "end": 67134, "idx": 479}, {"begin": 67135, "end": 67313, "idx": 480}, {"begin": 67314, "end": 67407, "idx": 481}, {"begin": 67408, "end": 67522, "idx": 482}, {"begin": 67523, "end": 67680, "idx": 483}, {"begin": 67681, "end": 67750, "idx": 484}, {"begin": 67751, "end": 67851, "idx": 485}, {"begin": 67865, "end": 68034, "idx": 486}, {"begin": 68035, "end": 68322, "idx": 487}, {"begin": 68323, "end": 68426, "idx": 488}, {"begin": 68446, "end": 68516, "idx": 489}, {"begin": 68517, "end": 68676, "idx": 490}, {"begin": 68677, "end": 68794, "idx": 491}, {"begin": 68795, "end": 68883, "idx": 492}, {"begin": 68884, "end": 68963, "idx": 493}, {"begin": 68964, "end": 69075, "idx": 494}, {"begin": 69076, "end": 69206, "idx": 495}, {"begin": 69207, "end": 69338, "idx": 496}, {"begin": 69339, "end": 69471, "idx": 497}, {"begin": 69472, "end": 69623, "idx": 498}, {"begin": 69624, "end": 69724, "idx": 499}, {"begin": 69760, "end": 69977, "idx": 500}, {"begin": 69978, "end": 70322, "idx": 501}, {"begin": 70323, "end": 70543, "idx": 502}, {"begin": 70544, "end": 70799, "idx": 503}, {"begin": 70843, "end": 70941, "idx": 504}, {"begin": 70942, "end": 71008, "idx": 505}, {"begin": 71009, "end": 71081, "idx": 506}, {"begin": 71082, "end": 71149, "idx": 507}, {"begin": 71150, "end": 71266, "idx": 508}, {"begin": 71267, "end": 71268, "idx": 509}, {"begin": 71269, "end": 71329, "idx": 510}, {"begin": 71330, "end": 71394, "idx": 511}, {"begin": 71395, "end": 71400, "idx": 512}, {"begin": 71401, "end": 71506, "idx": 513}, {"begin": 71507, "end": 71681, "idx": 514}, {"begin": 71682, "end": 71820, "idx": 515}, {"begin": 71832, "end": 71978, "idx": 516}, {"begin": 71979, "end": 72060, "idx": 517}, {"begin": 72061, "end": 72083, "idx": 518}, {"begin": 72122, "end": 72375, "idx": 519}, {"begin": 72376, "end": 72519, "idx": 520}, {"begin": 72520, "end": 72602, "idx": 521}, {"begin": 72603, "end": 72671, "idx": 522}, {"begin": 72672, "end": 72746, "idx": 523}, {"begin": 72747, "end": 72828, "idx": 524}, {"begin": 72829, "end": 72925, "idx": 525}, {"begin": 72926, "end": 72998, "idx": 526}, {"begin": 72999, "end": 73110, "idx": 527}, {"begin": 73111, "end": 73209, "idx": 528}, {"begin": 73210, "end": 73309, "idx": 529}, {"begin": 73310, "end": 73360, "idx": 530}, {"begin": 73361, "end": 73408, "idx": 531}, {"begin": 73409, "end": 73691, "idx": 532}, {"begin": 73692, "end": 73778, "idx": 533}, {"begin": 73803, "end": 73962, "idx": 534}, {"begin": 73963, "end": 74119, "idx": 535}, {"begin": 74120, "end": 74155, "idx": 536}, {"begin": 74156, "end": 74355, "idx": 537}, {"begin": 74356, "end": 74473, "idx": 538}, {"begin": 74474, "end": 74538, "idx": 539}, {"begin": 74539, "end": 74642, "idx": 540}, {"begin": 74659, "end": 74807, "idx": 541}, {"begin": 74821, "end": 74870, "idx": 542}, {"begin": 74871, "end": 74916, "idx": 543}, {"begin": 74917, "end": 75084, "idx": 544}, {"begin": 75085, "end": 75209, "idx": 545}, {"begin": 75210, "end": 75296, "idx": 546}, {"begin": 75297, "end": 75350, "idx": 547}, {"begin": 75351, "end": 75458, "idx": 548}, {"begin": 75459, "end": 75822, "idx": 549}, {"begin": 75823, "end": 75844, "idx": 550}, {"begin": 75865, "end": 75890, "idx": 551}, {"begin": 75891, "end": 76051, "idx": 552}, {"begin": 76052, "end": 76160, "idx": 553}, {"begin": 76161, "end": 76379, "idx": 554}, {"begin": 76423, "end": 76632, "idx": 555}, {"begin": 76633, "end": 76712, "idx": 556}, {"begin": 76725, "end": 77143, "idx": 557}, {"begin": 77144, "end": 77328, "idx": 558}, {"begin": 77329, "end": 77413, "idx": 559}, {"begin": 77414, "end": 77550, "idx": 560}, {"begin": 77551, "end": 77612, "idx": 561}, {"begin": 77613, "end": 77706, "idx": 562}, {"begin": 77707, "end": 77748, "idx": 563}, {"begin": 77767, "end": 77851, "idx": 564}, {"begin": 77852, "end": 77874, "idx": 565}, {"begin": 77875, "end": 78123, "idx": 566}, {"begin": 78124, "end": 78202, "idx": 567}, {"begin": 78203, "end": 78259, "idx": 568}, {"begin": 78260, "end": 78302, "idx": 569}, {"begin": 78303, "end": 78517, "idx": 570}, {"begin": 78518, "end": 78549, "idx": 571}, {"begin": 78550, "end": 78650, "idx": 572}, {"begin": 78651, "end": 78792, "idx": 573}, {"begin": 78793, "end": 78824, "idx": 574}, {"begin": 78825, "end": 79076, "idx": 575}, {"begin": 79077, "end": 79222, "idx": 576}, {"begin": 79223, "end": 79749, "idx": 577}, {"begin": 79750, "end": 79768, "idx": 578}, {"begin": 79769, "end": 79917, "idx": 579}, {"begin": 79918, "end": 80089, "idx": 580}, {"begin": 80090, "end": 80279, "idx": 581}, {"begin": 80280, "end": 80420, "idx": 582}, {"begin": 80435, "end": 80507, "idx": 583}, {"begin": 80508, "end": 80612, "idx": 584}, {"begin": 80631, "end": 80777, "idx": 585}, {"begin": 80778, "end": 80941, "idx": 586}, {"begin": 80942, "end": 81089, "idx": 587}, {"begin": 81090, "end": 81244, "idx": 588}, {"begin": 81245, "end": 81350, "idx": 589}, {"begin": 81351, "end": 81534, "idx": 590}, {"begin": 81535, "end": 82057, "idx": 591}, {"begin": 82058, "end": 82281, "idx": 592}, {"begin": 82282, "end": 82353, "idx": 593}, {"begin": 82354, "end": 82499, "idx": 594}, {"begin": 82500, "end": 82631, "idx": 595}, {"begin": 82632, "end": 82972, "idx": 596}, {"begin": 82973, "end": 83112, "idx": 597}, {"begin": 83113, "end": 83433, "idx": 598}, {"begin": 83434, "end": 83785, "idx": 599}, {"begin": 83786, "end": 83868, "idx": 600}, {"begin": 83869, "end": 83885, "idx": 601}, {"begin": 83886, "end": 83935, "idx": 602}, {"begin": 83936, "end": 84007, "idx": 603}, {"begin": 84008, "end": 84063, "idx": 604}, {"begin": 84064, "end": 84129, "idx": 605}, {"begin": 84130, "end": 84168, "idx": 606}, {"begin": 84169, "end": 84266, "idx": 607}, {"begin": 84267, "end": 84285, "idx": 608}, {"begin": 84286, "end": 84318, "idx": 609}, {"begin": 84319, "end": 84338, "idx": 610}, {"begin": 84339, "end": 84346, "idx": 611}, {"begin": 84347, "end": 84359, "idx": 612}, {"begin": 84360, "end": 84414, "idx": 613}, {"begin": 84415, "end": 84477, "idx": 614}, {"begin": 84478, "end": 84500, "idx": 615}, {"begin": 84501, "end": 84535, "idx": 616}, {"begin": 84536, "end": 84621, "idx": 617}, {"begin": 84622, "end": 84671, "idx": 618}, {"begin": 84672, "end": 84688, "idx": 619}, {"begin": 84689, "end": 84748, "idx": 620}, {"begin": 84749, "end": 84996, "idx": 621}, {"begin": 84997, "end": 85087, "idx": 622}, {"begin": 85088, "end": 85265, "idx": 623}, {"begin": 85266, "end": 85467, "idx": 624}, {"begin": 85468, "end": 85728, "idx": 625}, {"begin": 85729, "end": 86040, "idx": 626}, {"begin": 86060, "end": 86175, "idx": 627}, {"begin": 86176, "end": 86446, "idx": 628}, {"begin": 86447, "end": 86661, "idx": 629}, {"begin": 86662, "end": 86795, "idx": 630}, {"begin": 86796, "end": 86957, "idx": 631}, {"begin": 86958, "end": 87084, "idx": 632}, {"begin": 87104, "end": 87196, "idx": 633}], "ReferenceToFigure": [{"begin": 8063, "end": 8065, "target": "#fig_9", "idx": 0}, {"begin": 17273, "end": 17274, "target": "#fig_1", "idx": 1}, {"begin": 18310, "end": 18311, "target": "#fig_1", "idx": 2}, {"begin": 20477, "end": 20478, "target": "#fig_2", "idx": 3}, {"begin": 24236, "end": 24237, "target": "#fig_4", "idx": 4}, {"begin": 28444, "end": 28445, "target": "#fig_6", "idx": 5}, {"begin": 30773, "end": 30774, "target": "#fig_7", "idx": 6}, {"begin": 31179, "end": 31181, "target": "#fig_12", "idx": 7}, {"begin": 32754, "end": 32756, "target": "#fig_2", "idx": 8}, {"begin": 33672, "end": 33674, "target": "#fig_3", "idx": 9}], "Div": [{"begin": 76, "end": 1056, "idx": 0}, {"begin": 1059, "end": 4777, "idx": 1}, {"begin": 4779, "end": 5448, "idx": 2}, {"begin": 5450, "end": 5840, "idx": 3}, {"begin": 5842, "end": 6472, "idx": 4}, {"begin": 6474, "end": 7007, "idx": 5}, {"begin": 7009, "end": 7424, "idx": 6}, {"begin": 7426, "end": 7957, "idx": 7}, {"begin": 7959, "end": 8578, "idx": 8}, {"begin": 8580, "end": 8929, "idx": 9}, {"begin": 8931, "end": 9527, "idx": 10}, {"begin": 9529, "end": 10059, "idx": 11}, {"begin": 10061, "end": 10556, "idx": 12}, {"begin": 10558, "end": 10806, "idx": 13}, {"begin": 10808, "end": 11258, "idx": 14}, {"begin": 11260, "end": 11646, "idx": 15}, {"begin": 11648, "end": 12052, "idx": 16}, {"begin": 12054, "end": 12632, "idx": 17}, {"begin": 12634, "end": 13101, "idx": 18}, {"begin": 13103, "end": 13420, "idx": 19}, {"begin": 13422, "end": 13823, "idx": 20}, {"begin": 13825, "end": 14103, "idx": 21}, {"begin": 14105, "end": 14453, "idx": 22}, {"begin": 14455, "end": 14730, "idx": 23}, {"begin": 14732, "end": 15270, "idx": 24}, {"begin": 15272, "end": 15903, "idx": 25}, {"begin": 15905, "end": 17076, "idx": 26}, {"begin": 17078, "end": 18722, "idx": 27}, {"begin": 18724, "end": 21919, "idx": 28}, {"begin": 21921, "end": 22315, "idx": 29}, {"begin": 22317, "end": 22864, "idx": 30}, {"begin": 22866, "end": 23587, "idx": 31}, {"begin": 23589, "end": 23877, "idx": 32}, {"begin": 23879, "end": 24239, "idx": 33}, {"begin": 24241, "end": 25146, "idx": 34}, {"begin": 25148, "end": 26553, "idx": 35}, {"begin": 26555, "end": 27721, "idx": 36}, {"begin": 27723, "end": 29565, "idx": 37}, {"begin": 29567, "end": 31182, "idx": 38}, {"begin": 31184, "end": 32106, "idx": 39}, {"begin": 32108, "end": 32934, "idx": 40}, {"begin": 32936, "end": 33895, "idx": 41}, {"begin": 33897, "end": 34960, "idx": 42}, {"begin": 34962, "end": 38952, "idx": 43}, {"begin": 38954, "end": 39624, "idx": 44}, {"begin": 39626, "end": 41667, "idx": 45}, {"begin": 41669, "end": 42597, "idx": 46}, {"begin": 42599, "end": 43261, "idx": 47}, {"begin": 43263, "end": 45106, "idx": 48}, {"begin": 45108, "end": 47760, "idx": 49}, {"begin": 47762, "end": 49880, "idx": 50}, {"begin": 49882, "end": 50473, "idx": 51}, {"begin": 50475, "end": 50910, "idx": 52}, {"begin": 50912, "end": 51811, "idx": 53}, {"begin": 51813, "end": 53127, "idx": 54}, {"begin": 53129, "end": 53383, "idx": 55}, {"begin": 53385, "end": 53648, "idx": 56}, {"begin": 53650, "end": 53683, "idx": 57}, {"begin": 53685, "end": 54187, "idx": 58}, {"begin": 54189, "end": 54519, "idx": 59}, {"begin": 54521, "end": 54997, "idx": 60}, {"begin": 54999, "end": 57421, "idx": 61}, {"begin": 57423, "end": 57547, "idx": 62}, {"begin": 57549, "end": 58206, "idx": 63}, {"begin": 58208, "end": 58972, "idx": 64}, {"begin": 58974, "end": 59458, "idx": 65}, {"begin": 59460, "end": 59505, "idx": 66}, {"begin": 59507, "end": 59704, "idx": 67}, {"begin": 59706, "end": 59877, "idx": 68}, {"begin": 59879, "end": 60156, "idx": 69}, {"begin": 60158, "end": 60589, "idx": 70}, {"begin": 60591, "end": 61562, "idx": 71}, {"begin": 61564, "end": 61875, "idx": 72}, {"begin": 61877, "end": 63731, "idx": 73}, {"begin": 63733, "end": 64794, "idx": 74}, {"begin": 64796, "end": 65473, "idx": 75}, {"begin": 65475, "end": 66660, "idx": 76}, {"begin": 66662, "end": 66783, "idx": 77}, {"begin": 66785, "end": 67851, "idx": 78}, {"begin": 67853, "end": 68426, "idx": 79}, {"begin": 68428, "end": 69724, "idx": 80}, {"begin": 69726, "end": 70799, "idx": 81}, {"begin": 70801, "end": 71820, "idx": 82}, {"begin": 71822, "end": 72083, "idx": 83}, {"begin": 72085, "end": 73778, "idx": 84}, {"begin": 73780, "end": 74642, "idx": 85}, {"begin": 74644, "end": 74807, "idx": 86}, {"begin": 74809, "end": 75844, "idx": 87}, {"begin": 75846, "end": 76379, "idx": 88}, {"begin": 76381, "end": 76712, "idx": 89}, {"begin": 76714, "end": 77748, "idx": 90}, {"begin": 77750, "end": 80420, "idx": 91}, {"begin": 80422, "end": 80612, "idx": 92}, {"begin": 80614, "end": 86040, "idx": 93}, {"begin": 86042, "end": 87084, "idx": 94}, {"begin": 87086, "end": 87196, "idx": 95}], "SectionMain": [{"begin": 1056, "end": 87196, "idx": 0}], "ScholarlyEntity": [{"label": "Task", "begin": 190, "end": 216, "seq_label": ["I-Method", "I-Method", "B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120cross", "-", "\u0120downstream", "\u0120general", "ization"], "seq_scores": [0.36978796124458313, 0.38814592361450195, 0.4814020097255707, 0.4762369394302368, 0.6117139458656311], "text": " downstream generalization", "score": 0.4654573559761047, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 294, "end": 299, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9468304514884949, 0.9748820066452026], "text": " Pile", "score": 0.9608562290668488, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 343, "end": 352, "seq_label": ["B-Method"], "seq_token": ["\u0120training"], "seq_scores": [0.5248779654502869], "text": " training", "score": 0.5248779654502869, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 385, "end": 390, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9325632452964783, 0.9770582914352417], "text": " Pile", "score": 0.95481076836586, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 583, "end": 589, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9909880757331848, 0.9949592351913452, 0.9950389266014099, 0.9948822259902954], "text": " GPT-2", "score": 0.9939671158790588, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 593, "end": 599, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.990614652633667, 0.9945936799049377, 0.9950128197669983, 0.9952034950256348], "text": " GPT-3", "score": 0.9938561618328094, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 606, "end": 611, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9468061923980713, 0.9657909274101257], "text": " Pile", "score": 0.9562985599040985, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 731, "end": 736, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9624393582344055, 0.9712075591087341], "text": " Pile", "score": 0.9668234586715698, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 768, "end": 775, "seq_label": ["B-Dataset", "I-MLModel"], "seq_token": ["\u0120Raw", "\u0120CC"], "seq_scores": [0.25063568353652954, 0.430147260427475], "text": " Raw CC", "score": 0.34039147198200226, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 779, "end": 786, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.6130411624908447, 0.7520356774330139, 0.847850501537323], "text": " CC-100", "score": 0.7376424471537272, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 811, "end": 816, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9013662338256836, 0.9641631245613098], "text": " Pile", "score": 0.9327646791934967, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 109, "end": 136, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120increased", "\u0120training", "\u0120dataset"], "seq_scores": [0.7935064435005188, 0.8402427434921265, 0.9512794613838196], "text": " increased training dataset", "score": 0.8616762161254883, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 231, "end": 259, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9928244352340698, 0.9982813596725464, 0.9984444975852966, 0.9979007244110107, 0.9982438087463379], "text": " large-scale language models", "score": 0.9971389651298523, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 300, "end": 331, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u01208", "25", "\u0120Gi", "B", "\u0120English", "\u0120text", "\u0120corpus"], "seq_scores": [0.9877232313156128, 0.9879719614982605, 0.9971422553062439, 0.9974725842475891, 0.9969289898872375, 0.9958059787750244, 0.9980674386024475, 0.9978874325752258], "text": " an 825 GiB English text corpus", "score": 0.9948749840259552, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 352, "end": 380, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9904984831809998, 0.9982483386993408, 0.9982194304466248, 0.996955156326294, 0.9985007047653198], "text": " large-scale language models", "score": 0.9964844226837158, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 410, "end": 442, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012022", "\u0120diverse", "\u0120high", "-", "quality", "\u0120subs", "ets"], "seq_scores": [0.9751890897750854, 0.9889180064201355, 0.9953901767730713, 0.9969276785850525, 0.9972077012062073, 0.9976932406425476, 0.9961689114570618], "text": " 22 diverse high-quality subsets", "score": 0.9924992578370231, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 622, "end": 635, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120models"], "seq_scores": [0.9902831315994263, 0.9899889230728149], "text": " these models", "score": 0.9901360273361206, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 709, "end": 716, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.983184278011322], "text": " models", "score": 0.983184278011322, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 807, "end": 816, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.5442436337471008, 0.772095263004303, 0.7911405563354492], "text": " the Pile", "score": 0.702493151028951, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 960, "end": 969, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9672013521194458, 0.9583912491798401], "text": " the data", "score": 0.9627963006496429, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1097, "end": 1131, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120general", "-", "purpose", "\u0120language", "\u0120modeling"], "seq_scores": [0.9839550256729126, 0.9939242005348206, 0.9953515529632568, 0.9944221377372742, 0.9926677346229553], "text": " general-purpose language modeling", "score": 0.9920641303062439, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1246, "end": 1266, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9979709982872009, 0.995091438293457, 0.9953916072845459, 0.995390772819519, 0.9948493838310242, 0.9952965378761292], "text": "Radford et al., 2019", "score": 0.9956651230653127, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1267, "end": 1288, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sh", "oe", "y", "bi", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9967948794364929, 0.9947894811630249, 0.9949954748153687, 0.9950492978096008, 0.9952234625816345, 0.9956732392311096, 0.9954740405082703, 0.9955829977989197], "text": " Shoeybi et al., 2019", "score": 0.9954478591680527, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1289, "end": 1309, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120R", "aff", "el", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9973726272583008, 0.9948694705963135, 0.9950773119926453, 0.995370090007782, 0.9955839514732361, 0.9956417083740234, 0.9960301518440247], "text": " Raffel et al., 2019", "score": 0.9957064730780465, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1310, "end": 1323, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ross", "et", ",", "\u01202019"], "seq_scores": [0.9962724447250366, 0.9965768456459045, 0.9966155886650085, 0.9969080090522766], "text": " Rosset, 2019", "score": 0.9965932220220566, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1324, "end": 1343, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9964602589607239, 0.9958307147026062, 0.9963382482528687, 0.9964084029197693, 0.9963046312332153], "text": " Brown et al., 2020", "score": 0.9962684512138367, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1344, "end": 1366, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Lep", "ikh", "in", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9963841438293457, 0.9944233298301697, 0.9949173927307129, 0.9955626726150513, 0.9957960844039917, 0.9958652257919312, 0.9960039258003235], "text": " Lepikhin et al., 2020", "score": 0.9955646821430751, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1403, "end": 1427, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120model", "\u0120training"], "seq_scores": [0.5913500785827637, 0.6315356492996216, 0.5927038192749023], "text": " language model training", "score": 0.6051965157190958, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1498, "end": 1517, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ka", "plan", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9980781078338623, 0.9960265159606934, 0.9967807531356812, 0.9965406060218811, 0.9960616230964661, 0.9961666464805603], "text": "Kaplan et al., 2020", "score": 0.9966090420881907, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1179, "end": 1194, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120massive", "\u0120models"], "seq_scores": [0.9891830682754517, 0.9948263764381409], "text": " massive models", "score": 0.9920047223567963, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1197, "end": 1216, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9944106936454773, 0.9968488812446594, 0.9984908103942871, 0.9960615038871765], "text": " large text corpora", "score": 0.9964529722929001, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1403, "end": 1418, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120model"], "seq_scores": [0.948184072971344, 0.8992486596107483], "text": " language model", "score": 0.9237163662910461, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1443, "end": 1474, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120high", "-", "quality", "\u0120massive", "\u0120text", "\u0120data"], "seq_scores": [0.9903383851051331, 0.9973375201225281, 0.9978237152099609, 0.9963646531105042, 0.9984532594680786, 0.99853515625], "text": " high-quality massive text data", "score": 0.9964754482110342, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1548, "end": 1566, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.8083555698394775, 0.8627818822860718], "text": " language modeling", "score": 0.8355687260627747, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1634, "end": 1647, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8061106204986572, 0.9381439089775085, 0.9548359513282776], "text": " Common Crawl", "score": 0.8996968269348145, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1679, "end": 1697, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9974637031555176, 0.9975098371505737, 0.9976067543029785, 0.9971850514411926, 0.9972127079963684], "text": "Brown et al., 2020", "score": 0.9973956108093261, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1698, "end": 1718, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120R", "aff", "el", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9972615242004395, 0.997528612613678, 0.9979810118675232, 0.9978908896446228, 0.9978998899459839, 0.9976824522018433, 0.997717022895813], "text": " Raffel et al., 2019", "score": 0.9977087719099862, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1726, "end": 1735, "seq_label": ["B-Method"], "seq_token": ["\u0120training"], "seq_scores": [0.5765162110328674], "text": " training", "score": 0.5765162110328674, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 1742, "end": 1755, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8316853642463684, 0.9529644846916199, 0.9640989899635315], "text": " Common Crawl", "score": 0.9162496129671732, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1865, "end": 1891, "seq_label": ["I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120https", "://", "p", "ile", ".", "ele", "uther", ".", "ai", "/", "\u0120vers", "ity", "\u0120downstream", "\u0120general", "ization"], "seq_scores": [0.7184617519378662, 0.8935090899467468, 0.9252027273178101, 0.9524872899055481, 0.9643532633781433, 0.9595071077346802, 0.9640246629714966, 0.9689481854438782, 0.9640671014785767, 0.9626979231834412, 0.88893061876297, 0.791493833065033, 0.7977606654167175, 0.720547616481781, 0.8307992219924927], "text": " downstream generalization", "score": 0.8868527372678121, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1904, "end": 1916, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ross", "et", ",", "\u01202019"], "seq_scores": [0.9974932670593262, 0.9982343912124634, 0.9979382157325745, 0.9979932308197021], "text": "Rosset, 2019", "score": 0.9979147762060165, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2098, "end": 2110, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ross", "et", ",", "\u01202019"], "seq_scores": [0.9974408149719238, 0.9981018900871277, 0.9978070855140686, 0.9977449178695679], "text": "Rosset, 2019", "score": 0.997773677110672, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2111, "end": 2130, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9968534111976624, 0.9974185228347778, 0.9977636337280273, 0.9976940751075745, 0.9975616931915283], "text": " Brown et al., 2020", "score": 0.9974582672119141, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2131, "end": 2152, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Car", "lin", "i", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9977039694786072, 0.997531533241272, 0.9976693987846375, 0.997576892375946, 0.9976922273635864, 0.997554361820221, 0.9976373910903931], "text": " Carlini et al., 2020", "score": 0.9976236820220947, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2278, "end": 2309, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120general", "\u0120cross", "-", "domain", "\u0120knowledge"], "seq_scores": [0.7175101041793823, 0.8689515590667725, 0.8011465072631836, 0.7892007827758789, 0.6391729712486267], "text": " general cross-domain knowledge", "score": 0.7631963849067688, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2313, "end": 2339, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120downstream", "\u0120general", "ization"], "seq_scores": [0.7812080979347229, 0.7878009080886841, 0.793340265750885], "text": " downstream generalization", "score": 0.7874497572580973, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1540, "end": 1545, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.8371642231941223], "text": " data", "score": 0.8371642231941223, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1577, "end": 1619, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120most", "\u0120existing", "\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9412776231765747, 0.9645694494247437, 0.9846380352973938, 0.9985413551330566, 0.9986870884895325, 0.9983898401260376, 0.9983569979667664], "text": " most existing large-scale language models", "score": 0.9834943413734436, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1651, "end": 1656, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120most"], "seq_scores": [0.5501552224159241], "text": " most", "score": 0.5501552224159241, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1666, "end": 1677, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120their", "\u0120data"], "seq_scores": [0.6486018300056458, 0.8952509760856628], "text": " their data", "score": 0.7719264030456543, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1738, "end": 1755, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8654116988182068, 0.917959451675415, 0.8708480000495911, 0.847891092300415], "text": " the Common Crawl", "score": 0.875527560710907, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 1802, "end": 1810, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120dataset"], "seq_scores": [0.9573829770088196], "text": " dataset", "score": 0.9573829770088196, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1932, "end": 1960, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9936096668243408, 0.9976602792739868, 0.9980223178863525, 0.9980102181434631, 0.9984057545661926], "text": " large-scale language models", "score": 0.9971416473388672, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2065, "end": 2079, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120small", "\u0120amounts", "\u0120of", "\u0120training", "\u0120data"], "seq_scores": [0.7458600997924805, 0.9177694320678711, 0.6732965111732483, 0.6677319407463074, 0.9944313168525696], "text": " training data", "score": 0.7998178601264954, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2200, "end": 2258, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120number", "\u0120of", "\u0120smaller", ",", "\u0120high", "\u0120quality", ",", "\u0120diverse", "\u0120datasets"], "seq_scores": [0.9499154090881348, 0.9631320834159851, 0.97661954164505, 0.969760537147522, 0.9609872102737427, 0.9958036541938782, 0.9955472946166992, 0.9978755712509155, 0.9953259229660034, 0.9953019618988037, 0.9987381100654602], "text": " a large number of smaller, high quality, diverse datasets", "score": 0.9817279360511086, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2355, "end": 2365, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9916683435440063, 0.9924446940422058], "text": " the model", "score": 0.9920565187931061, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2377, "end": 2384, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9524049162864685], "text": " models", "score": 0.9524049162864685, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2466, "end": 2471, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9718768000602722, 0.9895480871200562], "text": " Pile", "score": 0.9807124435901642, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2561, "end": 2566, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9803482890129089, 0.9881643056869507], "text": " Pile", "score": 0.9842562973499298, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2784, "end": 2789, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9761487245559692, 0.983636200428009], "text": " Pile", "score": 0.9798924624919891, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 2865, "end": 2873, "seq_label": ["B-Task"], "seq_token": ["\u0120general"], "seq_scores": [0.5144215822219849], "text": " general", "score": 0.5144215822219849, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 2970, "end": 2985, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PubMed", "\u0120Central"], "seq_scores": [0.8770782351493835, 0.9154356122016907], "text": " PubMed Central", "score": 0.8962569236755371, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 2986, "end": 2992, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Ar", "X", "iv"], "seq_scores": [0.8410606980323792, 0.9211866855621338, 0.9242605566978455], "text": " ArXiv", "score": 0.8955026467641195, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 2993, "end": 3000, "seq_label": ["B-Datasource"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.8721106052398682], "text": " GitHub", "score": 0.8721106052398682, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3001, "end": 3005, "seq_label": ["B-Datasource"], "seq_token": ["\u0120the"], "seq_scores": [0.8244341015815735], "text": " the", "score": 0.8244341015815735, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3005, "end": 3021, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Free", "Law", "\u0120Project"], "seq_scores": [0.5272819995880127, 0.9103797078132629, 0.8961026668548584], "text": " FreeLaw Project", "score": 0.777921458085378, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3022, "end": 3037, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Exchange"], "seq_scores": [0.8643669486045837, 0.8991641402244568], "text": " Stack Exchange", "score": 0.8817655444145203, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3038, "end": 3042, "seq_label": ["B-Datasource"], "seq_token": ["\u0120the"], "seq_scores": [0.8045396208763123], "text": " the", "score": 0.8045396208763123, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3042, "end": 3073, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120US", "\u0120Patent", "\u0120and", "\u0120Trad", "emark", "\u0120Office"], "seq_scores": [0.5068919062614441, 0.8857762813568115, 0.8924013376235962, 0.8736051321029663, 0.8907186985015869, 0.8840267658233643], "text": " US Patent and Trademark Office", "score": 0.8222366869449615, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3074, "end": 3081, "seq_label": ["B-Datasource"], "seq_token": ["\u0120PubMed"], "seq_scores": [0.8552348017692566], "text": " PubMed", "score": 0.8552348017692566, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3082, "end": 3093, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Ubuntu", "\u0120IRC"], "seq_scores": [0.7787970304489136, 0.8675749897956848], "text": " Ubuntu IRC", "score": 0.8231860101222992, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3094, "end": 3105, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Hacker", "News"], "seq_scores": [0.8304185271263123, 0.8909991383552551], "text": " HackerNews", "score": 0.8607088327407837, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3106, "end": 3114, "seq_label": ["B-Datasource"], "seq_token": ["\u0120YouTube"], "seq_scores": [0.8488485813140869], "text": " YouTube", "score": 0.8488485813140869, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3115, "end": 3126, "seq_label": ["B-Dataset", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.47972583770751953, 0.5431160926818848, 0.5507683157920837], "text": " PhilPapers", "score": 0.5245367487271627, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3131, "end": 3144, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120NIH", "\u0120Ex", "P", "orter"], "seq_scores": [0.4746696650981903, 0.5513321161270142, 0.6370230913162231, 0.5722158551216125], "text": " NIH ExPorter", "score": 0.55881018191576, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3163, "end": 3176, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9947677850723267, 0.9969768524169922, 0.9973565340042114, 0.9962705373764038], "text": " OpenWebText2", "score": 0.9963429272174835, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3180, "end": 3192, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us", "2"], "seq_scores": [0.9934197068214417, 0.9971168041229248, 0.9972547888755798, 0.9961218237876892], "text": " BookCorpus2", "score": 0.9959782809019089, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3230, "end": 3242, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text"], "seq_scores": [0.9931730628013611, 0.9965969920158386, 0.9971912503242493], "text": " OpenWebText", "score": 0.995653768380483, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3244, "end": 3268, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "ok", "as", "lan", "\u0120and", "\u0120Cohen", ",", "\u01202019"], "seq_scores": [0.996595561504364, 0.9970691800117493, 0.9977741837501526, 0.9980171918869019, 0.998043417930603, 0.9979403614997864, 0.997469425201416, 0.9972646236419678], "text": "Gokaslan and Cohen, 2019", "score": 0.9975217431783676, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3273, "end": 3284, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9924188852310181, 0.9965898990631104, 0.9966437816619873], "text": " BookCorpus", "score": 0.995217521985372, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3286, "end": 3302, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Z", "hu", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9971510767936707, 0.996634304523468, 0.9975837469100952, 0.9975191354751587, 0.9969784021377563, 0.9964345693588257], "text": "Zhu et al., 2015", "score": 0.9970502058664957, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3303, "end": 3319, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kob", "ay", "ashi", ",", "\u01202018"], "seq_scores": [0.9563841819763184, 0.9974379539489746, 0.9978384375572205, 0.9977738261222839, 0.9974555373191833], "text": " Kobayashi, 2018", "score": 0.9893779873847961, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2472, "end": 2506, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u01208", "25", ".", "18", "\u0120Gi", "B", "\u0120English", "\u0120text", "\u0120dataset"], "seq_scores": [0.9916512370109558, 0.9937354922294617, 0.9979442954063416, 0.9978795051574707, 0.9984322190284729, 0.9989714622497559, 0.9986292123794556, 0.9971816539764404, 0.999126136302948, 0.9992063641548157], "text": " a 825.18 GiB English text dataset", "score": 0.9972757577896119, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2528, "end": 2556, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "\u0120scale", "\u0120language", "\u0120models"], "seq_scores": [0.991828978061676, 0.9974685907363892, 0.9973738193511963, 0.9977169036865234], "text": " large scale language models", "score": 0.9960970729589462, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2581, "end": 2618, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012022", "\u0120diverse", "\u0120and", "\u0120high", "-", "quality", "\u0120datasets"], "seq_scores": [0.9771042466163635, 0.9919495582580566, 0.9968451261520386, 0.9971151351928711, 0.9964739680290222, 0.9978790283203125, 0.9988441467285156], "text": " 22 diverse and high-quality datasets", "score": 0.9937444584710258, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2634, "end": 2683, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120established", "\u0120natural", "\u0120language", "\u0120processing", "\u0120datasets"], "seq_scores": [0.9710147380828857, 0.9945167899131775, 0.998674750328064, 0.9988019466400146, 0.9988478422164917], "text": " established natural language processing datasets", "score": 0.9923712134361267, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2687, "end": 2717, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120newly", "\u0120introduced", "\u0120ones"], "seq_scores": [0.9620166420936584, 0.9757340550422668, 0.989270031452179, 0.9878978133201599], "text": " several newly introduced ones", "score": 0.978729635477066, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2757, "end": 2779, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "\u0120language", "\u0120models"], "seq_scores": [0.9928380846977234, 0.9970819354057312, 0.9976595640182495], "text": " large language models", "score": 0.9958598613739014, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2807, "end": 2834, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120broad", "-", "co", "verage", "\u0120benchmark"], "seq_scores": [0.8579153418540955, 0.8791700601577759, 0.8859833478927612, 0.8705755472183228, 0.9131529331207275, 0.8864136934280396], "text": " a broad-coverage benchmark", "score": 0.8822018206119537, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2891, "end": 2907, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9942051768302917, 0.996802806854248], "text": " language models", "score": 0.9955039918422699, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2921, "end": 2934, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120new", "\u0120datasets"], "seq_scores": [0.9933150410652161, 0.9962360262870789], "text": " new datasets", "score": 0.9947755336761475, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3217, "end": 3242, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120original", "\u0120Open", "Web", "Text"], "seq_scores": [0.8143967390060425, 0.9599094986915588, 0.9591436386108398, 0.9458452463150024, 0.9111865162849426], "text": " the original OpenWebText", "score": 0.9180963277816773, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3411, "end": 3418, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Books", "3"], "seq_scores": [0.9761443734169006, 0.9862031936645508], "text": " Books3", "score": 0.9811737835407257, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3420, "end": 3433, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Press", "er", ",", "\u01202020"], "seq_scores": [0.9947328567504883, 0.9985053539276123, 0.9983478784561157, 0.9982776641845703], "text": "Presser, 2020", "score": 0.9974659383296967, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3435, "end": 3453, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Project", "\u0120Gutenberg"], "seq_scores": [0.8947892189025879, 0.9472376704216003], "text": " Project Gutenberg", "score": 0.9210134446620941, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3455, "end": 3460, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["PG", "-", "19"], "seq_scores": [0.9768857955932617, 0.9919154047966003, 0.9931727051734924], "text": "PG-19", "score": 0.9873246351877848, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3463, "end": 3479, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "ae", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9977520108222961, 0.9983916878700256, 0.9986170530319214, 0.998600423336029, 0.998519241809845, 0.9982312321662903], "text": "Rae et al., 2019", "score": 0.9983519415060679, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3481, "end": 3496, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "-", "Sub", "t", "itles"], "seq_scores": [0.9779532551765442, 0.9878407120704651, 0.9898945689201355, 0.9904244542121887, 0.9913541674613953], "text": " Open-Subtitles", "score": 0.9874934315681457, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3498, "end": 3513, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["T", "ied", "em", "ann", ",", "\u01202016"], "seq_scores": [0.9968515038490295, 0.9982499480247498, 0.9983604550361633, 0.9986323714256287, 0.9985499978065491, 0.9982132911682129], "text": "Tiedemann, 2016", "score": 0.9981429278850555, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3515, "end": 3533, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120English", "\u0120Wikipedia"], "seq_scores": [0.7390950322151184, 0.7779979705810547], "text": " English Wikipedia", "score": 0.7585465013980865, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3534, "end": 3549, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120DM", "\u0120Mathematics"], "seq_scores": [0.9632358551025391, 0.9759953618049622], "text": " DM Mathematics", "score": 0.9696156084537506, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3551, "end": 3570, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sa", "xton", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9977548718452454, 0.9982360601425171, 0.998566210269928, 0.9985257983207703, 0.9984293580055237, 0.9980630278587341], "text": "Saxton et al., 2019", "score": 0.9982625544071198, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3572, "end": 3581, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Euro", "P", "arl"], "seq_scores": [0.9866569638252258, 0.9923563003540039, 0.9935936331748962], "text": " EuroParl", "score": 0.9908689657847086, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3583, "end": 3594, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["K", "oe", "hn", ",", "\u01202005"], "seq_scores": [0.9973663687705994, 0.9975925087928772, 0.9982627034187317, 0.9982014894485474, 0.9978596568107605], "text": "Koehn, 2005", "score": 0.9978565454483033, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3604, "end": 3617, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120En", "ron", "\u0120Emails"], "seq_scores": [0.9865167140960693, 0.9857609868049622, 0.9742530584335327], "text": " Enron Emails", "score": 0.9821769197781881, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3626, "end": 3646, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["K", "lim", "t", "\u0120and", "\u0120Yang", ",", "\u01202004"], "seq_scores": [0.9973945617675781, 0.9972196817398071, 0.998162567615509, 0.9984667897224426, 0.998498797416687, 0.998512327671051, 0.9979878664016724], "text": "Klimt and Yang, 2004", "score": 0.998034656047821, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3763, "end": 3768, "seq_label": ["I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "B-Dataset", "I-Dataset"], "seq_token": ["X", "iv", ":", "21", "01", ".", "000", "27", "v", "cs", ".", "CL", "\u0120P", "ile"], "seq_scores": [0.4236912429332733, 0.4654519557952881, 0.6809048652648926, 0.7621651291847229, 0.8280711770057678, 0.8530116081237793, 0.8638095259666443, 0.757729172706604, 0.5874946117401123, 0.4427231252193451, 0.5789856910705566, 0.675547182559967, 0.9042845368385315, 0.8756672143936157], "text": " Pile", "score": 0.6928240742002215, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3804, "end": 3817, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.909115195274353, 0.7529537081718445, 0.9074589610099792], "text": " Common Crawl", "score": 0.856509288152059, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 3876, "end": 3882, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9660776853561401, 0.9878607392311096, 0.986055314540863, 0.9864040613174438], "text": " GPT-2", "score": 0.9815994501113892, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 3886, "end": 3892, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9710512757301331, 0.9870110750198364, 0.9856427311897278, 0.9855846762657166], "text": " GPT-3", "score": 0.9823224395513535, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3940, "end": 3945, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8476276993751526, 0.8242362141609192], "text": " Pile", "score": 0.8359319567680359, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3977, "end": 3982, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9156222939491272, 0.8826144933700562], "text": " Pile", "score": 0.8991183936595917, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4029, "end": 4042, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.719474732875824, 0.5049648284912109, 0.7248435616493225], "text": " Common Crawl", "score": 0.6497610410054525, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4156, "end": 4161, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8506681323051453, 0.7783793807029724], "text": " Pile", "score": 0.8145237565040588, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4292, "end": 4297, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8596729040145874, 0.808832049369812], "text": " Pile", "score": 0.8342524766921997, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3372, "end": 3410, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120existing", "\u0120high", "quality", "\u0120datasets"], "seq_scores": [0.9744001626968384, 0.9645984172821045, 0.9946324825286865, 0.9977772831916809, 0.998356282711029], "text": " several existing highquality datasets", "score": 0.9859529256820678, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3799, "end": 3822, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120pure", "\u0120Common", "\u0120C", "rawl", "\u0120data"], "seq_scores": [0.9859225749969482, 0.9928781390190125, 0.9980352520942688, 0.9985462427139282, 0.99811851978302], "text": " pure Common Crawl data", "score": 0.9947001457214355, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3936, "end": 3945, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.5277650952339172, 0.7999798655509949, 0.7564067840576172], "text": " the Pile", "score": 0.6947172482808431, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3955, "end": 3962, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9850391149520874], "text": " models", "score": 0.9850391149520874, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3973, "end": 3982, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.6639930605888367, 0.5737771391868591, 0.49942296743392944], "text": " the Pile", "score": 0.5790643890698751, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4012, "end": 4049, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120raw", "\u0120and", "\u0120filtered", "\u0120Common", "\u0120C", "rawl", "\u0120models"], "seq_scores": [0.9497220516204834, 0.9906845092773438, 0.9926925897598267, 0.9947338104248047, 0.9967657327651978, 0.996726393699646, 0.9974721074104309], "text": " raw and filtered Common Crawl models", "score": 0.9883995992796761, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4136, "end": 4145, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120text"], "seq_scores": [0.6649070978164673, 0.6643405556678772], "text": " the text", "score": 0.6646238267421722, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4194, "end": 4203, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120P", "\u0120the", "\u0120data"], "seq_scores": [0.5665371417999268, 0.9697086215019226, 0.9605005979537964], "text": " the data", "score": 0.8322487870852152, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4447, "end": 4472, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.9859073162078857, 0.991954505443573, 0.9930591583251953], "text": " the constituent datasets", "score": 0.9903069933255514, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4514, "end": 4526, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120alternative"], "seq_scores": [0.5300189256668091], "text": " alternative", "score": 0.5300189256668091, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4619, "end": 4632, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9144092202186584, 0.8559383749961853], "text": " each dataset", "score": 0.8851737976074219, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4733, "end": 4746, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9549035429954529, 0.9537523984909058], "text": " each dataset", "score": 0.9543279707431793, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 4902, "end": 4920, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.7648131847381592, 0.7325171828269958], "text": " language modeling", "score": 0.7486651837825775, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4861, "end": 4898, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u01208", "25", ".", "18", "\u0120Gi", "B", "\u0120english", "language", "\u0120dataset"], "seq_scores": [0.9922457337379456, 0.9953508377075195, 0.9981940388679504, 0.9981269240379333, 0.9984782338142395, 0.9985600113868713, 0.9983822107315063, 0.9960557222366333, 0.998759388923645, 0.998849630355835], "text": " a 825.18 GiB englishlanguage dataset", "score": 0.9973002731800079, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4973, "end": 5007, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012014", "\u0120new", "\u0120language", "\u0120modeling", "\u0120datasets"], "seq_scores": [0.989364743232727, 0.9875556826591492, 0.9958938360214233, 0.9982524514198303, 0.9986066222190857], "text": " 14 new language modeling datasets", "score": 0.9939346671104431, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 5147, "end": 5153, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.7040232419967651, 0.8434252738952637, 0.7629407048225403, 0.6806232333183289], "text": " GPT-2", "score": 0.7477531135082245, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 5218, "end": 5225, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.9930905699729919, 0.9906542897224426, 0.9930192828178406], "text": " CC-100", "score": 0.9922547141710917, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 5233, "end": 5246, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9870553016662598, 0.9896973967552185, 0.9920809864997864], "text": " Common Crawl", "score": 0.9896112283070883, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5147, "end": 5165, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120G", "PT", "-", "2", "sized", "\u0120models"], "seq_scores": [0.9568937420845032, 0.9651968479156494, 0.9741604924201965, 0.9702532887458801, 0.9658595323562622, 0.9816633462905884], "text": " GPT-2sized models", "score": 0.9690045416355133, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5176, "end": 5193, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120new", "\u0120dataset"], "seq_scores": [0.9837328791618347, 0.987536609172821, 0.9956307411193848], "text": " this new dataset", "score": 0.9889667431513468, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5229, "end": 5246, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8608871102333069, 0.955227255821228, 0.9468846321105957, 0.9620590806007385], "text": " raw Common Crawl", "score": 0.9312645196914673, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5289, "end": 5302, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.915338397026062, 0.9656559824943542], "text": " this dataset", "score": 0.9404971897602081, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5432, "end": 5447, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120their", "\u0120own", "\u0120data"], "seq_scores": [0.7018844485282898, 0.4674426019191742, 0.8910074830055237], "text": " their own data", "score": 0.6867781778176626, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5470, "end": 5475, "seq_label": ["B-Method", "I-Method"], "seq_token": ["The", "\u0120P"], "seq_scores": [0.4411250948905945, 0.5750409960746765], "text": "The P", "score": 0.5080830454826355, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5553, "end": 5574, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", "),"], "seq_scores": [0.9889711737632751, 0.9988992214202881, 0.9989882111549377, 0.9990431666374207, 0.9978718757629395, 0.9978548884391785, 0.996213972568512], "text": " Brown et al. (2020),", "score": 0.9968346442495074, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 5671, "end": 5681, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wikipedia"], "seq_scores": [0.7197839021682739], "text": " Wikipedia", "score": 0.7197839021682739, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5493, "end": 5521, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012022", "\u0120constituent", "\u0120sub", "-", "dat", "as", "ets"], "seq_scores": [0.982180118560791, 0.9929261803627014, 0.9969459176063538, 0.9974306225776672, 0.997775137424469, 0.9961367249488831, 0.9958192110061646], "text": " 22 constituent sub-datasets", "score": 0.9941734160695758, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5641, "end": 5663, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120high", "-", "quality", "\u0120datasets"], "seq_scores": [0.6022451519966125, 0.9911307096481323, 0.9902860522270203, 0.9959178566932678], "text": " high-quality datasets", "score": 0.8948949426412582, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5799, "end": 5812, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9760355949401855, 0.9912887811660767], "text": " each dataset", "score": 0.9836621880531311, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 5854, "end": 5866, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Common", "\u0120C", "rawl"], "seq_scores": [0.9804107546806335, 0.9797073602676392, 0.9860964417457581], "text": "Common Crawl", "score": 0.9820715188980103, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6013, "end": 6026, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9812936186790466, 0.9813432097434998, 0.9870498776435852], "text": " Common Crawl", "score": 0.9832289020220438, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6145, "end": 6158, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9529269933700562, 0.9614428281784058, 0.9754670858383179], "text": " Common Crawl", "score": 0.9632789691289266, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6181, "end": 6206, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120well", "-", "designed", "\u0120extraction"], "seq_scores": [0.5742874145507812, 0.5212448835372925, 0.5788359642028809, 0.3733994662761688], "text": " well-designed extraction", "score": 0.5119419321417809, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6225, "end": 6238, "seq_label": ["I-Method", "B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120filtering", "\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.4458233118057251, 0.8976103067398071, 0.8929207921028137, 0.9251367449760437], "text": " Common Crawl", "score": 0.7903727889060974, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 6253, "end": 6261, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile", "-", "CC"], "seq_scores": [0.9943084120750427, 0.9941990375518799, 0.9944809675216675, 0.9958237409591675], "text": " Pile-CC", "score": 0.9947030395269394, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6267, "end": 6276, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "-", "Text"], "seq_scores": [0.9785904884338379, 0.9940904974937439, 0.9947863817214966, 0.9950186014175415], "text": " jus-Text", "score": 0.990621492266655, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6278, "end": 6301, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["End", "r\u00c3\u00a9", "dy", "\u0120and", "\u0120Nov", "\u00c3\u00a1", "k", ",", "\u01202013"], "seq_scores": [0.9935654997825623, 0.9957903027534485, 0.9965097308158875, 0.9974310994148254, 0.9966211318969727, 0.9972968697547913, 0.9972876310348511, 0.9970569610595703, 0.9961273074150085], "text": "Endr\u00e9dy and Nov\u00e1k, 2013", "score": 0.9964096148808798, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5869, "end": 5900, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120collection", "\u0120of", "\u0120website", "\u0120craw", "ls"], "seq_scores": [0.9793917536735535, 0.9755982160568237, 0.894476592540741, 0.7336992621421814, 0.9910234212875366, 0.9850267767906189], "text": " a collection of website crawls", "score": 0.9265360037485758, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5929, "end": 5943, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120web", "\u0120pages"], "seq_scores": [0.9330009818077087, 0.9631640315055847, 0.9559800028800964], "text": " raw web pages", "score": 0.9507150053977966, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5944, "end": 5953, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120metadata"], "seq_scores": [0.8175224661827087], "text": " metadata", "score": 0.8175224661827087, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5957, "end": 5974, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120text", "\u0120extract", "ions"], "seq_scores": [0.884423017501831, 0.926582396030426, 0.8897313475608826], "text": " text extractions", "score": 0.9002455870310465, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6000, "end": 6012, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9911670088768005, 0.9964119791984558], "text": " the dataset", "score": 0.9937894940376282, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6057, "end": 6062, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120text"], "seq_scores": [0.5835726261138916], "text": " text", "score": 0.5835726261138916, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6221, "end": 6252, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Our", "\u0120Common", "\u0120C", "rawl", "-", "based", "\u0120dataset"], "seq_scores": [0.9818219542503357, 0.9613000750541687, 0.9974773526191711, 0.9980366826057434, 0.9972338080406189, 0.9977784752845764, 0.9981955885887146], "text": " Our Common Crawl-based dataset", "score": 0.9902634194919041, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6325, "end": 6343, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["raw", "\u0120HTTP", "\u0120responses"], "seq_scores": [0.5411756038665771, 0.6561651229858398, 0.5934128165245056], "text": "raw HTTP responses", "score": 0.5969178477923075, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6493, "end": 6507, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["Pub", "Med", "\u0120Central"], "seq_scores": [0.8196748495101929, 0.9301650524139404, 0.9371593594551086], "text": "PubMed Central", "score": 0.8956664204597473, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6509, "end": 6512, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["PM", "C"], "seq_scores": [0.8242394328117371, 0.9154626131057739], "text": "PMC", "score": 0.8698510229587555, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6532, "end": 6557, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120PubMed", "\u0120online", "\u0120repository"], "seq_scores": [0.7742381691932678, 0.743908166885376, 0.6421957612037659], "text": " PubMed online repository", "score": 0.7201140324274699, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6772, "end": 6776, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PM", "C"], "seq_scores": [0.7912704944610596, 0.9114070534706116], "text": " PMC", "score": 0.8513387739658356, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6912, "end": 6916, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PM", "C"], "seq_scores": [0.7982445955276489, 0.9050416946411133], "text": " PMC", "score": 0.8516431450843811, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6561, "end": 6581, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120biomedical", "\u0120articles"], "seq_scores": [0.8743144273757935, 0.8687886595726013], "text": " biomedical articles", "score": 0.8715515434741974, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7020, "end": 7026, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["Books", "3"], "seq_scores": [0.988150954246521, 0.9840608239173889], "text": "Books3", "score": 0.986105889081955, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7091, "end": 7117, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120B", "ibli", "ot", "ik", "\u0120private", "\u0120tracker"], "seq_scores": [0.6148656010627747, 0.8650206923484802, 0.8814403414726257, 0.8680034279823303, 0.5711209177970886, 0.6794458627700806], "text": " Bibliotik private tracker", "score": 0.7466494739055634, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7166, "end": 7176, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["er", ",", "\u01202020", "\u0120B", "ibli", "ot", "ik"], "seq_scores": [0.7327626347541809, 0.7145599126815796, 0.5680681467056274, 0.9490741491317749, 0.9590010046958923, 0.9596912860870361, 0.9629186987876892], "text": " Bibliotik", "score": 0.8351536904062543, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7317, "end": 7327, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120B", "ibli", "ot", "ik"], "seq_scores": [0.9538750648498535, 0.9653152823448181, 0.9661589860916138, 0.9676253199577332], "text": " Bibliotik", "score": 0.9632436633110046, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7360, "end": 7388, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120long", "-", "range", "\u0120context", "\u0120modeling"], "seq_scores": [0.6253016591072083, 0.5006632208824158, 0.5087067484855652, 0.5155416131019592, 0.538935124874115], "text": " long-range context modeling", "score": 0.5378296732902527, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7029, "end": 7039, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9911317229270935, 0.9880015254020691], "text": " a dataset", "score": 0.9895666241645813, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7042, "end": 7048, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120books"], "seq_scores": [0.8630244731903076], "text": " books", "score": 0.8630244731903076, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7197, "end": 7226, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120fiction", "\u0120and", "\u0120non", "fiction", "\u0120books"], "seq_scores": [0.7246758341789246, 0.9855906963348389, 0.9812057018280029, 0.9942605495452881, 0.9848427772521973], "text": " fiction and nonfiction books", "score": 0.9341151118278503, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7274, "end": 7304, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120next", "\u0120largest", "\u0120book", "\u0120dataset"], "seq_scores": [0.9710255265235901, 0.9391757845878601, 0.9900734424591064, 0.991682767868042, 0.9974430799484253], "text": " our next largest book dataset", "score": 0.9778801202774048, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7335, "end": 7341, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120books"], "seq_scores": [0.5262429714202881], "text": " books", "score": 0.5262429714202881, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7443, "end": 7455, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Open", "Web", "Text", "2"], "seq_scores": [0.9928544163703918, 0.9964888095855713, 0.9972730278968811, 0.9970567226409912], "text": "OpenWebText2", "score": 0.9959182441234589, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7457, "end": 7461, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["OW", "T", "2"], "seq_scores": [0.9923747181892395, 0.9966092109680176, 0.9966683983802795], "text": "OWT2", "score": 0.9952174425125122, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7510, "end": 7518, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Web", "Text"], "seq_scores": [0.9899594187736511, 0.9946351647377014], "text": " WebText", "score": 0.9922972917556763, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 7520, "end": 7540, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9977028965950012, 0.998017430305481, 0.9983206391334534, 0.9982312321662903, 0.9979782700538635, 0.9977074861526489], "text": "Radford et al., 2019", "score": 0.997992992401123, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7545, "end": 7563, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "Corp", "us"], "seq_scores": [0.9918408393859863, 0.9963889122009277, 0.9970768690109253, 0.9969075322151184, 0.9968320727348328], "text": " OpenWebTextCorpus", "score": 0.9958092451095581, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 7565, "end": 7589, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "ok", "as", "lan", "\u0120and", "\u0120Cohen", ",", "\u01202019"], "seq_scores": [0.9975180625915527, 0.9978881478309631, 0.9982361793518066, 0.9984462857246399, 0.9984843134880066, 0.9984543323516846, 0.9983708262443542, 0.9982075691223145], "text": "Gokaslan and Cohen, 2019", "score": 0.9982007145881653, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7615, "end": 7623, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Web", "Text"], "seq_scores": [0.9863429665565491, 0.9960716962814331], "text": " WebText", "score": 0.9912073314189911, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7646, "end": 7653, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.7329465746879578], "text": " Reddit", "score": 0.7329465746879578, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7703, "end": 7716, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9930538535118103, 0.9968878626823425, 0.9972323775291443, 0.9966894388198853], "text": " OpenWebText2", "score": 0.9959658831357956, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7750, "end": 7757, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.8629615902900696], "text": " Reddit", "score": 0.8629615902900696, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7909, "end": 7914, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120O", "WT", "2"], "seq_scores": [0.9923993945121765, 0.9960911870002747, 0.9963758587837219], "text": " OWT2", "score": 0.9949554800987244, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7465, "end": 7498, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120generalized", "\u0120web", "\u0120scrape", "\u0120dataset"], "seq_scores": [0.9937340617179871, 0.9958215951919556, 0.9975988268852234, 0.9983269572257996, 0.9984696507453918], "text": " a generalized web scrape dataset", "score": 0.9967902183532715, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7917, "end": 7956, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120dataset", "\u0120a", "\u0120high", "\u0120quality", "\u0120general", "\u0120purpose", "\u0120dataset"], "seq_scores": [0.6016190648078918, 0.9854745864868164, 0.9923382997512817, 0.9959995746612549, 0.9950543642044067, 0.9977462887763977, 0.9980970025062561], "text": " a high quality general purpose dataset", "score": 0.9380470258849007, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7969, "end": 7974, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["Ar", "X", "iv"], "seq_scores": [0.40126150846481323, 0.7179253101348877, 0.7507550120353699], "text": "ArXiv", "score": 0.6233139435450236, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8066, "end": 8072, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120ar", "X", "iv"], "seq_scores": [0.6586413383483887, 0.7577643990516663, 0.7816160917282104], "text": " arXiv", "score": 0.7326739430427551, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8163, "end": 8169, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120ar", "X", "iv"], "seq_scores": [0.5935126543045044, 0.7843871712684631, 0.8094292879104614], "text": " arXiv", "score": 0.7291097044944763, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8322, "end": 8328, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Ar", "X", "iv"], "seq_scores": [0.6327372789382935, 0.7911580801010132, 0.7987995147705078], "text": " ArXiv", "score": 0.7408982912699381, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7999, "end": 8015, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120research", "\u0120papers"], "seq_scores": [0.9350727796554565, 0.9535011053085327], "text": " research papers", "score": 0.9442869424819946, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8066, "end": 8069, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120ar"], "seq_scores": [0.658592700958252], "text": " ar", "score": 0.658592700958252, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8210, "end": 8228, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["iv", "\u0120papers", "\u0120high", "\u0120quality", "\u0120text"], "seq_scores": [0.5256657600402832, 0.6297502517700195, 0.7351762056350708, 0.8172218799591064, 0.7844803333282471], "text": " high quality text", "score": 0.6984588861465454, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8322, "end": 8335, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Ar", "X", "iv", "\u0120papers"], "seq_scores": [0.9717618823051453, 0.9772031307220459, 0.9780207276344299, 0.9653488993644714], "text": " ArXiv papers", "score": 0.9730836600065231, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 8466, "end": 8483, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120language", "\u0120model"], "seq_scores": [0.9834543466567993, 0.9907838106155396, 0.9918397665023804], "text": " a language model", "score": 0.9886926412582397, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8506, "end": 8513, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120papers"], "seq_scores": [0.792017936706543], "text": " papers", "score": 0.792017936706543, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8591, "end": 8597, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["G", "it", "Hub"], "seq_scores": [0.7918028235435486, 0.876425564289093, 0.9044982194900513], "text": "GitHub", "score": 0.857575535774231, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 8677, "end": 8683, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9715431928634644, 0.9827709794044495, 0.9834876656532288, 0.9851894974708557], "text": " GPT-3", "score": 0.9807478338479996, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8685, "end": 8703, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9947928786277771, 0.9968644976615906, 0.9969524145126343, 0.9967226386070251, 0.995698094367981], "text": "Brown et al., 2020", "score": 0.9962061047554016, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8835, "end": 8842, "seq_label": ["B-Datasource"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.8674696087837219], "text": " GitHub", "score": 0.8674696087837219, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8600, "end": 8615, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120corpus"], "seq_scores": [0.9824540019035339, 0.9754003882408142, 0.9747458100318909], "text": " a large corpus", "score": 0.9775334000587463, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8751, "end": 8769, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120open", "-", "source", "\u0120code", "\u0120repositories", "\u0120its", "\u0120training", "\u0120data"], "seq_scores": [0.46801304817199707, 0.9284316897392273, 0.915744423866272, 0.9013472199440002, 0.9281356930732727, 0.9627001285552979, 0.569180965423584, 0.9902961850166321], "text": " its training data", "score": 0.8329811692237854, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8788, "end": 8822, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120explicitly", "\u0120gathered", "\u0120code", "\u0120datasets"], "seq_scores": [0.7803288102149963, 0.9876460433006287, 0.9820558428764343, 0.9971578121185303], "text": " explicitly gathered code datasets", "score": 0.9367971271276474, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9079, "end": 9093, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Court", "Listener"], "seq_scores": [0.4377821087837219, 0.6269267201423645], "text": " CourtListener", "score": 0.5323544144630432, "type": "ScholarlyEntity"}, {"label": "URL", "begin": 9344, "end": 9375, "seq_label": ["B-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL"], "seq_token": ["\u0120https", "://", "www", ".", "court", "list", "ener", ".", "com", "/"], "seq_scores": [0.42882612347602844, 0.8279417157173157, 0.8528124690055847, 0.9153769612312317, 0.8185856342315674, 0.8305762410163879, 0.8407121896743774, 0.9034020900726318, 0.9011459350585938, 0.8934475183486938], "text": " https://www.courtlistener.com/", "score": 0.8212826877832413, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9154, "end": 9181, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120millions", "\u0120of", "\u0120legal", "\u0120opinions"], "seq_scores": [0.8011481761932373, 0.8649799227714539, 0.7964962720870972, 0.9338006973266602], "text": " millions of legal opinions", "score": 0.8491062670946121, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9218, "end": 9235, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120full", "\u0120dataset"], "seq_scores": [0.9937620759010315, 0.997149646282196, 0.9980565905570984], "text": " the full dataset", "score": 0.996322770913442, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9422, "end": 9437, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120court", "\u0120opinions"], "seq_scores": [0.776378333568573, 0.8701662421226501], "text": " court opinions", "score": 0.8232722878456116, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9479, "end": 9489, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["-", "text", "\u0120This", "\u0120data"], "seq_scores": [0.588965654373169, 0.6844038963317871, 0.9397503137588501, 0.9520924687385559], "text": " This data", "score": 0.7913030833005905, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9551, "end": 9576, "seq_label": ["B-Datasource", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Stack", "\u0120Exchange", "\u0120Data", "\u0120D", "ump"], "seq_scores": [0.48824387788772583, 0.5496336221694946, 0.908197820186615, 0.9596431255340576, 0.9610278010368347], "text": " Stack Exchange Data Dump", "score": 0.7733492493629456, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9644, "end": 9659, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Exchange"], "seq_scores": [0.8202223181724548, 0.831300675868988], "text": " Stack Exchange", "score": 0.8257614970207214, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 9933, "end": 9948, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Exchange"], "seq_scores": [0.8486986756324768, 0.8273293375968933], "text": " Stack Exchange", "score": 0.8380140066146851, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 9986, "end": 10005, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120question", "\u0120answering"], "seq_scores": [0.7035314440727234, 0.7977595329284668], "text": " question answering", "score": 0.7506454885005951, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9587, "end": 9637, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120anonym", "ized", "\u0120set", "\u0120of", "\u0120all", "\u0120user", "-", "cont", "ributed", "\u0120content"], "seq_scores": [0.9836937785148621, 0.9923506379127502, 0.9888219833374023, 0.9904748797416687, 0.8774447441101074, 0.7186769843101501, 0.8541746139526367, 0.9732104539871216, 0.9724077582359314, 0.9661211967468262, 0.9646539688110352], "text": " an anonymized set of all user-contributed content", "score": 0.9347300908782266, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9717, "end": 9744, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120user", "-", "cont", "ributed", "\u0120questions"], "seq_scores": [0.9332796931266785, 0.8963950276374817, 0.9030975699424744, 0.9112398624420166, 0.8931090235710144], "text": " user-contributed questions", "score": 0.9074242353439331, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9748, "end": 9756, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120answers"], "seq_scores": [0.5463103652000427], "text": " answers", "score": 0.5463103652000427, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9770, "end": 9814, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120largest", "\u0120publicly", "\u0120available", "\u0120repositories"], "seq_scores": [0.7154530882835388, 0.9030901789665222, 0.9054598212242126, 0.9175866842269897, 0.9702847003936768], "text": " the largest publicly available repositories", "score": 0.882374894618988, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9817, "end": 9839, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120question", "-", "answer", "\u0120pairs"], "seq_scores": [0.935484766960144, 0.997951328754425, 0.998428225517273, 0.9972448348999023], "text": " question-answer pairs", "score": 0.9822772890329361, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 10021, "end": 10039, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120downstream", "\u0120models"], "seq_scores": [0.9874070286750793, 0.9936277866363525], "text": " downstream models", "score": 0.9905174076557159, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10083, "end": 10100, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["US", "P", "TO", "\u0120Background", "s"], "seq_scores": [0.9677259922027588, 0.9726340770721436, 0.9737896919250488, 0.9763993620872498, 0.980219304561615], "text": "USPTO Backgrounds", "score": 0.9741536855697632, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10426, "end": 10444, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120US", "P", "TO", "\u0120Background", "s"], "seq_scores": [0.9115484952926636, 0.9264957308769226, 0.9372556805610657, 0.9514260292053223, 0.9573740363121033], "text": " USPTO Backgrounds", "score": 0.9368199944496155, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10103, "end": 10113, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9878087043762207, 0.9795260429382324], "text": " a dataset", "score": 0.9836673736572266, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 10583, "end": 10592, "seq_label": ["B-Datasource"], "seq_token": ["Wikipedia"], "seq_scores": [0.9148439764976501], "text": "Wikipedia", "score": 0.9148439764976501, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 10638, "end": 10656, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.9320613145828247, 0.9568328261375427], "text": " language modeling", "score": 0.9444470703601837, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10616, "end": 10634, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120high", "-", "quality", "\u0120text"], "seq_scores": [0.48567789793014526, 0.9463471174240112, 0.962084174156189, 0.9619480967521667], "text": " high-quality text", "score": 0.839014321565628, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 10830, "end": 10846, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["Pub", "Med", "\u0120Abstract", "s"], "seq_scores": [0.7492064833641052, 0.886408269405365, 0.8836723566055298, 0.8242765665054321], "text": "PubMed Abstracts", "score": 0.835890918970108, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 10904, "end": 10911, "seq_label": ["B-Datasource"], "seq_token": ["\u0120PubMed"], "seq_scores": [0.8362867832183838], "text": " PubMed", "score": 0.8362867832183838, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 11009, "end": 11013, "seq_label": ["I-Datasource", "B-Datasource", "I-Datasource"], "seq_token": ["\u0120repository", "\u0120PM", "C"], "seq_scores": [0.45604416728019714, 0.5980758666992188, 0.7516064047813416], "text": " PMC", "score": 0.6019088129202524, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 11146, "end": 11153, "seq_label": ["B-Datasource"], "seq_token": ["\u0120PubMed"], "seq_scores": [0.8340603113174438], "text": " PubMed", "score": 0.8340603113174438, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 11171, "end": 11180, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120MED", "-", "LINE"], "seq_scores": [0.6473658680915833, 0.8207919597625732, 0.8242812156677246], "text": " MED-LINE", "score": 0.7641463478406271, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10858, "end": 10872, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120abstract", "s"], "seq_scores": [0.9169431924819946, 0.9560779929161072, 0.9509325623512268], "text": " the abstracts", "score": 0.9413179159164429, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10877, "end": 10901, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012030", "\u0120million", "\u0120publications"], "seq_scores": [0.8685784935951233, 0.9029914140701294, 0.8456360101699829], "text": " 30 million publications", "score": 0.8724019726117452, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10938, "end": 10958, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120biomedical", "\u0120articles"], "seq_scores": [0.8797485828399658, 0.9808995723724365], "text": " biomedical articles", "score": 0.9303240776062012, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11211, "end": 11232, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120biomedical", "\u0120abstract", "s"], "seq_scores": [0.8971226215362549, 0.9319412112236023, 0.9201282858848572], "text": " biomedical abstracts", "score": 0.9163973728815714, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11283, "end": 11300, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["Project", "\u0120Gutenberg"], "seq_scores": [0.8204822540283203, 0.7115929126739502], "text": "Project Gutenberg", "score": 0.7660375833511353, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 11357, "end": 11375, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Project", "\u0120Gutenberg"], "seq_scores": [0.539386510848999, 0.5748515725135803], "text": " Project Gutenberg", "score": 0.5571190416812897, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11400, "end": 11406, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120PG", "-", "19"], "seq_scores": [0.9947570562362671, 0.9952196478843689, 0.9954511523246765], "text": " PG-19", "score": 0.9951426188151041, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 11419, "end": 11437, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Project", "\u0120Gutenberg"], "seq_scores": [0.5484004616737366, 0.5634268522262573], "text": " Project Gutenberg", "score": 0.555913656949997, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11462, "end": 11478, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "ae", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9978044629096985, 0.9984900951385498, 0.9988387227058411, 0.99888676404953, 0.9987612962722778, 0.9985753297805786], "text": "Rae et al., 2019", "score": 0.998559445142746, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11533, "end": 11540, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Books", "3"], "seq_scores": [0.991334855556488, 0.9914244413375854], "text": " Books3", "score": 0.9913796484470367, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11544, "end": 11555, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9902804493904114, 0.9928560256958008, 0.9939854741096497], "text": " BookCorpus", "score": 0.9923739830652872, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11574, "end": 11580, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120PG", "-", "19"], "seq_scores": [0.9947709441184998, 0.995444655418396, 0.9950308799743652], "text": " PG-19", "score": 0.995082159837087, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11614, "end": 11645, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120long", "-", "distance", "\u0120context", "\u0120modeling"], "seq_scores": [0.57856285572052, 0.6921103000640869, 0.68990558385849, 0.7115525007247925, 0.7635260224342346], "text": " long-distance context modeling", "score": 0.6871314525604248, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11303, "end": 11313, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9914236664772034, 0.9964853525161743], "text": " a dataset", "score": 0.9939545094966888, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11316, "end": 11343, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120classic", "\u0120Western", "\u0120literature"], "seq_scores": [0.6964784860610962, 0.807426929473877, 0.8041439652442932], "text": " classic Western literature", "score": 0.7693497935930887, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11344, "end": 11391, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120specific", "\u0120Project", "\u0120Gutenberg", "\u0120derived", "\u0120dataset"], "seq_scores": [0.9923283457756042, 0.993766188621521, 0.9885791540145874, 0.9974036812782288, 0.9972111582756042, 0.9981537461280823], "text": " The specific Project Gutenberg derived dataset", "score": 0.994573712348938, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11419, "end": 11443, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Project", "\u0120Gutenberg", "\u0120books"], "seq_scores": [0.9675270915031433, 0.9813671708106995, 0.9637983441352844], "text": " Project Gutenberg books", "score": 0.9708975354830424, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 11670, "end": 11684, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Sub", "t", "itles"], "seq_scores": [0.9874380230903625, 0.9959510564804077, 0.9955303072929382, 0.9945293068885803], "text": " OpenSubtitles", "score": 0.9933621734380722, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11781, "end": 11799, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120T", "ied", "em", "ann", "\u0120(", "2016", ")."], "seq_scores": [0.9840636253356934, 0.9983799457550049, 0.9987978935241699, 0.9989604949951172, 0.9979815483093262, 0.9975669384002686, 0.9945560693740845], "text": " Tiedemann (2016).", "score": 0.9957580736705235, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 11950, "end": 11978, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120creative", "\u0120writing", "\u0120generation"], "seq_scores": [0.8525785207748413, 0.8952337503433228, 0.8826315402984619], "text": " creative writing generation", "score": 0.876814603805542, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 11992, "end": 12006, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120screen", "writing"], "seq_scores": [0.938584566116333, 0.9220579862594604], "text": " screenwriting", "score": 0.9303212761878967, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 12007, "end": 12021, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120speech", "writing"], "seq_scores": [0.8978389501571655, 0.9199113249778748], "text": " speechwriting", "score": 0.9088751375675201, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 12026, "end": 12051, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120interactive", "\u0120storytelling"], "seq_scores": [0.9072566032409668, 0.8652717471122742], "text": " interactive storytelling", "score": 0.8862641751766205, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11695, "end": 11723, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120English", "\u0120language", "\u0120dataset"], "seq_scores": [0.9924992918968201, 0.9954478144645691, 0.9984092116355896, 0.9987047910690308], "text": " an English language dataset", "score": 0.9962652772665024, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11726, "end": 11736, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120subtitles"], "seq_scores": [0.7927650213241577], "text": " subtitles", "score": 0.7927650213241577, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11799, "end": 11809, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Sub", "t", "itles"], "seq_scores": [0.9635059237480164, 0.9511444568634033, 0.9668055176734924], "text": " Subtitles", "score": 0.9604852994283041, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12083, "end": 12104, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Deep", "Mind", "\u0120Mathematics"], "seq_scores": [0.9867245554924011, 0.9846658706665039, 0.9678342938423157], "text": " DeepMind Mathematics", "score": 0.9797415733337402, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12285, "end": 12304, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sa", "xton", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9980375170707703, 0.998401939868927, 0.998678982257843, 0.9985944628715515, 0.9984219074249268, 0.9982819557189941], "text": "Saxton et al., 2019", "score": 0.9984027942021688, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12395, "end": 12413, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9978423118591309, 0.9984316229820251, 0.9985615611076355, 0.998464822769165, 0.9984537363052368], "text": "Brown et al., 2020", "score": 0.9983508110046386, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12626, "end": 12631, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8354609608650208, 0.7846696376800537], "text": " Pile", "score": 0.8100652992725372, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12124, "end": 12162, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120collection", "\u0120of", "\u0120mathematical", "\u0120problems"], "seq_scores": [0.9806844592094421, 0.9822767376899719, 0.9364284873008728, 0.606266438961029, 0.9757866859436035], "text": " a collection of mathematical problems", "score": 0.8962885618209839, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12258, "end": 12283, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120natural", "\u0120language", "\u0120prompts"], "seq_scores": [0.9276862144470215, 0.9012656807899475, 0.8661520481109619], "text": " natural language prompts", "score": 0.8983679811159769, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12328, "end": 12350, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "\u0120language", "\u0120models"], "seq_scores": [0.9941413998603821, 0.996311604976654, 0.9969021081924438], "text": " large language models", "score": 0.9957850376764933, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12453, "end": 12467, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120math", "\u0120problems"], "seq_scores": [0.8677206039428711, 0.8268851041793823], "text": " math problems", "score": 0.8473028540611267, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12470, "end": 12487, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.9916039109230042, 0.9930746555328369, 0.9922873973846436], "text": " the training set", "score": 0.9923219879468282, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12512, "end": 12525, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset", "\u0120of"], "seq_scores": [0.9910789132118225, 0.9959198236465454, 0.7320796847343445], "text": " a dataset of", "score": 0.9063594738642374, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12525, "end": 12547, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120mathematical", "\u0120problems"], "seq_scores": [0.6718523502349854, 0.9849829077720642], "text": " mathematical problems", "score": 0.8284176290035248, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12595, "end": 12611, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9941425919532776, 0.9950293898582458], "text": " language models", "score": 0.9945859909057617, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12651, "end": 12662, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Book", "Corp", "us", "2"], "seq_scores": [0.9889344573020935, 0.9957072734832764, 0.9959365129470825, 0.9941493272781372], "text": "BookCorpus2", "score": 0.9936818927526474, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12701, "end": 12712, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9876413941383362, 0.9935691356658936, 0.994864284992218], "text": " BookCorpus", "score": 0.9920249382654825, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12714, "end": 12730, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Z", "hu", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9978316426277161, 0.9979402422904968, 0.9981411695480347, 0.9980165958404541, 0.9973912239074707, 0.9971572160720825], "text": "Zhu et al., 2015", "score": 0.9977463483810425, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12835, "end": 12846, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9857894778251648, 0.9939369559288025, 0.994569718837738], "text": " BookCorpus", "score": 0.9914320508639017, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12901, "end": 12919, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Project", "\u0120Gutenberg"], "seq_scores": [0.6826638579368591, 0.8101158142089844], "text": " Project Gutenberg", "score": 0.7463898360729218, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12923, "end": 12930, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Books", "3"], "seq_scores": [0.9343330264091492, 0.949081301689148], "text": " Books3", "score": 0.9417071640491486, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 12965, "end": 12976, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9825685024261475, 0.9934638142585754, 0.9942259192466736], "text": " BookCorpus", "score": 0.9900860786437988, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13040, "end": 13060, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9975525736808777, 0.996344268321991, 0.9962309002876282, 0.996025800704956, 0.9949826002120972, 0.9944661259651184], "text": "Radford et al., 2018", "score": 0.9959337115287781, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13061, "end": 13081, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Dev", "lin", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9968554973602295, 0.9951287508010864, 0.9947197437286377, 0.9948145747184753, 0.9940629601478577, 0.9931180477142334], "text": " Devlin et al., 2019", "score": 0.9947832624117533, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13082, "end": 13099, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Liu", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9961430430412292, 0.9962524771690369, 0.9967946410179138, 0.9966711401939392, 0.9966773986816406], "text": " Liu et al., 2019", "score": 0.9965077400207519, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12732, "end": 12771, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120expanded", "\u0120version", "\u0120the", "\u0120original", "\u0120Book", "Corp", "\u0120a", "\u0120widely", "\u0120used", "\u0120language", "\u0120modeling", "\u0120corpus"], "seq_scores": [0.5156771540641785, 0.5499354600906372, 0.5592592358589172, 0.8888041377067566, 0.630437970161438, 0.6564410924911499, 0.983241081237793, 0.9914246797561646, 0.9962462782859802, 0.9947350025177002, 0.9976315498352051, 0.9979569911956787], "text": " a widely used language modeling corpus", "score": 0.8134825527667999, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12785, "end": 12791, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120books"], "seq_scores": [0.9356163740158081], "text": " books", "score": 0.9356163740158081, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12948, "end": 12964, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120published", "\u0120books"], "seq_scores": [0.9676662683486938, 0.980798065662384], "text": " published books", "score": 0.9742321670055389, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13001, "end": 13009, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120dataset"], "seq_scores": [0.9086635112762451], "text": " dataset", "score": 0.9086635112762451, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13022, "end": 13038, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.989623486995697, 0.9960643649101257], "text": " language models", "score": 0.9928439259529114, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13122, "end": 13133, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Ubuntu", "\u0120IRC"], "seq_scores": [0.9516568779945374, 0.9420785903930664], "text": " Ubuntu IRC", "score": 0.9468677341938019, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 13229, "end": 13238, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120F", "reen", "ode"], "seq_scores": [0.3894929587841034, 0.5542212724685669, 0.4921553134918213], "text": " Freenode", "score": 0.4786231815814972, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13157, "end": 13170, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120publicly"], "seq_scores": [0.6161565780639648, 0.5141213536262512], "text": " the publicly", "score": 0.565138965845108, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13255, "end": 13268, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["log", "\u0120Chat", "log", "\u0120data"], "seq_scores": [0.525124192237854, 0.9566975235939026, 0.9773428440093994, 0.9749177694320679], "text": " Chatlog data", "score": 0.858520582318306, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13436, "end": 13444, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Euro", "P", "arl"], "seq_scores": [0.9890773296356201, 0.9934091567993164, 0.994492769241333], "text": "EuroParl", "score": 0.9923264185587565, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13446, "end": 13457, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["K", "oe", "hn", ",", "\u01202005"], "seq_scores": [0.9969668984413147, 0.9968208074569702, 0.9971275925636292, 0.9967382550239563, 0.9966553449630737], "text": "Koehn, 2005", "score": 0.9968617796897888, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13518, "end": 13538, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120translation"], "seq_scores": [0.8213420510292053, 0.8697091341018677], "text": " machine translation", "score": 0.8455255925655365, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13599, "end": 13619, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gro", "ves", "\u0120and", "\u0120Way", ",", "\u01202006"], "seq_scores": [0.9972586631774902, 0.9966562986373901, 0.9974971413612366, 0.9972116351127625, 0.9975268244743347, 0.9968043565750122], "text": "Groves and Way, 2006", "score": 0.9971591532230377, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13620, "end": 13639, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Van", "\u0120H", "alt", "eren", ",", "\u01202008"], "seq_scores": [0.9960200190544128, 0.9959995746612549, 0.9967169165611267, 0.9971531629562378, 0.9974174499511719, 0.9970188140869141], "text": " Van Halteren, 2008", "score": 0.9967209895451864, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13640, "end": 13661, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ci", "ob", "anu", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9970917701721191, 0.9957985281944275, 0.9969533681869507, 0.9970393180847168, 0.9974162578582764, 0.9974673986434937, 0.9976490139961243], "text": " Ciobanu et al., 2017", "score": 0.9970593793051583, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13461, "end": 13492, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120mult", "ilingual", "\u0120parallel", "\u0120corpus"], "seq_scores": [0.9915483593940735, 0.9959555864334106, 0.9977068901062012, 0.9970539808273315, 0.9974972605705261], "text": " a multilingual parallel corpus", "score": 0.9959524154663086, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 13851, "end": 13869, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120YouTube", "\u0120Sub", "t", "itles"], "seq_scores": [0.9684060215950012, 0.9931282997131348, 0.9953985810279846, 0.9951801300048828], "text": " YouTube Subtitles", "score": 0.9880282580852509, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 13954, "end": 13962, "seq_label": ["B-Datasource"], "seq_token": ["\u0120YouTube"], "seq_scores": [0.9076568484306335], "text": " YouTube", "score": 0.9076568484306335, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 14007, "end": 14025, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Youtube", "\u0120Sub", "t", "itles"], "seq_scores": [0.9669532775878906, 0.9936774373054504, 0.9952499866485596, 0.9950523972511292], "text": " Youtube Subtitles", "score": 0.9877332746982574, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13880, "end": 13906, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120parallel", "\u0120corpus", "\u0120of", "\u0120text"], "seq_scores": [0.9900990128517151, 0.9944406151771545, 0.9958048462867737, 0.7565965056419373, 0.7754198312759399], "text": " a parallel corpus of text", "score": 0.9024721622467041, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13988, "end": 14006, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120mult", "ilingual", "\u0120data"], "seq_scores": [0.9203891754150391, 0.9428815841674805, 0.929119348526001], "text": " multilingual data", "score": 0.9307967027028402, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14045, "end": 14065, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120educational", "\u0120content"], "seq_scores": [0.7191828489303589, 0.6778723001480103], "text": " educational content", "score": 0.6985275745391846, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 14124, "end": 14135, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.9846310019493103, 0.9866073131561279, 0.9869916439056396], "text": " PhilPapers", "score": 0.9860766530036926, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 14322, "end": 14333, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.9811675548553467, 0.97847580909729, 0.9818170070648193], "text": " PhilPapers", "score": 0.980486790339152, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14157, "end": 14193, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120open", "-", "access", "\u0120philosophy", "\u0120publications"], "seq_scores": [0.9300030469894409, 0.9723843336105347, 0.9700864553451538, 0.9514285922050476, 0.9354812502861023], "text": " open-access philosophy publications", "score": 0.9518767356872558, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14198, "end": 14224, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120international", "\u0120database"], "seq_scores": [0.9426289796829224, 0.9533018469810486, 0.9175330996513367], "text": " an international database", "score": 0.9378213087717692, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14522, "end": 14545, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120bulk", "-", "data", "\u0120repository"], "seq_scores": [0.9490553140640259, 0.9704659581184387, 0.9555982947349548, 0.9651512503623962, 0.8808571696281433], "text": " a bulk-data repository", "score": 0.9442255973815918, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14653, "end": 14665, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9762513637542725, 0.9724679589271545], "text": " the dataset", "score": 0.9743596613407135, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14749, "end": 14762, "seq_label": ["B-Method", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["H", "acker", "\u0120News", "\u01209"], "seq_scores": [0.5334888100624084, 0.47065454721450806, 0.5501664876937866, 0.4157935678958893], "text": "Hacker News 9", "score": 0.4925258532166481, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14863, "end": 14872, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120articles"], "seq_scores": [0.6188398599624634], "text": " articles", "score": 0.6188398599624634, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14943, "end": 14962, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120submitted", "\u0120articles"], "seq_scores": [0.7454913854598999, 0.7045620083808899], "text": " submitted articles", "score": 0.7250266969203949, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15293, "end": 15306, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120En", "ron", "\u0120Emails"], "seq_scores": [0.9932140111923218, 0.9927570223808289, 0.992634117603302], "text": " Enron Emails", "score": 0.9928683837254842, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 15316, "end": 15336, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["K", "lim", "t", "\u0120and", "\u0120Yang", ",", "\u01202004"], "seq_scores": [0.9950700998306274, 0.9952331185340881, 0.9963964819908142, 0.9970439076423645, 0.9971156120300293, 0.9973896145820618, 0.9965468049049377], "text": "Klimt and Yang, 2004", "score": 0.9963993770735604, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15432, "end": 15445, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120En", "ron", "\u0120Emails"], "seq_scores": [0.9901143312454224, 0.9907858371734619, 0.9909132719039917], "text": " Enron Emails", "score": 0.9906044801076254, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15340, "end": 15358, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120valuable", "\u0120corpus"], "seq_scores": [0.9859857559204102, 0.9876070618629456, 0.9941450953483582], "text": " a valuable corpus", "score": 0.9892459710439047, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15546, "end": 15565, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120other", "\u0120datasets"], "seq_scores": [0.8892138004302979, 0.8303620219230652, 0.99429851770401], "text": " our other datasets", "score": 0.9046247800191244, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15581, "end": 15597, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Language", "\u0120Models"], "seq_scores": [0.9177510738372803, 0.9465530514717102], "text": " Language Models", "score": 0.9321520626544952, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15621, "end": 15626, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.4661032259464264, 0.9577580094337463], "text": " Pile", "score": 0.7119306176900864, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15824, "end": 15829, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.5532270669937134, 0.9300365447998047], "text": " Pile", "score": 0.741631805896759, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15643, "end": 15662, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120training", "\u0120dataset"], "seq_scores": [0.986393392086029, 0.9788100719451904, 0.9948333501815796], "text": " a training dataset", "score": 0.986678938070933, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15666, "end": 15694, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9883381724357605, 0.9951593279838562, 0.9965754151344299, 0.9967221617698669, 0.9971119165420532], "text": " large-scale language models", "score": 0.9947813987731934, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15764, "end": 15786, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120evaluation", "\u0120dataset"], "seq_scores": [0.9846991896629333, 0.9816319942474365, 0.996068000793457], "text": " an evaluation dataset", "score": 0.9874663949012756, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15844, "end": 15869, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120broad", "-", "co", "verage", "\u0120dataset"], "seq_scores": [0.9824438691139221, 0.9856297373771667, 0.990623414516449, 0.9918292164802551, 0.994236409664154, 0.9946488738059998], "text": " a broad-coverage dataset", "score": 0.9899019201596578, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15886, "end": 15902, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9841455817222595, 0.996266782283783], "text": " language models", "score": 0.9902061820030212, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 15936, "end": 15938, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.7798683643341064], "text": " P", "score": 0.7798683643341064, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15956, "end": 15994, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120train", ",", "\u0120validation", ",", "\u0120and", "\u0120testing", "\u0120splits"], "seq_scores": [0.9656375050544739, 0.9934843182563782, 0.9941126704216003, 0.9911808371543884, 0.9910604357719421, 0.9939597845077515, 0.995091438293457], "text": " train, validation, and testing splits", "score": 0.9892181413514274, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15995, "end": 16033, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120validation", "\u0120and", "\u0120testing", "\u0120components"], "seq_scores": [0.9772408604621887, 0.9885799288749695, 0.9821034073829651, 0.9878144264221191, 0.9829922318458557], "text": " The validation and testing components", "score": 0.9837461709976196, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16046, "end": 16063, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01200", ".", "1", "%", "\u0120of", "\u0120the", "\u0120data"], "seq_scores": [0.7953433990478516, 0.844571053981781, 0.8690805435180664, 0.8078229427337646, 0.6732468008995056, 0.511654257774353, 0.9854029417037964], "text": " 0.1% of the data", "score": 0.7838745628084455, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16137, "end": 16151, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120most", "\u0120datasets"], "seq_scores": [0.9270538687705994, 0.9766538739204407], "text": " most datasets", "score": 0.95185387134552, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16170, "end": 16182, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9835832715034485, 0.9615315794944763], "text": " the dataset", "score": 0.9725574254989624, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16198, "end": 16235, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01201", "\u0120Gi", "B", "\u0120of", "\u0120validation", "\u0120and", "\u0120testing", "\u0120data"], "seq_scores": [0.6775466799736023, 0.7795609831809998, 0.7233666777610779, 0.7710305452346802, 0.54929119348526, 0.9949907660484314, 0.9968196153640747, 0.9970269799232483], "text": " 1 GiB of validation and testing data", "score": 0.8112041801214218, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16301, "end": 16311, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.7211322784423828], "text": " documents", "score": 0.7211322784423828, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16410, "end": 16443, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120train", "/", "valid", "ation", "/", "test", "\u0120splits"], "seq_scores": [0.9787122011184692, 0.9877278804779053, 0.9944778680801392, 0.9948613047599792, 0.9941978454589844, 0.9924997687339783, 0.9932010769844055, 0.9906867146492004], "text": " the train/validation/test splits", "score": 0.9907955825328827, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16468, "end": 16496, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120bits", "\u0120per", "\u0120UTF", "-", "8", "\u0120encoded", "\u0120byte"], "seq_scores": [0.6516382098197937, 0.6008554697036743, 0.7416551113128662, 0.7014707922935486, 0.6829445362091064, 0.5800772905349731, 0.5408493280410767], "text": " bits per UTF-8 encoded byte", "score": 0.6427843911307198, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16498, "end": 16501, "seq_label": ["B-Method", "I-Method"], "seq_token": ["BP", "B"], "seq_scores": [0.6353533864021301, 0.7547519207000732], "text": "BPB", "score": 0.6950526535511017, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16503, "end": 16517, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Bits", "\u0120per", "\u0120byte"], "seq_scores": [0.5784624814987183, 0.5263278484344482, 0.5872712135314941], "text": " Bits per byte", "score": 0.5640205144882202, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16535, "end": 16554, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120bits", "\u0120per", "\u0120character"], "seq_scores": [0.5919214487075806, 0.6024117469787598, 0.7186787128448486], "text": " bits per character", "score": 0.637670636177063, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16865, "end": 16877, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9652089476585388, 0.9322620034217834], "text": " the dataset", "score": 0.9487354755401611, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16912, "end": 16924, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9615787267684937, 0.9541018009185791], "text": " the dataset", "score": 0.9578402638435364, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 17184, "end": 17189, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8362148404121399, 0.48576512932777405], "text": " Pile", "score": 0.660989984869957, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17195, "end": 17201, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9907712936401367, 0.9948227405548096, 0.9947513937950134, 0.994734525680542], "text": " GPT-2", "score": 0.9937699884176254, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 17203, "end": 17223, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9965115189552307, 0.9973762035369873, 0.9980486631393433, 0.9978407621383667, 0.9977729916572571, 0.9969696402549744], "text": "Radford et al., 2019", "score": 0.9974199632803599, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17228, "end": 17234, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9922035932540894, 0.995377779006958, 0.995649516582489, 0.9956672191619873], "text": " GPT-3", "score": 0.9947245270013809, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 17236, "end": 17254, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.99539715051651, 0.9973928928375244, 0.9974651336669922, 0.9973453879356384, 0.9965280890464783], "text": "Brown et al., 2020", "score": 0.9968257308006286, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17308, "end": 17314, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9896755814552307, 0.9953624606132507, 0.995418906211853, 0.9952325224876404], "text": " GPT-2", "score": 0.9939223676919937, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17340, "end": 17346, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9890865087509155, 0.99543696641922, 0.9958369731903076, 0.9955435395240784], "text": " GPT-3", "score": 0.9939759969711304, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17152, "end": 17177, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.9939606189727783, 0.997214138507843, 0.9975324869155884], "text": " the constituent datasets", "score": 0.9962357481320699, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17282, "end": 17314, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120available", "\u0120versions", "\u0120of", "\u0120G", "PT", "-", "2"], "seq_scores": [0.5587514638900757, 0.49088430404663086, 0.542116641998291, 0.5704777240753174, 0.714637041091919, 0.7831257581710815, 0.8339054584503174, 0.7580861449241638], "text": " all available versions of GPT-2", "score": 0.6564980670809746, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17319, "end": 17346, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120all", "\u0120four", "\u0120versions", "\u0120of", "\u0120G", "PT", "-", "3"], "seq_scores": [0.6578577756881714, 0.4556770622730255, 0.5663618445396423, 0.6153556108474731, 0.6632295846939087, 0.790210485458374, 0.8056945204734802, 0.8077630400657654], "text": " all four versions of GPT-3", "score": 0.6702687405049801, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17449, "end": 17487, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120one", "-", "t", "enth", "\u0120of", "\u0120the", "\u0120respective", "\u0120test", "\u0120sets"], "seq_scores": [0.9231946468353271, 0.9610313177108765, 0.975381076335907, 0.9808774590492249, 0.9453892707824707, 0.9610947370529175, 0.9964236617088318, 0.9943122863769531, 0.9968653321266174], "text": " one-tenth of the respective test sets", "score": 0.9705077542199029, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17491, "end": 17499, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120most", "\u0120of"], "seq_scores": [0.9012876749038696, 0.7702584862709045], "text": " most of", "score": 0.8357730805873871, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17499, "end": 17524, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.5129768252372742, 0.9966264963150024, 0.9982060194015503], "text": " the constituent datasets", "score": 0.835936446984609, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17682, "end": 17695, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9350672364234924, 0.9270532131195068], "text": " each dataset", "score": 0.9310602247714996, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17788, "end": 17802, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120corpor", "a"], "seq_scores": [0.9803506731987, 0.9929607510566711, 0.9806210398674011], "text": " large corpora", "score": 0.9846441547075907, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17892, "end": 17915, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120larger", "\u0120language", "\u0120models"], "seq_scores": [0.9936867356300354, 0.9964286684989929, 0.9967650175094604], "text": " larger language models", "score": 0.9956268072128296, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17961, "end": 17976, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120smaller", "\u0120models"], "seq_scores": [0.9920497536659241, 0.9940361976623535], "text": " smaller models", "score": 0.9930429756641388, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18021, "end": 18043, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120empirical", "\u0120scaling"], "seq_scores": [0.612417459487915, 0.6799622178077698, 0.656083345413208], "text": " the empirical scaling", "score": 0.6494876742362976, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18069, "end": 18088, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ka", "plan", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9981656670570374, 0.997141420841217, 0.9977279305458069, 0.9976996779441833, 0.9970500469207764, 0.996436595916748], "text": "Kaplan et al., 2020", "score": 0.9973702232042948, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18089, "end": 18111, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hen", "igh", "an", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9973613619804382, 0.997520387172699, 0.9980819225311279, 0.9978612065315247, 0.9978400468826294, 0.9974566102027893, 0.9974452257156372], "text": " Henighan et al., 2020", "score": 0.997652394430978, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18161, "end": 18167, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9722031354904175, 0.991740345954895, 0.9911297559738159, 0.9910270571708679], "text": " GPT-2", "score": 0.9865250736474991, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18171, "end": 18177, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9791479110717773, 0.9915852546691895, 0.9915257096290588, 0.9920555949211121], "text": " GPT-3", "score": 0.9885786175727844, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 18199, "end": 18207, "seq_label": ["B-Task"], "seq_token": ["\u0120perplex"], "seq_scores": [0.5620104074478149], "text": " perplex", "score": 0.5620104074478149, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18224, "end": 18233, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.7386277318000793, 0.7343143224716187, 0.8975605368614197], "text": " the Pile", "score": 0.7901675303777059, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18267, "end": 18273, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9727309346199036, 0.9890469908714294, 0.9893354773521423, 0.9905310273170471], "text": " GPT-3", "score": 0.9854111075401306, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18434, "end": 18440, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.988174319267273, 0.9942129254341125, 0.9943574070930481, 0.9944705963134766], "text": " GPT-2", "score": 0.9928038120269775, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18444, "end": 18450, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9899078607559204, 0.9948424696922302, 0.9952607750892639, 0.9950662851333618], "text": " GPT-3", "score": 0.9937693476676941, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18470, "end": 18479, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.7939223051071167, 0.8295415639877319, 0.9191670417785645], "text": " the Pile", "score": 0.8475436369578043, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18683, "end": 18701, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120scaling"], "seq_scores": [0.7242128849029541, 0.6365827918052673, 0.6850427389144897, 0.6795796751976013], "text": " zero-shot scaling", "score": 0.6813545227050781, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18051, "end": 18067, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9933822154998779, 0.994749128818512], "text": " language models", "score": 0.994065672159195, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18263, "end": 18290, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120families", "\u0120of", "\u0120models", "\u0120the", "\u0120G", "PT", "-", "3", "\u0120family", "\u0120of", "\u0120models"], "seq_scores": [0.7946604490280151, 0.9536867737770081, 0.9550420641899109, 0.8278799653053284, 0.8935660719871521, 0.9190670251846313, 0.9392470717430115, 0.9590229392051697, 0.9763150215148926, 0.990050196647644, 0.9924863576889038], "text": " the GPT-3 family of models", "score": 0.9273658123883334, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18632, "end": 18645, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120models"], "seq_scores": [0.980579674243927, 0.9860911965370178], "text": " these models", "score": 0.9833354353904724, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18802, "end": 18808, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9898722171783447, 0.9943071007728577, 0.9956076741218567, 0.9957464337348938], "text": " GPT-3", "score": 0.9938833564519882, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18858, "end": 18863, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.3457683324813843, 0.4184114336967468], "text": " Pile", "score": 0.38208988308906555, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18949, "end": 18955, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9876091480255127, 0.9939565658569336, 0.995148241519928, 0.995309054851532], "text": " GPT-3", "score": 0.9930057525634766, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19049, "end": 19055, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9613185524940491, 0.9853566288948059, 0.9886593818664551, 0.9887275695800781], "text": " GPT-3", "score": 0.981015533208847, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19184, "end": 19189, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.5989753603935242, 0.675923228263855], "text": " Pile", "score": 0.6374492943286896, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18852, "end": 18874, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120which", "\u0120P", "ile", "\u0120components"], "seq_scores": [0.6814859509468079, 0.5408218502998352, 0.916143536567688, 0.9069685339927673], "text": " which Pile components", "score": 0.7613549679517746, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18917, "end": 18922, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120text"], "seq_scores": [0.9047192931175232], "text": " text", "score": 0.9047192931175232, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18924, "end": 18943, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["web", "\u0120pages", "\u0120and", "\u0120books"], "seq_scores": [0.8489299416542053, 0.9881392121315002, 0.8333370089530945, 0.807887852191925], "text": "web pages and books", "score": 0.8695735037326813, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 18971, "end": 18988, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120These", "\u0120components"], "seq_scores": [0.9691039323806763, 0.9281195402145386], "text": " These components", "score": 0.9486117362976074, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19133, "end": 19142, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["\u0120training", "\u0120data", "\u0120datasets"], "seq_scores": [0.474211722612381, 0.9230654239654541, 0.535622775554657], "text": " datasets", "score": 0.6442999740441641, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19279, "end": 19285, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.99280846118927, 0.9954648613929749, 0.9961193799972534, 0.9961022138595581], "text": " GPT-3", "score": 0.9951237291097641, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19394, "end": 19400, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9904550909996033, 0.994205892086029, 0.9949827194213867, 0.9950857758522034], "text": " GPT-3", "score": 0.9936823695898056, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19504, "end": 19510, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.989087700843811, 0.9941856861114502, 0.995103120803833, 0.9951484799385071], "text": " GPT-3", "score": 0.9933812469244003, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19561, "end": 19567, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9901490211486816, 0.9945420622825623, 0.9954434633255005, 0.9954219460487366], "text": " GPT-2", "score": 0.9938891232013702, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19718, "end": 19729, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2", "-", "P", "ile"], "seq_scores": [0.9907656908035278, 0.9959927797317505, 0.9965318441390991, 0.9959762692451477, 0.9938244819641113, 0.9927933216094971, 0.9855107069015503], "text": " GPT-2-Pile", "score": 0.9930564420563834, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19738, "end": 19744, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9925098419189453, 0.9958004355430603, 0.9964815378189087, 0.9963299632072449], "text": " GPT-3", "score": 0.9952804446220398, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19819, "end": 19832, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9898993372917175, 0.9933925271034241, 0.9946320056915283, 0.9930559992790222], "text": " OpenWebText2", "score": 0.992744967341423, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19226, "end": 19245, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120different", "\u0120datasets"], "seq_scores": [0.8488882184028625, 0.9562947750091553], "text": " different datasets", "score": 0.9025914967060089, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19288, "end": 19314, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120different", "\u0120P", "ile", "\u0120components"], "seq_scores": [0.7298938632011414, 0.7332100868225098, 0.8569431304931641, 0.7366343140602112], "text": " different Pile components", "score": 0.7641703486442566, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19966, "end": 19976, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "2", "-", "P", "ile"], "seq_scores": [0.9911344051361084, 0.99573814868927, 0.9961647987365723, 0.9957108497619629, 0.9952777624130249, 0.9943625926971436], "text": " GPT2-Pile", "score": 0.9947314262390137, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19996, "end": 20001, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120O", "WT", "2"], "seq_scores": [0.9915660619735718, 0.9934999346733093, 0.9913433790206909], "text": " OWT2", "score": 0.9921364585558573, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20239, "end": 20245, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9921668767929077, 0.9951273202896118, 0.995747983455658, 0.9956488013267517], "text": " GPT-3", "score": 0.9946727454662323, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20250, "end": 20255, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120O", "WT", "2"], "seq_scores": [0.9927943348884583, 0.9935153722763062, 0.9914253950119019], "text": " OWT2", "score": 0.992578367392222, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20307, "end": 20313, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9921628832817078, 0.9952314496040344, 0.9958771467208862, 0.9961791038513184], "text": " GPT-3", "score": 0.9948626458644867, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20349, "end": 20354, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120O", "WT", "2"], "seq_scores": [0.9926464557647705, 0.99457848072052, 0.9936443567276001], "text": " OWT2", "score": 0.9936230977376302, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20397, "end": 20403, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9905508756637573, 0.9945738911628723, 0.9953042268753052, 0.9958180785179138], "text": " GPT-3", "score": 0.9940617680549622, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20438, "end": 20443, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8348826766014099, 0.8746466040611267], "text": " Pile", "score": 0.8547646403312683, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20005, "end": 20017, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9894253015518188, 0.9733788371086121], "text": " the dataset", "score": 0.9814020693302155, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20128, "end": 20145, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120two", "\u0120datasets"], "seq_scores": [0.8618860244750977, 0.8646397590637207, 0.9570143818855286], "text": " the two datasets", "score": 0.8945133884747823, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20201, "end": 20213, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9832725524902344, 0.9761992692947388], "text": " the dataset", "score": 0.9797359108924866, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20328, "end": 20333, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.6775579452514648], "text": " data", "score": 0.6775579452514648, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20575, "end": 20583, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3", "'s"], "seq_scores": [0.9759284853935242, 0.9880008101463318, 0.9895703792572021, 0.9896708130836487, 0.8390719890594482], "text": " GPT-3's", "score": 0.956448495388031, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20598, "end": 20604, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["Books", "3"], "seq_scores": [0.980963945388794, 0.9875841736793518], "text": "Books3", "score": 0.9842740595340729, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20605, "end": 20619, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Wikipedia", "\u0120(", "en"], "seq_scores": [0.7916388511657715, 0.6259083151817322, 0.5507447123527527], "text": " Wikipedia (en", "score": 0.6560972929000854, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20621, "end": 20629, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile", "-", "CC"], "seq_scores": [0.986307680606842, 0.9883872866630554, 0.9907947182655334, 0.9922423362731934], "text": " Pile-CC", "score": 0.9894330054521561, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20633, "end": 20651, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Project", "\u0120Gutenberg"], "seq_scores": [0.8368584513664246, 0.9294822812080383], "text": " Project Gutenberg", "score": 0.8831703662872314, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20514, "end": 20523, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9782689213752747], "text": " datasets", "score": 0.9782689213752747, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20575, "end": 20596, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120G", "PT", "-", "3", "'s", "\u0120training", "\u0120set"], "seq_scores": [0.7764273285865784, 0.8796335458755493, 0.8430831432342529, 0.8405448198318481, 0.7341521978378296, 0.8821941018104553, 0.9441658854484558], "text": " GPT-3's training set", "score": 0.8428858603749957, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20688, "end": 20693, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["G", "PT", "-", "3"], "seq_scores": [0.9775586724281311, 0.9896411299705505, 0.9909161925315857, 0.9916006326675415], "text": "GPT-3", "score": 0.9874291568994522, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20779, "end": 20794, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120PubMed", "\u0120Central"], "seq_scores": [0.7830755114555359, 0.6968446969985962], "text": " PubMed Central", "score": 0.739960104227066, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20795, "end": 20812, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120PubMed", "\u0120Abstract", "s"], "seq_scores": [0.6305567026138306, 0.6652608513832092, 0.7481258511543274], "text": " PubMed Abstracts", "score": 0.6813144683837891, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20817, "end": 20823, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Ar", "X", "iv"], "seq_scores": [0.814081072807312, 0.6970211863517761, 0.719219446182251], "text": " ArXiv", "score": 0.743440568447113, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20854, "end": 20862, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Free", "Law"], "seq_scores": [0.9337536096572876, 0.9303632378578186], "text": " FreeLaw", "score": 0.9320584237575531, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20863, "end": 20875, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Hack", "-", "er", "News"], "seq_scores": [0.8986027836799622, 0.8994398713111877, 0.9090973138809204, 0.9061644673347473], "text": " Hack-erNews", "score": 0.9033261090517044, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20880, "end": 20898, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120US", "P", "TO", "\u0120Background", "s"], "seq_scores": [0.9246241450309753, 0.9235418438911438, 0.9303922057151794, 0.9298065900802612, 0.9447241425514221], "text": " USPTO Backgrounds", "score": 0.9306177854537964, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20982, "end": 20989, "seq_label": ["B-Dataset"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.724781334400177], "text": " GitHub", "score": 0.724781334400177, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 20993, "end": 21008, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120DM", "\u0120Mathematics"], "seq_scores": [0.9289734363555908, 0.8973750472068787], "text": " DM Mathematics", "score": 0.9131742417812347, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20722, "end": 20731, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9850559234619141], "text": " datasets", "score": 0.9850559234619141, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20824, "end": 20849, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120domain", "-", "specific", "\u0120datasets"], "seq_scores": [0.9940841794013977, 0.9980410933494568, 0.9984946250915527, 0.9988942742347717], "text": " domain-specific datasets", "score": 0.9973785430192947, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20906, "end": 20915, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9799880981445312], "text": " datasets", "score": 0.9799880981445312, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20940, "end": 20945, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120text"], "seq_scores": [0.6063007712364197], "text": " text", "score": 0.6063007712364197, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21079, "end": 21092, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9941224455833435, 0.9953415393829346, 0.9964326620101929, 0.9955655932426453], "text": " OpenWebText2", "score": 0.995365560054779, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21114, "end": 21120, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.937360405921936, 0.9740109443664551, 0.9714621901512146, 0.9675902724266052], "text": " GPT-3", "score": 0.9626059532165527, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21143, "end": 21148, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9772396087646484, 0.9157407283782959], "text": " Pile", "score": 0.9464901685714722, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21183, "end": 21206, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120research", "\u0120related", "\u0120tasks"], "seq_scores": [0.8948540687561035, 0.893002450466156, 0.7421709895133972], "text": " research related tasks", "score": 0.8433425029118856, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21207, "end": 21222, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120software", "\u0120tasks"], "seq_scores": [0.8151490688323975, 0.918367862701416], "text": " software tasks", "score": 0.8667584657669067, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 21227, "end": 21247, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120symbol", "\u0120manipulation"], "seq_scores": [0.9359790086746216, 0.9542620778083801], "text": " symbol manipulation", "score": 0.9451205432415009, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21343, "end": 21348, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8795233368873596, 0.6326753497123718], "text": " Pile", "score": 0.7560993432998657, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21410, "end": 21416, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.8461166024208069, 0.9444701075553894, 0.9613410234451294, 0.9605233669281006], "text": " GPT-3", "score": 0.9281127750873566, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21022, "end": 21047, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120majority", "\u0120of", "\u0120datasets"], "seq_scores": [0.9622947573661804, 0.9815576672554016, 0.9925493001937866, 0.9975515007972717], "text": " the majority of datasets", "score": 0.9834883064031601, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21112, "end": 21132, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120G", "PT", "-", "3", "\u0120sized", "\u0120model"], "seq_scores": [0.9277955889701843, 0.8952338695526123, 0.9262800812721252, 0.9366801977157593, 0.9343874454498291, 0.9454481601715088, 0.9414600729942322], "text": " a GPT-3 sized model", "score": 0.9296122023037502, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21258, "end": 21273, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120base", "\u0120model"], "seq_scores": [0.9902064204216003, 0.9907214641571045, 0.9894880056381226], "text": " the base model", "score": 0.9901386300722758, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21327, "end": 21359, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120majority", "\u0120of", "\u0120P", "ile", "\u0120components"], "seq_scores": [0.9024606347084045, 0.9619892835617065, 0.962384819984436, 0.9291881918907166, 0.9763422012329102, 0.9770016074180603], "text": " the majority of Pile components", "score": 0.9515611231327057, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21382, "end": 21430, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120predominantly", "\u0120web", "-", "based", "\u0120G", "PT", "-", "3", "\u0120training", "\u0120data"], "seq_scores": [0.9327621459960938, 0.973248302936554, 0.9858459830284119, 0.9932750463485718, 0.9939226508140564, 0.9893366098403931, 0.9890907406806946, 0.9884963035583496, 0.9866287708282471, 0.9898980855941772, 0.9955417513847351], "text": " the predominantly web-based GPT-3 training data", "score": 0.9834587628191168, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21672, "end": 21678, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9932199120521545, 0.9959241151809692, 0.9967511892318726, 0.996874213218689], "text": " GPT-3", "score": 0.9956923574209213, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21691, "end": 21702, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2", "\u0120P", "ile"], "seq_scores": [0.9909994006156921, 0.9948871731758118, 0.9961588382720947, 0.9957770109176636, 0.9926538467407227, 0.993061363697052], "text": " GPT-2 Pile", "score": 0.9939229389031728, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21723, "end": 21729, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9921905398368835, 0.995882511138916, 0.9966202974319458, 0.9967116117477417], "text": " GPT-3", "score": 0.9953512400388718, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21650, "end": 21663, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120datasets"], "seq_scores": [0.9824597835540771, 0.9918000102043152], "text": " the datasets", "score": 0.9871298968791962, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 21753, "end": 21768, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9772593975067139, 0.9913569688796997], "text": " these datasets", "score": 0.9843081831932068, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21826, "end": 21836, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.6486443281173706, 0.5475764870643616], "text": " the model", "score": 0.5981104075908661, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21965, "end": 21974, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.5493580102920532, 0.8144505620002747, 0.8059542775154114], "text": " the Pile", "score": 0.7232542832692465, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21988, "end": 22006, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.39805150032043457, 0.563505232334137], "text": " language modeling", "score": 0.48077836632728577, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22097, "end": 22117, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", ")"], "seq_scores": [0.9902836680412292, 0.9989339709281921, 0.998870313167572, 0.9990432858467102, 0.9982576966285706, 0.9979690909385681, 0.9979066848754883], "text": " Brown et al. (2020)", "score": 0.9973235300609044, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22159, "end": 22168, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Wiki", "Text"], "seq_scores": [0.9905575513839722, 0.9948403239250183], "text": " WikiText", "score": 0.9926989376544952, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22172, "end": 22180, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "B", "ADA"], "seq_scores": [0.9930229187011719, 0.9958570599555969, 0.9957629442214966, 0.9935114979743958], "text": " LAMBADA", "score": 0.9945386052131653, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 22203, "end": 22221, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.4198457598686218, 0.34475046396255493], "text": " language modeling", "score": 0.3822981119155884, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22256, "end": 22265, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.6338117122650146, 0.7943869829177856, 0.832843005657196], "text": " the Pile", "score": 0.7536805669466654, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22024, "end": 22079, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120architect", "urally", "-", "ident", "ical", "\u01201", ".", "3", "\u0120billion", "\u0120parameter", "\u0120models"], "seq_scores": [0.9824351668357849, 0.9916043877601624, 0.9930605888366699, 0.9950640797615051, 0.9957375526428223, 0.9884415864944458, 0.9967034459114075, 0.9972447156906128, 0.9968745708465576, 0.9970309734344482, 0.997281551361084], "text": " architecturally-identical 1.3 billion parameter models", "score": 0.9937707835977728, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22120, "end": 22139, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120different", "\u0120datasets"], "seq_scores": [0.9311937689781189, 0.9760599732398987], "text": " different datasets", "score": 0.9536268711090088, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22465, "end": 22491, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u012013", "-", "gram", "\u0120overlap", "\u0120filtering"], "seq_scores": [0.8736937642097473, 0.9160847067832947, 0.9008124470710754, 0.8935451507568359, 0.830894947052002], "text": " 13-gram overlap filtering", "score": 0.8830062031745911, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22497, "end": 22517, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", ")"], "seq_scores": [0.9915414452552795, 0.9988021850585938, 0.9989582300186157, 0.9990116357803345, 0.9971285462379456, 0.9972559809684753, 0.9974683523178101], "text": " Brown et al. (2020)", "score": 0.9971666250910077, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22649, "end": 22656, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.7788633108139038, 0.8670368194580078, 0.9146111607551575], "text": " CC-100", "score": 0.8535037636756897, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22367, "end": 22376, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9758538007736206], "text": " datasets", "score": 0.9758538007736206, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22430, "end": 22450, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120evaluation", "\u0120sets"], "seq_scores": [0.9885181784629822, 0.9931215643882751, 0.9933701753616333], "text": " the evaluation sets", "score": 0.9916699727376302, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22587, "end": 22595, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120dataset"], "seq_scores": [0.5806248188018799], "text": " dataset", "score": 0.5806248188018799, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22753, "end": 22758, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9814192056655884, 0.9791409373283386], "text": " Pile", "score": 0.9802800714969635, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22763, "end": 22780, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120En", "-", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9928098917007446, 0.9958406090736389, 0.9982871413230896, 0.9989966750144958, 0.9989135265350342, 0.9988797307014465], "text": " En- et al., 2019", "score": 0.997287929058075, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22781, "end": 22802, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Con", "neau", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9924980998039246, 0.9964821338653564, 0.9984534978866577, 0.9988287091255188, 0.9988411068916321, 0.9987911581993103], "text": " Conneau et al., 2020", "score": 0.9973157842954, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22725, "end": 22748, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120following", "\u0120datasets"], "seq_scores": [0.5171157121658325, 0.612176239490509, 0.8674379587173462], "text": " the following datasets", "score": 0.6655766367912292, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22808, "end": 22820, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120sample", "\u0120of"], "seq_scores": [0.9307507276535034, 0.7457774877548218, 0.5689017176628113], "text": " a sample of", "score": 0.7484766443570455, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22820, "end": 22837, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120CC", "\u0120W", "ET", "\u0120files"], "seq_scores": [0.5661896467208862, 0.9932710528373718, 0.9961593151092529, 0.9956682920455933, 0.9922295212745667], "text": " raw CC WET files", "score": 0.9087035655975342, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 22892, "end": 22910, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.5033613443374634, 0.5844162702560425], "text": " language modeling", "score": 0.5438888072967529, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22957, "end": 22966, "seq_label": ["I-Method", "I-Method", "B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile", "\u0120Wiki", "Text"], "seq_scores": [0.36150291562080383, 0.4138687551021576, 0.9549895524978638, 0.9842962622642517], "text": " WikiText", "score": 0.6786643713712692, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 22998, "end": 23006, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120L", "AM", "B", "ADA"], "seq_scores": [0.9837737083435059, 0.9777471423149109, 0.9739320278167725, 0.9803614616394043], "text": " LAMBADA", "score": 0.9789535850286484, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23034, "end": 23039, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9570063948631287, 0.9367275834083557], "text": " Pile", "score": 0.9468669891357422, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23071, "end": 23078, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Raw", "\u0120CC"], "seq_scores": [0.4368768632411957, 0.6511945128440857], "text": " Raw CC", "score": 0.5440356880426407, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23082, "end": 23089, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.6915532946586609, 0.7426856756210327, 0.7592946290969849], "text": " CC-100", "score": 0.7311778664588928, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23167, "end": 23171, "seq_label": ["I-Method", "I-Method", "B-Method"], "seq_token": ["\u0120P", "ile", "\u0120the"], "seq_scores": [0.5342676639556885, 0.4233502447605133, 0.5464417338371277], "text": " the", "score": 0.5013532141844431, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23171, "end": 23198, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120perplex", "ity", "\u0120based", "\u0120filtering"], "seq_scores": [0.566035807132721, 0.9116815328598022, 0.9380918741226196, 0.9211920499801636], "text": " perplexity based filtering", "score": 0.8342503160238266, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23206, "end": 23213, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.7212996482849121, 0.7483078837394714, 0.7967879176139832], "text": " CC-100", "score": 0.7554651498794556, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 23251, "end": 23261, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wikipedia"], "seq_scores": [0.8492047786712646], "text": " Wikipedia", "score": 0.8492047786712646, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 23398, "end": 23408, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wikipedia"], "seq_scores": [0.8566311001777649], "text": " Wikipedia", "score": 0.8566311001777649, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 23512, "end": 23525, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8337697386741638, 0.959118664264679, 0.9677888751029968], "text": " Common Crawl", "score": 0.9202257593472799, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22880, "end": 22921, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120traditional", "\u0120language", "\u0120modeling", "\u0120benchmarks"], "seq_scores": [0.9215694665908813, 0.96645587682724, 0.963925838470459, 0.9458239078521729], "text": " traditional language modeling benchmarks", "score": 0.9494437724351883, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23016, "end": 23023, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.972036600112915], "text": " models", "score": 0.972036600112915, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23092, "end": 23107, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120components"], "seq_scores": [0.5708695650100708, 0.7887191772460938], "text": " all components", "score": 0.6797943711280823, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23110, "end": 23119, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.520352303981781, 0.8221757411956787, 0.8185559511184692], "text": " the Pile", "score": 0.720361332098643, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23220, "end": 23237, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120language", "\u0120model"], "seq_scores": [0.9926591515541077, 0.9937866926193237, 0.9967156648635864], "text": " a language model", "score": 0.994387169679006, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23265, "end": 23274, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120data"], "seq_scores": [0.7840443253517151, 0.8715313673019409], "text": " all data", "score": 0.827787846326828, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23448, "end": 23467, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120data", "\u0120the", "\u0120collected", "\u0120data"], "seq_scores": [0.6022640466690063, 0.9856516122817993, 0.9924684166908264, 0.9905783534049988], "text": " the collected data", "score": 0.8927406072616577, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23667, "end": 23679, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9798591136932373, 0.9858946800231934], "text": " the dataset", "score": 0.9828768968582153, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23858, "end": 23876, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile", "\u0120datasets"], "seq_scores": [0.7598040699958801, 0.8354711532592773, 0.9123865365982056, 0.9391396045684814], "text": " the Pile datasets", "score": 0.8617003411054611, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 24070, "end": 24076, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9259359240531921, 0.9599208235740662, 0.9700791239738464, 0.9660113453865051], "text": " GPT-2", "score": 0.9554868042469025, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23917, "end": 23929, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["Each", "\u0120dataset"], "seq_scores": [0.9887006878852844, 0.9904873967170715], "text": "Each dataset", "score": 0.989594042301178, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 23941, "end": 23969, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120number", "\u0120of", "\u0120documents"], "seq_scores": [0.9789556264877319, 0.9904071688652039, 0.9916930794715881, 0.985870897769928, 0.9826237559318542], "text": " a large number of documents", "score": 0.9859101057052613, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24134, "end": 24160, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120majority", "\u0120of", "\u0120documents"], "seq_scores": [0.8322798013687134, 0.90766841173172, 0.8955680727958679, 0.9079908728599548], "text": " the majority of documents", "score": 0.885876789689064, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24207, "end": 24227, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120very", "\u0120long", "\u0120documents"], "seq_scores": [0.8538311123847961, 0.9913695454597473, 0.9881258010864258], "text": " very long documents", "score": 0.9444421529769897, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24381, "end": 24390, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.8813332319259644, 0.9003537893295288, 0.9758468866348267], "text": " the Pile", "score": 0.9191779692967733, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 24441, "end": 24461, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", ")"], "seq_scores": [0.9880000948905945, 0.9985312223434448, 0.9987780451774597, 0.9987699389457703, 0.996276319026947, 0.9961732029914856, 0.9973593354225159], "text": " Brown et al. (2020)", "score": 0.9962697369711739, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 24787, "end": 24796, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.8174537420272827, 0.8575122952461243, 0.9723262786865234], "text": " the Pile", "score": 0.8824307719866434, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24421, "end": 24433, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9906222224235535, 0.9891011118888855], "text": " the dataset", "score": 0.9898616671562195, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24571, "end": 24584, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120own", "\u0120data"], "seq_scores": [0.8881438374519348, 0.817467451095581, 0.920521080493927], "text": " our own data", "score": 0.8753774563471476, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24601, "end": 24624, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120mult", "ilingual", "\u0120dataset"], "seq_scores": [0.982029378414154, 0.9890819787979126, 0.9952926635742188, 0.9956039190292358], "text": " a multilingual dataset", "score": 0.9905019849538803, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24694, "end": 24706, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9791616201400757, 0.981582760810852], "text": " the dataset", "score": 0.9803721904754639, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24818, "end": 24827, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120fast", "text"], "seq_scores": [0.945287823677063, 0.9780063629150391], "text": " fasttext", "score": 0.961647093296051, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 24829, "end": 24849, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Su", "\u00c3\u00a1", "rez", "\u0120et", "\u0120al", ".,", "\u01202019", "a"], "seq_scores": [0.9980952143669128, 0.9982203841209412, 0.9985380172729492, 0.9988086223602295, 0.9987521171569824, 0.998494029045105, 0.9984960556030273, 0.998079776763916], "text": "Su\u00e1rez et al., 2019a", "score": 0.9984355270862579, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 24928, "end": 24952, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120identification"], "seq_scores": [0.8765239715576172, 0.9442975521087646], "text": " language identification", "score": 0.9104107618331909, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 24986, "end": 25009, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Cas", "well", "\u0120et", "\u0120al", ".", "\u0120(", "2020", "),"], "seq_scores": [0.9951584935188293, 0.9985536932945251, 0.9992133378982544, 0.9992425441741943, 0.999268114566803, 0.9989277720451355, 0.9988895058631897, 0.9976757168769836], "text": " Caswell et al. (2020),", "score": 0.9983661472797394, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 24971, "end": 24986, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120rare", "\u0120languages"], "seq_scores": [0.9514404535293579, 0.9526709914207458], "text": " rare languages", "score": 0.9520557224750519, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25109, "end": 25132, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120low", "-", "resource", "\u0120languages"], "seq_scores": [0.9682639241218567, 0.9757481217384338, 0.9816495776176453, 0.9812828302383423], "text": " low-resource languages", "score": 0.9767361134290695, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25208, "end": 25216, "seq_label": ["B-Method"], "seq_token": ["\u0120machine"], "seq_scores": [0.5225810408592224], "text": " machine", "score": 0.5225810408592224, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25327, "end": 25351, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Pr", "ab", "hu", "\u0120and", "\u0120Bir", "h", "ane", ",", "\u01202020"], "seq_scores": [0.9979694485664368, 0.9976567029953003, 0.9980348944664001, 0.9978839755058289, 0.9977757334709167, 0.9981207251548767, 0.9978494644165039, 0.9977942705154419, 0.9971120357513428], "text": "Prabhu and Birhane, 2020", "score": 0.9977996945381165, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25352, "end": 25380, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ider", "man", "\u0120and", "\u0120Sche", "ire", "r", ",", "\u01202020"], "seq_scores": [0.997368574142456, 0.9970049262046814, 0.9977021813392639, 0.997875452041626, 0.9976911544799805, 0.9979615211486816, 0.9978566765785217, 0.9979078769683838, 0.9974991679191589], "text": " Biderman and Scheirer, 2020", "score": 0.9976519478691949, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25451, "end": 25472, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "ovy", "\u0120and", "\u0120Sp", "ruit", ",", "\u01202016"], "seq_scores": [0.9981465339660645, 0.9964390397071838, 0.9974225759506226, 0.9974632263183594, 0.9974207878112793, 0.9970690608024597, 0.9961460828781128], "text": "Hovy and Spruit, 2016", "score": 0.9971581867762974, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25473, "end": 25497, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hutchinson", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9963041543960571, 0.996894121170044, 0.9974473714828491, 0.9973019361495972, 0.9969070553779602], "text": " Hutchinson et al., 2020", "score": 0.9969709277153015, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25498, "end": 25520, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bl", "od", "get", "t", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9974433183670044, 0.9956608414649963, 0.9968039989471436, 0.9971753358840942, 0.9975115060806274, 0.997668445110321, 0.997480571269989, 0.9974163770675659], "text": " Blodgett et al., 2020", "score": 0.9971450492739677, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25574, "end": 25592, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.5226079225540161, 0.8045159578323364], "text": " language modeling", "score": 0.6635619401931763, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25686, "end": 25704, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "eb", "ru", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.998028576374054, 0.9967362284660339, 0.9972757697105408, 0.9976020455360413, 0.9975278973579407, 0.9971086382865906, 0.9965881109237671], "text": "Gebru et al., 2018", "score": 0.9972667523792812, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25705, "end": 25731, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bender", "\u0120and", "\u0120Friedman", ",", "\u01202018"], "seq_scores": [0.9959927201271057, 0.9970518350601196, 0.9970194697380066, 0.9972646236419678, 0.9966200590133667], "text": " Bender and Friedman, 2018", "score": 0.9967897415161133, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25732, "end": 25751, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Jo", "\u0120and", "\u0120G", "eb", "ru", ",", "\u01202020"], "seq_scores": [0.9969125986099243, 0.9969567060470581, 0.997223973274231, 0.9975956082344055, 0.9974141120910645, 0.9975390434265137, 0.997342050075531], "text": " Jo and Gebru, 2020", "score": 0.9972834416798183, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25273, "end": 25298, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120ever", "\u0120larger", "\u0120datasets"], "seq_scores": [0.9829235672950745, 0.9810155630111694, 0.9939783811569214, 0.9972133040428162], "text": " the ever larger datasets", "score": 0.9887827038764954, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 25303, "end": 25310, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9157658219337463], "text": " models", "score": 0.9157658219337463, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25675, "end": 25684, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9700872302055359], "text": " datasets", "score": 0.9700872302055359, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 25782, "end": 25806, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120massive", "\u0120language", "\u0120models"], "seq_scores": [0.9920624494552612, 0.9961481094360352, 0.9956231713294983], "text": " massive language models", "score": 0.9946112434069315, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26027, "end": 26067, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Natural", "\u0120language", "\u0120processing", "\u0120technologies"], "seq_scores": [0.8816965222358704, 0.8896172642707825, 0.9073607325553894, 0.7072985768318176], "text": "Natural language processing technologies", "score": 0.846493273973465, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 26157, "end": 26174, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120appropriate", "\u0120data"], "seq_scores": [0.5038654804229736, 0.7227820158004761], "text": " appropriate data", "score": 0.6133237481117249, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 26344, "end": 26353, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9207203984260559], "text": " datasets", "score": 0.9207203984260559, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26412, "end": 26444, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120general", "-", "purpose", "\u0120language", "\u0120models"], "seq_scores": [0.9905297160148621, 0.9971008896827698, 0.9972259402275085, 0.9969310760498047, 0.9971977472305298], "text": " general-purpose language models", "score": 0.995797073841095, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26743, "end": 26761, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["G", "eb", "ru", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9971000552177429, 0.9958279728889465, 0.9968646168708801, 0.9970054030418396, 0.9966822266578674, 0.9958046078681946, 0.9952090382575989], "text": "Gebru et al., 2018", "score": 0.9963562744004386, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26844, "end": 26863, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aji", "\u0120and", "\u0120Yang", ",", "\u01202019"], "seq_scores": [0.9976498484611511, 0.9967641830444336, 0.9973414540290833, 0.9975672960281372, 0.9970999956130981, 0.9966291785240173], "text": "Raji and Yang, 2019", "score": 0.9971753259499868, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26864, "end": 26892, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ider", "man", "\u0120and", "\u0120Sche", "ire", "r", ",", "\u01202020"], "seq_scores": [0.9969491362571716, 0.9957754015922546, 0.9968985319137573, 0.9973457455635071, 0.9971170425415039, 0.997349739074707, 0.9973161816596985, 0.9972303509712219, 0.997008740901947], "text": " Biderman and Scheirer, 2020", "score": 0.9969989856084188, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26970, "end": 26987, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Se", "ck", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9973399043083191, 0.9957913160324097, 0.9968817234039307, 0.9969985485076904, 0.9967353940010071, 0.9961414933204651], "text": "Seck et al., 2018", "score": 0.9966480632623037, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26988, "end": 27013, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Costa", "-", "j", "uss", "\u00c3\u0142", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9943378567695618, 0.9931341409683228, 0.9940122961997986, 0.9941446185112, 0.9938700199127197, 0.995752215385437, 0.9966505169868469, 0.9964904189109802, 0.9960259199142456], "text": " Costa-juss\u00e0 et al., 2020", "score": 0.9949353337287903, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27014, "end": 27034, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Th", "iem", "e", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9966287016868591, 0.9956459403038025, 0.9960566759109497, 0.9963441491127014, 0.9968134760856628, 0.9968737363815308, 0.9967668056488037], "text": " Thieme et al., 2020", "score": 0.99644706930433, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27082, "end": 27107, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "ender", "\u0120and", "\u0120Friedman", ",", "\u01202018"], "seq_scores": [0.9976219534873962, 0.9958503246307373, 0.997385561466217, 0.9967803955078125, 0.9961931705474854, 0.9955530762672424], "text": "Bender and Friedman, 2018", "score": 0.9965640803178152, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27389, "end": 27410, "seq_label": ["I-Datasource", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["X", "B", "ider", "man", "\u0120et", "\u0120al", ".,", "\u01202021"], "seq_scores": [0.43217721581459045, 0.9978227615356445, 0.9966341853141785, 0.9976022839546204, 0.9978390336036682, 0.9979077577590942, 0.9974141120910645, 0.99720698595047], "text": "Biderman et al., 2021", "score": 0.9268255420029163, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27411, "end": 27426, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ider", "man", ",", "\u01202021"], "seq_scores": [0.997524082660675, 0.9975293278694153, 0.998137354850769, 0.9982910752296448, 0.9982327222824097], "text": " Biderman, 2021", "score": 0.9979429125785828, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27443, "end": 27457, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120datas", "heet"], "seq_scores": [0.759186327457428, 0.8682174682617188, 0.6223873496055603], "text": " the datasheet", "score": 0.7499303817749023, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27548, "end": 27564, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9818633198738098, 0.9958627223968506], "text": " language models", "score": 0.9888630211353302, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27569, "end": 27585, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120documents"], "seq_scores": [0.7599820494651794, 0.8404130339622498], "text": " these documents", "score": 0.8001975417137146, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 27820, "end": 27822, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.5676921606063843], "text": " P", "score": 0.5676921606063843, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27841, "end": 27865, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120topic", "\u0120modeling", "\u0120analysis"], "seq_scores": [0.4989841878414154, 0.9451114535331726, 0.8158116936683655], "text": " topic modeling analysis", "score": 0.7533024450143179, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27890, "end": 27897, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120G", "ens", "im"], "seq_scores": [0.9403706192970276, 0.9720476865768433, 0.965448796749115], "text": " Gensim", "score": 0.959289034207662, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27899, "end": 27919, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Re", "h", "ure", "k", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.995503842830658, 0.9960506558418274, 0.9968151450157166, 0.9967966675758362, 0.9968482851982117, 0.9963789582252502, 0.993455708026886, 0.9912499189376831], "text": "Rehurek et al., 2011", "score": 0.9953873977065086, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27971, "end": 27988, "seq_label": ["I-Method", "I-Method", "I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["let", "\u0120All", "ocation", "Ble", "i", "\u0120et", "\u0120al", ".,", "\u01202003"], "seq_scores": [0.4174771010875702, 0.42320749163627625, 0.4314257502555847, 0.9870768189430237, 0.9792914986610413, 0.9852650761604309, 0.9845471978187561, 0.9559587836265564, 0.9226263165473938], "text": "Blei et al., 2003", "score": 0.7874306705262926, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28043, "end": 28045, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.5292577147483826], "text": " P", "score": 0.5292577147483826, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28085, "end": 28105, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "off", "man", "\u0120et", "\u0120al", ".,", "\u01202010"], "seq_scores": [0.9969819188117981, 0.9951165914535522, 0.996863842010498, 0.997366726398468, 0.9970012307167053, 0.9958286881446838, 0.9950130581855774], "text": "Hoffman et al., 2010", "score": 0.996310293674469, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28123, "end": 28125, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.5773016810417175], "text": " P", "score": 0.5773016810417175, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 28233, "end": 28240, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["P", "ile", "-", "CC"], "seq_scores": [0.6936848759651184, 0.4444911777973175, 0.5326604843139648, 0.5297427177429199], "text": "Pile-CC", "score": 0.5501448139548302, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28375, "end": 28377, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.5867969393730164], "text": " P", "score": 0.5867969393730164, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28419, "end": 28432, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.601614773273468, 0.637404203414917, 0.7452731132507324], "text": " Common Crawl", "score": 0.6614306966463724, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27868, "end": 27872, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120its"], "seq_scores": [0.5571715831756592], "text": " its", "score": 0.5571715831756592, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27932, "end": 27996, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u012016", "-", "topic", "\u0120Lat", "ent", "\u0120Dir", "ich", "let", "\u0120All", "ocation", "\u0120(", "Ble", "i", "\u0120et", "\u0120al", ".,", "\u01202003", ")", "\u0120models"], "seq_scores": [0.9057754278182983, 0.9913263916969299, 0.994666576385498, 0.9943304061889648, 0.9959630370140076, 0.9937963485717773, 0.9921457171440125, 0.9913007616996765, 0.9910297393798828, 0.9912307262420654, 0.9688401818275452, 0.9714762568473816, 0.9859376549720764, 0.973587155342102, 0.9814056754112244, 0.9593618512153625, 0.9526714086532593, 0.8853784799575806, 0.9862042665481567], "text": " 16-topic Latent Dirichlet Allocation (Blei et al., 2003) models", "score": 0.9740225296271475, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27999, "end": 28014, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120component"], "seq_scores": [0.7015973925590515, 0.6304377317428589], "text": " each component", "score": 0.6660175621509552, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28017, "end": 28039, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120validation", "\u0120set", "\u0120of"], "seq_scores": [0.915787398815155, 0.9933121800422668, 0.9872928857803345, 0.6438384056091309], "text": " the validation set of", "score": 0.8850577175617218, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28039, "end": 28048, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.7100586295127869, 0.8159739375114441, 0.6000446677207947], "text": " the Pile", "score": 0.7086924115816752, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28119, "end": 28123, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.5913021564483643], "text": " the", "score": 0.5913021564483643, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 28206, "end": 28253, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120Common", "\u0120C", "rawl", "-", "derived", "\u0120(", "P", "ile", "-", "CC", ")", "\u0120topic", "\u0120model"], "seq_scores": [0.9532855749130249, 0.9578415751457214, 0.9577034711837769, 0.9443637132644653, 0.9310716390609741, 0.9190129041671753, 0.920045018196106, 0.9275583028793335, 0.8878925442695618, 0.9123117327690125, 0.9346062541007996, 0.9254140257835388, 0.9761438965797424, 0.9794349670410156], "text": " the Common Crawl-derived (Pile-CC) topic model", "score": 0.9376204013824463, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28256, "end": 28277, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120document", "\u0120sets", "\u0120of"], "seq_scores": [0.9824371933937073, 0.9949021339416504, 0.9930562376976013, 0.5161994099617004], "text": " the document sets of", "score": 0.8716487437486649, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28277, "end": 28298, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120other", "\u0120components"], "seq_scores": [0.8387535810470581, 0.9683070778846741, 0.9624539613723755], "text": " the other components", "score": 0.9231715401013693, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28419, "end": 28432, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.6534523963928223, 0.5557929873466492, 0.6013766527175903], "text": " Common Crawl", "score": 0.6035406788190206, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 28546, "end": 28554, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120P", "ile", "-", "CC"], "seq_scores": [0.825481653213501, 0.5698984861373901, 0.7379927039146423, 0.7337619066238403], "text": " Pile-CC", "score": 0.7167836874723434, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28596, "end": 28609, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9781122803688049, 0.9828090071678162, 0.9866037368774414, 0.9848418235778809], "text": " OpenWebText2", "score": 0.9830917119979858, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 28773, "end": 28778, "seq_label": ["B-Datasource"], "seq_token": ["\u0120open"], "seq_scores": [0.36674201488494873], "text": " open", "score": 0.36674201488494873, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 28790, "end": 28803, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.8589359521865845, 0.8924572467803955, 0.9304035902023315], "text": " Common Crawl", "score": 0.8939322630564371, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 28879, "end": 28887, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120P", "ile", "-", "CC"], "seq_scores": [0.7636513113975525, 0.7100568413734436, 0.8115925192832947, 0.8129070401191711], "text": " Pile-CC", "score": 0.7745519280433655, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29048, "end": 29055, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Github"], "seq_scores": [0.775685727596283], "text": " Github", "score": 0.775685727596283, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29056, "end": 29067, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.9369651675224304, 0.9570557475090027, 0.9648933410644531], "text": " PhilPapers", "score": 0.9529714186986288, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29072, "end": 29081, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Euro", "P", "arl"], "seq_scores": [0.9658253788948059, 0.9694221019744873, 0.9740307331085205], "text": " EuroParl", "score": 0.9697594046592712, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 28542, "end": 28566, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile", "-", "CC", "\u0120topic", "\u0120model"], "seq_scores": [0.7730469703674316, 0.7360159754753113, 0.6955342888832092, 0.7199355959892273, 0.8010534644126892, 0.9562543034553528, 0.9671759605407715], "text": " the Pile-CC topic model", "score": 0.8070023655891418, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28579, "end": 28593, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120documents"], "seq_scores": [0.9368293285369873, 0.9269745349884033], "text": " the documents", "score": 0.9319019317626953, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28751, "end": 28766, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["fil", "tered", "\u0120craw", "ls"], "seq_scores": [0.9697393178939819, 0.9729462265968323, 0.971927285194397, 0.9488646984100342], "text": "filtered crawls", "score": 0.9658693820238113, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 28786, "end": 28803, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.82135009765625, 0.889447033405304, 0.8655878305435181, 0.8592854738235474], "text": " the Common Crawl", "score": 0.8589176088571548, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 29136, "end": 29151, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120L", "DA", "\u0120models"], "seq_scores": [0.9486531615257263, 0.9277719855308533, 0.9807058572769165, 0.983881950378418], "text": " our LDA models", "score": 0.9602532386779785, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29223, "end": 29255, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120larger", "\u0120CC", "-", "derived", "\u0120component"], "seq_scores": [0.8271130919456482, 0.877650797367096, 0.8893618583679199, 0.8702074885368347, 0.8639825582504272, 0.7844430208206177], "text": " the larger CC-derived component", "score": 0.8521264692147573, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29430, "end": 29447, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120other", "\u0120components"], "seq_scores": [0.4954344630241394, 0.5583887100219727], "text": " other components", "score": 0.526911586523056, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29469, "end": 29484, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["ile", "\u0120the", "\u0120data", "\u0120modes"], "seq_scores": [0.5187970399856567, 0.6343428492546082, 0.6515526175498962, 0.557206928730011], "text": " the data modes", "score": 0.590474858880043, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 29850, "end": 29868, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120prof", "anity", "-", "check", "er"], "seq_scores": [0.7735561728477478, 0.8890495300292969, 0.8643186092376709, 0.8656957149505615, 0.7916465401649475], "text": " profanity-checker", "score": 0.836853313446045, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 29885, "end": 29895, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Z", "hou", ",", "\u01202019"], "seq_scores": [0.9968163371086121, 0.9971804618835449, 0.997296154499054, 0.9969589710235596], "text": "Zhou, 2019", "score": 0.9970629811286926, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 29989, "end": 30013, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Wik", "id", "et", "ox", "\u0120Toxic", "\u0120Comment"], "seq_scores": [0.7377272844314575, 0.8451411128044128, 0.7411764860153198, 0.6854569911956787, 0.7192422747612, 0.6686862111091614], "text": " Wikidetox Toxic Comment", "score": 0.7329050600528717, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 30023, "end": 30043, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["W", "ul", "cz", "yn", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9973625540733337, 0.9963777661323547, 0.9979099631309509, 0.9981092214584351, 0.9979250431060791, 0.9981719255447388, 0.9979614019393921, 0.9971517324447632], "text": "Wulczyn et al., 2016", "score": 0.997621200978756, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 29919, "end": 29937, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120\"", "t", "oxicity", "\u0120model"], "seq_scores": [0.9821385145187378, 0.9773887395858765, 0.9790316820144653, 0.9871299862861633, 0.9829607605934143], "text": " a \"toxicity model", "score": 0.9817299365997314, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 29949, "end": 29974, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120multiple", "\u0120prof", "anity", "\u0120lists"], "seq_scores": [0.8723253607749939, 0.9321292042732239, 0.9782310128211975, 0.9693109393119812], "text": " multiple profanity lists", "score": 0.9379991292953491, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30126, "end": 30148, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120English", "\u0120sentences"], "seq_scores": [0.9534482955932617, 0.9703205227851868, 0.9809451103210449], "text": " the English sentences", "score": 0.9682379762331644, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30151, "end": 30164, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9887086749076843, 0.9968110918998718], "text": " each dataset", "score": 0.9927598834037781, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 30170, "end": 30199, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120same", "\u0120language", "\u0120class", "ifier"], "seq_scores": [0.9891480803489685, 0.995457649230957, 0.9935036301612854, 0.9977763295173645, 0.996523916721344], "text": " the same language classifier", "score": 0.9944819211959839, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 30927, "end": 30940, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9439729452133179, 0.977810263633728], "text": " each dataset", "score": 0.960891604423523, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31030, "end": 31039, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.6760434508323669], "text": " datasets", "score": 0.6760434508323669, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 31223, "end": 31239, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.989332914352417, 0.9936069250106812], "text": " language models", "score": 0.9914699196815491, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31274, "end": 31292, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120data"], "seq_scores": [0.9922967553138733, 0.995699405670166, 0.9957882761955261], "text": " the training data", "score": 0.9945948123931885, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 31388, "end": 31395, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric"], "seq_token": ["\u0120P", "\u0120models"], "seq_scores": [0.5370664000511169, 0.9483678936958313], "text": " models", "score": 0.7427171468734741, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31444, "end": 31453, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120P", "ile"], "seq_scores": [0.801060676574707, 0.8427242636680603, 0.7902663946151733], "text": " the Pile", "score": 0.8113504449526469, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31489, "end": 31498, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9893201589584351, 0.9790305495262146], "text": " the data", "score": 0.9841753542423248, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 31506, "end": 31523, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120specific", "\u0120model"], "seq_scores": [0.9183930158615112, 0.9419052600860596, 0.9819283485412598], "text": " a specific model", "score": 0.9474088748296102, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31784, "end": 31796, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120surrounding"], "seq_scores": [0.530159056186676], "text": " surrounding", "score": 0.530159056186676, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 31890, "end": 31903, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9569945931434631, 0.9721712470054626], "text": " this dataset", "score": 0.9645829200744629, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 32399, "end": 32419, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["L", "oper", "\u0120and", "\u0120Bird", ",", "\u01202002"], "seq_scores": [0.9927724003791809, 0.9919038414955139, 0.9962263107299805, 0.9929133057594299, 0.9959973096847534, 0.9954662322998047], "text": "Loper and Bird, 2002", "score": 0.9942132333914439, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 32650, "end": 32674, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "acc", "ian", "ella", "\u0120et", "\u0120al", ".,", "\u01202010"], "seq_scores": [0.9870203733444214, 0.988286554813385, 0.9905431866645813, 0.9918500185012817, 0.9939275979995728, 0.9952263832092285, 0.9947494864463806, 0.9928902387619019], "text": "Baccianella et al., 2010", "score": 0.9918117299675941, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 32730, "end": 32743, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9881411790847778, 0.9911754131317139], "text": " each dataset", "score": 0.9896582961082458, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 32864, "end": 32876, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9951646327972412, 0.9962527751922607], "text": " the dataset", "score": 0.995708703994751, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 33636, "end": 33661, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.9338285326957703, 0.9951213002204895, 0.9962853193283081], "text": " the constituent datasets", "score": 0.9750783840815226, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 33680, "end": 33699, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120entire", "\u0120dataset"], "seq_scores": [0.9890508651733398, 0.9956191182136536, 0.9963147044181824], "text": " the entire dataset", "score": 0.9936615626017252, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 34906, "end": 34920, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120news", "\u0120articles"], "seq_scores": [0.7530247569084167, 0.5885175466537476], "text": " news articles", "score": 0.6707711517810822, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 35302, "end": 35312, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["O", "bar", ",", "\u01202020"], "seq_scores": [0.9969151020050049, 0.9986408352851868, 0.9986899495124817, 0.9986593723297119], "text": "Obar, 2020", "score": 0.9982263147830963, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 35313, "end": 35338, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Pr", "ab", "hu", "\u0120and", "\u0120Bir", "h", "ane", ",", "\u01202020"], "seq_scores": [0.9973336458206177, 0.9986265897750854, 0.9988822340965271, 0.9988526105880737, 0.9987210631370544, 0.9989957213401794, 0.9988465309143066, 0.9989171028137207, 0.9985224604606628], "text": " Prabhu and Birhane, 2020", "score": 0.9986331065495809, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 35489, "end": 35507, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["V", "it", "ak", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9979439377784729, 0.9983183145523071, 0.9985488057136536, 0.9985960125923157, 0.9986763596534729, 0.9984550476074219, 0.998096764087677], "text": "Vitak et al., 2016", "score": 0.9983764631407601, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 35508, "end": 35529, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120F", "ies", "ler", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9975036978721619, 0.9977626800537109, 0.9980145692825317, 0.998422384262085, 0.9985356330871582, 0.9983487129211426, 0.9982234835624695], "text": " Fiesler et al., 2020", "score": 0.9981158801487514, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 35750, "end": 35764, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Hal", "ava", "is", ",", "\u01202019"], "seq_scores": [0.9967261552810669, 0.9987523555755615, 0.998703122138977, 0.9988085031509399, 0.9987241625785828], "text": "Halavais, 2019", "score": 0.9983428597450257, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 35026, "end": 35032, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120texts"], "seq_scores": [0.6963971257209778], "text": " texts", "score": 0.6963971257209778, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 35668, "end": 35680, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120public", "\u0120data"], "seq_scores": [0.8342849016189575, 0.9120398759841919], "text": " public data", "score": 0.8731623888015747, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 36280, "end": 36292, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Public", "\u0120data"], "seq_scores": [0.9002468585968018, 0.7662102580070496], "text": " Public data", "score": 0.8332285583019257, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 36295, "end": 36300, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.7353813052177429], "text": " data", "score": 0.7353813052177429, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 36459, "end": 36464, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.5919744372367859], "text": " data", "score": 0.5919744372367859, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 36947, "end": 36966, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120To", "S", "\u0120compliant", "\u0120data"], "seq_scores": [0.6616630554199219, 0.7830243706703186, 0.6030429601669312, 0.6925819516181946], "text": " ToS compliant data", "score": 0.6850780844688416, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 36970, "end": 36995, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120author", "ial", "\u0120cons", "ented", "\u0120data"], "seq_scores": [0.8393023610115051, 0.8958630561828613, 0.8651276230812073, 0.8435522317886353, 0.8192751407623291], "text": " authorial consented data", "score": 0.8526240825653076, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 37501, "end": 37507, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "3"], "seq_scores": [0.9882599115371704, 0.9921425580978394], "text": " Book3", "score": 0.9902012348175049, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 37511, "end": 37525, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Sub", "t", "itles"], "seq_scores": [0.9877650141716003, 0.9932318925857544, 0.9899184107780457, 0.9882888793945312], "text": " OpenSubtitles", "score": 0.9898010492324829, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 37749, "end": 37762, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120En", "ron", "\u0120Emails"], "seq_scores": [0.9907546639442444, 0.9922322034835815, 0.9782462120056152], "text": " Enron Emails", "score": 0.9870776931444804, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 37943, "end": 37949, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120En", "ron"], "seq_scores": [0.9886162877082825, 0.990790843963623], "text": " Enron", "score": 0.9897035658359528, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 37345, "end": 37358, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120datasets"], "seq_scores": [0.9285620450973511, 0.9401919841766357], "text": " the datasets", "score": 0.9343770146369934, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 37375, "end": 37379, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120Dat"], "seq_scores": [0.5438455939292908], "text": " Dat", "score": 0.5438455939292908, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 37701, "end": 37710, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120couple", "\u0120datasets", "\u0120the", "\u0120data"], "seq_scores": [0.5380270481109619, 0.7522465586662292, 0.5028573870658875, 0.5070818662643433], "text": " the data", "score": 0.5750532150268555, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 38360, "end": 38378, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120YouTube", "\u0120Sub", "t", "itles"], "seq_scores": [0.9663341045379639, 0.9911608695983887, 0.9910058975219727, 0.9895957112312317], "text": " YouTube Subtitles", "score": 0.9845241457223892, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 38490, "end": 38508, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120YouTube", "\u0120Sub", "t", "itles"], "seq_scores": [0.962853193283081, 0.9914253950119019, 0.9911244511604309, 0.9897924661636353], "text": " YouTube Subtitles", "score": 0.9837988764047623, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 38643, "end": 38648, "seq_label": ["B-Method"], "seq_token": ["\u0120Cond"], "seq_scores": [0.4788016676902771], "text": " Cond", "score": 0.4788016676902771, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 38654, "end": 38661, "seq_label": ["B-Datasource"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.52225661277771], "text": " GitHub", "score": 0.52225661277771, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 38063, "end": 38077, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120five", "\u0120datasets"], "seq_scores": [0.9575343728065491, 0.9787347316741943], "text": " five datasets", "score": 0.9681345522403717, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 38251, "end": 38266, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.8258408308029175, 0.9123512506484985], "text": " these datasets", "score": 0.869096040725708, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 38395, "end": 38410, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.8317832946777344, 0.9188454747200012], "text": " these datasets", "score": 0.8753143846988678, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 38799, "end": 38814, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.7987061738967896, 0.913419783115387], "text": " these datasets", "score": 0.8560629785060883, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 38936, "end": 38951, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.756679356098175, 0.9226589202880859], "text": " these datasets", "score": 0.8396691381931305, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 39246, "end": 39267, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "ru", "nd", "age", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9977793097496033, 0.9961119294166565, 0.9975044131278992, 0.9973318576812744, 0.9979000091552734, 0.9979155659675598, 0.9976383447647095, 0.9969283938407898], "text": "Brundage et al., 2018", "score": 0.9973887279629707, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 39268, "end": 39288, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Am", "ode", "i", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9968361854553223, 0.9963014125823975, 0.9968981742858887, 0.9971626400947571, 0.9975444674491882, 0.9971785545349121, 0.9964770674705505], "text": " Amodei et al., 2016", "score": 0.9969140716961452, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 39289, "end": 39317, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ost", "rom", "\u0120and", "\u0120Y", "ud", "k", "owsky", ",", "\u01202014"], "seq_scores": [0.9972358345985413, 0.9941951632499695, 0.9954628348350525, 0.996837854385376, 0.9968642592430115, 0.997020423412323, 0.9969519376754761, 0.9966111779212952, 0.9970507621765137, 0.9961904287338257], "text": " Bostrom and Yudkowsky, 2014", "score": 0.9964420676231385, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 39318, "end": 39332, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ost", "rom", ",", "\u01202014"], "seq_scores": [0.9963024854660034, 0.9955198764801025, 0.9962239265441895, 0.9971432089805603, 0.9967255592346191], "text": " Bostrom, 2014", "score": 0.996383011341095, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 39333, "end": 39358, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Crit", "ch", "\u0120and", "\u0120Kr", "ue", "ger", ",", "\u01202020"], "seq_scores": [0.9967412352561951, 0.996260941028595, 0.9972615242004395, 0.9969674944877625, 0.9971306920051575, 0.9967660903930664, 0.9974555373191833, 0.9971652626991272], "text": " Critch and Krueger, 2020", "score": 0.9969685971736908, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 39516, "end": 39529, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120AI", "\u0120alignment"], "seq_scores": [0.799920380115509, 0.7145403027534485], "text": " AI alignment", "score": 0.7572303414344788, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 39061, "end": 39068, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.8761064410209656], "text": " models", "score": 0.8761064410209656, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 39072, "end": 39081, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.5798839330673218], "text": " datasets", "score": 0.5798839330673218, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 39179, "end": 39213, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120progressively", "\u0120stronger", "\u0120AI", "\u0120systems"], "seq_scores": [0.7436767816543579, 0.7782262563705444, 0.8924506902694702, 0.9244972467422485], "text": " progressively stronger AI systems", "score": 0.8347127437591553, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 39597, "end": 39623, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120un", "aligned", "\u0120language", "\u0120models"], "seq_scores": [0.9503520131111145, 0.9568483233451843, 0.9750053882598877, 0.9951186180114746], "text": " unaligned language models", "score": 0.9693310856819153, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 40551, "end": 40560, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["2013", "2015", "ful", ",", "\u01202003"], "seq_scores": [0.6739410758018494, 0.8757946491241455, 0.9780730605125427, 0.9969527721405029, 0.9962196946144104], "text": "ful, 2003", "score": 0.9041962504386902, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 40696, "end": 40712, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["D", "ai", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9980776309967041, 0.9980812072753906, 0.9983945488929749, 0.9984433054924011, 0.9981967806816101, 0.9979830980300903], "text": "Dai et al., 2019", "score": 0.9981960952281952, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 40713, "end": 40730, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Rae", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9973203539848328, 0.9979470372200012, 0.998445451259613, 0.9982901215553284, 0.9980568289756775], "text": " Rae et al., 2019", "score": 0.9980119585990905, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 40731, "end": 40753, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hen", "igh", "an", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9977095127105713, 0.9978506565093994, 0.9980277419090271, 0.9980320334434509, 0.998244047164917, 0.9980598092079163, 0.9976744055747986], "text": " Henighan et al., 2020", "score": 0.99794260093144, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 40754, "end": 40771, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Liu", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9971045851707458, 0.9981690645217896, 0.9985419511795044, 0.9984048008918762, 0.9982643723487854], "text": " Liu et al., 2018", "score": 0.9980969548225402, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 39745, "end": 39752, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.883350133895874], "text": " models", "score": 0.883350133895874, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 39755, "end": 39770, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120copyright", "\u0120data"], "seq_scores": [0.9345559477806091, 0.9411459565162659], "text": " copyright data", "score": 0.9378509521484375, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 39998, "end": 40013, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120copyright", "\u0120data"], "seq_scores": [0.8305953741073608, 0.8250905871391296], "text": " copyright data", "score": 0.8278429806232452, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 40304, "end": 40313, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.8373426198959351, 0.7904226779937744], "text": " the data", "score": 0.8138826489448547, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 40361, "end": 40370, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.8909717798233032, 0.8875592350959778], "text": " the data", "score": 0.8892655074596405, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 41825, "end": 41843, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gr", "ace", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.998024582862854, 0.9972901344299316, 0.9979938268661499, 0.9977943897247314, 0.9975870847702026, 0.9971600770950317], "text": "Grace et al., 2018", "score": 0.9976416826248169, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 41844, "end": 41860, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Y", "ud", "k", "owsky", ",", "\u01202013"], "seq_scores": [0.9970564842224121, 0.9973741769790649, 0.9980560541152954, 0.9978609681129456, 0.9978402853012085, 0.9975747466087341], "text": " Yudkowsky, 2013", "score": 0.9976271192232767, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 41994, "end": 42021, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "ost", "rom", "\u0120and", "\u0120Y", "ud", "k", "owsky", ",", "\u01202014"], "seq_scores": [0.9975534081459045, 0.9949164390563965, 0.9959579110145569, 0.9968996047973633, 0.9968252182006836, 0.9967617392539978, 0.9965883493423462, 0.9963589310646057, 0.996552586555481, 0.9959157109260559], "text": "Bostrom and Yudkowsky, 2014", "score": 0.9964329898357391, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42022, "end": 42036, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Russell", ",", "\u01202019"], "seq_scores": [0.9957167506217957, 0.9978583455085754, 0.9978086352348328], "text": " Russell, 2019", "score": 0.997127910455068, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42037, "end": 42051, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120B", "ost", "rom", ",", "\u01202014"], "seq_scores": [0.9963799118995667, 0.996890127658844, 0.9971668124198914, 0.9977395534515381, 0.9973422884941101], "text": " Bostrom, 2014", "score": 0.9971037387847901, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42052, "end": 42072, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Am", "ode", "i", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9972774386405945, 0.9975106716156006, 0.9978612065315247, 0.9978151321411133, 0.9980034232139587, 0.9979479908943176, 0.9976757168769836], "text": " Amodei et al., 2016", "score": 0.9977273685591561, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42125, "end": 42147, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["S", "otal", "a", "\u0120and", "\u0120Gl", "oor", ",", "\u01202017"], "seq_scores": [0.9978417158126831, 0.9974844455718994, 0.9981071949005127, 0.9980641007423401, 0.9980006814002991, 0.99820876121521, 0.9980790615081787, 0.9977043271064758], "text": "Sotala and Gloor, 2017", "score": 0.9979362860321999, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42148, "end": 42174, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sh", "ul", "man", "\u0120and", "\u0120B", "ost", "rom", ",", "\u01202020"], "seq_scores": [0.9970833659172058, 0.9976837635040283, 0.9981204867362976, 0.9981049299240112, 0.9981988072395325, 0.9981344938278198, 0.9979629516601562, 0.9982454776763916, 0.9980547428131104], "text": " Shulman and Bostrom, 2020", "score": 0.997954335477617, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42345, "end": 42358, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "ost", "rom", ",", "\u01202014"], "seq_scores": [0.9977520108222961, 0.9969332218170166, 0.997747004032135, 0.9981022477149963, 0.9981082677841187], "text": "Bostrom, 2014", "score": 0.9977285504341126, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 41731, "end": 41742, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120AI", "\u0120systems"], "seq_scores": [0.9172734618186951, 0.9050998091697693], "text": " AI systems", "score": 0.9111866354942322, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 41951, "end": 41971, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120powerful", "\u0120AI", "\u0120systems"], "seq_scores": [0.41504549980163574, 0.925690770149231, 0.9304947257041931], "text": " powerful AI systems", "score": 0.7570769985516866, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 42241, "end": 42266, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120such", "\u0120powerful", "\u0120AI", "\u0120systems"], "seq_scores": [0.6841854453086853, 0.7399555444717407, 0.9374523758888245, 0.9461573958396912], "text": " such powerful AI systems", "score": 0.8269376903772354, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42556, "end": 42569, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Russell", ",", "\u01202019"], "seq_scores": [0.9868489503860474, 0.9973645806312561, 0.9978390336036682], "text": "Russell, 2019", "score": 0.9940175215403239, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 42572, "end": 42596, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Crit", "ch", "\u0120and", "\u0120Kr", "ue", "ger", ",", "\u01202020"], "seq_scores": [0.9888255000114441, 0.9971693158149719, 0.9974178075790405, 0.997171938419342, 0.9971259236335754, 0.9969768524169922, 0.9972164630889893, 0.9969817996025085], "text": "Critch and Krueger, 2020", "score": 0.996110700070858, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 42692, "end": 42704, "seq_label": ["B-Method"], "seq_token": ["\u0120development"], "seq_scores": [0.8096045851707458], "text": " development", "score": 0.8096045851707458, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 42757, "end": 42786, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120High", "\u0120powered", "\u0120language", "\u0120models"], "seq_scores": [0.990120530128479, 0.9971702694892883, 0.9978098273277283, 0.9975358247756958], "text": " High powered language models", "score": 0.9956591129302979, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 42798, "end": 42828, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120their", "\u0120more", "\u0120general", "\u0120successors"], "seq_scores": [0.8303884267807007, 0.9450895190238953, 0.9534762501716614, 0.9621983766555786], "text": " their more general successors", "score": 0.922788143157959, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 43171, "end": 43180, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120data"], "seq_scores": [0.9421076774597168, 0.9400098919868469], "text": " our data", "score": 0.9410587847232819, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 43395, "end": 43413, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9975372552871704, 0.9977285265922546, 0.9978600144386292, 0.9975196719169617, 0.9970259070396423], "text": "Brown et al., 2020", "score": 0.9975342750549316, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 43414, "end": 43436, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bru", "nd", "age", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9972785115242004, 0.9975444674491882, 0.9977527260780334, 0.9980231523513794, 0.998011589050293, 0.9977265000343323, 0.9977633953094482], "text": " Brundage et al., 2018", "score": 0.9977286202566964, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 43546, "end": 43573, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Search", "\u0120Engine", "\u0120Optim", "ization"], "seq_scores": [0.8457494974136353, 0.872624933719635, 0.8684337735176086, 0.8643016815185547], "text": " Search Engine Optimization", "score": 0.8627774715423584, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 43898, "end": 43925, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["F", "erguson", "\u0120and", "\u0120Schne", "ier", ",", "\u01202003"], "seq_scores": [0.9967561364173889, 0.9892686009407043, 0.9951640367507935, 0.9936032891273499, 0.9935460686683655, 0.9929499626159668, 0.9907709360122681], "text": "Ferguson and Schneier, 2003", "score": 0.9931512900761196, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 43355, "end": 43380, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120powerful", "\u0120language", "\u0120models"], "seq_scores": [0.9879192113876343, 0.9883896112442017, 0.9933609962463379], "text": " powerful language models", "score": 0.989889939626058, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 43507, "end": 43527, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120low", "\u0120quality", "\u0120content"], "seq_scores": [0.5455893278121948, 0.6738418936729431, 0.5221319794654846], "text": " low quality content", "score": 0.5805210669835409, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 43680, "end": 43696, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120content", "\u0120language", "\u0120models"], "seq_scores": [0.5892524123191833, 0.9889411330223083, 0.9891467094421387], "text": " language models", "score": 0.8557800849278768, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 44122, "end": 44137, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Christian", ",", "\u01202020"], "seq_scores": [0.994522213935852, 0.9977918863296509, 0.9982348680496216], "text": "Christian, 2020", "score": 0.9968496561050415, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 44820, "end": 44842, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120human", "-", "guided", "\u0120learning"], "seq_scores": [0.9842154383659363, 0.9963745474815369, 0.9971281886100769, 0.9959017634391785], "text": " human-guided learning", "score": 0.9934049844741821, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 44844, "end": 44865, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["St", "i", "enn", "on", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.997302770614624, 0.997996985912323, 0.9984118938446045, 0.998524010181427, 0.9983477592468262, 0.9986086487770081, 0.9982064962387085, 0.9980277419090271], "text": "Stiennon et al., 2020", "score": 0.9981782883405685, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 43960, "end": 43972, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120such", "\u0120models"], "seq_scores": [0.9727581143379211, 0.9834359288215637], "text": " such models", "score": 0.9780970215797424, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 43975, "end": 43989, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120huge", "\u0120datasets"], "seq_scores": [0.9945656061172485, 0.9975913763046265], "text": " huge datasets", "score": 0.9960784912109375, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 44056, "end": 44076, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120their", "\u0120training", "\u0120sets"], "seq_scores": [0.6229608058929443, 0.6686297059059143, 0.981166660785675], "text": " their training sets", "score": 0.7575857241948446, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 44146, "end": 44153, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9543375372886658], "text": " models", "score": 0.9543375372886658, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 44650, "end": 44657, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric"], "seq_token": ["\u0120set", "\u0120models"], "seq_scores": [0.7085496187210083, 0.9709822535514832], "text": " models", "score": 0.8397659361362457, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 44664, "end": 44679, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120\"", "d", "irt", "ier", "\"", "\u0120data"], "seq_scores": [0.9762750267982483, 0.973599910736084, 0.9954894185066223, 0.9962540864944458, 0.995807409286499, 0.9934157133102417], "text": " \"dirtier\" data", "score": 0.9884735941886902, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 45043, "end": 45056, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120these", "\u0120models"], "seq_scores": [0.9601682424545288, 0.9601527452468872], "text": " these models", "score": 0.960160493850708, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 45123, "end": 45147, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Self", "-", "super", "vised", "\u0120training"], "seq_scores": [0.9824585318565369, 0.9912416934967041, 0.9937509894371033, 0.9935116171836853, 0.9845304489135742], "text": "Self-supervised training", "score": 0.9890986561775208, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 45297, "end": 45303, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Gl", "o", "Ve"], "seq_scores": [0.9475914239883423, 0.9462328553199768, 0.9555191993713379], "text": " GloVe", "score": 0.9497811595598856, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45305, "end": 45328, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Pen", "nington", "\u0120et", "\u0120al", ".,", "\u01202014"], "seq_scores": [0.9971902966499329, 0.9964332580566406, 0.9977987408638, 0.9976662397384644, 0.9969817996025085, 0.9960677623748779], "text": "Pennington et al., 2014", "score": 0.9970230162143707, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 45333, "end": 45342, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120word", "2", "vec"], "seq_scores": [0.9013556838035583, 0.8218135237693787, 0.8613122701644897], "text": " word2vec", "score": 0.8614938259124756, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45344, "end": 45364, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["M", "ik", "ol", "ov", "\u0120et", "\u0120al", ".,", "\u01202013"], "seq_scores": [0.9973133206367493, 0.9975886344909668, 0.9981303811073303, 0.9980716109275818, 0.9981841444969177, 0.9980271458625793, 0.9974806904792786, 0.997047483921051], "text": "Mikolov et al., 2013", "score": 0.9977304264903069, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 45398, "end": 45408, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Wikipedia"], "seq_scores": [0.8350887894630432], "text": " Wikipedia", "score": 0.8350887894630432, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45409, "end": 45418, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Gig", "aw", "ord"], "seq_scores": [0.9345648288726807, 0.9733564853668213, 0.9809533953666687], "text": " Gigaword", "score": 0.9629582365353903, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45420, "end": 45438, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Gra", "ff", "\u0120et", "\u0120al", ".,", "\u01202003"], "seq_scores": [0.9975359439849854, 0.9979457259178162, 0.9983785152435303, 0.9982460737228394, 0.9980589747428894, 0.9978164434432983], "text": "Graff et al., 2003", "score": 0.9979969461758932, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 45456, "end": 45468, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Google", "\u0120News"], "seq_scores": [0.7378332018852234, 0.5221984386444092], "text": " Google News", "score": 0.6300158202648163, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45509, "end": 45529, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9973282814025879, 0.997759222984314, 0.9985270500183105, 0.9983727931976318, 0.9981639981269836, 0.9979332685470581], "text": "Radford et al., 2018", "score": 0.998014102379481, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45531, "end": 45553, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u0120,", "\u01202019"], "seq_scores": [0.9896706938743591, 0.9966577291488647, 0.9976325035095215, 0.9978234767913818, 0.9975320100784302, 0.9934917092323303, 0.9971900582313538], "text": "Radford et al., , 2019", "score": 0.9957140258380345, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45555, "end": 45574, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9954126477241516, 0.9979713559150696, 0.998218834400177, 0.9981770515441895, 0.9980849027633667], "text": " Brown et al., 2020", "score": 0.9975729584693909, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45575, "end": 45588, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ross", "et", ",", "\u01202019"], "seq_scores": [0.9952415227890015, 0.9982241988182068, 0.9984384179115295, 0.9983300566673279], "text": " Rosset, 2019", "score": 0.9975585490465164, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45589, "end": 45610, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sh", "oe", "y", "bi", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9970105886459351, 0.9981960654258728, 0.9984089732170105, 0.9985803365707397, 0.9985042810440063, 0.9985514283180237, 0.9984452128410339, 0.998141884803772], "text": " Shoeybi et al., 2019", "score": 0.9982298463582993, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45640, "end": 45659, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Dev", "lin", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.997860848903656, 0.9978786706924438, 0.9981970191001892, 0.9981564879417419, 0.9978682994842529, 0.9976242184638977], "text": "Devlin et al., 2019", "score": 0.9979309240976969, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45660, "end": 45677, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Liu", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9969358444213867, 0.9979442954063416, 0.9982944130897522, 0.998191773891449, 0.9980564117431641], "text": " Liu et al., 2019", "score": 0.9978845477104187, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 45678, "end": 45698, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120R", "aff", "el", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9975047707557678, 0.9977237582206726, 0.9983829259872437, 0.9983521699905396, 0.9983339905738831, 0.9982593655586243, 0.99825519323349], "text": " Raffel et al., 2019", "score": 0.9981160249028888, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45707, "end": 45710, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120C", "4"], "seq_scores": [0.9919725656509399, 0.9943055510520935], "text": " C4", "score": 0.9931390583515167, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45737, "end": 45742, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9730456471443176, 0.9697023630142212], "text": " Pile", "score": 0.9713740050792694, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45749, "end": 45753, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120m", "C", "4"], "seq_scores": [0.9908155202865601, 0.9941818118095398, 0.9949374198913574], "text": " mC4", "score": 0.9933115839958191, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45757, "end": 45764, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.9915466904640198, 0.9942806959152222, 0.9950124621391296], "text": " CC-100", "score": 0.9936132828394572, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45808, "end": 45811, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120C", "4"], "seq_scores": [0.9925041198730469, 0.9933390021324158], "text": " C4", "score": 0.9929215610027313, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 45812, "end": 45815, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["m", "C", "4"], "seq_scores": [0.9897558093070984, 0.994335949420929, 0.9941658973693848], "text": "mC4", "score": 0.9927525520324707, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 46014, "end": 46021, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CC", "-", "100"], "seq_scores": [0.992960512638092, 0.9943395256996155, 0.9951019287109375], "text": " CC-100", "score": 0.994133989016215, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 46117, "end": 46122, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9603942632675171, 0.9635005593299866], "text": " Pile", "score": 0.9619474112987518, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 46187, "end": 46200, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.7168028950691223, 0.7897537350654602, 0.8480528593063354], "text": " Common Crawl", "score": 0.7848698298136393, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 46376, "end": 46381, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.870715320110321, 0.9069293737411499], "text": " Pile", "score": 0.8888223469257355, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 46667, "end": 46675, "seq_label": ["I-Method", "B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120analysis", "\u0120P", "ile", "-", "CC"], "seq_scores": [0.5169679522514343, 0.9908571839332581, 0.9922211766242981, 0.9908599257469177, 0.9942989349365234], "text": " Pile-CC", "score": 0.8970410346984863, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 46676, "end": 46691, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PubMed", "\u0120Central"], "seq_scores": [0.6177499890327454, 0.4940250813961029], "text": " PubMed Central", "score": 0.5558875352144241, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 46692, "end": 46698, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Ar", "X", "iv"], "seq_scores": [0.5042950510978699, 0.5478408336639404, 0.5099812746047974], "text": " ArXiv", "score": 0.5207057197888693, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 46703, "end": 46714, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Ubuntu", "\u0120IRC"], "seq_scores": [0.57025146484375, 0.5307174324989319], "text": " Ubuntu IRC", "score": 0.5504844486713409, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47007, "end": 47025, "seq_label": ["I-ReferenceLink", "I-ReferenceLink", "B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["ider", "man", "\u0120YouTube", "\u0120Sub", "t", "itles"], "seq_scores": [0.7906739115715027, 0.6599855422973633, 0.7457209825515747, 0.9711709022521973, 0.9861499667167664, 0.9852686524391174], "text": " YouTube Subtitles", "score": 0.8564949929714203, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 47026, "end": 47041, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Exchange"], "seq_scores": [0.8236727118492126, 0.7101671695709229], "text": " Stack Exchange", "score": 0.7669199407100677, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 47046, "end": 47053, "seq_label": ["B-Datasource"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.8283319473266602], "text": " GitHub", "score": 0.8283319473266602, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 47083, "end": 47097, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120ded", "u", "pl", "ication"], "seq_scores": [0.8874151706695557, 0.9306727647781372, 0.9335541129112244, 0.9318248629570007], "text": " deduplication", "score": 0.9208667278289795, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 47112, "end": 47128, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120n", "-", "gram", "\u0120analysis"], "seq_scores": [0.6339296698570251, 0.8168736100196838, 0.8227626085281372, 0.8151345252990723], "text": " n-gram analysis", "score": 0.7721751034259796, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47143, "end": 47156, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.980801522731781, 0.9890915155410767, 0.9923266768455505, 0.9904257655143738], "text": " OpenWebText2", "score": 0.9881613701581955, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47180, "end": 47188, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Free", "Law"], "seq_scores": [0.9676796793937683, 0.9850279092788696], "text": " FreeLaw", "score": 0.976353794336319, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47189, "end": 47206, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Pub", "med", "\u0120Abstract", "s"], "seq_scores": [0.966293215751648, 0.9846404790878296, 0.9860987663269043, 0.9889100790023804], "text": " Pubmed Abstracts", "score": 0.9814856350421906, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47207, "end": 47216, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Ex", "P", "orter"], "seq_scores": [0.9791103005409241, 0.9859125018119812, 0.9886835217475891], "text": " ExPorter", "score": 0.9845687747001648, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47221, "end": 47232, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.9792150259017944, 0.9888523817062378, 0.9903344511985779], "text": " PhilPapers", "score": 0.9861339529355367, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 47262, "end": 47277, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120topic", "\u0120modeling"], "seq_scores": [0.5902571678161621, 0.7671157121658325], "text": " topic modeling", "score": 0.6786864399909973, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 47353, "end": 47371, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120US", "P", "TO", "\u0120Background", "s"], "seq_scores": [0.9774576425552368, 0.9878218770027161, 0.9898654818534851, 0.9807964563369751, 0.9858570098876953], "text": " USPTO Backgrounds", "score": 0.9843596935272216, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 47414, "end": 47418, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT"], "seq_scores": [0.9837319850921631, 0.9809250235557556], "text": " GPT", "score": 0.9823285043239594, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 45150, "end": 45185, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120natural", "\u0120language", "\u0120processing", "\u0120models"], "seq_scores": [0.9905960559844971, 0.9955339431762695, 0.9967459440231323, 0.9945060610771179], "text": " natural language processing models", "score": 0.9943455010652542, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45188, "end": 45218, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", ",", "\u0120unl", "abel", "ed", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.9918625354766846, 0.9950582981109619, 0.9964526891708374, 0.9983927607536316, 0.9983142614364624, 0.998222291469574, 0.99822598695755, 0.9940910935401917], "text": " large, unlabeled text corpora", "score": 0.9963274896144867, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 45262, "end": 45289, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Word", "\u0120representation", "\u0120models"], "seq_scores": [0.992251455783844, 0.9968563318252563, 0.9967861175537109], "text": " Word representation models", "score": 0.9952979683876038, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45381, "end": 45390, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.91881263256073], "text": " datasets", "score": 0.91881263256073, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45443, "end": 45475, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120non", "-", "public", "\u0120Google", "\u0120News", "\u0120corpus"], "seq_scores": [0.9475162029266357, 0.9450395703315735, 0.9586899280548096, 0.973457932472229, 0.9686053991317749, 0.9703577756881714, 0.9579541087150574], "text": " a non-public Google News corpus", "score": 0.9602315596171788, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 45491, "end": 45507, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9896921515464783, 0.9921181797981262], "text": " language models", "score": 0.9909051656723022, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 45615, "end": 45638, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120masked", "\u0120language", "\u0120models"], "seq_scores": [0.9929444193840027, 0.996475875377655, 0.9958449006080627], "text": " masked language models", "score": 0.9950883984565735, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45768, "end": 45798, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120larger", ",", "\u0120mult", "ilingual", "\u0120datasets"], "seq_scores": [0.9783052802085876, 0.9943336248397827, 0.9909296631813049, 0.9981487989425659, 0.998293936252594], "text": " larger, multilingual datasets", "score": 0.992002260684967, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45869, "end": 45878, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9300256371498108, 0.8969226479530334], "text": " the data", "score": 0.9134741425514221, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 45998, "end": 46013, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.8910057544708252, 0.9392086863517761], "text": " these datasets", "score": 0.9151072204113007, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 46136, "end": 46157, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120three", "\u0120datasets"], "seq_scores": [0.9441795945167542, 0.8047590255737305, 0.9949042797088623], "text": " these three datasets", "score": 0.9146142999331156, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 46258, "end": 46285, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120larg", "esc", "ale", "\u0120language", "\u0120models"], "seq_scores": [0.9720406532287598, 0.9973234534263611, 0.9970742464065552, 0.9972983002662659, 0.997808039188385], "text": " largescale language models", "score": 0.9923089385032654, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 46304, "end": 46322, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120web", "\u0120scrap", "es"], "seq_scores": [0.937301754951477, 0.9726237058639526, 0.9757894277572632, 0.9696472883224487], "text": " large web scrapes", "score": 0.9638405442237854, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 46326, "end": 46365, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120more", "\u0120targeted", ",", "\u0120higher", "-", "quality", "\u0120datasets"], "seq_scores": [0.9846704006195068, 0.9944648146629333, 0.9952221512794495, 0.9921367168426514, 0.9971858859062195, 0.9976750016212463, 0.9983242154121399], "text": " more targeted, higher-quality datasets", "score": 0.9942398837634495, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 46637, "end": 46652, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120final", "\u0120data"], "seq_scores": [0.981999933719635, 0.9868794083595276, 0.9599369764328003], "text": " the final data", "score": 0.9762721061706543, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 47875, "end": 47892, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120datasets"], "seq_scores": [0.9729645252227783, 0.9928225874900818], "text": " several datasets", "score": 0.98289355635643, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 47905, "end": 47922, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120datasets"], "seq_scores": [0.976791262626648, 0.9936503767967224], "text": " several datasets", "score": 0.9852208197116852, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48002, "end": 48024, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120English", "\u0120component"], "seq_scores": [0.7927377820014954, 0.9228277206420898, 0.8394842147827148], "text": " the English component", "score": 0.8516832391421, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48027, "end": 48036, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9265044331550598, 0.9934555888175964], "text": " the data", "score": 0.9599800109863281, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48107, "end": 48125, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120several", "\u0120data", "\u0120sets"], "seq_scores": [0.9733328819274902, 0.9857310652732849, 0.991442859172821], "text": " several data sets", "score": 0.9835022687911987, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 48188, "end": 48212, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120US", "\u0120Congressional", "\u0120Record"], "seq_scores": [0.42963534593582153, 0.39543667435646057, 0.394588828086853], "text": " US Congressional Record", "score": 0.40655361612637836, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 48612, "end": 48646, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120general", "-", "purpose", "\u0120language", "\u0120modeling"], "seq_scores": [0.9336954951286316, 0.9598290920257568, 0.9610540270805359, 0.966049313545227, 0.9630298018455505], "text": " general-purpose language modeling", "score": 0.9567315459251404, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48507, "end": 48524, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120quantity"], "seq_scores": [0.6403853893280029, 0.738313615322113, 0.7280540466308594], "text": " a large quantity", "score": 0.7022510170936584, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48589, "end": 48599, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9851964712142944, 0.9925096035003662], "text": " a dataset", "score": 0.9888530373573303, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 48650, "end": 48661, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Fan", "fiction"], "seq_scores": [0.813014030456543, 0.7342639565467834], "text": " Fanfiction", "score": 0.7736389935016632, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 48681, "end": 48685, "seq_label": ["B-Dataset"], "seq_token": ["\u0120fan"], "seq_scores": [0.3873455226421356], "text": " fan", "score": 0.3873455226421356, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 48751, "end": 48770, "seq_label": ["B-Datasource", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL"], "seq_token": ["\u0120www", ".", "fan", "fiction", ".", "net"], "seq_scores": [0.35483092069625854, 0.7036235928535461, 0.6283504962921143, 0.7922830581665039, 0.829017698764801, 0.807496964931488], "text": " www.fanfiction.net", "score": 0.685933788617452, "type": "ScholarlyEntity"}, {"label": "URL", "begin": 48774, "end": 48807, "seq_label": ["B-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL", "I-URL"], "seq_token": ["\u0120www", ".", "https", "://", "archive", "of", "ou", "rown", ".", "\u0120org"], "seq_scores": [0.33421576023101807, 0.8266498446464539, 0.8016518950462341, 0.9347575902938843, 0.942987322807312, 0.9613272547721863, 0.9645656943321228, 0.9685505032539368, 0.9696914553642273, 0.9673863053321838], "text": " www.https://archiveofourown. org", "score": 0.8671783626079559, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 48861, "end": 48879, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.7902255058288574, 0.8146252036094666], "text": " language modeling", "score": 0.802425354719162, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49036, "end": 49047, "seq_label": ["I-Method", "B-Dataset", "I-Dataset"], "seq_token": ["\u0120modeling", "\u0120fan", "fiction"], "seq_scores": [0.49831387400627136, 0.7213944792747498, 0.5931484699249268], "text": " fanfiction", "score": 0.6042856077353159, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 48970, "end": 49002, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["fiction", "\u0120most", "\u0120language", "\u0120modeling", "\u0120datasets"], "seq_scores": [0.6927462220191956, 0.9691916704177856, 0.9595000743865967, 0.9975345134735107, 0.9982261061668396], "text": " most language modeling datasets", "score": 0.9234397172927856, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49132, "end": 49143, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.955245852470398, 0.9637270569801331, 0.9696798324584961], "text": " Literotica", "score": 0.962884247303009, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49144, "end": 49155, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.8947128653526306, 0.9138026833534241, 0.9359123110771179], "text": " Literotica", "score": 0.9148092865943909, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49430, "end": 49441, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.9474761486053467, 0.9623154997825623, 0.9680323004722595], "text": " Literotica", "score": 0.9592746496200562, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49570, "end": 49581, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.967965304851532, 0.979442298412323, 0.9822925925254822], "text": " Literotica", "score": 0.976566731929779, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49696, "end": 49707, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.9468268752098083, 0.9629355072975159, 0.9693818688392639], "text": " Literotica", "score": 0.9597147504488627, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 49787, "end": 49798, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Liter", "ot", "ica"], "seq_scores": [0.945777416229248, 0.9660657644271851, 0.9737269282341003], "text": " Literotica", "score": 0.9618567029635111, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 49191, "end": 49217, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120short", "-", "form", "\u0120erotic", "\u0120fiction"], "seq_scores": [0.8906499743461609, 0.9371979832649231, 0.925842821598053, 0.8845091462135315, 0.8603118062019348], "text": " short-form erotic fiction", "score": 0.8997023463249206, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 49472, "end": 49491, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120short", "-", "form", "\u0120fiction"], "seq_scores": [0.823859691619873, 0.9478839635848999, 0.9467073082923889, 0.9149627089500427], "text": " short-form fiction", "score": 0.9083534181118011, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 49541, "end": 49559, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120trained", "\u0120model"], "seq_scores": [0.9912745356559753, 0.9940209984779358, 0.9955440163612366], "text": " the trained model", "score": 0.9936131834983826, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 49667, "end": 49686, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120other", "\u0120datasets"], "seq_scores": [0.9843743443489075, 0.990707278251648, 0.9961484670639038], "text": " the other datasets", "score": 0.9904100298881531, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 50162, "end": 50167, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.3985360264778137, 0.4008905291557312], "text": " Pile", "score": 0.39971327781677246, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 49950, "end": 49963, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9217113852500916, 0.942499577999115], "text": " each dataset", "score": 0.9321054816246033, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 50250, "end": 50277, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120and", "\u0120similar", "\u0120datasets"], "seq_scores": [0.9689022898674011, 0.9154168367385864, 0.5805033445358276, 0.9950262904167175], "text": " these and similar datasets", "score": 0.8649621903896332, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 50497, "end": 50510, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9109430909156799, 0.8975330591201782, 0.9251440763473511], "text": " Common Crawl", "score": 0.9112067421277364, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 50516, "end": 50524, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.8307512402534485, 0.9458135962486267, 0.9551592469215393], "text": " jusText", "score": 0.9105746944745382, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 50526, "end": 50549, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["End", "r\u00c3\u00a9", "dy", "\u0120and", "\u0120Nov", "\u00c3\u00a1", "k", ",", "\u01202013"], "seq_scores": [0.9965358972549438, 0.997738242149353, 0.9978030323982239, 0.9985117316246033, 0.9981673955917358, 0.998417854309082, 0.9984036087989807, 0.9983512163162231, 0.9979464411735535], "text": "Endr\u00e9dy and Nov\u00e1k, 2013", "score": 0.9979861577351888, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 50573, "end": 50598, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120document", "\u0120level", "\u0120filtering"], "seq_scores": [0.9080890417098999, 0.9141814708709717, 0.9256603121757507], "text": " document level filtering", "score": 0.9159769415855408, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 50616, "end": 50620, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "ET"], "seq_scores": [0.832647442817688, 0.6919918656349182], "text": " WET", "score": 0.7623196542263031, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 50658, "end": 50662, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120W", "ET"], "seq_scores": [0.7651323676109314, 0.652258038520813], "text": " WET", "score": 0.7086952030658722, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 50750, "end": 50773, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120R", "aff", "el", "\u0120et", "\u0120al", ".", "\u0120(", "\u01202019", "),"], "seq_scores": [0.994900643825531, 0.9982964396476746, 0.9988930821418762, 0.9989978671073914, 0.9990304708480835, 0.9990284442901611, 0.9980624318122864, 0.9981738328933716, 0.9976033568382263], "text": " Raffel et al. ( 2019),", "score": 0.9981096188227335, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 50863, "end": 50888, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u01203", "-", "sent", "ence", "\u0120ded", "u", "pl", "ication"], "seq_scores": [0.9361132383346558, 0.9752520322799683, 0.9819034337997437, 0.9821404814720154, 0.984241783618927, 0.9741916060447693, 0.9748730659484863, 0.9700803756713867], "text": " 3-sentence deduplication", "score": 0.972349502146244, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 50497, "end": 50510, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9156180620193481, 0.9023230671882629, 0.9044890999794006], "text": " Common Crawl", "score": 0.9074767430623373, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 50616, "end": 50626, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120W", "ET", "\u0120files"], "seq_scores": [0.9697341918945312, 0.9682433605194092, 0.9356082081794739], "text": " WET files", "score": 0.9578619201978048, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 50658, "end": 50668, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120W", "ET", "\u0120files"], "seq_scores": [0.9771113991737366, 0.977882981300354, 0.9638293981552124], "text": " WET files", "score": 0.972941259543101, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 50943, "end": 50951, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.9707415699958801, 0.964849591255188, 0.9756198525428772], "text": " jusText", "score": 0.9704036712646484, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 50971, "end": 50983, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Tra", "fil", "atur", "a"], "seq_scores": [0.8382101058959961, 0.8356102705001831, 0.8849661350250244, 0.8931173086166382], "text": " Trafilatura", "score": 0.8629759550094604, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 50984, "end": 50994, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Newspaper"], "seq_scores": [0.7052882313728333], "text": " Newspaper", "score": 0.7052882313728333, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 50995, "end": 51002, "seq_label": ["B-Dataset", "I-MLModel"], "seq_token": ["\u0120Goose", "3"], "seq_scores": [0.5695852041244507, 0.5265423059463501], "text": " Goose3", "score": 0.5480637550354004, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51007, "end": 51015, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120Drag", "Net"], "seq_scores": [0.7815962433815002, 0.9046220183372498], "text": " DragNet", "score": 0.843109130859375, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51155, "end": 51163, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.9381314516067505, 0.938708484172821, 0.9508187770843506], "text": " jusText", "score": 0.942552904287974, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51236, "end": 51244, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.9608909487724304, 0.9681384563446045, 0.9750285148620605], "text": " jusText", "score": 0.9680193066596985, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51425, "end": 51435, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120j", "us", "Text", "'s"], "seq_scores": [0.9150622487068176, 0.9167755246162415, 0.9385856986045837, 0.5863363146781921], "text": " jusText's", "score": 0.8391899466514587, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 51460, "end": 51465, "seq_label": ["B-Task"], "seq_token": ["\u0120text"], "seq_scores": [0.5422182679176331], "text": " text", "score": 0.5422182679176331, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51496, "end": 51508, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120tra", "fil", "atur", "a"], "seq_scores": [0.5805628895759583, 0.517703652381897, 0.5079016089439392, 0.6216011047363281], "text": " trafilatura", "score": 0.5569423139095306, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 51698, "end": 51710, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120tra", "fil", "atur", "a"], "seq_scores": [0.7436122298240662, 0.685566782951355, 0.7029262185096741, 0.7690250873565674], "text": " trafilatura", "score": 0.7252825796604156, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 51748, "end": 51769, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120intra", "-", "page", "\u0120filtering"], "seq_scores": [0.540597140789032, 0.6660820245742798, 0.6610715389251709, 0.5694933533668518], "text": " intra-page filtering", "score": 0.6093110144138336, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 51063, "end": 51087, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120an", "\u0120extraction", "\u0120benchmark"], "seq_scores": [0.964605987071991, 0.9549882411956787, 0.9592236876487732], "text": " an extraction benchmark", "score": 0.9596059719721476, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 51299, "end": 51321, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120many", "\u0120other", "\u0120extract", "ors"], "seq_scores": [0.6716296672821045, 0.7614579796791077, 0.9054819941520691, 0.8865455389022827], "text": " many other extractors", "score": 0.806278795003891, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 51460, "end": 51473, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120volume", "\u0120of", "\u0120CC", "\u0120data", "\u0120text", "\u0120corpor", "a"], "seq_scores": [0.6256505250930786, 0.8423758745193481, 0.8231425881385803, 0.6500757932662964, 0.99172443151474, 0.9590856432914734, 0.97052401304245, 0.8784863352775574], "text": " text corpora", "score": 0.8426331505179405, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 51834, "end": 51842, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.9281045198440552, 0.9818698763847351, 0.9842961430549622], "text": " jusText", "score": 0.9647568464279175, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52033, "end": 52041, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.9136394262313843, 0.9626667499542236, 0.9665429592132568], "text": " jusText", "score": 0.9476163784662882, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 52295, "end": 52298, "seq_label": ["B-Dataset"], "seq_token": ["\u0120CC"], "seq_scores": [0.9559267163276672], "text": " CC", "score": 0.9559267163276672, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 52368, "end": 52379, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120WAR", "C", "-", "based"], "seq_scores": [0.9885140061378479, 0.9910526871681213, 0.884780764579773, 0.6844682693481445], "text": " WARC-based", "score": 0.8872039318084717, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 52379, "end": 52382, "seq_label": ["B-Dataset"], "seq_token": ["\u0120CC"], "seq_scores": [0.788827657699585], "text": " CC", "score": 0.788827657699585, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52451, "end": 52459, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120py", "-", "c", "ld", "2"], "seq_scores": [0.8979554176330566, 0.9796819686889648, 0.9821340441703796, 0.9834538698196411, 0.973314106464386], "text": " py-cld2", "score": 0.9633078813552857, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 52291, "end": 52306, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120languages", "\u0120the", "\u0120CC", "\u0120dataset"], "seq_scores": [0.5422513484954834, 0.7673147320747375, 0.8863392472267151, 0.8105653524398804], "text": " the CC dataset", "score": 0.7516176700592041, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 52332, "end": 52396, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120high", "-", "quality", ",", "\u0120fully", "\u0120mult", "ilingual", ",", "\u0120WAR", "C", "-", "based", "\u0120CC", "-", "based", "\u0120dataset"], "seq_scores": [0.955596923828125, 0.9689503312110901, 0.9806354641914368, 0.9865618348121643, 0.9788929224014282, 0.9691994190216064, 0.9896865487098694, 0.991127073764801, 0.9666296243667603, 0.936756432056427, 0.956530749797821, 0.9773856997489929, 0.9846205711364746, 0.9884365797042847, 0.9945987462997437, 0.9953069090843201, 0.9961777925491333], "text": " a high-quality, fully multilingual, WARC-based CC-based dataset", "score": 0.9774760954520282, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52565, "end": 52572, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120py", "c", "ld", "2"], "seq_scores": [0.9525114297866821, 0.9792933464050293, 0.9820257425308228, 0.9818539619445801], "text": " pycld2", "score": 0.9739211201667786, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52583, "end": 52592, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120fast", "text"], "seq_scores": [0.9177886843681335, 0.9626769423484802], "text": " fasttext", "score": 0.9402328133583069, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 52617, "end": 52629, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120class", "ifying"], "seq_scores": [0.6687129139900208, 0.559685230255127], "text": " classifying", "score": 0.6141990721225739, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52676, "end": 52684, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.5975632071495056, 0.7544684410095215, 0.7306501865386963], "text": " jusText", "score": 0.6942272782325745, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52767, "end": 52774, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120py", "c", "ld", "2"], "seq_scores": [0.9528265595436096, 0.9771913290023804, 0.9824233055114746, 0.9837761521339417], "text": " pycld2", "score": 0.9740543365478516, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52804, "end": 52813, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "-", "Text"], "seq_scores": [0.86130291223526, 0.9335042238235474, 0.9363176822662354, 0.9524069428443909], "text": " jus-Text", "score": 0.9208829402923584, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52885, "end": 52892, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120py", "c", "ld", "2"], "seq_scores": [0.9226331114768982, 0.9596524238586426, 0.9687796831130981, 0.9655452370643616], "text": " pycld2", "score": 0.9541526138782501, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 52842, "end": 52860, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120j", "us", "Text", "\u0120documents"], "seq_scores": [0.9658344984054565, 0.9747334718704224, 0.9691638946533203, 0.9672442674636841], "text": " jusText documents", "score": 0.9692440330982208, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 52986, "end": 53004, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.6535940170288086, 0.7199538350105286], "text": " language modeling", "score": 0.6867739260196686, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 53020, "end": 53041, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120mult", "ilingual", "\u0120corpor", "a"], "seq_scores": [0.9870032668113708, 0.9957683086395264, 0.9968773126602173, 0.9872051477432251], "text": " multilingual corpora", "score": 0.9917135089635849, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 53154, "end": 53157, "seq_label": ["B-Dataset"], "seq_token": ["\u0120CC"], "seq_scores": [0.4949248433113098], "text": " CC", "score": 0.4949248433113098, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 53180, "end": 53205, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", ")", "\u01202020"], "seq_scores": [0.992716372013092, 0.9984265565872192, 0.998519241809845, 0.9983134269714355, 0.9956491589546204, 0.9974076151847839, 0.9970640540122986, 0.9874158501625061], "text": " Brown et al. (2020) 2020", "score": 0.9956890344619751, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 53283, "end": 53300, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120subset", "\u0120of", "\u0120CC"], "seq_scores": [0.9357787370681763, 0.9437524676322937, 0.8500308990478516, 0.8271529674530029], "text": " our subset of CC", "score": 0.8891787678003311, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 53410, "end": 53419, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120pand", "oc", "\u01201"], "seq_scores": [0.7281835079193115, 0.8325003385543823, 0.6740429997444153], "text": " pandoc 1", "score": 0.7449089487393697, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 53429, "end": 53448, "seq_label": ["I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-Method"], "seq_token": ["\u012019", "Mac", "F", "arlane", ",", "\u01202006", "\u012019"], "seq_scores": [0.4865531325340271, 0.9064508676528931, 0.9231578707695007, 0.9095519781112671, 0.9725270867347717, 0.9158191680908203, 0.3467947542667389], "text": "MacFarlane, 2006 19", "score": 0.7801221225942884, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 53454, "end": 53471, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Mac", "F", "arlane", ",", "\u0120-", "2020"], "seq_scores": [0.9253210425376892, 0.9797794222831726, 0.969714343547821, 0.9916346073150635, 0.978076159954071, 0.9863646030426025], "text": "MacFarlane, -2020", "score": 0.97181502978007, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 53525, "end": 53530, "seq_label": ["B-Method"], "seq_token": ["\u0120mark"], "seq_scores": [0.5201874375343323], "text": " mark", "score": 0.5201874375343323, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 53603, "end": 53610, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120pand", "oc"], "seq_scores": [0.648472785949707, 0.6450079083442688], "text": " pandoc", "score": 0.6467403471469879, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 53485, "end": 53501, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120J", "ATS", "\u0120format"], "seq_scores": [0.5465095639228821, 0.7758563756942749, 0.8871464729309082, 0.8403816223144531], "text": " the JATS format", "score": 0.7624735087156296, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 53790, "end": 53797, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.8285694122314453], "text": " Reddit", "score": 0.8285694122314453, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54054, "end": 54072, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Newspaper", "\u0120scra", "per"], "seq_scores": [0.6363926529884338, 0.6284459829330444, 0.5418806672096252], "text": " Newspaper scraper", "score": 0.6022397677103678, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54073, "end": 54080, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120D", "edu", "pl"], "seq_scores": [0.6618443131446838, 0.5559340119361877, 0.5342262983322144], "text": " Dedupl", "score": 0.5840015411376953, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54139, "end": 54150, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Min", "Hash", "LS", "H"], "seq_scores": [0.8188245892524719, 0.6532655954360962, 0.7471246719360352, 0.6231842041015625], "text": " MinHashLSH", "score": 0.7105997651815414, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54162, "end": 54168, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Data", "S"], "seq_scores": [0.675646185874939, 0.5205041170120239], "text": " DataS", "score": 0.5980751514434814, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 53712, "end": 53724, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9873769283294678, 0.9875470995903015], "text": " the dataset", "score": 0.9874620139598846, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 54227, "end": 54237, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Newspaper"], "seq_scores": [0.3701379597187042], "text": " Newspaper", "score": 0.3701379597187042, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54248, "end": 54256, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120j", "us", "Text"], "seq_scores": [0.4946461319923401, 0.8184927105903625, 0.85093092918396], "text": " jusText", "score": 0.7213565905888876, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 54260, "end": 54273, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9668983221054077, 0.9833787083625793, 0.9882204532623291, 0.9889475107192993], "text": " OpenWebText2", "score": 0.9818612486124039, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 54294, "end": 54313, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "-", "Text", "Corp", "us"], "seq_scores": [0.9906988739967346, 0.9955397248268127, 0.9962232112884521, 0.9965761303901672, 0.9964247345924377, 0.996181845664978], "text": " OpenWeb-TextCorpus", "score": 0.9952740867932638, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54404, "end": 54409, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Method", "I-Method"], "seq_token": ["\u0120html", "\u0120extract", "ors", "\u0120P", "ile"], "seq_scores": [0.393247127532959, 0.6449323296546936, 0.6141093373298645, 0.7199262380599976, 0.7793139219284058], "text": " Pile", "score": 0.6303057909011841, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 54506, "end": 54518, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9870554208755493, 0.9845859408378601], "text": " the dataset", "score": 0.9858206808567047, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 54579, "end": 54585, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120ar", "X", "iv"], "seq_scores": [0.7591161131858826, 0.8432814478874207, 0.8455828428268433], "text": " arXiv", "score": 0.8159934679667155, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54725, "end": 54740, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["X", "iv", "\u0120pand", "oc", "\u01201", ".", "19", ".", "2", "."], "seq_scores": [0.4789888262748718, 0.46840640902519226, 0.662590503692627, 0.8271461129188538, 0.8055856823921204, 0.7359254360198975, 0.6623179912567139, 0.6134406328201294, 0.5668217539787292, 0.4958548843860626], "text": " pandoc 1.19.2.", "score": 0.6317078232765198, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 54637, "end": 54641, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120our"], "seq_scores": [0.6663630604743958], "text": " our", "score": 0.6663630604743958, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 54878, "end": 54895, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01201", ",", "264", ",", "405", "\u0120papers"], "seq_scores": [0.7818438410758972, 0.9784702062606812, 0.9885866045951843, 0.9849415421485901, 0.9839584827423096, 0.9511424899101257], "text": " 1,264,405 papers", "score": 0.9448238611221313, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 54952, "end": 54959, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120pand", "oc"], "seq_scores": [0.609981894493103, 0.7675659656524658], "text": " pandoc", "score": 0.6887739300727844, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 55170, "end": 55188, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120modeling"], "seq_scores": [0.6716058254241943, 0.7570879459381104], "text": " language modeling", "score": 0.7143468856811523, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 55145, "end": 55159, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120text", "\u0120data"], "seq_scores": [0.9546907544136047, 0.894230306148529, 0.9925892353057861], "text": " all text data", "score": 0.94717009862264, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 55259, "end": 55267, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Web", "Text"], "seq_scores": [0.957894504070282, 0.9640076160430908], "text": " WebText", "score": 0.9609510600566864, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 55283, "end": 55290, "seq_label": ["B-Datasource"], "seq_token": ["\u0120GitHub"], "seq_scores": [0.7814241647720337], "text": " GitHub", "score": 0.7814241647720337, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 55509, "end": 55522, "seq_label": ["B-Datasource", "I-Method", "I-Method"], "seq_token": ["\u0120Github", "'s", "\u0120API"], "seq_scores": [0.6796676516532898, 0.5040159821510315, 0.5032424926757812], "text": " Github's API", "score": 0.5623087088267008, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 56165, "end": 56172, "seq_label": ["I-Method", "I-Method", "B-Datasource"], "seq_token": ["\u0120initial", "\u0120search", "\u0120Github"], "seq_scores": [0.6952652931213379, 0.7266380786895752, 0.8395849466323853], "text": " Github", "score": 0.7538294394810995, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 56616, "end": 56628, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120git", "\u0120cloning"], "seq_scores": [0.940399169921875, 0.8961004614830017], "text": " git cloning", "score": 0.9182498157024384, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 56632, "end": 56648, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120text", "\u0120extraction"], "seq_scores": [0.8552849888801575, 0.8632848262786865], "text": " text extraction", "score": 0.859284907579422, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 56371, "end": 56405, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120gathered", "\u0120list", "\u0120of", "\u0120repositories"], "seq_scores": [0.8871901035308838, 0.8996122479438782, 0.8867964744567871, 0.8736273050308228, 0.8560146689414978], "text": " the gathered list of repositories", "score": 0.880648159980774, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 56853, "end": 56896, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120repositories", "\u0120larger", "\u0120repositories", "\u0120very", "\u0120repetitive", "\u0120aut", "og", "ener", "ated", "\u0120source", "\u0120files"], "seq_scores": [0.593448281288147, 0.7127389311790466, 0.8383090496063232, 0.8473095893859863, 0.9019201993942261, 0.8530839085578918, 0.9726094603538513, 0.9733138680458069, 0.9715760350227356, 0.9665843844413757, 0.9680597186088562], "text": " very repetitive autogenerated source files", "score": 0.8726321296258406, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 56899, "end": 56910, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120data", "\u0120files"], "seq_scores": [0.8846513032913208, 0.9491536021232605], "text": " data files", "score": 0.9169024527072906, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 56989, "end": 56998, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.755587637424469, 0.905486524105072], "text": " the data", "score": 0.8305370807647705, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 57058, "end": 57063, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.748584508895874, 0.48022231459617615], "text": " Pile", "score": 0.6144034117460251, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 57114, "end": 57121, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Github"], "seq_scores": [0.6536508798599243], "text": " Github", "score": 0.6536508798599243, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 57158, "end": 57176, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120quality", "\u0120filtering"], "seq_scores": [0.566167414188385, 0.69580078125], "text": " quality filtering", "score": 0.6309840977191925, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 57212, "end": 57228, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120code", "\u0120generation"], "seq_scores": [0.8472604751586914, 0.84671550989151], "text": " code generation", "score": 0.8469879925251007, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57114, "end": 57126, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": [".", "\u0120630", ".", "64", "\u0120Gi", "B", "\u0120of", "\u0120Github", "\u0120data"], "seq_scores": [0.5091301798820496, 0.5539140105247498, 0.8360199332237244, 0.7513329386711121, 0.8338877558708191, 0.7400619387626648, 0.5265944004058838, 0.5145400166511536, 0.9772053360939026], "text": " Github data", "score": 0.6936318344540067, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 57275, "end": 57291, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120language", "\u0120models"], "seq_scores": [0.9836056232452393, 0.9874300360679626], "text": " language models", "score": 0.985517829656601, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57392, "end": 57405, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9686412811279297, 0.9752791523933411], "text": " this dataset", "score": 0.9719602167606354, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 57532, "end": 57546, "seq_label": ["I-Datasource", "B-Method", "I-Method", "I-Method"], "seq_token": ["Listener", "\u0120Beautiful", "S", "oup"], "seq_scores": [0.4913061857223511, 0.7622073292732239, 0.663752555847168, 0.574334442615509], "text": " BeautifulSoup", "score": 0.622900128364563, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57446, "end": 57470, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120court", "\u0120opinions", "\u0120data"], "seq_scores": [0.987114429473877, 0.985985279083252, 0.9954864382743835, 0.99140465259552], "text": " the court opinions data", "score": 0.9899976998567581, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 57621, "end": 57636, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Exchange"], "seq_scores": [0.7800092697143555, 0.8384484648704529], "text": " Stack Exchange", "score": 0.8092288672924042, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 57903, "end": 57920, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120few", "-", "shot", "\u0120ability"], "seq_scores": [0.6156563758850098, 0.7194926738739014, 0.7118520736694336, 0.5272054076194763], "text": " few-shot ability", "score": 0.6435516327619553, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 57922, "end": 57940, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9967273473739624, 0.9968776702880859, 0.9973698854446411, 0.9972246885299683, 0.9964742064476013], "text": "Brown et al., 2020", "score": 0.9969347596168519, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57580, "end": 57592, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9949736595153809, 0.9958160519599915], "text": " the dataset", "score": 0.9953948557376862, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57688, "end": 57710, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120top", "\u0120three", "\u0120answers"], "seq_scores": [0.9360963106155396, 0.9624271988868713, 0.9720603823661804, 0.98064124584198], "text": " the top three answers", "score": 0.9628062844276428, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57750, "end": 57770, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120other", "\u0120responses"], "seq_scores": [0.8977700471878052, 0.9749590158462524, 0.9824793338775635], "text": " all other responses", "score": 0.9517361323038737, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 57874, "end": 57902, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "-", "scale", "\u0120language", "\u0120models"], "seq_scores": [0.9703935980796814, 0.9932591915130615, 0.9959654808044434, 0.9958223104476929, 0.9919536709785461], "text": " large-scale language models", "score": 0.9894788503646851, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57975, "end": 57989, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120questions"], "seq_scores": [0.8398475646972656, 0.8300050497055054], "text": " all questions", "score": 0.8349263072013855, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 57993, "end": 58001, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120answers"], "seq_scores": [0.5928778052330017], "text": " answers", "score": 0.5928778052330017, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 58123, "end": 58139, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "\u0120Ex", "changes"], "seq_scores": [0.4620952010154724, 0.6903080344200134, 0.6407143473625183], "text": " Stack Exchanges", "score": 0.5977058609326681, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 58143, "end": 58164, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Meta", "-", "Stack", "\u0120Ex", "changes"], "seq_scores": [0.4973595142364502, 0.6798473596572876, 0.6995552778244019, 0.7105122208595276, 0.6651131510734558], "text": " Meta-Stack Exchanges", "score": 0.6504775047302246, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 58191, "end": 58205, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Stack", "Over", "flow"], "seq_scores": [0.8530364036560059, 0.8656126260757446, 0.8661885261535645], "text": " StackOverflow", "score": 0.8616125186284384, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 58039, "end": 58060, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["The", "\u0120resulting", "\u0120dataset"], "seq_scores": [0.983426570892334, 0.9950510859489441, 0.9980263113975525], "text": "The resulting dataset", "score": 0.9921679894129435, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 58080, "end": 58101, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012015", ",", "6", "22", ",", "475", "\u0120documents"], "seq_scores": [0.9601460695266724, 0.9900862574577332, 0.9959787130355835, 0.9966509938240051, 0.9956359267234802, 0.9946684241294861, 0.9815263152122498], "text": " 15,622,475 documents", "score": 0.9878132428441729, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 58697, "end": 58701, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120AP", "S"], "seq_scores": [0.5649610757827759, 0.47336360812187195], "text": " APS", "score": 0.5191623419523239, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 58703, "end": 58708, "seq_label": ["B-Dataset"], "seq_token": ["Autom"], "seq_scores": [0.3586714565753937], "text": "Autom", "score": 0.3586714565753937, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 58649, "end": 58662, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120datasets"], "seq_scores": [0.8768925070762634, 0.9934949278831482], "text": " the datasets", "score": 0.9351937174797058, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 58740, "end": 58749, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9651097059249878, 0.9763106107711792], "text": " the data", "score": 0.9707101583480835, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 59131, "end": 59146, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PubMed", "\u0120Central"], "seq_scores": [0.8041929006576538, 0.9299633502960205], "text": " PubMed Central", "score": 0.8670781254768372, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 59263, "end": 59267, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120PM", "C"], "seq_scores": [0.6630153656005859, 0.8659902215003967], "text": " PMC", "score": 0.7645027935504913, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59030, "end": 59042, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120articles", "\u0120the", "\u0120dataset"], "seq_scores": [0.6822851300239563, 0.8980560898780823, 0.9892633557319641, 0.997663140296936], "text": " the dataset", "score": 0.8918169289827347, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59285, "end": 59294, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120set"], "seq_scores": [0.7192875742912292, 0.6771144270896912], "text": " this set", "score": 0.6982010006904602, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59306, "end": 59315, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9848944544792175, 0.9902864694595337], "text": " the data", "score": 0.9875904619693756, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59394, "end": 59416, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120remaining", "\u0120dataset"], "seq_scores": [0.9920634627342224, 0.9964736104011536, 0.9978765249252319], "text": " The remaining dataset", "score": 0.995471199353536, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59425, "end": 59457, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012015", ",", "518", ",", "009", "\u0120titles", "\u0120and", "\u0120abstract", "s"], "seq_scores": [0.9172568917274475, 0.9831472039222717, 0.9767969250679016, 0.9794154167175293, 0.9630564451217651, 0.9691976308822632, 0.8644629120826721, 0.9555388689041138, 0.929165780544281], "text": " 15,518,009 titles and abstracts", "score": 0.9486708972189162, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 59652, "end": 59670, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120T", "ied", "em", "ann", "\u0120(", "2016", "),"], "seq_scores": [0.9883472323417664, 0.9982196688652039, 0.998902440071106, 0.9988676309585571, 0.9960416555404663, 0.9977537989616394, 0.9927903413772583], "text": " Tiedemann (2016),", "score": 0.9958461097308567, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59535, "end": 59552, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120text", "\u0120dataset"], "seq_scores": [0.9963891506195068, 0.99703049659729, 0.9988601207733154], "text": " the text dataset", "score": 0.9974265893300375, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59571, "end": 59589, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120subtitle", "\u0120text"], "seq_scores": [0.7588560581207275, 0.8559672832489014, 0.7698732018470764], "text": " the subtitle text", "score": 0.7948988477389017, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59611, "end": 59640, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120English", "\u0120language", "\u0120dataset"], "seq_scores": [0.9943951368331909, 0.9969636797904968, 0.998957633972168, 0.9986664056777954], "text": " the English language dataset", "score": 0.9972457140684128, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 59741, "end": 59760, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120wik", "ipedia", "/", "20", "2003", "01"], "seq_scores": [0.6388182044029236, 0.797359049320221, 0.679107129573822, 0.7459980845451355, 0.6946130394935608, 0.6856198310852051], "text": " wikipedia/20200301", "score": 0.7069192230701447, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59776, "end": 59796, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120T", "ensor", "Flow", "\u0120Dat", "as", "ets"], "seq_scores": [0.8868036270141602, 0.8904617428779602, 0.9346904158592224, 0.954901933670044, 0.9404478073120117, 0.935987651348114], "text": " TensorFlow Datasets", "score": 0.9238821963469187, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 59930, "end": 59957, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120Easy", ",", "\u0120Medium", ",", "\u0120and", "\u0120Hard"], "seq_scores": [0.5591744184494019, 0.7385169863700867, 0.6399673223495483, 0.7308425903320312, 0.5241382122039795, 0.6367615461349487, 0.5835416316986084], "text": " the Easy, Medium, and Hard", "score": 0.6304203867912292, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60186, "end": 60190, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120all"], "seq_scores": [0.736635148525238], "text": " all", "score": 0.736635148525238, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60251, "end": 60260, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9516298770904541, 0.9014896154403687], "text": " the data", "score": 0.9265597462654114, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 60620, "end": 60631, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9870979189872742, 0.9935682415962219, 0.9948292374610901], "text": " BookCorpus", "score": 0.9918317993481954, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 60714, "end": 60725, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9850435256958008, 0.991706371307373, 0.9941040873527527], "text": " BookCorpus", "score": 0.9902846614519755, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 60826, "end": 60837, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9866858720779419, 0.9911710619926453, 0.9936601519584656], "text": " BookCorpus", "score": 0.9905056953430176, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 60868, "end": 60886, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kob", "ay", "ashi", "\u0120(", "2018", ")."], "seq_scores": [0.9875819683074951, 0.9971016049385071, 0.9986916184425354, 0.9980144500732422, 0.9975194334983826, 0.9964768290519714], "text": " Kobayashi (2018).", "score": 0.995897650718689, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 60901, "end": 60912, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "Corp", "us"], "seq_scores": [0.9876003265380859, 0.9919117093086243, 0.9943509101867676], "text": " BookCorpus", "score": 0.9912876486778259, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60643, "end": 60656, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012011", ",", "0", "38", "\u0120books"], "seq_scores": [0.9843508005142212, 0.9935050010681152, 0.9934846758842468, 0.9925611615180969, 0.985912024974823], "text": " 11,038 books", "score": 0.9899627327919006, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60767, "end": 60784, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120larger", "\u0120version"], "seq_scores": [0.6318697333335876, 0.7457423210144043, 0.6071254014968872], "text": " a larger version", "score": 0.6615791519482931, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60807, "end": 60826, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120own", "\u0120version", "\u0120of"], "seq_scores": [0.9694271087646484, 0.9797258973121643, 0.9694017171859741, 0.580322265625], "text": " our own version of", "score": 0.8747192472219467, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60886, "end": 60901, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["Corp", "us", "\u0120Our", "\u0120version", "\u0120of"], "seq_scores": [0.559117317199707, 0.6029544472694397, 0.930428147315979, 0.9237940311431885, 0.7740818858146667], "text": " Our version of", "score": 0.7580751657485962, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 60921, "end": 60934, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["Corp", "us", "\u012017", ",", "8", "68", "\u0120books"], "seq_scores": [0.6053765416145325, 0.6176188588142395, 0.9881840944290161, 0.9960553646087646, 0.9970506429672241, 0.9971989393234253, 0.9944513440132141], "text": " 17,868 books", "score": 0.885133683681488, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 60983, "end": 61009, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120ep", "ub", "to", "-", "text", "\u0120converter"], "seq_scores": [0.9360155463218689, 0.836750864982605, 0.9900144338607788, 0.9906055331230164, 0.9859593510627747, 0.9888412356376648, 0.9826184511184692], "text": " the epubto-text converter", "score": 0.9586864880153111, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 61012, "end": 61030, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kob", "ay", "ashi", "\u0120(", "\u01202018", ")"], "seq_scores": [0.9858930706977844, 0.9962678551673889, 0.9970243573188782, 0.9948923587799072, 0.9965974688529968, 0.9944429993629456], "text": " Kobayashi ( 2018)", "score": 0.9941863516966502, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 61198, "end": 61207, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120html", "2", "txt"], "seq_scores": [0.6108897924423218, 0.781984269618988, 0.7806208729743958], "text": " html2txt", "score": 0.7244983116785685, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 61163, "end": 61170, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120tables"], "seq_scores": [0.776354968547821], "text": " tables", "score": 0.776354968547821, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 61241, "end": 61248, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120tables"], "seq_scores": [0.5191956758499146], "text": " tables", "score": 0.5191956758499146, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 61421, "end": 61432, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Spe", "er", ",", "\u01202019"], "seq_scores": [0.9955223798751831, 0.9956579208374023, 0.9954944849014282, 0.9948181509971619], "text": "Speer, 2019", "score": 0.9953732341527939, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 61589, "end": 61598, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.950951874256134, 0.9500256776809692], "text": " the data", "score": 0.9504887759685516, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 61909, "end": 61929, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource", "I-Dataset"], "seq_token": ["\u0120Hack", "ern", "ews", "\u0120Big", "Query"], "seq_scores": [0.6041732430458069, 0.7547052502632141, 0.6840382814407349, 0.28401318192481995, 0.5401895046234131], "text": " Hackernews BigQuery", "score": 0.5734238922595978, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 61998, "end": 62000, "seq_label": ["B-Dataset"], "seq_token": ["\u0120P"], "seq_scores": [0.467402845621109], "text": " P", "score": 0.467402845621109, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 62020, "end": 62037, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Hacker", "\u0120News", "\u0120post"], "seq_scores": [0.8247644305229187, 0.7883321046829224, 0.4526542127132416], "text": " Hacker News post", "score": 0.6885835826396942, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 62153, "end": 62162, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Big", "Query"], "seq_scores": [0.5914331078529358, 0.5275920033454895], "text": " BigQuery", "score": 0.5595125555992126, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 62228, "end": 62237, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Big", "Query"], "seq_scores": [0.6876080632209778, 0.6511433720588684], "text": " BigQuery", "score": 0.6693757176399231, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 62331, "end": 62364, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120official", "\u0120Hacker", "\u0120News", "\u0120API", "\u0120story", "\u0120and", "\u0120comment", "\u0120text", "\u0120retrieval"], "seq_scores": [0.4749933183193207, 0.4001386761665344, 0.4846453368663788, 0.6574763059616089, 0.8569995164871216, 0.8794981241226196, 0.8273680210113525, 0.8653495907783508, 0.8317463994026184], "text": " story and comment text retrieval", "score": 0.697579476568434, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62180, "end": 62186, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120story"], "seq_scores": [0.5700788497924805], "text": " story", "score": 0.5700788497924805, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 62366, "end": 62377, "seq_label": ["B-Method", "I-Method", "I-Datasource"], "seq_token": ["H", "acker", "\u0120News"], "seq_scores": [0.4057319462299347, 0.446164071559906, 0.48718586564064026], "text": "Hacker News", "score": 0.44636062781016034, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62397, "end": 62406, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120comments"], "seq_scores": [0.627144455909729], "text": " comments", "score": 0.627144455909729, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 62490, "end": 62511, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120most", "\u0120language", "\u0120models"], "seq_scores": [0.9783003926277161, 0.9713003039360046, 0.9955701231956482], "text": " most language models", "score": 0.9817236065864563, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62519, "end": 62530, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120input", "\u0120data"], "seq_scores": [0.9570684432983398, 0.9731470346450806], "text": " input data", "score": 0.9651077389717102, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62817, "end": 62834, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["-", "du", "pl", "icate", "\u0120comment", "\u0120sequences", "\u0120the", "\u0120comment", "\u0120data"], "seq_scores": [0.7005341649055481, 0.8120864033699036, 0.8439050912857056, 0.7879903316497803, 0.706154465675354, 0.6886497139930725, 0.8999443650245667, 0.9922176599502563, 0.9858813285827637], "text": " the comment data", "score": 0.8241515027152168, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62856, "end": 62865, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120comments"], "seq_scores": [0.7589672207832336], "text": " comments", "score": 0.7589672207832336, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 62961, "end": 62963, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.6163854002952576], "text": " a", "score": 0.6163854002952576, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 63768, "end": 63780, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.985650360584259, 0.9817793965339661], "text": " the dataset", "score": 0.9837148785591125, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 63853, "end": 63859, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9694724082946777, 0.9895946979522705, 0.9901547431945801, 0.989151120185852], "text": " GPT-3", "score": 0.9845932424068451, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 63809, "end": 63838, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120large", "\u0120list", "\u0120of", "\u0120search", "\u0120terms"], "seq_scores": [0.9324033260345459, 0.9239461421966553, 0.8873250484466553, 0.6471344232559204, 0.6647842526435852, 0.8435577154159546], "text": " a large list of search terms", "score": 0.8165251513322195, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 63851, "end": 63853, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.5559390187263489], "text": " a", "score": 0.5559390187263489, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 63870, "end": 63906, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120manually", "\u0120selected", "\u0120list", "\u0120of", "\u0120queries"], "seq_scores": [0.9361755847930908, 0.9705224633216858, 0.9589793086051941, 0.9725499749183655, 0.9717108607292175, 0.9458670020103455], "text": " a manually selected list of queries", "score": 0.9593008657296499, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 63926, "end": 63940, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120responses"], "seq_scores": [0.7349367141723633, 0.7962657809257507], "text": " the responses", "score": 0.765601247549057, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 64101, "end": 64115, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120requests", "-", "html"], "seq_scores": [0.6520920991897583, 0.7258407473564148, 0.7442187070846558], "text": " requests-html", "score": 0.7073838512102762, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 64251, "end": 64272, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Youtube", "Trans", "cript", "A", "pi"], "seq_scores": [0.5964452624320984, 0.6211780905723572, 0.7294428944587708, 0.6669428944587708, 0.678327202796936], "text": " YoutubeTranscriptApi", "score": 0.6584672689437866, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 64285, "end": 64321, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120human", "\u0120generated", "\u0120closed", "\u0120capt", "ions"], "seq_scores": [0.7482819557189941, 0.5454298257827759, 0.981792151927948, 0.9841436147689819, 0.9811796545982361, 0.9738908410072327], "text": " all human generated closed captions", "score": 0.8691196739673615, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 64750, "end": 64760, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120subtitles"], "seq_scores": [0.6716470122337341], "text": " subtitles", "score": 0.6716470122337341, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 64764, "end": 64779, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120173", ",", "651", "\u0120videos"], "seq_scores": [0.8586939573287964, 0.9676323533058167, 0.9539559483528137, 0.8701132535934448], "text": " 173,651 videos", "score": 0.9125988781452179, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 64815, "end": 64826, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.7527350187301636, 0.7796990275382996, 0.7426444292068481], "text": " PhilPapers", "score": 0.7583594918251038, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 64828, "end": 64830, "seq_label": ["B-Dataset"], "seq_token": ["PP"], "seq_scores": [0.6046395897865295], "text": "PP", "score": 0.6046395897865295, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 65018, "end": 65033, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["-", "MP", "\u0120p", "yo", "ai", "har", "ves", "ter"], "seq_scores": [0.4783344268798828, 0.4924064576625824, 0.7613874673843384, 0.8168669939041138, 0.8397775292396545, 0.8163856863975525, 0.7448570728302002, 0.6747450828552246], "text": " pyoaiharvester", "score": 0.7030950896441936, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 65180, "end": 65187, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120pdf", "box"], "seq_scores": [0.7156497836112976, 0.6934629678726196], "text": " pdfbox", "score": 0.7045563757419586, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 65400, "end": 65411, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method"], "seq_token": ["_", "\u0120PDF", "ext", "ract"], "seq_scores": [0.5154671669006348, 0.7192659974098206, 0.7112433314323425, 0.6063129305839539], "text": " PDFextract", "score": 0.6380723565816879, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 64956, "end": 64965, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.968725860118866, 0.9512679576873779], "text": " the data", "score": 0.959996908903122, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65143, "end": 65150, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120Papers"], "seq_scores": [0.5325760245323181], "text": " Papers", "score": 0.5325760245323181, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65355, "end": 65359, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120The"], "seq_scores": [0.5110910534858704], "text": " The", "score": 0.5110910534858704, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65526, "end": 65549, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120bulk", "-", "data", "\u0120repository"], "seq_scores": [0.920400857925415, 0.9542173743247986, 0.9131277799606323, 0.9563175439834595, 0.705132782459259], "text": " a bulk-data repository", "score": 0.8898392677307129, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65643, "end": 65654, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120applications", "\u0120These", "\u0120data"], "seq_scores": [0.5774025321006775, 0.8935118913650513, 0.867204487323761], "text": " These data", "score": 0.7793729702631632, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 65913, "end": 65922, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Ex", "POR", "TER"], "seq_scores": [0.9815180897712708, 0.9744362831115723, 0.9692927002906799], "text": " ExPORTER", "score": 0.9750823577245077, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 65926, "end": 65932, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120CR", "IS", "P"], "seq_scores": [0.9748753905296326, 0.9667452573776245, 0.9580463767051697], "text": " CRISP", "score": 0.966555674870809, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65945, "end": 65968, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["POR", "\u0120data", "\u0120a", "\u0120consolidated", "\u0120dataset"], "seq_scores": [0.499773770570755, 0.5072726011276245, 0.9919121265411377, 0.9939432740211487, 0.9950200319290161], "text": " a consolidated dataset", "score": 0.7975843608379364, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 65971, "end": 65992, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120awarded", "\u0120applications"], "seq_scores": [0.5998053550720215, 0.7931408286094666], "text": " awarded applications", "score": 0.696473091840744, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66617, "end": 66653, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01209", "39", ",", "668", "\u0120grant", "\u0120application", "\u0120abstract", "s"], "seq_scores": [0.7432998418807983, 0.9023333787918091, 0.8988734483718872, 0.9159075617790222, 0.8349979519844055, 0.8456486463546753, 0.8458284139633179, 0.7518112063407898], "text": " 939,668 grant application abstracts", "score": 0.8423375561833382, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 66712, "end": 66723, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120mail", "parser"], "seq_scores": [0.6893495321273804, 0.902799129486084], "text": " mailparser", "score": 0.7960743308067322, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66690, "end": 66699, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9848644733428955, 0.9885890483856201], "text": " the data", "score": 0.9867267608642578, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66862, "end": 66880, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120multiple", "\u0120datasets"], "seq_scores": [0.9827410578727722, 0.9942606687545776], "text": " multiple datasets", "score": 0.9885008633136749, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 67265, "end": 67270, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.4830562472343445, 0.6522064208984375], "text": " Pile", "score": 0.567631334066391, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66892, "end": 66917, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.9456300139427185, 0.9796253442764282, 0.9866906404495239], "text": " the constituent datasets", "score": 0.9706486662228903, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66947, "end": 66966, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120output", "\u0120dataset"], "seq_scores": [0.9271341562271118, 0.9245557188987732, 0.9459105134010315], "text": " the output dataset", "score": 0.9325334628423055, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 66995, "end": 67005, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.9137426614761353], "text": " documents", "score": 0.9137426614761353, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67010, "end": 67019, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.8568224310874939], "text": " datasets", "score": 0.8568224310874939, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67056, "end": 67066, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.819952666759491], "text": " documents", "score": 0.819952666759491, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67069, "end": 67082, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.8746390342712402, 0.8525148630142212], "text": " each dataset", "score": 0.8635769486427307, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67120, "end": 67133, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120that", "\u0120dataset"], "seq_scores": [0.9347715377807617, 0.9261189699172974], "text": " that dataset", "score": 0.9304452538490295, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67142, "end": 67146, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.562030553817749], "text": " the", "score": 0.562030553817749, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67156, "end": 67166, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120documents"], "seq_scores": [0.6243061423301697], "text": " documents", "score": 0.6243061423301697, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67237, "end": 67250, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.8540199995040894, 0.9159168004989624], "text": " each dataset", "score": 0.8849684000015259, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 67417, "end": 67432, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hard", "in", "\u0120(", "2018", "),"], "seq_scores": [0.9800302386283875, 0.9966895580291748, 0.981126606464386, 0.9964463114738464, 0.9787142872810364], "text": " Hardin (2018),", "score": 0.9866014003753663, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 67456, "end": 67466, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Rao", "\u0120(", "19", "61"], "seq_scores": [0.8061314821243286, 0.9430415034294128, 0.9885889291763306, 0.99367755651474], "text": " Rao (1961", "score": 0.932859867811203, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67323, "end": 67333, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120dataset"], "seq_scores": [0.9840669631958008, 0.9873912930488586], "text": " a dataset", "score": 0.9857291281223297, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67548, "end": 67562, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012010", "Gi", "B", "\u0120of", "\u0120data"], "seq_scores": [0.7077298164367676, 0.8430984020233154, 0.7228162288665771, 0.6634602546691895, 0.8470417857170105], "text": " 10GiB of data", "score": 0.756829297542572, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67586, "end": 67591, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01202", "Gi", "B"], "seq_scores": [0.6644840836524963, 0.7409927845001221, 0.600774347782135], "text": " 2GiB", "score": 0.6687504053115845, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67610, "end": 67641, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120validation", "\u0120and", "\u0120test", "\u0120splits"], "seq_scores": [0.9745075702667236, 0.9811421036720276, 0.9704626202583313, 0.9747085571289062, 0.9774911403656006], "text": " the validation and test splits", "score": 0.9756623983383179, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67685, "end": 67702, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.976800799369812, 0.9878952503204346, 0.9892202019691467], "text": " the training set", "score": 0.9846387505531311, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67812, "end": 67830, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120held", "\u0120out", "\u0120data"], "seq_scores": [0.9344403147697449, 0.9700601696968079, 0.9608855843544006, 0.9576454162597656], "text": " the held out data", "score": 0.9557578712701797, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 67875, "end": 67896, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".", "\u0120(", "2020", "),"], "seq_scores": [0.9875819683074951, 0.9987886548042297, 0.9990441203117371, 0.9990929365158081, 0.9981991648674011, 0.9981029033660889, 0.996793806552887], "text": " Brown et al. (2020),", "score": 0.9968005078179496, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 67983, "end": 67988, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.7641194462776184], "text": " data", "score": 0.7641194462776184, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68096, "end": 68105, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9482247829437256, 0.8853694200515747], "text": " the data", "score": 0.9167971014976501, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68121, "end": 68133, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9919468760490417, 0.9917917251586914], "text": " the dataset", "score": 0.9918693006038666, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68159, "end": 68174, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120academic", "\u0120texts"], "seq_scores": [0.9822133779525757, 0.9906133413314819], "text": " academic texts", "score": 0.9864133596420288, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68207, "end": 68232, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120highest", "\u0120quality", "\u0120data"], "seq_scores": [0.9837548136711121, 0.990352213382721, 0.9885520935058594, 0.9921401143074036], "text": " the highest quality data", "score": 0.988699808716774, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68244, "end": 68257, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120smaller", "\u0120sets"], "seq_scores": [0.9873192310333252, 0.9944218993186951], "text": " smaller sets", "score": 0.9908705651760101, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68312, "end": 68321, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9798672199249268, 0.9797276258468628], "text": " the data", "score": 0.9797974228858948, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68345, "end": 68354, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120any", "\u0120data"], "seq_scores": [0.5706011652946472, 0.5014185309410095], "text": " any data", "score": 0.5360098481178284, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68490, "end": 68515, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120P", "ile", "\u0120wide", "\u0120de", "-", "du", "pl", "ication"], "seq_scores": [0.9820197224617004, 0.9928570985794067, 0.9941721558570862, 0.9734010696411133, 0.9850143790245056, 0.9852553606033325, 0.9839037656784058, 0.9827964305877686], "text": " Pile wide de-duplication", "score": 0.9849274978041649, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68525, "end": 68540, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120de", "-", "du", "pl", "ication"], "seq_scores": [0.9630106687545776, 0.9784556031227112, 0.9800055027008057, 0.9790210723876953, 0.9748743772506714], "text": " de-duplication", "score": 0.9750734448432923, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68583, "end": 68597, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Open", "-", "Web", "Text", "2"], "seq_scores": [0.5586161017417908, 0.7405083179473877, 0.6503201127052307, 0.6887733936309814, 0.688992440700531], "text": " Open-WebText2", "score": 0.6654420733451843, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68601, "end": 68609, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120P", "ile", "-", "CC"], "seq_scores": [0.9378163814544678, 0.9848651885986328, 0.9761881232261658, 0.9721783399581909], "text": " Pile-CC", "score": 0.9677620083093643, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68612, "end": 68623, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120those", "\u0120sets"], "seq_scores": [0.934033989906311, 0.9386225938796997], "text": " those sets", "score": 0.9363282918930054, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 68655, "end": 68675, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120duplicate", "\u0120documents"], "seq_scores": [0.9327811598777771, 0.9543737173080444], "text": " duplicate documents", "score": 0.9435774385929108, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 68713, "end": 68727, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "-", "Text", "2"], "seq_scores": [0.9877562522888184, 0.9903472661972046, 0.991091787815094, 0.9904422163963318, 0.9912882447242737], "text": " OpenWeb-Text2", "score": 0.9901851534843444, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 68731, "end": 68744, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9916216135025024, 0.9940074682235718, 0.994857668876648], "text": " Common Crawl", "score": 0.9934955835342407, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68745, "end": 68755, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Min", "Hash", "LS", "H"], "seq_scores": [0.6016010642051697, 0.9214906692504883, 0.9124879240989685, 0.8842872381210327], "text": "MinHashLSH", "score": 0.8299667239189148, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68764, "end": 68771, "seq_label": ["B-Method"], "seq_token": ["\u0120Python"], "seq_scores": [0.39533355832099915], "text": " Python", "score": 0.39533355832099915, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68771, "end": 68782, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Dat", "ask", "etch"], "seq_scores": [0.7027952671051025, 0.8737083077430725, 0.8464564085006714], "text": " Datasketch", "score": 0.8076533277829488, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 68829, "end": 68837, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Min", "hash"], "seq_scores": [0.9075291752815247, 0.8143139481544495], "text": " Minhash", "score": 0.8609215617179871, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 68924, "end": 68937, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.9842005372047424, 0.9872037768363953, 0.9892097115516663, 0.9893313646316528], "text": " OpenWebText2", "score": 0.9874863475561142, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 68949, "end": 68962, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9915283918380737, 0.9929825067520142, 0.9949235320091248], "text": " Common Crawl", "score": 0.9931448101997375, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69050, "end": 69054, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120L", "SH"], "seq_scores": [0.9041421413421631, 0.9112868905067444], "text": " LSH", "score": 0.9077145159244537, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69084, "end": 69113, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120quad", "r", "atic", "\u0120Min", "hash", "\u0120comparison"], "seq_scores": [0.5597019791603088, 0.8051162958145142, 0.8231401443481445, 0.638857901096344, 0.8856707811355591, 0.7449401617050171], "text": " quadratic Minhash comparison", "score": 0.742904543876648, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69201, "end": 69205, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120L", "SH"], "seq_scores": [0.9549351930618286, 0.9483304619789124], "text": " LSH", "score": 0.9516328275203705, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69252, "end": 69265, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120in", "memory", "\u0120L", "SH"], "seq_scores": [0.9210213422775269, 0.796266496181488, 0.5265703797340393, 0.9519846439361572], "text": " inmemory LSH", "score": 0.7989607155323029, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69286, "end": 69296, "seq_label": ["B-Method"], "seq_token": ["\u0120Cassandra"], "seq_scores": [0.5286182165145874], "text": " Cassandra", "score": 0.5286182165145874, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69324, "end": 69337, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Open", "Web", "Text", "2"], "seq_scores": [0.8633253574371338, 0.9609606266021729, 0.9600518941879272, 0.9411253929138184], "text": " OpenWebText2", "score": 0.9313658177852631, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 69457, "end": 69470, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.9889640808105469, 0.9859442710876465, 0.9896327257156372], "text": " Common Crawl", "score": 0.9881803592046102, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69538, "end": 69544, "seq_label": ["B-Method"], "seq_token": ["\u0120Mongo"], "seq_scores": [0.7578694820404053], "text": " Mongo", "score": 0.7578694820404053, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69609, "end": 69615, "seq_label": ["B-Method"], "seq_token": ["\u0120Mongo"], "seq_scores": [0.6659203171730042], "text": " Mongo", "score": 0.6659203171730042, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 69642, "end": 69656, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120in", "-", "memory", "\u0120L", "SH"], "seq_scores": [0.931216835975647, 0.926746666431427, 0.9042514562606812, 0.6648809313774109, 0.9668598175048828], "text": " in-memory LSH", "score": 0.8787911415100098, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 69689, "end": 69702, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.989490807056427, 0.9850752353668213, 0.989913821220398], "text": " Common Crawl", "score": 0.9881599545478821, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 69116, "end": 69130, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120all", "\u0120documents"], "seq_scores": [0.5746684670448303, 0.5115944743156433], "text": " all documents", "score": 0.5431314706802368, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 69396, "end": 69417, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120corrupted", "\u0120database"], "seq_scores": [0.8428207039833069, 0.7927104830741882, 0.7888946533203125], "text": " a corrupted database", "score": 0.8081419467926025, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 69827, "end": 69847, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Rad", "ford", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.997731626033783, 0.998225748538971, 0.9983401298522949, 0.9981333613395691, 0.9978048205375671, 0.997626006603241], "text": "Radford et al., 2019", "score": 0.997976948817571, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 69848, "end": 69867, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9970331192016602, 0.9979925155639648, 0.9982461929321289, 0.9980699419975281, 0.9979133009910583], "text": " Brown et al., 2020", "score": 0.9978510141372681, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 69868, "end": 69889, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Sh", "oe", "y", "bi", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9972314238548279, 0.9980210065841675, 0.9981860518455505, 0.9982578158378601, 0.9982998967170715, 0.9984251260757446, 0.9981733560562134, 0.9981353282928467], "text": " Shoeybi et al., 2019", "score": 0.9980912506580353, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70316, "end": 70321, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8923335075378418, 0.4291522800922394], "text": " Pile", "score": 0.6607428938150406, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 69914, "end": 69931, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120data", "\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.4924364686012268, 0.9883286952972412, 0.9956568479537964, 0.9937122464179993], "text": " the training set", "score": 0.8675335645675659, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 70298, "end": 70305, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9122768044471741], "text": " models", "score": 0.9122768044471741, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70344, "end": 70349, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9578543901443481, 0.8320212364196777], "text": " Pile", "score": 0.8949378132820129, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70385, "end": 70390, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.913171112537384, 0.7130136489868164], "text": " Pile", "score": 0.8130923807621002, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70470, "end": 70475, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9326038956642151, 0.7521503567695618], "text": " Pile", "score": 0.8423771262168884, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70707, "end": 70712, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.9041612148284912, 0.6916348934173584], "text": " Pile", "score": 0.7978980541229248, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 70326, "end": 70333, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9068568348884583], "text": " models", "score": 0.9068568348884583, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 70385, "end": 70421, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120P", "ile", "'s", "\u0120own", "\u0120validation", "\u0120and", "\u0120test", "\u0120sets"], "seq_scores": [0.9524236917495728, 0.9707499742507935, 0.9793715476989746, 0.981149435043335, 0.9785526394844055, 0.9971083998680115, 0.9959374666213989, 0.9958614706993103], "text": " Pile's own validation and test sets", "score": 0.9813943281769753, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 70479, "end": 70499, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120validation", "\u0120data"], "seq_scores": [0.9882681965827942, 0.9954429864883423, 0.9943313598632812], "text": " the validation data", "score": 0.9926808476448059, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 70622, "end": 70642, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120validation", "\u0120sets"], "seq_scores": [0.9878009557723999, 0.9956423044204712, 0.9952025413513184], "text": " the validation sets", "score": 0.9928819338480631, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 70788, "end": 70798, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120P", "ile", "\u0120any", "\u0120model"], "seq_scores": [0.4987986385822296, 0.6518642902374268, 0.8822539448738098, 0.8227332830429077], "text": " any model", "score": 0.7139125391840935, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 70927, "end": 70940, "seq_label": ["I-Method", "I-Method", "B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["atory", "\u0120analysis", "\u0120Common", "\u0120C", "rawl"], "seq_scores": [0.6974287629127502, 0.668028712272644, 0.8492550253868103, 0.8038573265075684, 0.8473255634307861], "text": " Common Crawl", "score": 0.7731790781021118, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 70988, "end": 71006, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Brown", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.9972146153450012, 0.9980660080909729, 0.9983476400375366, 0.9983360171318054, 0.998149037361145], "text": "Brown et al., 2020", "score": 0.9980226635932923, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 71067, "end": 71080, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120dataset"], "seq_scores": [0.9763039946556091, 0.9890524744987488], "text": " this dataset", "score": 0.982678234577179, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 71743, "end": 71756, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120top", "\u01201000"], "seq_scores": [0.7277209758758545, 0.6956481337547302, 0.6245085597038269], "text": " the top 1000", "score": 0.6826258897781372, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 71832, "end": 71850, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["Tokens", "\u0120Trip", "Ad", "visor"], "seq_scores": [0.49183645844459534, 0.7622649669647217, 0.7387914061546326, 0.7150487899780273], "text": "Tokens TripAdvisor", "score": 0.6769854053854942, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 71851, "end": 71863, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Simply", "H", "ired"], "seq_scores": [0.6700772643089294, 0.7678350806236267, 0.7653474807739258], "text": " SimplyHired", "score": 0.7344199419021606, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 71864, "end": 71881, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120Associated", "\u0120Press"], "seq_scores": [0.7124930620193481, 0.9005799889564514], "text": " Associated Press", "score": 0.8065365254878998, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 71882, "end": 71893, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Post", "-", "Media"], "seq_scores": [0.6997908353805542, 0.9083542823791504, 0.9035975933074951], "text": " Post-Media", "score": 0.8372475703557333, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 71894, "end": 71902, "seq_label": ["B-Datasource", "I-Datasource"], "seq_token": ["\u0120The", "\u0120FCC"], "seq_scores": [0.6782300472259521, 0.8621635437011719], "text": " The FCC", "score": 0.770196795463562, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 72307, "end": 72313, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9902603030204773, 0.9941944479942322, 0.9950556755065918, 0.9947273135185242], "text": " GPT-2", "score": 0.9935594350099564, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 72323, "end": 72329, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9913727045059204, 0.9943192601203918, 0.9953806400299072, 0.9950339794158936], "text": " GPT-3", "score": 0.9940266460180283, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 72576, "end": 72601, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120causal", "\u0120attention", "\u0120mask", "ing"], "seq_scores": [0.8693563342094421, 0.971549928188324, 0.9583652019500732, 0.9173296093940735], "text": " causal attention masking", "score": 0.9291502684354782, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 72857, "end": 72862, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120P", "ile"], "seq_scores": [0.8876190185546875, 0.7817326188087463], "text": " Pile", "score": 0.8346758186817169, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73003, "end": 73009, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9908316731452942, 0.9943385720252991, 0.9946247935295105, 0.9946579337120056], "text": " GPT-2", "score": 0.9936132431030273, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73013, "end": 73019, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "3"], "seq_scores": [0.9919754862785339, 0.9949520826339722, 0.9953569769859314, 0.9952237010002136], "text": " GPT-3", "score": 0.9943770617246628, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 73136, "end": 73153, "seq_label": ["I-Method", "I-Method", "I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Hug", "ging", "\u0120Face", "Wolf", "\u0120et", "\u0120al", ".,", "\u01202020"], "seq_scores": [0.5662969946861267, 0.9116345643997192, 0.8903183341026306, 0.9865041971206665, 0.9928459525108337, 0.9937870502471924, 0.9887040257453918, 0.9663439989089966], "text": "Wolf et al., 2020", "score": 0.9120543897151947, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73172, "end": 73178, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9890711307525635, 0.9924619793891907, 0.9930431842803955, 0.9927674531936646], "text": " GPT-2", "score": 0.9918359369039536, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73202, "end": 73208, "seq_label": ["I-Method", "I-Method", "I-Method", "B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Open", "AI", "\u0120API", "\u0120G", "PT", "-", "3"], "seq_scores": [0.4226933717727661, 0.7561601400375366, 0.6391202211380005, 0.9912109971046448, 0.9936416745185852, 0.994227409362793, 0.9938530921936035], "text": " GPT-3", "score": 0.8272724151611328, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73213, "end": 73221, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120d", "avin", "ci"], "seq_scores": [0.8339403867721558, 0.9195095896720886, 0.9586606025695801], "text": " davinci", "score": 0.9040368596712748, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73302, "end": 73308, "seq_label": ["I-Method", "I-Method", "B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["AI", "\u0120API", "\u0120G", "PT", "-", "3"], "seq_scores": [0.6460166573524475, 0.5328893065452576, 0.9892008304595947, 0.9928015470504761, 0.9936222434043884, 0.9935033917427063], "text": " GPT-3", "score": 0.8580056627591451, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 72151, "end": 72167, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120given", "\u0120dataset"], "seq_scores": [0.9731733202934265, 0.973919153213501, 0.9829893708229065], "text": " a given dataset", "score": 0.9766939481099447, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 72280, "end": 72290, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9799086451530457, 0.9713118076324463], "text": " the model", "score": 0.975610226392746, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 72389, "end": 72399, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9907208681106567, 0.9916913509368896], "text": " the model", "score": 0.9912061095237732, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 72519, "end": 72565, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120respective", "\u0120language", "\u0120model", "\u0120implementations"], "seq_scores": [0.9680068492889404, 0.9814413785934448, 0.9853777885437012, 0.986674964427948, 0.959995448589325], "text": " The respective language model implementations", "score": 0.9762992858886719, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 72635, "end": 72647, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9573180675506592, 0.8921703696250916], "text": " the dataset", "score": 0.9247442185878754, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 72769, "end": 72787, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120whole", "\u0120dataset"], "seq_scores": [0.987199604511261, 0.9915820956230164, 0.9920743703842163], "text": " the whole dataset", "score": 0.9902853568394979, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 72894, "end": 72919, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120constituent", "\u0120datasets"], "seq_scores": [0.9849562048912048, 0.9933904409408569, 0.9947532415390015], "text": " the constituent datasets", "score": 0.9910332957903544, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 73117, "end": 73125, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120Hug"], "seq_scores": [0.5069856643676758, 0.611255943775177], "text": " the Hug", "score": 0.5591208040714264, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 73209, "end": 73213, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric"], "seq_token": ["\u0120Face", "\u0120(", "Wolf", "\u0120et", "\u0120al", ".,", "\u01202020", ")", "\u0120implementation", "PT", "-", "2", "\u0120The"], "seq_scores": [0.6920537352561951, 0.5818052291870117, 0.5794188976287842, 0.6256181597709656, 0.6540278792381287, 0.5668636560440063, 0.5516228675842285, 0.5619356632232666, 0.6127715110778809, 0.5501282215118408, 0.6625856161117554, 0.6388828158378601, 0.6144211888313293], "text": " The", "score": 0.6070873416387118, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 73274, "end": 73308, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120175", "B", "\u0120parameter", "\u0120version", "\u0120of", "\u0120G", "PT", "-", "3"], "seq_scores": [0.9813931584358215, 0.9922239780426025, 0.989560604095459, 0.9912177920341492, 0.9891242980957031, 0.9840437769889832, 0.9878368973731995, 0.9875749945640564, 0.9864826798439026, 0.9853366017341614], "text": " a 175B parameter version of GPT-3", "score": 0.9874794781208038, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 73455, "end": 73462, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Open", "AI"], "seq_scores": [0.555631160736084, 0.6463350057601929], "text": " OpenAI", "score": 0.6009830832481384, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 73649, "end": 73660, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Ubuntu", "\u0120IRC"], "seq_scores": [0.9793821573257446, 0.9877107739448547], "text": " Ubuntu IRC", "score": 0.9835464656352997, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 73661, "end": 73674, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Book", "-", "Corp", "us", "2"], "seq_scores": [0.9855015873908997, 0.9905685782432556, 0.9926847219467163, 0.9923614263534546, 0.9896201491355896], "text": " Book-Corpus2", "score": 0.9901472926139832, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 73679, "end": 73690, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Phil", "P", "apers"], "seq_scores": [0.9824855327606201, 0.9901077747344971, 0.9911348819732666], "text": " PhilPapers", "score": 0.9879093964894613, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 73764, "end": 73770, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120G", "PT", "-", "2"], "seq_scores": [0.9571824073791504, 0.9774737358093262, 0.9747060537338257, 0.9609304070472717], "text": " GPT-2", "score": 0.9675731509923935, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73588, "end": 73598, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["-", "t", "enth", "\u0120of", "\u0120the", "\u0120test", "\u0120set", "\u0120one", "-", "t", "enth"], "seq_scores": [0.712346613407135, 0.792435884475708, 0.8093230128288269, 0.6503759026527405, 0.7588599324226379, 0.9896299242973328, 0.9828985929489136, 0.5607553720474243, 0.7389085292816162, 0.7179079651832581, 0.6910462975502014], "text": " one-tenth", "score": 0.7640443660996177, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73618, "end": 73631, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120documents", "\u0120each", "\u0120dataset"], "seq_scores": [0.7349013686180115, 0.9225662350654602, 0.9013646841049194, 0.9260219931602478], "text": " each dataset", "score": 0.8712135702371597, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73739, "end": 73757, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120full", "\u0120test", "\u0120set"], "seq_scores": [0.9757732152938843, 0.9853057265281677, 0.9930904507637024, 0.9856491088867188], "text": " the full test set", "score": 0.9849546253681183, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73837, "end": 73856, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120pe", "j", "orative", "\u0120content"], "seq_scores": [0.7381739616394043, 0.6500218510627747, 0.6719069480895996, 0.5397502183914185], "text": " pejorative content", "score": 0.6499632447957993, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73871, "end": 73875, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120sex"], "seq_scores": [0.5901597738265991], "text": " sex", "score": 0.5901597738265991, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 73973, "end": 74003, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120public", "\u0120\"", "n", "aughty", "\u0120words", "\"", "\u0120list"], "seq_scores": [0.9490898251533508, 0.9761447310447693, 0.9846893548965454, 0.9916594624519348, 0.9926750063896179, 0.9875932335853577, 0.9668529629707336, 0.9159113168716431], "text": " a public \"naughty words\" list", "score": 0.9705769866704941, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 74105, "end": 74118, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9713026881217957, 0.9854647517204285], "text": " each dataset", "score": 0.9783837199211121, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 74179, "end": 74185, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120words"], "seq_scores": [0.5920228362083435], "text": " words", "score": 0.5920228362083435, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 74290, "end": 74296, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120words"], "seq_scores": [0.5548927783966064], "text": " words", "score": 0.5548927783966064, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 74368, "end": 74374, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120words"], "seq_scores": [0.5639392137527466], "text": " words", "score": 0.5639392137527466, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 74507, "end": 74517, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120words"], "seq_scores": [0.5550025701522827, 0.5182487368583679], "text": " the words", "score": 0.5366256535053253, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 75286, "end": 75295, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Facebook"], "seq_scores": [0.5257775187492371], "text": " Facebook", "score": 0.5257775187492371, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 85060, "end": 85071, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Amazon", "\u0120EC", "2"], "seq_scores": [0.325930118560791, 0.6214511394500732, 0.6241965293884277], "text": " Amazon EC2", "score": 0.5238592624664307, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 85075, "end": 85086, "seq_label": ["B-Datasource", "I-Method", "I-Datasource"], "seq_token": ["\u0120Amazon", "\u0120SQ", "S"], "seq_scores": [0.5249108076095581, 0.5178444385528564, 0.4337654709815979], "text": " Amazon SQS", "score": 0.49217357238133747, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 86681, "end": 86747, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120most", "\u0120extensive", "\u0120computer", "ized", "\u0120demographic", "\u0120and", "\u0120genetics", "\u0120database"], "seq_scores": [0.7879400253295898, 0.7748382687568665, 0.843569278717041, 0.8472575545310974, 0.8913764953613281, 0.8687847256660461, 0.8244913816452026, 0.6622574925422668, 0.8337470889091492], "text": " the most extensive computerized demographic and genetics database", "score": 0.8149180346065097, "type": "ScholarlyEntity"}]}, "filename": "00036_2101_00027.json", "id": "00036_2101_00027"}