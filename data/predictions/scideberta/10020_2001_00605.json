{"text": "Zero-Shot Reinforcement Learning with Deep Attention Convolutional Neural Networks\n\nAbstract:\nSimulation-to-simulation and simulation-to-real world transfer of neural network models have been a difficult problem. To close the reality gap, prior methods to simulation-to-real world transfer focused on domain adaptation, decoupling perception and dynamics and solving each problem separately, and randomization of agent parameters and environment conditions to expose the learning agent to a variety of conditions. While these methods provide acceptable performance, the computational complexity required to capture a large variation of parameters for comprehensive scenarios on a given task such as autonomous driving or robotic manipulation is high. Our key contribution is to theoretically prove and empirically demonstrate that a deep attention convolutional neural network (DACNN) with specific visual sensor configuration performs as well as training on a dataset with high domain and parameter variation at lower computational complexity. Specifically, the attention network weights are learned through policy optimization to focus on local dependencies that lead to optimal actions, and does not require tuning in real-world for generalization. Our new architecture adapts perception with respect to the control objective, resulting in zero-shot learning without pre-training a perception network. To measure the impact of our new deep network architecture on domain adaptation, we consider autonomous driving as a use case. We perform an extensive set of experiments in simulation-to-simulation and simulation-toreal scenarios to compare our approach to several baselines including the current state-of-art models.\n\nMain:\n\n\n\n1 Introduction\nMost of the recent examples in deep reinforcement learning of autonomous control agents utilize realistic simulation environments to learn various tasks including but not limited to locomotion, motion planning, and robotic-arm manipulation with limited or no human guidance (see [1] and references therein). These realistic simulation environments are safe for the agent to experience both desired and unwanted behavior. On the other hand, in general, a controller learned in a simulation environment performs poorly in the real world or does not generalize without additional tuning in the real world.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n\narXiv:2001.00605v1 [cs.LG] 2 Jan 2020\nThere is no single approach for zero-shot reinforcement learning of a robotic controller agent. In [2], the authors apply domain adaptation at the feature level. In [3] and [4], the authors used domain and dynamics randomization, respectively. In [5], the authors propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. More recently, domain adaptation has been studied for robotic manipulators [6] [7] [8] [9] [10] [11] in which the authors use raw (pixel) images as state for deep reinforcement learning.\nTo achieve zero-shot RL requires addressing the uncertainty, un-modeled dynamics, and perception challenges across all three components, namely, agent, environment, and interpreter. There are currently two schools of thought, one focusing on improving dynamics and the other on perception. We argue that the key to achieving robust zero-shot reinforcement learning requires jointly addressing uncertainty in dynamics and variability in perception.\nWe propose a new deep neural network architecture named Deep Attention Convolutional Neural Network (DACNN). An overview of the steps of our proposed approach is shown in Figure 1. Our key contribution is that our attention model uniquely captures underlying components in the modern control theoretic approach, i.e., image-based servo-ing, without the need for separation of perception and control. The image-based servoing have been succesully applied to robotic control use cases including but not limited to drones. The recent image-based servo-ing methods use image feature vectors which are specific transformation of the raw pixels. We prove that our attention model uniquely captures the image feature vectors, i.e., in image-based visual servo control, via annotation vectors. Annotation vectors are extracted from a CNN as described in [12]. By defining the image features as annotation vectors, the full image error is defined in term of the weights of the annotation vectors. We assume that the annotation vectors have fixed orientation in the inertial frame. This assumption allows the passivity-like features of the dynamic system to transfer to the full image error when a spherical camera geometry is used. Therefore, we jointly solve the perception and control problem via the attention model that results in robust domain adaptation with zero-shot RL. A complete characterization of the class of systems which can be rendered passive from is beyond the scope of this paper. However, this class is broad, and encompasses mechanical systems modeled by Euler-Lagrange equations [13].\nFigure 1 : The block diagram of our proposed approach using an attention mechanism that preserves passivity-like features of the dynamic system for optimal motion planning. A complete characterization of the class of systems which can be rendered passive from to is beyond the scope of this paper. However, this class is broad, and encompasses mechanical systems modeled by Euler-Lagrange equations.\n\n2 Our Approach: Attention Models in Optimal Visual Control\nIn this section, we describe each constructing block of the overall architecture of our proposed approach in Figure 1. First, we describe the core of our architecture, attention neural networks. Second, we describe the underlying assumptions that enable attention networks to perform better than the current state-of-the art approaches. Finally, we provide how autonomous driving physics satisfy the assumptions for deep reinforcement learning with attention networks.\nOur hypothesis is that under certain assumptions on the control system and sensor configuration, a specific type of neural network, i.e., attention network, enables joint perception and stable control of the system even there are significant changes in the environment such as texture and lighting that transforms the observation space. There are several formulation of attention networks for image-captioning and natural language processing. To the best of our knowledge, the attention mechanism in these applications are used in conjunction with recurrent neural networks. There are several formulations of attention models in recurrent neural networks [14, 12, 15, 16]. Attention enables neural networks to \"focus\" selectively on different parts of the input while generating the corresponding parts of the output sequence. This selective \"focus\" for a corresponding input is then learned through back-propogation.\nOur formulation is inspired from the attention model in [14] where the attention-based model can attend to salient part of an image while generating its caption. Intuitively, attention enables the model to see what parts of the image to focus on as it generates a caption. It is very much equivalent to how humans perceive when performing image caption generation or long length machine translation. In the context of autonomous driving, the vehicle needs to focus on the ques from the road such as white and yellow lines but not on the entire road, i.e., grey asphalt, unless there is an obstacle or another vehicle.\nOur main goal is to learn the shortest path on an arbitrarily-drawn track using a ground robot or vehicle as the highest possible speed without going off-the-track, hitting obstacles or other vehicles. We assume that the ground vehicle control is based only on raw (pixel) image and there is no other sensor for position or velocity. In the following, first, we consider the image-based control formulation using construct from the image-based visual servo (IBVS) theory and the kinematic model of our vehicle as defined in [17]. We provide the necessary conditions required to preserve passivity-like properties. Second, we consider the full image error for the control problem. We propose that the image features and full image error is defined in terms of annotation vectors and their corresponding weights. We show that the proposed formulation guarantees a stabilizing controller. Finally, we describe the learning task based on attention model.\nThe image-based only IBVS approach for our ground vehicle problem can be solved if and only if the image geometry is spherical. In the classical setting, the state of our vehicle is defined as s = [x, y, \u03c8, v] where (x, y) is the position of the vehicle on a 2D plane, \u03c8 is the steering angle of the center of the mass, and v is the velocity of the vehicle in 2D plane. The state transition function is defined by the discrete-time formulation of the kinematic model in Equations 1-4 as formulated in [17] :\u1e8b = v cos(\u03c8 + \u03b2) (1) \u1e8f = v sin(\u03c8 + \u03b2) (2) \u03c8 = (v/l r ) sin(\u03b2) (3) v = a (4) \u03b2 = tan \u22121 ((l r /L) tan(\u03b4 f ))\nl r is the distance of the center of mass to the rear axle, L is the length of the vehicle, a is the acceleration, and \u03b4 f is the steering of the front vehicle 1. The proposed kinematic performs at par with the dynamic model for model predictive control off-road test using actual-size passenger automobiles. This kinematic model satisfies the passivity property for the control system.\nIn the classical IBVS formulation, the control state is inferred from the sensor-based observations. That is, the sensor state is the raw-pixel image and a transformation matrix is used to map observations into the control state. In our deep reinforcement learning formulation, we consider observations as the state and the control state transformation is implicitly embedded into the neural network and will not be inferred directly. The deep reinforcement learning algorithm will infer on the control state through a reward function by making trial-and-error based decision on the observation space, i.e., raw pixel images.\nIn our and IBVS formulation, the observation state is defined as S \u2208 R L where S = [a 1 , . . . a L ] \u2208 S is a matrix and each column a i \u2208 R D corresponds to a D-dimensional feature extracted from the observed image, L and D are finite integers. The geometry of the camera is modeled by its image surface S relative to its focal point. Therefore, the image feature a i can be written as a function of is projection P i onto the image surface S in the body fixed frame. The image feature in our formulation is the output of the convolutional neural network layers, and the input to the convolutional network layers is the raw-pixel image.\nTheorem 2.1. The passivity-like properties of the body fixed frame dynamics of a rigid object in the image space are preserved if and only if the image geometry is of a spherical camera.\nThe proof of Theorem 2.1 is in [18]. Our kinematic equations, Equations 1-4, already exhibits a simple linear cascade system, i.e., \u1e8b = vR and v = a where R is a rotation matrix. Since cascade systems exhibit passivity-like properties, to guarantee that the controlled system inherits passivity-like properties, we need to show that the gradient of the full image error contains a skew-symmetric matrix on angular velocities. In the next section, we define the full image error in terms of an attention network.\nFigure 2 : The block diagram of the our attention-CNN network.\nThe overall neural network architecture with the deep attention network for our approach is shown in Figure 2. Intuitively, the objective of a visual servo algorithm in image space is to match the observed image to a known \"model\" image of the target. The target is an image of the environment with the desired outcome. In the context of autonomous vehicle, a target is an image of the road where the vehicle is within the white lines and away from obstacles and other vehicles. Our approach does not require a known model image of the target for controls. However, we need to engineer a reward function that will indirectly provide the means to discriminate between desired versus undesired behavior. Therefore, it is necessary to examine the error in the image space. Furthermore, we hypothesize that our attention network model reduces the image space error compared to naively feeding image features extracted from CNN layers. The full image error between the observed and known model image is defined using a combination matrix approach\u03b4 1 := C(a i \u2212 a * i )\nwhere a * i are the desired image features and C is the combination matrix that preserve the passivitylike properties. Assuming that C is full rank, we can rewrite the full image error as a weighted sum\u03b4 1 = \u03a3 i \u03b1 i (a i \u2212 a * i )\nwhere \u03b1 i \u2265 0. The choice of \u03b1 i becomes the design component for the control algorithm. We propose that in the above formulation a i can be chosen as the annotation vectors in a our modified attention model and \u03b1 i are the corresponding weights of the annotation vectors. Suppose that a set of image features, annotation vectors are extracted from a CNN as in [14]. The output of the attention layer from the annotation vectors corresponds to the features extracted at different image locations. The extractor produces a finite number L of annotation vectors a = {a 1 , ..., a L }, a i \u2208 R D where D is a finite integer. These annotation vectors form the state space of our MDP. We define a context vector \u1e91 where \u03c6 is a weighted sum,\u1e91 = i \u03b1 i a i\nFor each location i, the mechanism generates a positive weight \u03b1 i which can be interpreted either as the probability that location i is the right place to focus for producing the control action (the \"hard\" but stochastic attention mechanism), or as the relative importance to give to location i in weighted sum of the the a i vectors. The weight \u03b1 i of each annotation vector a i is computed by an attention model f att . The weight \u03b1 i is calculated by the softmax function\u03b1 ti = exp(e ti )/\u03a3 L k exp(e tk )\nwheree ti = f att (a i , h t\u22121 ),\nh t\u22121 are hidden state vectors from the previous LSTM cell and f att an attention model. In the literature, there are multiple formulations of the f att model, i.e., additive and multiplicative. In the following, we describe our formulation of f att based on additive models\nThe only input to the attention model f att is the annotation vectors. Additive attention (or multilayer perceptron(MLP) attention) and multiplicative attention (or dot-product attention) are the most commonly used attention mechanisms. They share the same and unified form of attention introduced above, but are different in how they compute the function f att . We use a modified MLP attention in our network to selectively pick the a i vector for computing the e ti , and not have any contextual vector for computing the attention weights. Since the annotation weights \u03b1 i are output of a softmax function, \u03b1 i are positive. Therefore, we preserve the combination matrix C in Equation 6 to be full rank. This property also ensures that we can stabilize the system if the velocity is available as a control input, i.e., kinematic control. This condition is satisfied by our design.\nWhile the visual features used as state provide means for robustness to camera and target calibration errors, the rigid-body dynamics of the camera ego-motion are highly coupled when expressed as target motion in the image plane. Therefore, a direct adaptive control approach provide better results. We consider a general formulation of the ground robot navigation as Markov decision process (MDP) and use a vanilla clipped proximate policy optimization algorithm [19]. In standard policy optimization, the policy to be learned is parametrized by the weights and bias parameters of the underlying neural network. In our case, the underlying neural network contains an attention mechanism in addition to more commonly used convolutional and dense layers. Therefore, the policy optimization solves for the full image error in the visual space while learning the optimal actions for navigation.\n\n3 Conclusion and Future Work\nMost traditional approaches to control robotic agents rely on extracting features from image or using non-image based sensors for state measurements. The optimal control algorithms that utilize pixel as a state typically need a simulation environment. The control policies trained in a specific simulation environment perform poorly in other environments with the same hardware model configuration. We have tackled two critical problems in our work on domain adaptation. First, we provided a theoretical foundation on the conditions under which joint perception and control will perform as well as the current state-of-the-art or better at lower computational complexity. Second, we implemented our approach for a mobile robot and empirically demonstrated the improvement.\nWe have proposed a new architecture (DACNN) that strives to attain joint perception and control by learning to focus through the lens of the CNN layers, and achieve zero-shot reinforcement learning, converging to an optimal policy that's transferable across perceptive differences in the environment. The attention model learns to capture the part of the image relevant to driving while the spherical geometry under which the image captures the real-time observation guarantees stable control under the passivity assumption.\nWe have demonstrated that additive attention can capture the focus required for optimal control theoretically in Section 2 and empirically in the context of autonomous-driving in Section 4. This is achieved by designing the context vector in the form of the full error in visual space with respect to the desired visual features. We empirically prove over comprehensive and targeted experiments in simulation and real world that this new mechanism provides a robust domain transfer performance across different textures, colors, and lighting. We have shown that our attention network performs at par or better compared to the current state-of-the-art method based on variational auto-encoders at lower computational complexity and the need to design extensive set of experiments with domain variation. Future work should also look to explore other attention mechanisms like self-attention [20], deep siamese attention [21] to have stronger capabilities to teach focus to the encoded features of our network.\n4 Supplementary Material\n\n4.1 Experiments and Results\nIn this section, we summarize our experimental results on sim2sim and sim2real. Our main conclusion is two-folds: 1) DACNN provides equivalent or better performance on domain transfer tasks with texture and light variation in the environment and 2) DACNN converges sooner in training compared to deeper neural networks with better performance, i.e., higher total reward, resulting in lower compute cost without degradation. The faster training is achieved by 1) not randomizing the model and environment conditions and 2) allowing the network to focus on jointly optimizing for perception and dynamics. There is no pre-training stage for learning the latent space representation for perception.\nAn in-depth discussion on our experiments and description of baselines adopted are included in the rest of the paper.\n\n4.1.1 Design of Experiments\nWe conducted two sets of experiments with increasing complexity:\nTask I -Time trial: For this task, the robocar is required to complete a given racing track as fast as possible without going off the track. The off-track condition is defined as when all the wheels of the car outside of the race track. Task II -Multi-car racing: For this task, the robocar is required to complete a given racing track as possible without crashing into two or more bot cars. The crash condition is defined as when the robocar is in the close vicnity of a bot car. The bot car is controlled by the simulation with no learning capability and models the worst case scenario by frequently changing the lanes to block the learner robocar. The number of the bot cars on the track is based on the length of the track.\nWe consider two sets of domain adaptation tasks in sim2sim and sim2real experiments. First, we use unseen color, texture, and lighting conditions in the evaluation phase and real track environment respectively but we keep the track shape the same in all experiments. Second, we modify the track shape as well as the color and texture.\nWe consider two baselines in both sim2sim and sim2real transfer experiments. First baseline has vanilla CNN layers to extract features without our attention mechanism. Second baseline is based on the DARLA approach where we use DARLA's \u03b2-VAE neural network to learn a latent state representation and then use the same output layers from our approach to learn the control task. Our implementation of DARLA is as described in [22], and applied to the autonomous vehicle use case.\nWe trained two baseline models with no domain adaptation, two models with DARLA using different \u03b2 values, two models with DACNN using different number of units, a total of 6 models. We used a vanilla policy optimization implementation and categorical exploration from a widely popular tool kit. We created an interface to our simulation environment which is open-source 2.\nWe performed evaluations on sim2sim and sim2real with textural variants -carpet, wood, concrete, and random lighting effects as seen in Figure 3 with five or more replications. Then, we changed the reward function from penalizing deviations from following center yellow, dotted line to penalizing crossing white lines on the inner and outer edge of the track, and repeated the experiments.\n\n4.1.2 Results: Task I -Time-trial\nIn total, we have more than 100+ and 20+ tests for sim2sim and sim2real respectively, to reach a statistically significant conclusion. Currently, we have several hundred hours of autonomous driving image, time-series, and event logs from the point-of-view of the car 3. In the simulation, we note that the baseline model was never able to finish on the concrete track. However the attention based model finished successfully on all surfaces and all of its converged iterations had significantly higher (80%+) completion rates. One interesting observation is that our deeper DACNN architecture achieved the highest cumulative reward. We anticipate that the deeper network implicitly extracts the non-linearities from vision-based controls implicitly. We summarize our sim2real experiment results in Table 1. In the real world, we observed DARLA and DACNN performed better than the baselines under lighting variations 4. Baseline I uses the probabilistic action space described for \"racetrack problem\" in [23] to account for uncertainty in dynamics, while Baseline II uses deterministic action decisions. We consider two types of DARLA models, one uses data augmentation and the other does not, when training the learn to see model. The data augmentation is performed by various perturbations such as shifting the camera image or adding Gaussian noise. DACNN models use a single layer and two-layer attention layers after vanilla input CNN layers. While our results in this task are preliminary, we observed that the reduced computational complexity transferred to the new task of avoiding moving objects. We used a different length and width track to accommodate up to three robocars without crashing into each other. For the same simulation configuration using five and three bot cars changing lanes at specific intervals, the DACNN model converged faster than the deep neural network but slower than the shallow network as shown in Figure 5 and Figure ??, respectively. However, the total reward achieved for the DACNN was higher than the shallow one and only slightly higher than the deeper network. The evaluation results on each\n\n4.1.4 Discussion\nWe use Gradient-weighted Class Activation Mapping (Grad-CAM) in [24] to visualize the impact of our baseline versus proposed approach on the image space prior to the output layers for control. Grad-CAM applies to CNNs used in reinforcement learning, without any architectural changes or re-training. Grad-CAM uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.\nIn Figure 7, we compared our baseline and basic DACNN model on the same image collected from the real world through the robocar's perspective. The warmer colors (yellow-red) correspond to focus areas where cooler colors (blue-purple) correspond to ignored areas. It is clear that the DACNN learns to focus on the track and not distracted by objects and surfaces outside the track. Note that both models are saved at the same iteration so that we can observe whether attention outperforms the baseline at same computational training step.\n\n4.2 Related Work\nVision-based servo control. In this paper, we consider image-based optimal control which is an extension of image-based visual servo (IBVS) control. Classic IBVS was developed for serial-link robotic manipulators [25, 26] and aims to control the dynamics of features in the image plane directly [27, 28]. More recent work on visual servoing focused on unmanned aerial vehicles (see [29] and reference therein). Our work differs from visual servoing most commonly used in unmanned aerial vehicles because we do not have a separate motion planning module. Our work is most similar to recent work on robotic manipulators [6, 7] in which the authors use raw (pixel) images as state for deep reinforcement learning. IBVS methods offer advantages in robustness to camera and target calibration errors, reduced computational complexity. One caveat of the classical IBVS is that it is necessary to determine the depth of each visual feature used in the image error criterion independently from the control algorithm. One of the approaches to overcome this issue is to use adaptive control, hence, the motivation to use reinforcement learning as a direct adaptive control method.\nDomain Adaptation for Robot Learning. In domain adaptation literature for robot learning, our approach is comparable to [5] where the authors propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. In our approach, we do not separate perception from dynamics but our intuition to create a latent attention space for dynamics is a common theme. Our approach differs from recent work on robotic manipulators because the state is based on image only and not augmented with other control state information such as position. Moreover, the use of attention network to create a latent space is new.\nIn [2], the authors apply domain adaptation at the feature level. In [3] and [4], the authors use domain and dynamics randomization, respectively.\nAttention Mechanisms in Reinforcement Learning. Attention models were applied with remarkable success to complex visual tasks such as image captioning [14] and machine translation [12]. However, attention models have mostly been applied to recurrent neural networks and not for optimal visual-servoing tasks. In [30] and [31], the authors use a recurrent neural network (RNN) which processes inputs sequentially and incrementally combines information to build up a dynamic internal representation of the scene or environment. We hypothesize that convolutional neural network (CNN) layers can capture local dependencies needed to create an approximate model for the optimal control task. Instead, our approach passes images sampled from a scene through multiple CNN layers prior to the attention network, hence, has the additional advantage of invariance to lighting and texture inherent in CNNs [32]. In other previous attempts to integrate attention with RL, the authors have largely used hand-crafted features as inputs to the attention model [33]. The hand-crafted features require a large number of hyper-parameters and are not invariant to lighting and texture. Our CNNbased attention network overcomes these challenges in lighting and texture. While it is possible to segment focus areas separately as described in [34], the delay caused by the model inference is too large to construct a stable controller.\n\nFootnotes:\n1: The rear wheels are fixed and do not steer\n2: https://github.com/aws-samples/aws-deepracer-workshops/tree/master/Advanced%20workshops/\n3: See racing competition links at https://driving-olympics.ai/\n4: A video of our findings from these experiments are attached to our submission.\n\nReferences:\n\n- J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, \"Sim-to-real: Learning agile locomotion for quadruped robots,\" CoRR, vol. abs/1804.10332, 2018. [Online]. Available: http://arxiv.org/abs/1804.10332- E. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell, \"Towards adapting deep visuomotor representations from simulated to real environments,\" CoRR, vol. abs/1511.07111, 2015. [Online]. Available: http://arxiv.org/abs/1511.07111\n\n- J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, \"Domain randomization for transferring deep neural networks from simulation to the real world,\" CoRR, vol. abs/1703.06907, 2017. [Online]. Available: http://arxiv.org/abs/1703.06907\n\n- I. Mordatch, K. Lowrey, and E. Todorov, \"Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids.\" in IROS. IEEE, 2015, pp. 5307-5314.\n\n- I. Higgins, A. Pal, A. A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner, \"DARLA: improving zero-shot transfer in reinforcement learning,\" in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, 2017, pp. 1480-1490. [Online]. Available: http://proceedings.mlr.press/v70/higgins17a.html\n\n- K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke, \"Using simulation and domain adaptation to improve efficiency of deep robotic grasping,\" CoRR, vol. abs/1709.07857, 2017. [Online]. Available: http://arxiv.org/abs/1709.07857\n\n- D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine, \"Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation,\" CoRR, vol. abs/1806.10293, 2018. [Online]. Available: http://arxiv.org/abs/1806.10293\n\n- S. James, A. J. Davison, and E. Johns, \"Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task,\" arXiv preprint arXiv:1707.02267, 2017.\n\n- M. Gualtieri and R. Platt, \"Learning 6-dof grasping and pick-place using attention focus,\" in Conference on Robot Learning, 2018, pp. 477-486.\n\n- A. A. Rusu, M. Ve\u010der\u00edk, T. Roth\u00f6rl, N. Heess, R. Pascanu, and R. Hadsell, \"Sim-to-real robot learning from pixels with progressive nets,\" in Conference on Robot Learning, 2017, pp. 262-270.\n\n- F. Golemo, A. A. Taiga, A. Courville, and P.-Y. Oudeyer, \"Sim-to-real transfer with neural- augmented robot simulation,\" in Conference on Robot Learning, 2018, pp. 817-828.\n\n- D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and translate,\" arXiv preprint arXiv:1409.0473, 2014.\n\n- M. Arcak, \"Passivity as a design tool for group coordination,\" IEEE Transactions on Automatic Control, vol. 52, no. 8, pp. 1380-1390, Aug 2007.\n\n- K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image caption generation with visual attention,\" arXiv preprint arXiv:1502.03044, 2015.\n\n- J. Ba, V. Mnih, and K. Kavukcuoglu, \"Multiple object recognition with visual attention,\" CoRR, vol. abs/1412.7755, 2015.\n\n- V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, \"Recurrent models of visual attention,\" CoRR, vol. abs/1406.6247, 2014. [Online]. Available: http://arxiv.org/abs/1406.6247\n\n- J. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli, \"Kinematic and dynamic vehicle models for autonomous driving control design,\" in 2015 IEEE Intelligent Vehicles Symposium (IV), June 2015, pp. 1094-1099.\n\n- T. Hamel and R. Mahony, \"Visual servoing of an under-actuated dynamic rigid-body system: an image-based approach,\" IEEE Transactions on Robotics and Automation, vol. 18, no. 2, pp. 187-198, April 2002.\n\n- J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \"Proximal policy optimization algorithms,\" arXiv preprint arXiv:1707.06347, 2017.\n\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\n- L. Wu, Y. Wang, J. Gao, and X. Li, \"Where-and-when to look: Deep siamese attention networks for video-based person re-identification,\" IEEE Transactions on Multimedia, 2018.\n\n- I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner, \"Darla: Improving zero-shot transfer in reinforcement learning,\" in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 1480-1490.\n\n- A. G. Barto, S. J. Bradtke, and S. P. Singh, \"Learning to act using real-time dynamic programming,\" Artificial Intelligence, vol. 72, no. 1, pp. 81 -138, 1995. [Online]. Available: http://www.sciencedirect.com/science/article/pii/000437029400011O\n\n- R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, \"Grad-CAM: Visual explanations from deep networks via gradient-based localization,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 618-626.\n\n- L. Weiss, A. Sanderson, and C. Neuman, \"Dynamic sensor-based control of robots with visual feedback,\" IEEE Journal on Robotics and Automation, vol. 3, no. 5, pp. 404-417, October 1987.\n\n- B. Espiau, F. Chaumette, and P. Rives, \"A new approach to visual servoing in robotics,\" IEEE Transactions on Robotics and Automation, vol. 8, no. 3, pp. 313-326, June 1992.\n\n- S. Hutchinson, G. D. Hager, and P. I. Corke, \"A tutorial on visual servo control,\" IEEE Transactions on Robotics and Automation, vol. 12, no. 5, pp. 651-670, Oct 1996.\n\n- F. Chaumette and S. Hutchinson, \"Visual servo control. ii. advanced approaches [tutorial],\" IEEE Robotics Automation Magazine, vol. 14, no. 1, pp. 109-118, March 2007.\n\n- Y. Lu, Z. Xue, G.-S. Xia, and L. Zhang, \"A survey on vision-based uav navigation,\" Geo-spatial Information Science, vol. 21, no. 1, pp. 21-32, 2018.\n\n- V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, \"Recurrent models of visual attention,\" CoRR, vol. abs/1406.6247, 2014. [Online]. Available: http://arxiv.org/abs/1406.6247\n\n- X. Liang, Q. Wang, Y. Feng, Z. Liu, and J. Huang, \"VMAV-C: A deep attention-based reinforcement learning algorithm for model-based control,\" CoRR, vol. abs/1812.09968, 2018. [Online]. Available: http://arxiv.org/abs/1812.09968\n\n- Y. LeCun and Y. Bengio, \"The handbook of brain theory and neural networks,\" M. A. Arbib, Ed. Cambridge, MA, USA: MIT Press, 1998, ch. Convolutional Networks for Images, Speech, and Time Series, pp. 255-258. [Online]. Available: http://dl.acm.org/citation.cfm?id=303568.303704\n\n- A. Manchin, E. Abbasnejad, and A. van den Hengel, \"Reinforcement learning with attention that works: A self-supervised approach,\" CoRR, vol. abs/1904.03367, 2019. [Online]. Available: http://arxiv.org/abs/1904.03367\n\n- J. Choi, B. Lee, and B. Zhang, \"Multi-focus attention network for efficient deep reinforcement learning,\" CoRR, vol. abs/1712.04603, 2017. [Online]. Available: http: //arxiv.org/abs/1712.04603\n\n", "annotations": {"ReferenceToTable": [{"begin": 22636, "end": 22637, "target": "#tab_0", "idx": 0}], "ReferenceToFootnote": [{"begin": 9303, "end": 9304, "target": "#foot_0", "idx": 0}, {"begin": 21404, "end": 21405, "target": "#foot_1", "idx": 1}, {"begin": 22099, "end": 22100, "target": "#foot_2", "idx": 2}, {"begin": 22748, "end": 22749, "target": "#foot_3", "idx": 3}], "SectionMain": [{"begin": 1730, "end": 28445, "idx": 0}], "ReferenceToFormula": [{"begin": 15108, "end": 15109, "target": "#formula_1", "idx": 0}], "SectionReference": [{"begin": 28743, "end": 35919, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1730, "idx": 0}], "Div": [{"begin": 94, "end": 1722, "idx": 0}, {"begin": 1733, "end": 2442, "idx": 1}, {"begin": 2444, "end": 5511, "idx": 2}, {"begin": 5513, "end": 16195, "idx": 3}, {"begin": 16197, "end": 18556, "idx": 4}, {"begin": 18558, "end": 19398, "idx": 5}, {"begin": 19400, "end": 21796, "idx": 6}, {"begin": 21798, "end": 23964, "idx": 7}, {"begin": 23966, "end": 25027, "idx": 8}, {"begin": 25029, "end": 28445, "idx": 9}], "Head": [{"begin": 1733, "end": 1747, "n": "1", "idx": 0}, {"begin": 2444, "end": 2481, "idx": 1}, {"begin": 5513, "end": 5571, "n": "2", "idx": 2}, {"begin": 16197, "end": 16225, "n": "3", "idx": 3}, {"begin": 18558, "end": 18585, "n": "4.1", "idx": 4}, {"begin": 19400, "end": 19427, "n": "4.1.1", "idx": 5}, {"begin": 21798, "end": 21831, "n": "4.1.2", "idx": 6}, {"begin": 23966, "end": 23982, "n": "4.1.4", "idx": 7}, {"begin": 25029, "end": 25045, "n": "4.2", "idx": 8}], "Paragraph": [{"begin": 94, "end": 1722, "idx": 0}, {"begin": 1748, "end": 2350, "idx": 1}, {"begin": 2351, "end": 2442, "idx": 2}, {"begin": 2482, "end": 3064, "idx": 3}, {"begin": 3065, "end": 3512, "idx": 4}, {"begin": 3513, "end": 5111, "idx": 5}, {"begin": 5112, "end": 5511, "idx": 6}, {"begin": 5572, "end": 6040, "idx": 7}, {"begin": 6041, "end": 6958, "idx": 8}, {"begin": 6959, "end": 7576, "idx": 9}, {"begin": 7577, "end": 8527, "idx": 10}, {"begin": 8528, "end": 9035, "idx": 11}, {"begin": 9143, "end": 9529, "idx": 12}, {"begin": 9530, "end": 10155, "idx": 13}, {"begin": 10156, "end": 10794, "idx": 14}, {"begin": 10795, "end": 10981, "idx": 15}, {"begin": 10982, "end": 11493, "idx": 16}, {"begin": 11494, "end": 11556, "idx": 17}, {"begin": 11557, "end": 12598, "idx": 18}, {"begin": 12621, "end": 12823, "idx": 19}, {"begin": 12852, "end": 13587, "idx": 20}, {"begin": 13601, "end": 14076, "idx": 21}, {"begin": 14111, "end": 14116, "idx": 22}, {"begin": 14145, "end": 14419, "idx": 23}, {"begin": 14420, "end": 15303, "idx": 24}, {"begin": 15304, "end": 16195, "idx": 25}, {"begin": 16226, "end": 16998, "idx": 26}, {"begin": 16999, "end": 17523, "idx": 27}, {"begin": 17524, "end": 18531, "idx": 28}, {"begin": 18532, "end": 18556, "idx": 29}, {"begin": 18586, "end": 19280, "idx": 30}, {"begin": 19281, "end": 19398, "idx": 31}, {"begin": 19428, "end": 19492, "idx": 32}, {"begin": 19493, "end": 20220, "idx": 33}, {"begin": 20221, "end": 20555, "idx": 34}, {"begin": 20556, "end": 21033, "idx": 35}, {"begin": 21034, "end": 21406, "idx": 36}, {"begin": 21407, "end": 21796, "idx": 37}, {"begin": 21832, "end": 23964, "idx": 38}, {"begin": 23983, "end": 24489, "idx": 39}, {"begin": 24490, "end": 25027, "idx": 40}, {"begin": 25046, "end": 26216, "idx": 41}, {"begin": 26217, "end": 26884, "idx": 42}, {"begin": 26885, "end": 27031, "idx": 43}, {"begin": 27032, "end": 28445, "idx": 44}], "ReferenceToBib": [{"begin": 2027, "end": 2030, "target": "#b0", "idx": 0}, {"begin": 2581, "end": 2584, "target": "#b1", "idx": 1}, {"begin": 2647, "end": 2650, "target": "#b2", "idx": 2}, {"begin": 2655, "end": 2658, "target": "#b3", "idx": 3}, {"begin": 2729, "end": 2732, "target": "#b4", "idx": 4}, {"begin": 2953, "end": 2956, "target": "#b5", "idx": 5}, {"begin": 2957, "end": 2960, "target": "#b6", "idx": 6}, {"begin": 2961, "end": 2964, "target": "#b7", "idx": 7}, {"begin": 2965, "end": 2968, "target": "#b8", "idx": 8}, {"begin": 2969, "end": 2973, "target": "#b9", "idx": 9}, {"begin": 2974, "end": 2978, "target": "#b10", "idx": 10}, {"begin": 4359, "end": 4363, "target": "#b11", "idx": 11}, {"begin": 5106, "end": 5110, "target": "#b12", "idx": 12}, {"begin": 6696, "end": 6700, "target": "#b13", "idx": 13}, {"begin": 6701, "end": 6704, "target": "#b11", "idx": 14}, {"begin": 6705, "end": 6708, "target": "#b14", "idx": 15}, {"begin": 6709, "end": 6712, "target": "#b15", "idx": 16}, {"begin": 7015, "end": 7019, "target": "#b13", "idx": 17}, {"begin": 8101, "end": 8105, "target": "#b16", "idx": 18}, {"begin": 9029, "end": 9033, "target": "#b16", "idx": 19}, {"begin": 11013, "end": 11017, "target": "#b17", "idx": 20}, {"begin": 13213, "end": 13217, "target": "#b13", "idx": 21}, {"begin": 15768, "end": 15772, "target": "#b18", "idx": 22}, {"begin": 18413, "end": 18417, "target": "#b19", "idx": 23}, {"begin": 18442, "end": 18446, "target": "#b20", "idx": 24}, {"begin": 20980, "end": 20984, "target": "#b21", "idx": 25}, {"begin": 22835, "end": 22839, "target": "#b22", "idx": 26}, {"begin": 24047, "end": 24051, "target": "#b23", "idx": 27}, {"begin": 25259, "end": 25263, "target": "#b24", "idx": 28}, {"begin": 25264, "end": 25267, "target": "#b25", "idx": 29}, {"begin": 25341, "end": 25345, "target": "#b26", "idx": 30}, {"begin": 25346, "end": 25349, "target": "#b27", "idx": 31}, {"begin": 25428, "end": 25432, "target": "#b28", "idx": 32}, {"begin": 25664, "end": 25667, "target": "#b5", "idx": 33}, {"begin": 25668, "end": 25670, "target": "#b6", "idx": 34}, {"begin": 26337, "end": 26340, "target": "#b4", "idx": 35}, {"begin": 26888, "end": 26891, "target": "#b1", "idx": 36}, {"begin": 26954, "end": 26957, "target": "#b2", "idx": 37}, {"begin": 26962, "end": 26965, "target": "#b3", "idx": 38}, {"begin": 27183, "end": 27187, "target": "#b13", "idx": 39}, {"begin": 27212, "end": 27216, "target": "#b11", "idx": 40}, {"begin": 27344, "end": 27348, "target": "#b29", "idx": 41}, {"begin": 27353, "end": 27357, "target": "#b30", "idx": 42}, {"begin": 27927, "end": 27931, "target": "#b31", "idx": 43}, {"begin": 28077, "end": 28081, "target": "#b32", "idx": 44}, {"begin": 28353, "end": 28357, "target": "#b33", "idx": 45}], "ReferenceString": [{"begin": 28758, "end": 28993, "id": "b0", "idx": 0}, {"begin": 28995, "end": 29256, "id": "b1", "idx": 1}, {"begin": 29260, "end": 29509, "id": "b2", "idx": 2}, {"begin": 29513, "end": 29676, "id": "b3", "idx": 3}, {"begin": 29680, "end": 30073, "id": "b4", "idx": 4}, {"begin": 30077, "end": 30397, "id": "b5", "idx": 5}, {"begin": 30401, "end": 30707, "id": "b6", "idx": 6}, {"begin": 30711, "end": 30887, "id": "b7", "idx": 7}, {"begin": 30891, "end": 31033, "id": "b8", "idx": 8}, {"begin": 31037, "end": 31226, "id": "b9", "idx": 9}, {"begin": 31230, "end": 31402, "id": "b10", "idx": 10}, {"begin": 31406, "end": 31552, "id": "b11", "idx": 11}, {"begin": 31556, "end": 31699, "id": "b12", "idx": 12}, {"begin": 31703, "end": 31910, "id": "b13", "idx": 13}, {"begin": 31914, "end": 32034, "id": "b14", "idx": 14}, {"begin": 32038, "end": 32211, "id": "b15", "idx": 15}, {"begin": 32215, "end": 32422, "id": "b16", "idx": 16}, {"begin": 32426, "end": 32627, "id": "b17", "idx": 17}, {"begin": 32631, "end": 32776, "id": "b18", "idx": 18}, {"begin": 32780, "end": 32985, "id": "b19", "idx": 19}, {"begin": 32989, "end": 33162, "id": "b20", "idx": 20}, {"begin": 33166, "end": 33454, "id": "b21", "idx": 21}, {"begin": 33458, "end": 33704, "id": "b22", "idx": 22}, {"begin": 33708, "end": 33958, "id": "b23", "idx": 23}, {"begin": 33962, "end": 34146, "id": "b24", "idx": 24}, {"begin": 34150, "end": 34322, "id": "b25", "idx": 25}, {"begin": 34326, "end": 34493, "id": "b26", "idx": 26}, {"begin": 34497, "end": 34664, "id": "b27", "idx": 27}, {"begin": 34668, "end": 34816, "id": "b28", "idx": 28}, {"begin": 34820, "end": 34993, "id": "b29", "idx": 29}, {"begin": 34997, "end": 35223, "id": "b30", "idx": 30}, {"begin": 35227, "end": 35502, "id": "b31", "idx": 31}, {"begin": 35506, "end": 35721, "id": "b32", "idx": 32}, {"begin": 35725, "end": 35917, "id": "b33", "idx": 33}], "Sentence": [{"begin": 94, "end": 212, "idx": 0}, {"begin": 213, "end": 513, "idx": 1}, {"begin": 514, "end": 750, "idx": 2}, {"begin": 751, "end": 1044, "idx": 3}, {"begin": 1045, "end": 1251, "idx": 4}, {"begin": 1252, "end": 1404, "idx": 5}, {"begin": 1405, "end": 1531, "idx": 6}, {"begin": 1532, "end": 1722, "idx": 7}, {"begin": 1748, "end": 2055, "idx": 8}, {"begin": 2056, "end": 2168, "idx": 9}, {"begin": 2169, "end": 2350, "idx": 10}, {"begin": 2351, "end": 2442, "idx": 11}, {"begin": 2482, "end": 2577, "idx": 12}, {"begin": 2578, "end": 2643, "idx": 13}, {"begin": 2644, "end": 2725, "idx": 14}, {"begin": 2726, "end": 2877, "idx": 15}, {"begin": 2878, "end": 3064, "idx": 16}, {"begin": 3065, "end": 3246, "idx": 17}, {"begin": 3247, "end": 3354, "idx": 18}, {"begin": 3355, "end": 3512, "idx": 19}, {"begin": 3513, "end": 3621, "idx": 20}, {"begin": 3622, "end": 3693, "idx": 21}, {"begin": 3694, "end": 3912, "idx": 22}, {"begin": 3913, "end": 4032, "idx": 23}, {"begin": 4033, "end": 4152, "idx": 24}, {"begin": 4153, "end": 4298, "idx": 25}, {"begin": 4299, "end": 4364, "idx": 26}, {"begin": 4365, "end": 4500, "idx": 27}, {"begin": 4501, "end": 4584, "idx": 28}, {"begin": 4585, "end": 4735, "idx": 29}, {"begin": 4736, "end": 4882, "idx": 30}, {"begin": 4883, "end": 5004, "idx": 31}, {"begin": 5005, "end": 5111, "idx": 32}, {"begin": 5112, "end": 5284, "idx": 33}, {"begin": 5285, "end": 5409, "idx": 34}, {"begin": 5410, "end": 5511, "idx": 35}, {"begin": 5572, "end": 5690, "idx": 36}, {"begin": 5691, "end": 5766, "idx": 37}, {"begin": 5767, "end": 5908, "idx": 38}, {"begin": 5909, "end": 6040, "idx": 39}, {"begin": 6041, "end": 6377, "idx": 40}, {"begin": 6378, "end": 6483, "idx": 41}, {"begin": 6484, "end": 6615, "idx": 42}, {"begin": 6616, "end": 6713, "idx": 43}, {"begin": 6714, "end": 6867, "idx": 44}, {"begin": 6868, "end": 6958, "idx": 45}, {"begin": 6959, "end": 7120, "idx": 46}, {"begin": 7121, "end": 7231, "idx": 47}, {"begin": 7232, "end": 7358, "idx": 48}, {"begin": 7359, "end": 7576, "idx": 49}, {"begin": 7577, "end": 7778, "idx": 50}, {"begin": 7779, "end": 7910, "idx": 51}, {"begin": 7911, "end": 8106, "idx": 52}, {"begin": 8107, "end": 8190, "idx": 53}, {"begin": 8191, "end": 8256, "idx": 54}, {"begin": 8257, "end": 8387, "idx": 55}, {"begin": 8388, "end": 8462, "idx": 56}, {"begin": 8463, "end": 8527, "idx": 57}, {"begin": 8528, "end": 8655, "idx": 58}, {"begin": 8656, "end": 8897, "idx": 59}, {"begin": 8898, "end": 9035, "idx": 60}, {"begin": 9143, "end": 9305, "idx": 61}, {"begin": 9306, "end": 9451, "idx": 62}, {"begin": 9452, "end": 9529, "idx": 63}, {"begin": 9530, "end": 9630, "idx": 64}, {"begin": 9631, "end": 9759, "idx": 65}, {"begin": 9760, "end": 9964, "idx": 66}, {"begin": 9965, "end": 10155, "idx": 67}, {"begin": 10156, "end": 10251, "idx": 68}, {"begin": 10252, "end": 10402, "idx": 69}, {"begin": 10403, "end": 10492, "idx": 70}, {"begin": 10493, "end": 10625, "idx": 71}, {"begin": 10626, "end": 10794, "idx": 72}, {"begin": 10795, "end": 10807, "idx": 73}, {"begin": 10808, "end": 10981, "idx": 74}, {"begin": 10982, "end": 11018, "idx": 75}, {"begin": 11019, "end": 11160, "idx": 76}, {"begin": 11161, "end": 11407, "idx": 77}, {"begin": 11408, "end": 11493, "idx": 78}, {"begin": 11494, "end": 11556, "idx": 79}, {"begin": 11557, "end": 11808, "idx": 80}, {"begin": 11809, "end": 11876, "idx": 81}, {"begin": 11877, "end": 12035, "idx": 82}, {"begin": 12036, "end": 12113, "idx": 83}, {"begin": 12114, "end": 12258, "idx": 84}, {"begin": 12259, "end": 12326, "idx": 85}, {"begin": 12327, "end": 12487, "idx": 86}, {"begin": 12488, "end": 12598, "idx": 87}, {"begin": 12621, "end": 12739, "idx": 88}, {"begin": 12740, "end": 12823, "idx": 89}, {"begin": 12852, "end": 12940, "idx": 90}, {"begin": 12941, "end": 13124, "idx": 91}, {"begin": 13125, "end": 13218, "idx": 92}, {"begin": 13219, "end": 13348, "idx": 93}, {"begin": 13349, "end": 13473, "idx": 94}, {"begin": 13474, "end": 13531, "idx": 95}, {"begin": 13532, "end": 13587, "idx": 96}, {"begin": 13601, "end": 13936, "idx": 97}, {"begin": 13937, "end": 14023, "idx": 98}, {"begin": 14024, "end": 14076, "idx": 99}, {"begin": 14111, "end": 14116, "idx": 100}, {"begin": 14145, "end": 14233, "idx": 101}, {"begin": 14234, "end": 14339, "idx": 102}, {"begin": 14340, "end": 14419, "idx": 103}, {"begin": 14420, "end": 14490, "idx": 104}, {"begin": 14491, "end": 14656, "idx": 105}, {"begin": 14657, "end": 14783, "idx": 106}, {"begin": 14784, "end": 14962, "idx": 107}, {"begin": 14963, "end": 15047, "idx": 108}, {"begin": 15048, "end": 15126, "idx": 109}, {"begin": 15127, "end": 15260, "idx": 110}, {"begin": 15261, "end": 15303, "idx": 111}, {"begin": 15304, "end": 15533, "idx": 112}, {"begin": 15534, "end": 15603, "idx": 113}, {"begin": 15604, "end": 15773, "idx": 114}, {"begin": 15774, "end": 15916, "idx": 115}, {"begin": 15917, "end": 16057, "idx": 116}, {"begin": 16058, "end": 16195, "idx": 117}, {"begin": 16226, "end": 16375, "idx": 118}, {"begin": 16376, "end": 16477, "idx": 119}, {"begin": 16478, "end": 16624, "idx": 120}, {"begin": 16625, "end": 16696, "idx": 121}, {"begin": 16697, "end": 16897, "idx": 122}, {"begin": 16898, "end": 16998, "idx": 123}, {"begin": 16999, "end": 17299, "idx": 124}, {"begin": 17300, "end": 17523, "idx": 125}, {"begin": 17524, "end": 17853, "idx": 126}, {"begin": 17854, "end": 18066, "idx": 127}, {"begin": 18067, "end": 18325, "idx": 128}, {"begin": 18326, "end": 18531, "idx": 129}, {"begin": 18532, "end": 18556, "idx": 130}, {"begin": 18586, "end": 18665, "idx": 131}, {"begin": 18666, "end": 19009, "idx": 132}, {"begin": 19010, "end": 19188, "idx": 133}, {"begin": 19189, "end": 19280, "idx": 134}, {"begin": 19281, "end": 19398, "idx": 135}, {"begin": 19428, "end": 19492, "idx": 136}, {"begin": 19493, "end": 19633, "idx": 137}, {"begin": 19634, "end": 19729, "idx": 138}, {"begin": 19730, "end": 19884, "idx": 139}, {"begin": 19885, "end": 19973, "idx": 140}, {"begin": 19974, "end": 20143, "idx": 141}, {"begin": 20144, "end": 20220, "idx": 142}, {"begin": 20221, "end": 20305, "idx": 143}, {"begin": 20306, "end": 20487, "idx": 144}, {"begin": 20488, "end": 20555, "idx": 145}, {"begin": 20556, "end": 20632, "idx": 146}, {"begin": 20633, "end": 20723, "idx": 147}, {"begin": 20724, "end": 20932, "idx": 148}, {"begin": 20933, "end": 21033, "idx": 149}, {"begin": 21034, "end": 21215, "idx": 150}, {"begin": 21216, "end": 21328, "idx": 151}, {"begin": 21329, "end": 21406, "idx": 152}, {"begin": 21407, "end": 21583, "idx": 153}, {"begin": 21584, "end": 21796, "idx": 154}, {"begin": 21832, "end": 21966, "idx": 155}, {"begin": 21967, "end": 22101, "idx": 156}, {"begin": 22102, "end": 22200, "idx": 157}, {"begin": 22201, "end": 22358, "idx": 158}, {"begin": 22359, "end": 22464, "idx": 159}, {"begin": 22465, "end": 22581, "idx": 160}, {"begin": 22582, "end": 22638, "idx": 161}, {"begin": 22639, "end": 22750, "idx": 162}, {"begin": 22751, "end": 22934, "idx": 163}, {"begin": 22935, "end": 23062, "idx": 164}, {"begin": 23063, "end": 23182, "idx": 165}, {"begin": 23183, "end": 23277, "idx": 166}, {"begin": 23278, "end": 23435, "idx": 167}, {"begin": 23436, "end": 23548, "idx": 168}, {"begin": 23549, "end": 23802, "idx": 169}, {"begin": 23803, "end": 23933, "idx": 170}, {"begin": 23934, "end": 23964, "idx": 171}, {"begin": 23983, "end": 24175, "idx": 172}, {"begin": 24176, "end": 24282, "idx": 173}, {"begin": 24283, "end": 24489, "idx": 174}, {"begin": 24490, "end": 24632, "idx": 175}, {"begin": 24633, "end": 24752, "idx": 176}, {"begin": 24753, "end": 24870, "idx": 177}, {"begin": 24871, "end": 25027, "idx": 178}, {"begin": 25046, "end": 25073, "idx": 179}, {"begin": 25074, "end": 25194, "idx": 180}, {"begin": 25195, "end": 25350, "idx": 181}, {"begin": 25351, "end": 25456, "idx": 182}, {"begin": 25457, "end": 25599, "idx": 183}, {"begin": 25600, "end": 25756, "idx": 184}, {"begin": 25757, "end": 25875, "idx": 185}, {"begin": 25876, "end": 26054, "idx": 186}, {"begin": 26055, "end": 26216, "idx": 187}, {"begin": 26217, "end": 26254, "idx": 188}, {"begin": 26255, "end": 26490, "idx": 189}, {"begin": 26491, "end": 26636, "idx": 190}, {"begin": 26637, "end": 26812, "idx": 191}, {"begin": 26813, "end": 26884, "idx": 192}, {"begin": 26885, "end": 26950, "idx": 193}, {"begin": 26951, "end": 27031, "idx": 194}, {"begin": 27032, "end": 27079, "idx": 195}, {"begin": 27080, "end": 27217, "idx": 196}, {"begin": 27218, "end": 27340, "idx": 197}, {"begin": 27341, "end": 27557, "idx": 198}, {"begin": 27558, "end": 27718, "idx": 199}, {"begin": 27719, "end": 27932, "idx": 200}, {"begin": 27933, "end": 28082, "idx": 201}, {"begin": 28083, "end": 28198, "idx": 202}, {"begin": 28199, "end": 28281, "idx": 203}, {"begin": 28282, "end": 28445, "idx": 204}], "ReferenceToFigure": [{"begin": 3691, "end": 3692, "idx": 0}, {"begin": 5119, "end": 5120, "idx": 1}, {"begin": 5688, "end": 5689, "idx": 2}, {"begin": 11501, "end": 11502, "idx": 3}, {"begin": 11665, "end": 11666, "idx": 4}, {"begin": 21550, "end": 21551, "target": "#fig_0", "idx": 5}, {"begin": 23772, "end": 23773, "target": "#fig_2", "idx": 6}, {"begin": 24500, "end": 24501, "target": "#fig_4", "idx": 7}], "Abstract": [{"begin": 84, "end": 1722, "idx": 0}], "SectionFootnote": [{"begin": 28447, "end": 28741, "idx": 0}], "Footnote": [{"begin": 28458, "end": 28503, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 28504, "end": 28595, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 28596, "end": 28659, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 28660, "end": 28741, "id": "foot_3", "n": "4", "idx": 3}], "ScholarlyEntity": [{"label": "Task", "begin": 94, "end": 118, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["Sim", "ulation", "-", "to", "-", "sim", "ulation"], "seq_scores": [0.6232102513313293, 0.6775181293487549, 0.6751257181167603, 0.6753073930740356, 0.6943577527999878, 0.5806601047515869, 0.6371329426765442], "text": "Simulation-to-simulation", "score": 0.6519017560141427, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 122, "end": 156, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120simulation", "-", "to", "-", "real", "\u0120world", "\u0120transfer"], "seq_scores": [0.6047003269195557, 0.884418249130249, 0.8792797327041626, 0.905187726020813, 0.8661162853240967, 0.8602965474128723, 0.8903783559799194], "text": " simulation-to-real world transfer", "score": 0.8414824604988098, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 255, "end": 289, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120simulation", "-", "to", "-", "real", "\u0120world", "\u0120transfer"], "seq_scores": [0.8564763069152832, 0.8462455868721008, 0.8916466236114502, 0.9253859519958496, 0.887765109539032, 0.903490424156189, 0.9254326224327087], "text": " simulation-to-real world transfer", "score": 0.890920375074659, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 300, "end": 318, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.7569566965103149, 0.8572012782096863], "text": " domain adaptation", "score": 0.8070789873600006, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 319, "end": 354, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120dec", "ou", "pling", "\u0120perception", "\u0120and", "\u0120dynamics"], "seq_scores": [0.9169638156890869, 0.9493590593338013, 0.9473884105682373, 0.8872813582420349, 0.654588520526886, 0.8573358058929443], "text": " decoupling perception and dynamics", "score": 0.8688194950421652, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 395, "end": 418, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120random", "ization", "\u0120of", "\u0120agent"], "seq_scores": [0.9006176590919495, 0.9134470224380493, 0.5073465704917908, 0.5783296823501587], "text": " randomization of agent", "score": 0.7249352335929871, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 698, "end": 717, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120autonomous", "\u0120driving"], "seq_scores": [0.8423234224319458, 0.8480796813964844], "text": " autonomous driving", "score": 0.8452015519142151, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 720, "end": 741, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120robotic", "\u0120manipulation"], "seq_scores": [0.6094105243682861, 0.7746688723564148], "text": " robotic manipulation", "score": 0.6920396983623505, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 832, "end": 876, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120attention", "\u0120conv", "olution", "al", "\u0120neural", "\u0120network"], "seq_scores": [0.9019424915313721, 0.9841874837875366, 0.9873630404472351, 0.985471785068512, 0.9840706586837769, 0.9825731515884399, 0.980241060256958], "text": " deep attention convolutional neural network", "score": 0.9722642387662616, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 878, "end": 883, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["D", "AC", "NN"], "seq_scores": [0.9157547950744629, 0.9621343612670898, 0.9610433578491211], "text": "DACNN", "score": 0.946310838063558, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 1062, "end": 1080, "seq_label": ["I-Method", "I-Method", "I-Method", "B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120visual", "\u0120sensor", "\u0120configuration", "\u0120attention", "\u0120network"], "seq_scores": [0.5487011075019836, 0.7184872627258301, 0.5711855292320251, 0.3618253171443939, 0.7440195679664612], "text": " attention network", "score": 0.5888437569141388, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1108, "end": 1128, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120policy", "\u0120optimization"], "seq_scores": [0.9533454775810242, 0.9478782415390015], "text": " policy optimization", "score": 0.9506118595600128, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1235, "end": 1250, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120general", "ization"], "seq_scores": [0.6828200221061707, 0.5956425070762634], "text": " generalization", "score": 0.639231264591217, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1342, "end": 1361, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120learning"], "seq_scores": [0.980067789554596, 0.9840478301048279, 0.9843289852142334, 0.9837642312049866], "text": " zero-shot learning", "score": 0.983052209019661, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1369, "end": 1382, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120pre", "-", "training"], "seq_scores": [0.9348652958869934, 0.9672812223434448, 0.9654315710067749], "text": " pre-training", "score": 0.955859363079071, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1466, "end": 1484, "seq_label": ["I-ModelArchitecture", "B-Task", "I-Task"], "seq_token": ["\u0120network", "\u0120domain", "\u0120adaptation"], "seq_scores": [0.621986985206604, 0.9238108396530151, 0.9496119618415833], "text": " domain adaptation", "score": 0.8318032622337341, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1497, "end": 1516, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120autonomous", "\u0120driving"], "seq_scores": [0.9544593691825867, 0.953605592250824], "text": " autonomous driving", "score": 0.9540324807167053, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1577, "end": 1602, "seq_label": ["B-Task", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120simulation", "-", "to", "-", "sim", "ulation"], "seq_scores": [0.526123583316803, 0.5152849555015564, 0.5217876434326172, 0.4978293180465698, 0.5785468816757202, 0.5769686102867126], "text": " simulation-to-simulation", "score": 0.5360901653766632, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1606, "end": 1624, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120simulation", "-", "t", "oreal"], "seq_scores": [0.2796180844306946, 0.5033026337623596, 0.4756023585796356, 0.49048900604248047], "text": " simulation-toreal", "score": 0.43725302070379257, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 159, "end": 181, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120neural", "\u0120network", "\u0120models"], "seq_scores": [0.98776775598526, 0.98870450258255, 0.9821493029594421], "text": " neural network models", "score": 0.9862071871757507, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 958, "end": 968, "seq_label": ["I-MLModelGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120learning", "\u0120a", "\u0120dataset"], "seq_scores": [0.5130711197853088, 0.9925388693809509, 0.9917234778404236], "text": " a dataset", "score": 0.8324444890022278, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1382, "end": 1384, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.5083361864089966], "text": " a", "score": 0.5083361864089966, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1661, "end": 1679, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120network", "\u0120approach", "\u0120several", "\u0120bas", "elines"], "seq_scores": [0.5983971953392029, 0.581696629524231, 0.6789577007293701, 0.8552879691123962, 0.923812747001648], "text": " several baselines", "score": 0.7276304483413696, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 1689, "end": 1721, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120current", "\u0120state", "-", "of", "-", "art", "\u0120models"], "seq_scores": [0.9378898739814758, 0.9761186838150024, 0.9891660809516907, 0.9964575171470642, 0.9963036775588989, 0.9966397285461426, 0.9964642524719238, 0.9939807653427124], "text": " the current state-of-art models", "score": 0.9853775724768639, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1778, "end": 1806, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120deep", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.9734672904014587, 0.983251690864563, 0.9701491594314575], "text": " deep reinforcement learning", "score": 0.9756227135658264, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1929, "end": 1940, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120locom", "otion"], "seq_scores": [0.7957656979560852, 0.853042721748352], "text": " locomotion", "score": 0.8244042098522186, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1941, "end": 1957, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120motion", "\u0120planning"], "seq_scores": [0.7590405941009521, 0.9011424779891968], "text": " motion planning", "score": 0.8300915360450745, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2028, "end": 2029, "seq_label": ["I-Task", "I-Task", "B-ReferenceLink"], "seq_token": ["arm", "\u0120manipulation", "1"], "seq_scores": [0.5545022487640381, 0.5807509422302246, 0.9914775490760803], "text": "1", "score": 0.7089102466901144, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2313, "end": 2324, "seq_label": ["B-Method"], "seq_token": ["\u0120additional"], "seq_scores": [0.5009633898735046], "text": " additional", "score": 0.5009633898735046, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2513, "end": 2546, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.9869074821472168, 0.9882693290710449, 0.9900586009025574, 0.9904887676239014, 0.9845070838928223], "text": " zero-shot reinforcement learning", "score": 0.9880462527275086, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2582, "end": 2583, "seq_label": ["B-ReferenceLink"], "seq_token": ["2"], "seq_scores": [0.9944274425506592], "text": "2", "score": 0.9944274425506592, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2603, "end": 2621, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.9671174883842468, 0.9799163341522217], "text": " domain adaptation", "score": 0.9735169112682343, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2648, "end": 2649, "seq_label": ["B-ReferenceLink"], "seq_token": ["3"], "seq_scores": [0.9944829344749451], "text": "3", "score": 0.9944829344749451, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2656, "end": 2657, "seq_label": ["B-ReferenceLink"], "seq_token": ["4"], "seq_scores": [0.9949900507926941], "text": "4", "score": 0.9949900507926941, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2676, "end": 2710, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120and", "\u0120dynamics", "\u0120random", "ization"], "seq_scores": [0.9811574816703796, 0.9754422307014465, 0.9740949869155884, 0.9896656274795532, 0.9904746413230896], "text": " domain and dynamics randomization", "score": 0.9821669936180115, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2730, "end": 2731, "seq_label": ["B-ReferenceLink"], "seq_token": ["5"], "seq_scores": [0.9956855773925781], "text": "5", "score": 0.9956855773925781, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 2781, "end": 2787, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9648910164833069, 0.9651363492012024], "text": " DARLA", "score": 0.9650136828422546, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 2789, "end": 2831, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["Dis", "ent", "Ang", "led", "\u0120Represent", "ation", "\u0120Learning", "\u0120Agent"], "seq_scores": [0.9285430908203125, 0.9072473049163818, 0.9303303360939026, 0.9502391815185547, 0.9492861032485962, 0.9473162889480591, 0.9514879584312439, 0.940739095211029], "text": "DisentAngled Representation Learning Agent", "score": 0.93814866989851, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 2892, "end": 2910, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.9472305178642273, 0.9226898550987244], "text": " domain adaptation", "score": 0.9349601864814758, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2954, "end": 2955, "seq_label": ["B-ReferenceLink"], "seq_token": ["6"], "seq_scores": [0.9961375594139099], "text": "6", "score": 0.9961375594139099, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2958, "end": 2959, "seq_label": ["B-ReferenceLink"], "seq_token": ["7"], "seq_scores": [0.9942071437835693], "text": "7", "score": 0.9942071437835693, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2962, "end": 2963, "seq_label": ["B-ReferenceLink"], "seq_token": ["8"], "seq_scores": [0.9946067929267883], "text": "8", "score": 0.9946067929267883, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2966, "end": 2967, "seq_label": ["B-ReferenceLink"], "seq_token": ["9"], "seq_scores": [0.9937281608581543], "text": "9", "score": 0.9937281608581543, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2970, "end": 2972, "seq_label": ["B-ReferenceLink"], "seq_token": ["10"], "seq_scores": [0.9936920404434204], "text": "10", "score": 0.9936920404434204, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2975, "end": 2977, "seq_label": ["B-ReferenceLink"], "seq_token": ["11"], "seq_scores": [0.9953919649124146], "text": "11", "score": 0.9953919649124146, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3035, "end": 3063, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120deep", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.8621710538864136, 0.9001998901367188, 0.8090701103210449], "text": " deep reinforcement learning", "score": 0.8571470181147257, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2753, "end": 2780, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120multi", "-", "stage", "\u0120RL", "\u0120agent"], "seq_scores": [0.8641720414161682, 0.8953148126602173, 0.8786992430686951, 0.8749763369560242, 0.890268862247467, 0.8701393604278564, 0.8383331298828125], "text": " a new multi-stage RL agent", "score": 0.8731291123798915, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3003, "end": 3022, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120(", "pixel", ")", "\u0120images"], "seq_scores": [0.9104479551315308, 0.9203277230262756, 0.9336936473846436, 0.9358502626419067, 0.9492046236991882], "text": " raw (pixel) images", "score": 0.929904842376709, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3075, "end": 3088, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120RL"], "seq_scores": [0.9850849509239197, 0.9908701777458191, 0.992420494556427, 0.9934971332550049], "text": " zero-shot RL", "score": 0.9904681891202927, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3389, "end": 3396, "seq_label": ["B-Method"], "seq_token": ["\u0120robust"], "seq_scores": [0.6056444644927979], "text": " robust", "score": 0.6056444644927979, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3396, "end": 3429, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.7197502851486206, 0.9915571212768555, 0.9930962920188904, 0.9926782846450806, 0.9905360341072083], "text": " zero-shot reinforcement learning", "score": 0.937523603439331, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 3529, "end": 3562, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120neural", "\u0120network", "\u0120architecture"], "seq_scores": [0.4565313756465912, 0.9194892644882202, 0.9431678056716919, 0.7117974758148193], "text": " deep neural network architecture", "score": 0.7577464804053307, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 3568, "end": 3612, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120Deep", "\u0120Attention", "\u0120Conv", "olution", "al", "\u0120Neural", "\u0120Network"], "seq_scores": [0.9454491138458252, 0.9799090027809143, 0.9834836721420288, 0.982030987739563, 0.9800683856010437, 0.9824976325035095, 0.9810910224914551], "text": " Deep Attention Convolutional Neural Network", "score": 0.9763614024434771, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 3614, "end": 3619, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["D", "AC", "NN"], "seq_scores": [0.9243870377540588, 0.9786348938941956, 0.9767306447029114], "text": "DACNN", "score": 0.9599175254503886, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 3726, "end": 3742, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120model"], "seq_scores": [0.48116305470466614, 0.7087500095367432], "text": " attention model", "score": 0.5949565321207047, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3830, "end": 3852, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120modern", "\u0120control", "\u0120image", "-", "based", "\u0120serv", "o", "-", "ing"], "seq_scores": [0.540543794631958, 0.5585368275642395, 0.7560921907424927, 0.8940436840057373, 0.8944079279899597, 0.8574264645576477, 0.8306129574775696, 0.8316184878349304, 0.8509076237678528], "text": " image-based servo-ing", "score": 0.7793544398413764, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 3912, "end": 3937, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120control", "\u0120The", "\u0120image", "-", "based", "\u0120serv", "oing"], "seq_scores": [0.5953537225723267, 0.6815072298049927, 0.9088801145553589, 0.9701274037361145, 0.9699071049690247, 0.9561743140220642, 0.949142336845398], "text": " The image-based servoing", "score": 0.8615846037864685, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4032, "end": 4073, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120The", "\u0120recent", "\u0120image", "-", "based", "\u0120serv", "o", "-", "ing", "\u0120methods"], "seq_scores": [0.5545679926872253, 0.791496992111206, 0.8646499514579773, 0.9569854140281677, 0.9597963690757751, 0.9441330432891846, 0.9220660328865051, 0.9254387617111206, 0.9337570071220398, 0.658126175403595], "text": " The recent image-based servo-ing methods", "score": 0.8511017739772797, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4240, "end": 4273, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120attention", "\u0120model", "\u0120image", "-", "based", "\u0120visual", "\u0120serv", "o", "\u0120control"], "seq_scores": [0.46955370903015137, 0.7825363278388977, 0.7078324556350708, 0.7803235054016113, 0.7529549598693848, 0.7526122331619263, 0.682903528213501, 0.6583235859870911, 0.7502938508987427], "text": " image-based visual servo control", "score": 0.7041482395595975, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4278, "end": 4297, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120annotation", "\u0120vectors"], "seq_scores": [0.9037124514579773, 0.878026008605957], "text": " annotation vectors", "score": 0.8908692300319672, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4298, "end": 4317, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120An", "notation", "\u0120vectors"], "seq_scores": [0.8495059013366699, 0.852078914642334, 0.7830414772033691], "text": " Annotation vectors", "score": 0.8282087643941244, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 4338, "end": 4342, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120CNN"], "seq_scores": [0.5500043630599976], "text": " CNN", "score": 0.5500043630599976, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4360, "end": 4362, "seq_label": ["B-ReferenceLink"], "seq_token": ["12"], "seq_scores": [0.9936373233795166], "text": "12", "score": 0.9936373233795166, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4398, "end": 4417, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120annotation", "\u0120vectors"], "seq_scores": [0.729645848274231, 0.5907193422317505], "text": " annotation vectors", "score": 0.6601825952529907, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 4806, "end": 4822, "seq_label": ["I-Method", "B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120annotation", "\u0120attention", "\u0120model"], "seq_scores": [0.4770723879337311, 0.5070392489433289, 0.7306972146034241], "text": " attention model", "score": 0.5716029504934946, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4838, "end": 4863, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120robust", "\u0120domain", "\u0120adaptation"], "seq_scores": [0.783223032951355, 0.8479938507080078, 0.8461848497390747], "text": " robust domain adaptation", "score": 0.8258005777994791, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4868, "end": 4881, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120RL"], "seq_scores": [0.9794394373893738, 0.9919941425323486, 0.9919615387916565, 0.9946727156639099], "text": " zero-shot RL", "score": 0.9895169585943222, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5107, "end": 5109, "seq_label": ["B-ReferenceLink"], "seq_token": ["13"], "seq_scores": [0.9945347309112549], "text": "13", "score": 0.9945347309112549, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 3722, "end": 3742, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120attention", "\u0120model"], "seq_scores": [0.883152186870575, 0.9179983139038086, 0.9196321964263916], "text": " our attention model", "score": 0.9069275657335917, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4077, "end": 4083, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120image"], "seq_scores": [0.6060731410980225], "text": " image", "score": 0.6060731410980225, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4166, "end": 4186, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120attention", "\u0120model"], "seq_scores": [0.9059240818023682, 0.9491270780563354, 0.9434728026390076], "text": " our attention model", "score": 0.9328413208325704, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4336, "end": 4338, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120a"], "seq_scores": [0.5169575214385986], "text": " a", "score": 0.5169575214385986, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4802, "end": 4822, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120attention", "\u0120model"], "seq_scores": [0.8700942397117615, 0.9248341917991638, 0.8988476395606995], "text": " the attention model", "score": 0.8979253570238749, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5171, "end": 5194, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120an", "\u0120attention", "\u0120mechanism"], "seq_scores": [0.90053790807724, 0.9134752750396729, 0.953777015209198], "text": " an attention mechanism", "score": 0.9225967327753702, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 5259, "end": 5283, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120optimal", "\u0120motion", "\u0120planning"], "seq_scores": [0.8889651894569397, 0.874199628829956, 0.9595372676849365], "text": " optimal motion planning", "score": 0.9075673619906107, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5485, "end": 5510, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120E", "uler", "-", "L", "ag", "range", "\u0120equations"], "seq_scores": [0.6308525204658508, 0.7712292671203613, 0.7623356580734253, 0.796461284160614, 0.7522907257080078, 0.7371624112129211, 0.699066698551178], "text": " Euler-Lagrange equations", "score": 0.7356283664703369, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 5739, "end": 5765, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120neural", "\u0120networks"], "seq_scores": [0.9726985692977905, 0.9913473725318909, 0.9880853891372681], "text": " attention neural networks", "score": 0.9840437769889832, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 5825, "end": 5844, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120networks"], "seq_scores": [0.9711748361587524, 0.9889561533927917], "text": " attention networks", "score": 0.9800654947757721, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5987, "end": 6015, "seq_label": ["I-Task", "B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120driving", "\u0120deep", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.3848239779472351, 0.9553644061088562, 0.9749054312705994, 0.958046019077301], "text": " deep reinforcement learning", "score": 0.8182849586009979, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6020, "end": 6039, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120networks"], "seq_scores": [0.97700035572052, 0.9884556531906128], "text": " attention networks", "score": 0.9827280044555664, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5867, "end": 5896, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120current", "\u0120state", "-", "of", "-", "the", "\u0120art"], "seq_scores": [0.5291132926940918, 0.5584846138954163, 0.6242024302482605, 0.6589488983154297, 0.6713841557502747, 0.6492921710014343, 0.5830837488174438, 0.5582921504974365], "text": " the current state-of-the art", "score": 0.6041001826524734, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6156, "end": 6171, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120neural", "\u0120network"], "seq_scores": [0.503578782081604, 0.8005895018577576], "text": " neural network", "score": 0.6520841419696808, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6178, "end": 6196, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120network"], "seq_scores": [0.9548442959785461, 0.9811347723007202], "text": " attention network", "score": 0.9679895341396332, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6410, "end": 6429, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120networks"], "seq_scores": [0.9617505073547363, 0.9838976263999939], "text": " attention networks", "score": 0.9728240668773651, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6433, "end": 6450, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120image", "-", "ca", "ption", "ing"], "seq_scores": [0.942992091178894, 0.9319883584976196, 0.9253844022750854, 0.9224086999893188, 0.9079472422599792], "text": " image-captioning", "score": 0.9261441588401794, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 6454, "end": 6482, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120natural", "\u0120language", "\u0120processing"], "seq_scores": [0.7991625070571899, 0.8204458951950073, 0.8208755254745483], "text": " natural language processing", "score": 0.8134946425755819, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6513, "end": 6537, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120attention", "\u0120mechanism"], "seq_scores": [0.741712749004364, 0.7406914234161377, 0.7116044759750366], "text": " the attention mechanism", "score": 0.7313362161318461, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6588, "end": 6614, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120recurrent", "\u0120neural", "\u0120networks"], "seq_scores": [0.9674282670021057, 0.987583577632904, 0.9871599674224854], "text": " recurrent neural networks", "score": 0.9807239373524984, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6649, "end": 6666, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120models"], "seq_scores": [0.9066387414932251, 0.7223368287086487], "text": " attention models", "score": 0.8144877851009369, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6669, "end": 6695, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120recurrent", "\u0120neural", "\u0120networks"], "seq_scores": [0.9379633665084839, 0.9903676509857178, 0.9889419078826904], "text": " recurrent neural networks", "score": 0.972424308458964, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6697, "end": 6699, "seq_label": ["B-ReferenceLink"], "seq_token": ["14"], "seq_scores": [0.997328519821167], "text": "14", "score": 0.997328519821167, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6700, "end": 6703, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012012"], "seq_scores": [0.9961022138595581], "text": " 12", "score": 0.9961022138595581, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6704, "end": 6707, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012015"], "seq_scores": [0.996105968952179], "text": " 15", "score": 0.996105968952179, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 6708, "end": 6711, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012016"], "seq_scores": [0.9961252808570862], "text": " 16", "score": 0.9961252808570862, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6713, "end": 6723, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120Attention"], "seq_scores": [0.5391710996627808], "text": " Attention", "score": 0.5391710996627808, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 6731, "end": 6747, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120neural", "\u0120networks"], "seq_scores": [0.9075061082839966, 0.9593741297721863], "text": " neural networks", "score": 0.9334401190280914, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6940, "end": 6957, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120back", "-", "prop", "og", "ation"], "seq_scores": [0.9624982476234436, 0.9762558341026306, 0.974307119846344, 0.9711715579032898, 0.9718266129493713], "text": " back-propogation", "score": 0.9712118744850159, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6137, "end": 6171, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120specific", "\u0120type", "\u0120of", "\u0120neural", "\u0120network"], "seq_scores": [0.9521338939666748, 0.9302226901054382, 0.884183406829834, 0.9173825979232788, 0.9439651966094971, 0.960844874382019], "text": " a specific type of neural network", "score": 0.9314554433027903, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6178, "end": 6196, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120attention", "\u0120network"], "seq_scores": [0.7783313989639282, 0.7190560102462769], "text": " attention network", "score": 0.7486937046051025, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6410, "end": 6429, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120attention", "\u0120networks"], "seq_scores": [0.8034026026725769, 0.859565258026123], "text": " attention networks", "score": 0.83148393034935, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6588, "end": 6605, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120recurrent", "\u0120neural"], "seq_scores": [0.6394191980361938, 0.5501192212104797], "text": " recurrent neural", "score": 0.5947692096233368, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6649, "end": 6666, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120attention", "\u0120models"], "seq_scores": [0.8021911382675171, 0.8905542492866516], "text": " attention models", "score": 0.8463726937770844, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6991, "end": 7011, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120attention", "\u0120model"], "seq_scores": [0.6657441258430481, 0.7746399641036987, 0.6142173409461975], "text": " the attention model", "score": 0.6848671436309814, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 7016, "end": 7018, "seq_label": ["B-ReferenceLink"], "seq_token": ["14"], "seq_scores": [0.9942476749420166], "text": "14", "score": 0.9942476749420166, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7133, "end": 7143, "seq_label": ["I-Task", "B-Method"], "seq_token": ["\u0120caption", "\u0120attention"], "seq_scores": [0.5337711572647095, 0.8551395535469055], "text": " attention", "score": 0.6944553554058075, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7297, "end": 7322, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120image", "\u0120caption", "\u0120generation"], "seq_scores": [0.9628569483757019, 0.966902494430542, 0.9758394956588745], "text": " image caption generation", "score": 0.9685329794883728, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7325, "end": 7357, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120long", "\u0120length", "\u0120machine", "\u0120translation"], "seq_scores": [0.9527473449707031, 0.9673978686332703, 0.9501237273216248, 0.972499430179596], "text": " long length machine translation", "score": 0.9606920927762985, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7376, "end": 7395, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120autonomous", "\u0120driving"], "seq_scores": [0.7284484505653381, 0.6070557832717896], "text": " autonomous driving", "score": 0.6677521169185638, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 6991, "end": 7011, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120formulation", "\u0120the", "\u0120attention", "\u0120model"], "seq_scores": [0.5263492465019226, 0.9522443413734436, 0.9769012331962585, 0.9576455354690552], "text": " the attention model", "score": 0.85328508913517, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7025, "end": 7051, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120attention", "-", "based", "\u0120model"], "seq_scores": [0.9858387112617493, 0.9909849166870117, 0.9920006990432739, 0.9917634725570679, 0.992474377155304], "text": " the attention-based model", "score": 0.9906124353408814, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7151, "end": 7161, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9794098138809204, 0.9880794882774353], "text": " the model", "score": 0.9837446510791779, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7947, "end": 7983, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120image", "-", "based", "\u0120control", "\u0120formulation"], "seq_scores": [0.9621632695198059, 0.9878567457199097, 0.9907816052436829, 0.9899995923042297, 0.9849342107772827, 0.9634179472923279], "text": " the image-based control formulation", "score": 0.9798588951428732, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8004, "end": 8047, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120image", "-", "based", "\u0120visual", "\u0120serv", "o", "\u0120(", "IB", "VS", ")", "\u0120theory"], "seq_scores": [0.87800133228302, 0.9125308990478516, 0.9768338799476624, 0.9779258966445923, 0.9744914770126343, 0.9693344831466675, 0.9634984731674194, 0.7819243669509888, 0.5570811629295349, 0.9572237133979797, 0.7935690879821777, 0.912320077419281], "text": " the image-based visual servo (IBVS) theory", "score": 0.8878945708274841, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8102, "end": 8104, "seq_label": ["B-ReferenceLink"], "seq_token": ["17"], "seq_scores": [0.996174156665802], "text": "17", "score": 0.996174156665802, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8210, "end": 8231, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120full", "\u0120image", "\u0120error"], "seq_scores": [0.8844220638275146, 0.8943777084350586, 0.876430869102478, 0.8711609840393066], "text": " the full image error", "score": 0.8815979063510895, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8295, "end": 8312, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120full", "\u0120image", "\u0120error"], "seq_scores": [0.6889190077781677, 0.7320453524589539, 0.7095973491668701], "text": " full image error", "score": 0.7101872364679972, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 8510, "end": 8526, "seq_label": ["I-Method", "B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120controller", "\u0120attention", "\u0120model"], "seq_scores": [0.5208114981651306, 0.6991121172904968, 0.6821715831756592], "text": " attention model", "score": 0.6340317328770956, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7837, "end": 7841, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120raw"], "seq_scores": [0.651556670665741], "text": " raw", "score": 0.651556670665741, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 8510, "end": 8526, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["pixel", ")", "\u0120image", "\u0120attention", "\u0120model"], "seq_scores": [0.5467655062675476, 0.6156485676765442, 0.5099508166313171, 0.9317114353179932, 0.8895447254180908], "text": " attention model", "score": 0.6987242102622986, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8528, "end": 8562, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["The", "\u0120image", "-", "based", "\u0120only", "\u0120IB", "VS", "\u0120approach"], "seq_scores": [0.6327696442604065, 0.6776291131973267, 0.974342405796051, 0.9741998314857483, 0.9792070388793945, 0.9599586129188538, 0.9112628102302551, 0.8318185210227966], "text": "The image-based only IBVS approach", "score": 0.8676484972238541, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8941, "end": 8971, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120state", "\u0120the", "\u0120discrete", "-", "time", "\u0120formulation"], "seq_scores": [0.4983958303928375, 0.5466287136077881, 0.723200798034668, 0.6382875442504883, 0.6866995096206665, 0.6038219332695007], "text": " the discrete-time formulation", "score": 0.6161723881959915, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 9030, "end": 9032, "seq_label": ["B-ReferenceLink"], "seq_token": ["17"], "seq_scores": [0.9949929118156433], "text": "17", "score": 0.9949929118156433, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 8528, "end": 8562, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["The", "\u0120image", "-", "based", "\u0120only", "\u0120IB", "VS", "\u0120approach"], "seq_scores": [0.6915887594223022, 0.761968195438385, 0.8381967544555664, 0.834967851638794, 0.7503230571746826, 0.6639129519462585, 0.6913382411003113, 0.5718300342559814], "text": "The image-based only IBVS approach", "score": 0.7255157306790352, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 9371, "end": 9396, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120model", "\u0120predictive", "\u0120control"], "seq_scores": [0.8287256360054016, 0.8083400726318359, 0.7801414728164673], "text": " model predictive control", "score": 0.805735727151235, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9305, "end": 9328, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120proposed", "\u0120k", "inem", "atic"], "seq_scores": [0.6959912776947021, 0.703760027885437, 0.6943213939666748, 0.7421534657478333, 0.7463298439979553], "text": " The proposed kinematic", "score": 0.7165112018585205, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9349, "end": 9367, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120dynamic", "\u0120model"], "seq_scores": [0.9129073619842529, 0.8928351998329163, 0.8854997158050537], "text": " the dynamic model", "score": 0.8970807592074076, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9451, "end": 9472, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120k", "inem", "atic", "\u0120model"], "seq_scores": [0.9147588014602661, 0.9242426156997681, 0.9556621313095093, 0.9648172855377197, 0.9204518795013428], "text": " This kinematic model", "score": 0.9359865427017212, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9532, "end": 9563, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120classical", "\u0120IB", "VS", "\u0120formulation"], "seq_scores": [0.7952967286109924, 0.9487168788909912, 0.9294966459274292, 0.9727373719215393, 0.8256711959838867], "text": " the classical IBVS formulation", "score": 0.8943837642669678, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9762, "end": 9806, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120transformation", "\u0120our", "\u0120deep", "\u0120reinforcement", "\u0120learning", "\u0120formulation"], "seq_scores": [0.5380693674087524, 0.8067994117736816, 0.8758760094642639, 0.962061882019043, 0.9580381512641907, 0.7919449806213379], "text": " our deep reinforcement learning formulation", "score": 0.8221316337585449, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9849, "end": 9882, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120control", "\u0120state", "\u0120transformation"], "seq_scores": [0.6864315867424011, 0.8040873408317566, 0.7055687308311462, 0.6850126385688782], "text": " the control state transformation", "score": 0.7202750742435455, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 9964, "end": 10006, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120The", "\u0120deep", "\u0120reinforcement", "\u0120learning", "\u0120algorithm"], "seq_scores": [0.7065670490264893, 0.8458271026611328, 0.8566526174545288, 0.8571507930755615, 0.7172933220863342], "text": " The deep reinforcement learning algorithm", "score": 0.7966981768608093, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 10046, "end": 10064, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120reward", "\u0120function"], "seq_scores": [0.5190389156341553, 0.6496056318283081, 0.5901137590408325], "text": " a reward function", "score": 0.586252768834432, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9532, "end": 9563, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120classical", "\u0120IB", "VS", "\u0120formulation"], "seq_scores": [0.7647050619125366, 0.8065577745437622, 0.7744957804679871, 0.6685125827789307, 0.5549740195274353], "text": " the classical IBVS formulation", "score": 0.7138490438461303, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9599, "end": 9629, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120sensor", "-", "based", "\u0120observations"], "seq_scores": [0.9079274535179138, 0.906510591506958, 0.9629737734794617, 0.9681835174560547, 0.9493966698646545], "text": " the sensor-based observations", "score": 0.9389984011650085, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9659, "end": 9663, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.5153141617774963], "text": " the", "score": 0.5153141617774963, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9722, "end": 9735, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric"], "seq_token": ["-", "pixel", "\u0120image", "\u0120observations"], "seq_scores": [0.5830214619636536, 0.5929825305938721, 0.5051796436309814, 0.8005837798118591], "text": " observations", "score": 0.6204418540000916, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9762, "end": 9771, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120deep"], "seq_scores": [0.53510582447052, 0.5365466475486755], "text": " our deep", "score": 0.5358262360095978, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9819, "end": 9832, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120observations"], "seq_scores": [0.809261679649353], "text": " observations", "score": 0.809261679649353, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 9910, "end": 9929, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120neural", "\u0120network"], "seq_scores": [0.9122839570045471, 0.9156930446624756, 0.8895612359046936], "text": " the neural network", "score": 0.9058460791905721, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10137, "end": 10154, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120pixel", "\u0120images"], "seq_scores": [0.9788130521774292, 0.986681342124939, 0.9767431616783142], "text": " raw pixel images", "score": 0.9807458519935608, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 10166, "end": 10171, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120IB", "VS"], "seq_scores": [0.6236138343811035, 0.9344618320465088], "text": " IBVS", "score": 0.7790378332138062, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 10679, "end": 10719, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120the", "\u0120conv", "olution", "al", "\u0120neural", "\u0120network", "\u0120layers"], "seq_scores": [0.7891901731491089, 0.789984405040741, 0.9434139132499695, 0.9501581192016602, 0.9533913135528564, 0.9519193172454834, 0.9312389492988586], "text": " the convolutional neural network layers", "score": 0.9013280272483826, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 10737, "end": 10770, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120the", "\u0120conv", "olution", "al", "\u0120network", "\u0120layers"], "seq_scores": [0.7215917706489563, 0.6397466063499451, 0.9168514013290405, 0.9279144406318665, 0.9418026804924011, 0.9094634652137756], "text": " the convolutional network layers", "score": 0.8428950607776642, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11014, "end": 11016, "seq_label": ["B-ReferenceLink"], "seq_token": ["18"], "seq_scores": [0.9963963627815247], "text": "18", "score": 0.9963963627815247, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11075, "end": 11106, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120simple", "\u0120linear", "\u0120cascade", "\u0120system"], "seq_scores": [0.5056490302085876, 0.7903115749359131, 0.8412094712257385, 0.8311833143234253, 0.7920176386833191], "text": " a simple linear cascade system", "score": 0.7520742058753968, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11166, "end": 11182, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120cascade", "\u0120systems"], "seq_scores": [0.6451482772827148, 0.6675702929496765], "text": " cascade systems", "score": 0.6563592851161957, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11471, "end": 11474, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120an"], "seq_scores": [0.4761575162410736], "text": " an", "score": 0.4761575162410736, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11474, "end": 11492, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120network"], "seq_scores": [0.599365770816803, 0.9637797474861145], "text": " attention network", "score": 0.7815727591514587, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11533, "end": 11547, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "-", "CNN"], "seq_scores": [0.7199054956436157, 0.9306345582008362, 0.9266741275787354], "text": " attention-CNN", "score": 0.8590713938077291, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11601, "end": 11605, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "B-ModelArchitecture"], "seq_token": ["\u0120neural", "\u0120network", "\u0120the"], "seq_scores": [0.4833434820175171, 0.8709216713905334, 0.573332667350769], "text": " the", "score": 0.6425326069196066, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 11605, "end": 11628, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120attention", "\u0120network"], "seq_scores": [0.7263790369033813, 0.9869099259376526, 0.9886988997459412], "text": " deep attention network", "score": 0.9006626208623251, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 11697, "end": 11722, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120visual", "\u0120serv", "o", "\u0120algorithm"], "seq_scores": [0.8567328453063965, 0.9791114926338196, 0.9698825478553772, 0.9631491303443909, 0.964288592338562], "text": " a visual servo algorithm", "score": 0.9466329216957092, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12142, "end": 12160, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120reward", "\u0120function"], "seq_scores": [0.5470441579818726, 0.6826297640800476, 0.682516872882843], "text": " a reward function", "score": 0.6373969316482544, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 12359, "end": 12381, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120our", "\u0120attention", "\u0120network"], "seq_scores": [0.6638046503067017, 0.5902565121650696, 0.9758703708648682], "text": " our attention network", "score": 0.7433105111122131, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 12475, "end": 12486, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120CNN", "\u0120layers"], "seq_scores": [0.7562851905822754, 0.6510639190673828], "text": " CNN layers", "score": 0.7036745548248291, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 12568, "end": 12598, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120combination", "\u0120matrix", "\u0120approach"], "seq_scores": [0.8921462893486023, 0.9562304615974426, 0.9340445399284363, 0.9014896154403687], "text": " a combination matrix approach", "score": 0.9209777265787125, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12359, "end": 12387, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120attention", "\u0120network", "\u0120model"], "seq_scores": [0.9712296724319458, 0.9815954566001892, 0.9805636405944824, 0.9707385301589966], "text": " our attention network model", "score": 0.9760318249464035, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 13202, "end": 13206, "seq_label": ["I-ModelArchitecture", "B-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120CNN"], "seq_scores": [0.49296894669532776, 0.6731370687484741], "text": " CNN", "score": 0.5830530077219009, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13214, "end": 13216, "seq_label": ["B-ReferenceLink"], "seq_token": ["14"], "seq_scores": [0.9958627223968506], "text": "14", "score": 0.9958627223968506, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13030, "end": 13059, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120modified", "\u0120attention", "\u0120model"], "seq_scores": [0.7107881903648376, 0.9706541895866394, 0.9828624129295349, 0.9805063009262085], "text": " our modified attention model", "score": 0.9112027734518051, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 13137, "end": 13143, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120set"], "seq_scores": [0.6527530550956726, 0.7054932713508606], "text": " a set", "score": 0.6791231632232666, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13200, "end": 13206, "seq_label": ["I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120features", "\u0120a", "\u0120CNN"], "seq_scores": [0.5367191433906555, 0.7328982353210449, 0.6110387444496155], "text": " a CNN", "score": 0.6268853743871053, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13348, "end": 13360, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120extract"], "seq_scores": [0.6154354214668274, 0.6176164150238037], "text": " The extract", "score": 0.6165259182453156, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 13797, "end": 13842, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["the", "\u0120\"", "hard", "\"", "\u0120but", "\u0120sto", "ch", "astic", "\u0120attention", "\u0120mechanism"], "seq_scores": [0.7391642332077026, 0.7220299243927002, 0.7590724229812622, 0.7629680037498474, 0.9068542718887329, 0.9501258730888367, 0.9691935777664185, 0.9707908034324646, 0.9737889170646667, 0.9564809799194336], "text": "the \"hard\" but stochastic attention mechanism", "score": 0.8710469007492065, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14015, "end": 14021, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120f", "\u0120att"], "seq_scores": [0.9150572419166565, 0.8285913467407227], "text": " f att", "score": 0.8718242943286896, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14055, "end": 14076, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120soft", "max", "\u0120function"], "seq_scores": [0.6295166611671448, 0.6297703385353088, 0.6982253789901733, 0.6231473088264465], "text": " the softmax function", "score": 0.6451649218797684, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13996, "end": 14015, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120an", "\u0120attention", "\u0120model"], "seq_scores": [0.9247938394546509, 0.9439601898193359, 0.9093931317329407], "text": " an attention model", "score": 0.9260490536689758, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14207, "end": 14213, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "B-MLModel", "I-MLModel"], "seq_token": ["\u0120L", "ST", "M", "\u0120cell", "\u0120f", "\u0120att"], "seq_scores": [0.5399689674377441, 0.9090209603309631, 0.9266082048416138, 0.8106240034103394, 0.8912981748580933, 0.8093700408935547], "text": " f att", "score": 0.8144817252953848, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14216, "end": 14226, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120attention"], "seq_scores": [0.5435531735420227], "text": " attention", "score": 0.5435531735420227, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14291, "end": 14297, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120f", "\u0120att"], "seq_scores": [0.8490871787071228, 0.8228664994239807], "text": " f att", "score": 0.8359768390655518, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 14388, "end": 14394, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120f", "\u0120att"], "seq_scores": [0.941818118095398, 0.9306325316429138], "text": " f att", "score": 0.9362253248691559, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14213, "end": 14232, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120an", "\u0120attention", "\u0120model"], "seq_scores": [0.49322709441185, 0.8281428217887878, 0.9093341827392578], "text": " an attention model", "score": 0.7435680329799652, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14287, "end": 14291, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120the"], "seq_scores": [0.5398221015930176], "text": " the", "score": 0.5398221015930176, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14369, "end": 14388, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120att", "\u0120model", "\u0120our", "\u0120formulation", "\u0120of"], "seq_scores": [0.6417006850242615, 0.5168052911758423, 0.8019968271255493, 0.7792712450027466, 0.5977540016174316], "text": " our formulation of", "score": 0.6675056099891663, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14403, "end": 14419, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120att", "\u0120additive", "\u0120models"], "seq_scores": [0.5598661303520203, 0.9309689998626709, 0.9633489847183228], "text": " additive models", "score": 0.818061371644338, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14441, "end": 14451, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120attention"], "seq_scores": [0.3931380808353424], "text": " attention", "score": 0.3931380808353424, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14457, "end": 14463, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120f", "\u0120att"], "seq_scores": [0.3796706199645996, 0.3922691345214844], "text": " f att", "score": 0.385969877243042, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14490, "end": 14509, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Add", "itive", "\u0120attention"], "seq_scores": [0.8247702717781067, 0.6503812670707703, 0.6823193430900574], "text": " Additive attention", "score": 0.7191569606463114, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14513, "end": 14535, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120mult", "il", "ayer", "\u0120percept", "ron"], "seq_scores": [0.694632887840271, 0.8622937798500061, 0.8714458346366882, 0.8581766486167908, 0.8536249995231628], "text": " multilayer perceptron", "score": 0.8280348300933837, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14536, "end": 14539, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["ML", "P"], "seq_scores": [0.465998113155365, 0.7860299348831177], "text": "MLP", "score": 0.6260140240192413, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14555, "end": 14580, "seq_label": ["I-ModelArchitecture", "B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120attention", "\u0120multipl", "icative", "\u0120attention"], "seq_scores": [0.568654477596283, 0.9149042367935181, 0.8573510050773621, 0.847317636013031], "text": " multiplicative attention", "score": 0.7970568388700485, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14584, "end": 14606, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120dot", "-", "product", "\u0120attention"], "seq_scores": [0.9086688160896301, 0.8280495405197144, 0.8506748080253601, 0.8132843375205994], "text": " dot-product attention", "score": 0.850169375538826, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14696, "end": 14706, "seq_label": ["I-Method", "I-Method", "B-Method"], "seq_token": ["\u0120attention", "\u0120mechanisms", "\u0120attention"], "seq_scores": [0.39264723658561707, 0.5851278901100159, 0.7214820981025696], "text": " attention", "score": 0.5664190749327341, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 14801, "end": 14815, "seq_label": ["I-Method", "B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120modified", "\u0120ML", "P", "\u0120attention"], "seq_scores": [0.529358983039856, 0.4750119149684906, 0.8508912920951843, 0.7742242813110352], "text": " MLP attention", "score": 0.6573716178536415, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14437, "end": 14457, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120attention", "\u0120model"], "seq_scores": [0.8974587321281433, 0.9259746670722961, 0.8610149621963501], "text": " the attention model", "score": 0.8948161204655966, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14818, "end": 14830, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120network"], "seq_scores": [0.8260758519172668, 0.7284996509552002], "text": " our network", "score": 0.7772877514362335, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15544, "end": 15579, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120visual", "\u0120features", "\u0120a", "\u0120direct", "\u0120adaptive", "\u0120control", "\u0120approach"], "seq_scores": [0.6018577814102173, 0.6569376587867737, 0.9332231879234314, 0.9601993560791016, 0.9764218330383301, 0.9704678654670715, 0.907848596572876], "text": " a direct adaptive control approach", "score": 0.8581366113254002, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15671, "end": 15695, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Mark", "ov", "\u0120decision", "\u0120process"], "seq_scores": [0.8400169014930725, 0.8903757333755493, 0.8918015360832214, 0.8792417645454407], "text": " Markov decision process", "score": 0.875358983874321, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15697, "end": 15700, "seq_label": ["B-Method", "I-Method"], "seq_token": ["M", "DP"], "seq_scores": [0.7065160274505615, 0.9240673780441284], "text": "MDP", "score": 0.815291702747345, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15709, "end": 15767, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120vanilla", "\u0120clipped", "\u0120prox", "imate", "\u0120policy", "\u0120optimization", "\u0120algorithm"], "seq_scores": [0.9229453802108765, 0.935433566570282, 0.9678505063056946, 0.9852572083473206, 0.9804650545120239, 0.9781180024147034, 0.9756487011909485, 0.9302734732627869], "text": " a vanilla clipped proximate policy optimization algorithm", "score": 0.9594989866018295, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 15769, "end": 15771, "seq_label": ["B-ReferenceLink"], "seq_token": ["19"], "seq_scores": [0.992915153503418], "text": "19", "score": 0.992915153503418, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15776, "end": 15805, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120standard", "\u0120policy", "\u0120optimization"], "seq_scores": [0.860755205154419, 0.8260169625282288, 0.9660395979881287], "text": " standard policy optimization", "score": 0.8842705885569254, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15968, "end": 15991, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120an", "\u0120attention", "\u0120mechanism"], "seq_scores": [0.7018861174583435, 0.8081054091453552, 0.7850954532623291], "text": " an attention mechanism", "score": 0.7650289932886759, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16068, "end": 16092, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "B-Method", "I-Method", "I-Method"], "seq_token": ["olution", "al", "\u0120layers", "\u0120the", "\u0120policy", "\u0120optimization"], "seq_scores": [0.392600417137146, 0.42601385712623596, 0.4196311831474304, 0.9041572213172913, 0.9340754151344299, 0.9208055138587952], "text": " the policy optimization", "score": 0.6662139346202215, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16150, "end": 16159, "seq_label": ["B-Method"], "seq_token": ["\u0120learning"], "seq_scores": [0.4949410855770111], "text": " learning", "score": 0.4949410855770111, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15885, "end": 15915, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120underlying", "\u0120neural", "\u0120network"], "seq_scores": [0.8215318322181702, 0.851487398147583, 0.8719574213027954, 0.832953155040741], "text": " the underlying neural network", "score": 0.8444824516773224, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15929, "end": 15959, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120underlying", "\u0120neural", "\u0120network"], "seq_scores": [0.8140987753868103, 0.9035922288894653, 0.8822681903839111, 0.8250137567520142], "text": " the underlying neural network", "score": 0.8562432378530502, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16375, "end": 16406, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["-", "image", "\u0120based", "\u0120sensors", "\u0120The", "\u0120optimal", "\u0120control", "\u0120algorithms"], "seq_scores": [0.5429189205169678, 0.5661802291870117, 0.5749650597572327, 0.6275632977485657, 0.7630316615104675, 0.8197572827339172, 0.7695806622505188, 0.6988422274589539], "text": " The optimal control algorithms", "score": 0.6703549176454544, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16677, "end": 16695, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.693294107913971, 0.5639981627464294], "text": " domain adaptation", "score": 0.6286461353302002, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16770, "end": 16799, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120joint", "\u0120perception", "\u0120and", "\u0120control"], "seq_scores": [0.8106349110603333, 0.8658998012542725, 0.7903424501419067, 0.8824292421340942], "text": " joint perception and control", "score": 0.8373266011476517, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 17036, "end": 17041, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["D", "AC", "NN"], "seq_scores": [0.925561785697937, 0.9650745987892151, 0.9572755098342896], "text": "DACNN", "score": 0.9493039647738138, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 17135, "end": 17150, "seq_label": ["I-Task", "B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120control", "\u0120the", "\u0120CNN", "\u0120layers"], "seq_scores": [0.4144103527069092, 0.5528454780578613, 0.5464591383934021, 0.6052572727203369], "text": " the CNN layers", "score": 0.5297430604696274, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17163, "end": 17196, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120zero", "-", "shot", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.9831957221031189, 0.9895673394203186, 0.9896530508995056, 0.9902254939079285, 0.9830381870269775], "text": " zero-shot reinforcement learning", "score": 0.9871359586715698, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 17303, "end": 17319, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120model"], "seq_scores": [0.5479627847671509, 0.6632339954376221], "text": " attention model", "score": 0.6055983901023865, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17015, "end": 17034, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120architecture"], "seq_scores": [0.8546843528747559, 0.9009227752685547, 0.9043097496032715], "text": " a new architecture", "score": 0.8866389592488607, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17299, "end": 17319, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120The", "\u0120attention", "\u0120model"], "seq_scores": [0.9850084781646729, 0.9923985600471497, 0.9923768043518066], "text": " The attention model", "score": 0.9899279475212097, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17549, "end": 17568, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120additive", "\u0120attention"], "seq_scores": [0.8274275064468384, 0.7431359887123108], "text": " additive attention", "score": 0.7852817475795746, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 17680, "end": 17699, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120autonomous", "-", "driving"], "seq_scores": [0.44942954182624817, 0.5099189877510071, 0.6249151825904846], "text": " autonomous-driving", "score": 0.5280879040559133, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18089, "end": 18107, "seq_label": ["I-Method", "I-Method", "B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120context", "\u0120vector", "\u0120attention", "\u0120network"], "seq_scores": [0.6192601919174194, 0.5139837861061096, 0.7121296525001526, 0.8517976403236389], "text": " attention network", "score": 0.6742928177118301, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18190, "end": 18216, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120vari", "ational", "\u0120auto", "-", "enc", "od", "ers"], "seq_scores": [0.8018285632133484, 0.8116434216499329, 0.9007903933525085, 0.9196698069572449, 0.8920016288757324, 0.8856878280639648, 0.8860878944396973], "text": " variational auto-encoders", "score": 0.8711013623646328, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18397, "end": 18412, "seq_label": ["I-Method", "I-Method", "B-Method", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120mechanisms", "\u0120self", "-", "att", "ention"], "seq_scores": [0.4402514398097992, 0.6587712168693542, 0.5784426927566528, 0.6264784932136536, 0.6130595803260803, 0.6387191414833069], "text": " self-attention", "score": 0.5926204274098078, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18414, "end": 18416, "seq_label": ["B-ReferenceLink"], "seq_token": ["20"], "seq_scores": [0.9951005578041077], "text": "20", "score": 0.9951005578041077, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18418, "end": 18441, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120si", "ames", "e", "\u0120attention"], "seq_scores": [0.779268741607666, 0.9113120436668396, 0.963038980960846, 0.9473076462745667, 0.8913989663124084], "text": " deep siamese attention", "score": 0.8984652757644653, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18443, "end": 18445, "seq_label": ["B-ReferenceLink"], "seq_token": ["21"], "seq_scores": [0.9964993000030518], "text": "21", "score": 0.9964993000030518, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18085, "end": 18107, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120attention", "\u0120network"], "seq_scores": [0.8660712838172913, 0.897721529006958, 0.8410130143165588], "text": " our attention network", "score": 0.868268609046936, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18145, "end": 18181, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120current", "\u0120state", "-", "of", "-", "the", "-", "art", "\u0120method"], "seq_scores": [0.702872097492218, 0.7135689854621887, 0.7989492416381836, 0.8294534683227539, 0.8382314443588257, 0.8393570184707642, 0.8397286534309387, 0.7981961369514465, 0.7884458303451538, 0.575620174407959], "text": " the current state-of-the-art method", "score": 0.7724423050880432, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18190, "end": 18216, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120vari", "ational", "\u0120auto", "-", "enc", "od", "ers"], "seq_scores": [0.6567417979240417, 0.6842588782310486, 0.7203586101531982, 0.7747899889945984, 0.7769558429718018, 0.7949886918067932, 0.6765843629837036], "text": " variational auto-encoders", "score": 0.7263825961521694, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18518, "end": 18530, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120network"], "seq_scores": [0.6756778955459595, 0.5462552309036255], "text": " our network", "score": 0.6109665632247925, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18643, "end": 18651, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120sim", "2", "sim"], "seq_scores": [0.9220308661460876, 0.8260751366615295, 0.8940245509147644], "text": " sim2sim", "score": 0.8807101845741272, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 18655, "end": 18664, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120sim", "2", "real"], "seq_scores": [0.9354224801063538, 0.845890462398529, 0.8890355825424194], "text": " sim2real", "score": 0.8901161750157675, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18702, "end": 18708, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.606302797794342, 0.8050994277000427], "text": " DACNN", "score": 0.7057011127471924, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 18753, "end": 18769, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120domain", "\u0120transfer"], "seq_scores": [0.9578320384025574, 0.9778945446014404], "text": " domain transfer", "score": 0.9678632915019989, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18834, "end": 18840, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.5787985920906067, 0.7750641107559204], "text": " DACNN", "score": 0.6769313514232635, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19140, "end": 19159, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120jointly", "\u0120optimizing"], "seq_scores": [0.7196168899536133, 0.7904927134513855], "text": " jointly optimizing", "score": 0.7550548017024994, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19200, "end": 19219, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120dynamics", "\u0120pre", "-", "training", "\u0120stage"], "seq_scores": [0.5180666446685791, 0.9156418442726135, 0.9528351426124573, 0.9569822549819946, 0.8277810215950012], "text": " pre-training stage", "score": 0.8342613816261292, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19223, "end": 19264, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120learning", "\u0120the", "\u0120latent", "\u0120space", "\u0120representation"], "seq_scores": [0.8858590722084045, 0.8068845868110657, 0.9536249041557312, 0.9144566655158997, 0.9312617778778076], "text": " learning the latent space representation", "score": 0.8984174013137818, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19268, "end": 19279, "seq_label": ["B-Method"], "seq_token": ["\u0120perception"], "seq_scores": [0.3969438970088959], "text": " perception", "score": 0.3969438970088959, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18881, "end": 18904, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120deeper", "\u0120neural", "\u0120networks"], "seq_scores": [0.9658417701721191, 0.9618867039680481, 0.9446362853050232], "text": " deeper neural networks", "score": 0.9574549198150635, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19063, "end": 19073, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.8565250039100647, 0.8313948512077332], "text": " the model", "score": 0.8439599275588989, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19116, "end": 19128, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120network"], "seq_scores": [0.6078981757164001, 0.6697531342506409], "text": " the network", "score": 0.6388256549835205, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19341, "end": 19351, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120bas", "elines"], "seq_scores": [0.8013898730278015, 0.7459335327148438], "text": " baselines", "score": 0.7736617028713226, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20244, "end": 20262, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.9357977509498596, 0.9397846460342407], "text": " domain adaptation", "score": 0.9377911984920502, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20589, "end": 20619, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Method", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120sim", "2", "sim", "\u0120and", "\u0120sim", "2", "real", "\u0120transfer"], "seq_scores": [0.7185844779014587, 0.6297573447227478, 0.6608830690383911, 0.3599879741668701, 0.5019682049751282, 0.695884644985199, 0.6598435044288635, 0.7908922433853149], "text": " sim2sim and sim2real transfer", "score": 0.6272251829504967, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20755, "end": 20761, "seq_label": ["I-ModelArchitecture", "I-Method", "I-Method", "B-MLModel", "I-MLModel"], "seq_token": ["\u0120CNN", "\u0120attention", "\u0120mechanism", "\u0120DAR", "LA"], "seq_scores": [0.518122673034668, 0.4690767824649811, 0.604811429977417, 0.8875300288200378, 0.9366090893745422], "text": " DARLA", "score": 0.6832300007343293, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20783, "end": 20791, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA", "'s"], "seq_scores": [0.9799812436103821, 0.9815933108329773, 0.7199677228927612], "text": " DARLA's", "score": 0.8938474257787069, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 20791, "end": 20812, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120\u00ce\u00b2", "-", "VA", "E", "\u0120neural", "\u0120network"], "seq_scores": [0.6662999987602234, 0.9817816019058228, 0.9786876440048218, 0.9791330695152283, 0.7315596342086792, 0.8974317908287048], "text": " \u03b2-VAE neural network", "score": 0.8724822898705801, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20918, "end": 20926, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Task"], "seq_token": ["\u0120latent", "\u0120state", "\u0120representation", "\u0120control"], "seq_scores": [0.6245850920677185, 0.6723382472991943, 0.6369472742080688, 0.5827147960662842], "text": " control", "score": 0.6291463524103165, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 20954, "end": 20960, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9446179866790771, 0.9700571298599243], "text": " DARLA", "score": 0.9573375582695007, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 20981, "end": 20983, "seq_label": ["B-ReferenceLink"], "seq_token": ["22"], "seq_scores": [0.9958741068840027], "text": "22", "score": 0.9958741068840027, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20567, "end": 20581, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120bas", "elines"], "seq_scores": [0.8589302897453308, 0.8720934987068176, 0.8481370210647583], "text": " two baselines", "score": 0.8597202698389689, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20632, "end": 20647, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120First", "\u0120baseline"], "seq_scores": [0.9198752045631409, 0.922264575958252], "text": " First baseline", "score": 0.9210698902606964, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20723, "end": 20739, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Second", "\u0120baseline"], "seq_scores": [0.9178385138511658, 0.9115923047065735], "text": " Second baseline", "score": 0.9147154092788696, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20932, "end": 20954, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["'s", "-", "\u0120Our", "\u0120implementation", "\u0120of"], "seq_scores": [0.5595279932022095, 0.5291070342063904, 0.7576573491096497, 0.7790662050247192, 0.5552558302879333], "text": " Our implementation of", "score": 0.6361228823661804, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21072, "end": 21090, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.5598735809326172, 0.9624003767967224], "text": " domain adaptation", "score": 0.7611369788646698, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 21107, "end": 21113, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9034630060195923, 0.9379096031188965], "text": " DARLA", "score": 0.9206863045692444, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 21155, "end": 21161, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.627705991268158, 0.5638983249664307], "text": " DACNN", "score": 0.5958021581172943, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21223, "end": 21268, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120vanilla", "\u0120policy", "\u0120optimization", "\u0120implementation"], "seq_scores": [0.7405585050582886, 0.7681733965873718, 0.9771100282669067, 0.981222927570343, 0.827336847782135], "text": " a vanilla policy optimization implementation", "score": 0.858880341053009, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 21272, "end": 21296, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120categ", "orical", "\u0120exploration"], "seq_scores": [0.9846600890159607, 0.9894809722900391, 0.9903586506843567], "text": " categorical exploration", "score": 0.9881665706634521, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21044, "end": 21064, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120baseline", "\u0120models"], "seq_scores": [0.9895156621932983, 0.9951061010360718, 0.9957148432731628], "text": " two baseline models", "score": 0.9934455355008444, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21091, "end": 21102, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120models"], "seq_scores": [0.9800533652305603, 0.9756923913955688], "text": " two models", "score": 0.9778728783130646, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21139, "end": 21150, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120models"], "seq_scores": [0.9855276346206665, 0.983900785446167], "text": " two models", "score": 0.9847142100334167, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 21205, "end": 21214, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u01206", "\u0120models"], "seq_scores": [0.9382027983665466, 0.9917277097702026], "text": " 6 models", "score": 0.9649652540683746, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21434, "end": 21442, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120sim", "2", "sim"], "seq_scores": [0.9809021949768066, 0.9771605730056763, 0.9827715754508972], "text": " sim2sim", "score": 0.9802781144777933, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21446, "end": 21455, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120sim", "2", "real"], "seq_scores": [0.9830133318901062, 0.9763790965080261, 0.9820998907089233], "text": " sim2real", "score": 0.9804974397023519, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 21894, "end": 21898, "seq_label": ["B-Dataset"], "seq_token": ["\u0120sim"], "seq_scores": [0.4616747498512268], "text": " sim", "score": 0.4616747498512268, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 22398, "end": 22424, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deeper", "\u0120DAC", "NN", "\u0120architecture"], "seq_scores": [0.655189037322998, 0.6496566534042358, 0.9502488970756531, 0.5988932847976685], "text": " deeper DACNN architecture", "score": 0.7134969681501389, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22547, "end": 22569, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120deeper", "\u0120network", "\u0120vision", "-", "based", "\u0120controls"], "seq_scores": [0.4172784090042114, 0.7843159437179565, 0.7879340052604675, 0.7677277326583862, 0.7794131636619568, 0.7478070855140686], "text": " vision-based controls", "score": 0.7140793899695078, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 22669, "end": 22675, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9853844046592712, 0.9905681014060974], "text": " DARLA", "score": 0.9879762530326843, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 22679, "end": 22685, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.8797879815101624, 0.8120495080947876], "text": " DACNN", "score": 0.845918744802475, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22766, "end": 22797, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120prob", "abil", "istic", "\u0120action", "\u0120space"], "seq_scores": [0.7369692325592041, 0.724882960319519, 0.852415144443512, 0.8683533668518066, 0.8437765836715698, 0.7459798455238342], "text": " the probabilistic action space", "score": 0.7953961888949076, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 22836, "end": 22838, "seq_label": ["B-ReferenceLink"], "seq_token": ["23"], "seq_scores": [0.9945157170295715], "text": "23", "score": 0.9945157170295715, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22902, "end": 22933, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120deter", "ministic", "\u0120action", "\u0120decisions"], "seq_scores": [0.731573760509491, 0.7020989060401917, 0.6601904034614563, 0.5274409055709839], "text": " deterministic action decisions", "score": 0.6553259938955307, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 22959, "end": 22965, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9811481833457947, 0.9784114360809326], "text": " DARLA", "score": 0.9797798097133636, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 22982, "end": 23000, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120data", "\u0120aug", "mentation"], "seq_scores": [0.9862167835235596, 0.9915211200714111, 0.9889459013938904], "text": " data augmentation", "score": 0.9888946016629537, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23062, "end": 23084, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120The", "\u0120data", "\u0120aug", "mentation"], "seq_scores": [0.9033342003822327, 0.7953376173973083, 0.9910962581634521, 0.9905011653900146], "text": " The data augmentation", "score": 0.920067310333252, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23166, "end": 23181, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method"], "seq_token": ["urb", "ations", "\u0120Ga", "ussian", "\u0120noise"], "seq_scores": [0.6837515234947205, 0.7018865346908569, 0.5674259662628174, 0.9672515392303467, 0.9724218845367432], "text": " Gaussian noise", "score": 0.778547489643097, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 23182, "end": 23188, "seq_label": ["B-MLModel", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.5880230665206909, 0.7548902034759521], "text": " DACNN", "score": 0.6714566349983215, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 23199, "end": 23245, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120a", "\u0120single", "\u0120layer", "\u0120and", "\u0120two", "-", "layer", "\u0120attention", "\u0120layers"], "seq_scores": [0.587039053440094, 0.7149406671524048, 0.7580483555793762, 0.7152776122093201, 0.7016885280609131, 0.9212155342102051, 0.9170771241188049, 0.8801494836807251, 0.9036272168159485], "text": " a single layer and two-layer attention layers", "score": 0.7887848416964213, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 23251, "end": 23276, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120vanilla", "\u0120input", "\u0120CNN", "\u0120layers"], "seq_scores": [0.7446340322494507, 0.7732768654823303, 0.8522078990936279, 0.7721976041793823], "text": " vanilla input CNN layers", "score": 0.7855791002511978, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 23658, "end": 23664, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.49897029995918274, 0.7445998191833496], "text": " DACNN", "score": 0.6217850595712662, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 23845, "end": 23851, "seq_label": ["I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120neural", "\u0120network", "\u0120shallow", "\u0120network", "\u0120DAC", "NN"], "seq_scores": [0.7141419649124146, 0.8232021927833557, 0.38244861364364624, 0.8302170634269714, 0.74996417760849, 0.8852033019065857], "text": " DACNN", "score": 0.7308628857135773, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22036, "end": 22048, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120autonomous", "\u0120driving", "\u0120image", "\u0120time", "-", "series"], "seq_scores": [0.5135645866394043, 0.7861162424087524, 0.8243672847747803, 0.9102285504341125, 0.9792168736457825, 0.984241783618927], "text": " time-series", "score": 0.8329558869202932, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 22053, "end": 22064, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120event", "\u0120logs"], "seq_scores": [0.8761011362075806, 0.9510893225669861], "text": " event logs", "score": 0.9135952293872833, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22133, "end": 22152, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120baseline", "\u0120model"], "seq_scores": [0.9902886152267456, 0.9921790361404419, 0.9832685589790344], "text": " the baseline model", "score": 0.988578736782074, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22208, "end": 22234, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120attention", "\u0120based", "\u0120model"], "seq_scores": [0.9880515336990356, 0.9923158884048462, 0.9912126660346985, 0.991003155708313], "text": " the attention based model", "score": 0.9906458109617233, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22394, "end": 22398, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric"], "seq_token": ["\u0120its", "\u0120our"], "seq_scores": [0.47204187512397766, 0.5821403861045837], "text": " our", "score": 0.5270911306142807, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22483, "end": 22502, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120DAC", "\u0120the", "\u0120deeper", "\u0120network"], "seq_scores": [0.4979431927204132, 0.7387852668762207, 0.7030754089355469, 0.5554262399673462], "text": " the deeper network", "score": 0.6238075271248817, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22707, "end": 22721, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120bas", "elines"], "seq_scores": [0.9329334497451782, 0.92557293176651, 0.9053879976272583], "text": " the baselines", "score": 0.9212981263796488, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22750, "end": 22761, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Bas", "eline", "\u0120I"], "seq_scores": [0.8463822603225708, 0.8042188286781311, 0.7482373714447021], "text": " Baseline I", "score": 0.799612820148468, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22946, "end": 22972, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120types", "\u0120of", "\u0120DAR", "LA", "\u0120models"], "seq_scores": [0.9597600698471069, 0.9611861705780029, 0.9575206637382507, 0.8876977562904358, 0.9856417775154114, 0.9788580536842346], "text": " two types of DARLA models", "score": 0.9551107486089071, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 22973, "end": 22977, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120one"], "seq_scores": [0.8327598571777344], "text": " one", "score": 0.8327598571777344, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23004, "end": 23014, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120other"], "seq_scores": [0.8134803771972656, 0.7593261003494263], "text": " the other", "score": 0.786403238773346, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23182, "end": 23195, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120DAC", "NN", "\u0120models"], "seq_scores": [0.9780133366584778, 0.9828192591667175, 0.9728178381919861], "text": " DACNN models", "score": 0.9778834780057272, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23654, "end": 23670, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120DAC", "NN", "\u0120model"], "seq_scores": [0.874333381652832, 0.8699527978897095, 0.7460405230522156, 0.7282246947288513], "text": " the DACNN model", "score": 0.8046378493309021, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23732, "end": 23752, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120shallow", "\u0120network"], "seq_scores": [0.9116677641868591, 0.9332334995269775, 0.9051380157470703], "text": " the shallow network", "score": 0.9166797598203024, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23867, "end": 23883, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120shallow", "\u0120one"], "seq_scores": [0.8969878554344177, 0.9092646837234497, 0.8599217534065247], "text": " the shallow one", "score": 0.8887247641881307, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 23913, "end": 23932, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120deeper", "\u0120network"], "seq_scores": [0.8546183109283447, 0.886681318283081, 0.8210330605506897], "text": " the deeper network", "score": 0.8541108965873718, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 23989, "end": 24032, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Grad", "ient", "-", "weight", "ed", "\u0120Class", "\u0120Activ", "ation", "\u0120M", "apping"], "seq_scores": [0.9922052025794983, 0.9957934617996216, 0.9965509176254272, 0.9964898228645325, 0.9957072734832764, 0.9950553178787231, 0.9953586459159851, 0.9945135116577148, 0.9947068095207214, 0.9944570064544678], "text": " Gradient-weighted Class Activation Mapping", "score": 0.9950837969779969, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24034, "end": 24042, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Grad", "-", "C", "AM"], "seq_scores": [0.9894911050796509, 0.9937577247619629, 0.995465874671936, 0.9952432513237], "text": "Grad-CAM", "score": 0.9934894889593124, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 24048, "end": 24050, "seq_label": ["B-ReferenceLink"], "seq_token": ["24"], "seq_scores": [0.9944933652877808], "text": "24", "score": 0.9944933652877808, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24175, "end": 24184, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Grad", "-", "C", "AM"], "seq_scores": [0.9931643605232239, 0.9965684413909912, 0.9971067309379578, 0.9971725344657898], "text": " Grad-CAM", "score": 0.9960030168294907, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24208, "end": 24231, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120reinforcement", "\u0120learning"], "seq_scores": [0.9361284971237183, 0.9088090062141418], "text": " reinforcement learning", "score": 0.92246875166893, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24269, "end": 24281, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120re", "-", "training"], "seq_scores": [0.5987240076065063, 0.5976492762565613, 0.5510625243186951], "text": " re-training", "score": 0.5824786027272543, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24282, "end": 24291, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Grad", "-", "C", "AM"], "seq_scores": [0.9932457804679871, 0.9964667558670044, 0.9970898628234863, 0.9971279501914978], "text": " Grad-CAM", "score": 0.9959825873374939, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24078, "end": 24091, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120baseline"], "seq_scores": [0.6612392663955688, 0.6527988314628601], "text": " our baseline", "score": 0.6570190489292145, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24195, "end": 24200, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120CNN", "s"], "seq_scores": [0.8517916798591614, 0.8401064872741699], "text": " CNNs", "score": 0.8459490835666656, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 24537, "end": 24543, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.506482720375061, 0.6250623464584351], "text": " DACNN", "score": 0.565772533416748, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 24773, "end": 24779, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120DAC", "NN"], "seq_scores": [0.8210261464118958, 0.8038483262062073], "text": " DACNN", "score": 0.8124372363090515, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 24955, "end": 24965, "seq_label": ["B-Method"], "seq_token": ["\u0120attention"], "seq_scores": [0.5551810264587402], "text": " attention", "score": 0.5551810264587402, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24514, "end": 24527, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120baseline"], "seq_scores": [0.9108006358146667, 0.9491487741470337], "text": " our baseline", "score": 0.9299747049808502, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24880, "end": 24892, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120both", "\u0120models"], "seq_scores": [0.9700073599815369, 0.9852049350738525], "text": " both models", "score": 0.9776061475276947, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 24977, "end": 24990, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120baseline"], "seq_scores": [0.8657086491584778, 0.9217413663864136], "text": " the baseline", "score": 0.8937250077724457, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25046, "end": 25072, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Vision", "-", "based", "\u0120serv", "o", "\u0120control"], "seq_scores": [0.9400849938392639, 0.966044545173645, 0.9692995548248291, 0.9578198194503784, 0.9327651858329773, 0.9402684569358826], "text": "Vision-based servo control", "score": 0.9510470926761627, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25100, "end": 25128, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120image", "-", "based", "\u0120optimal", "\u0120control"], "seq_scores": [0.9314013719558716, 0.9495057463645935, 0.9564138054847717, 0.9543327689170837, 0.9343393445014954], "text": " image-based optimal control", "score": 0.9451986074447631, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25153, "end": 25180, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120image", "-", "based", "\u0120visual", "\u0120serv", "o", "\u0120("], "seq_scores": [0.9659796953201294, 0.9939403533935547, 0.9948758482933044, 0.9940826296806335, 0.9918457865715027, 0.9903606176376343, 0.8252643346786499], "text": " image-based visual servo (", "score": 0.9651927522250584, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25180, "end": 25193, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["IB", "VS", ")", "\u0120control"], "seq_scores": [0.6607434153556824, 0.9861611127853394, 0.8585532903671265, 0.9862164258956909], "text": "IBVS) control", "score": 0.8729185611009598, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25194, "end": 25207, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Classic", "\u0120IB", "VS"], "seq_scores": [0.5200368165969849, 0.6758742332458496, 0.9836154580116272], "text": " Classic IBVS", "score": 0.7265088359514872, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25260, "end": 25262, "seq_label": ["B-ReferenceLink"], "seq_token": ["25"], "seq_scores": [0.9959111213684082], "text": "25", "score": 0.9959111213684082, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25263, "end": 25266, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012026"], "seq_scores": [0.9899660348892212], "text": " 26", "score": 0.9899660348892212, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25342, "end": 25344, "seq_label": ["B-ReferenceLink"], "seq_token": ["27"], "seq_scores": [0.9970417618751526], "text": "27", "score": 0.9970417618751526, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25345, "end": 25348, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u012028"], "seq_scores": [0.9917560815811157], "text": " 28", "score": 0.9917560815811157, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25370, "end": 25386, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120visual", "\u0120serv", "oing"], "seq_scores": [0.8268595933914185, 0.9014609456062317, 0.8595774173736572], "text": " visual servoing", "score": 0.8626326521237692, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25429, "end": 25431, "seq_label": ["B-ReferenceLink"], "seq_token": ["29"], "seq_scores": [0.9954997897148132], "text": "29", "score": 0.9954997897148132, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25478, "end": 25494, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120visual", "\u0120serv", "oing"], "seq_scores": [0.9122386574745178, 0.9459888339042664, 0.9373945593833923], "text": " visual servoing", "score": 0.9318740169207255, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25564, "end": 25598, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120separate", "\u0120motion", "\u0120planning", "\u0120module"], "seq_scores": [0.5457972288131714, 0.7925695180892944, 0.6890471577644348, 0.8134832978248596, 0.713066577911377], "text": " a separate motion planning module", "score": 0.7107927560806274, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25665, "end": 25666, "seq_label": ["B-ReferenceLink"], "seq_token": ["6"], "seq_scores": [0.9974018335342407], "text": "6", "score": 0.9974018335342407, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 25667, "end": 25669, "seq_label": ["B-ReferenceLink"], "seq_token": ["\u01207"], "seq_scores": [0.9945007562637329], "text": " 7", "score": 0.9945007562637329, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25727, "end": 25755, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120deep", "\u0120reinforcement", "\u0120learning"], "seq_scores": [0.955310046672821, 0.9822628498077393, 0.9718124270439148], "text": " deep reinforcement learning", "score": 0.9697951078414917, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 25756, "end": 25769, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120IB", "VS", "\u0120methods"], "seq_scores": [0.9702310562133789, 0.9897510409355164, 0.5124491453170776], "text": " IBVS methods", "score": 0.8241437474886576, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26109, "end": 26126, "seq_label": ["I-Method", "I-Method", "I-Method", "B-Method", "I-Method"], "seq_token": ["\u0120classical", "\u0120IB", "VS", "\u0120adaptive", "\u0120control"], "seq_scores": [0.8768176436424255, 0.817614734172821, 0.9903576970100403, 0.9357985854148865, 0.9590342044830322], "text": " adaptive control", "score": 0.9159245729446411, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26156, "end": 26179, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120reinforcement", "\u0120learning"], "seq_scores": [0.9622491598129272, 0.9577789306640625], "text": " reinforcement learning", "score": 0.9600140452384949, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 25695, "end": 25714, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120raw", "\u0120(", "pixel", ")", "\u0120images"], "seq_scores": [0.9711161255836487, 0.9792296290397644, 0.978909432888031, 0.9784979224205017, 0.9800761342048645], "text": " raw (pixel) images", "score": 0.9775658488273621, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 26217, "end": 26234, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["Domain", "\u0120Adapt", "ation"], "seq_scores": [0.6525701880455017, 0.7419900894165039, 0.7554476857185364], "text": "Domain Adaptation", "score": 0.7166693210601807, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 26238, "end": 26253, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120Robot", "\u0120Learning"], "seq_scores": [0.5853760242462158, 0.5971142649650574], "text": " Robot Learning", "score": 0.5912451446056366, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26257, "end": 26275, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.7324357032775879, 0.7530226111412048], "text": " domain adaptation", "score": 0.7427291572093964, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26338, "end": 26339, "seq_label": ["B-ReferenceLink"], "seq_token": ["5"], "seq_scores": [0.9961219429969788], "text": "5", "score": 0.9961219429969788, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 26394, "end": 26400, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120DAR", "LA"], "seq_scores": [0.9606485366821289, 0.9635892510414124], "text": " DARLA", "score": 0.9621188938617706, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 26402, "end": 26444, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["Dis", "ent", "Ang", "led", "\u0120Represent", "ation", "\u0120Learning", "\u0120Agent"], "seq_scores": [0.9162603616714478, 0.8825419545173645, 0.905394434928894, 0.9309899806976318, 0.9355477094650269, 0.9346451163291931, 0.943024218082428, 0.9401189684867859], "text": "DisentAngled Representation Learning Agent", "score": 0.9235653430223465, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26579, "end": 26604, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120latent", "\u0120attention", "\u0120space"], "seq_scores": [0.685276210308075, 0.9268782734870911, 0.9467712044715881, 0.8888473510742188], "text": " a latent attention space", "score": 0.8619432598352432, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 26833, "end": 26851, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120network"], "seq_scores": [0.8678846955299377, 0.8810696601867676], "text": " attention network", "score": 0.8744771778583527, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 26366, "end": 26393, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120new", "\u0120multi", "-", "stage", "\u0120RL", "\u0120agent"], "seq_scores": [0.9470521211624146, 0.9677612781524658, 0.9677332639694214, 0.9622060060501099, 0.9670386910438538, 0.9630827903747559, 0.9443030953407288], "text": " a new multi-stage RL agent", "score": 0.9598824637276786, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26889, "end": 26890, "seq_label": ["B-ReferenceLink"], "seq_token": ["2"], "seq_scores": [0.9934321045875549], "text": "2", "score": 0.9934321045875549, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26910, "end": 26928, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120adaptation"], "seq_scores": [0.9845848679542542, 0.992911159992218], "text": " domain adaptation", "score": 0.9887480139732361, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26955, "end": 26956, "seq_label": ["B-ReferenceLink"], "seq_token": ["3"], "seq_scores": [0.9935367703437805], "text": "3", "score": 0.9935367703437805, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 26963, "end": 26964, "seq_label": ["B-ReferenceLink"], "seq_token": ["4"], "seq_scores": [0.9945921301841736], "text": "4", "score": 0.9945921301841736, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 26982, "end": 27016, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120domain", "\u0120and", "\u0120dynamics", "\u0120random", "ization"], "seq_scores": [0.9861263632774353, 0.9831933975219727, 0.980868935585022, 0.9908581376075745, 0.9905275106430054], "text": " domain and dynamics randomization", "score": 0.9863148689270019, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27032, "end": 27052, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["Att", "ention", "\u0120Mechan", "isms"], "seq_scores": [0.8936902284622192, 0.9041098952293396, 0.8894857168197632, 0.8598845601081848], "text": "Attention Mechanisms", "score": 0.8867926001548767, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27055, "end": 27078, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Rein", "forcement", "\u0120Learning"], "seq_scores": [0.954371988773346, 0.9686762094497681, 0.94505375623703], "text": " Reinforcement Learning", "score": 0.956033984820048, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27079, "end": 27089, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120Attention"], "seq_scores": [0.4696938991546631], "text": " Attention", "score": 0.4696938991546631, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27165, "end": 27182, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120image", "\u0120caption", "ing"], "seq_scores": [0.9863422513008118, 0.9858158826828003, 0.9836316108703613], "text": " image captioning", "score": 0.9852632482846578, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27184, "end": 27186, "seq_label": ["B-ReferenceLink"], "seq_token": ["14"], "seq_scores": [0.9969680905342102], "text": "14", "score": 0.9969680905342102, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27191, "end": 27211, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120machine", "\u0120translation"], "seq_scores": [0.9852592945098877, 0.9798129796981812], "text": " machine translation", "score": 0.9825361371040344, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27213, "end": 27215, "seq_label": ["B-ReferenceLink"], "seq_token": ["12"], "seq_scores": [0.9973652958869934], "text": "12", "score": 0.9973652958869934, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27226, "end": 27236, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120attention"], "seq_scores": [0.6179372072219849], "text": " attention", "score": 0.6179372072219849, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27271, "end": 27297, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120recurrent", "\u0120neural", "\u0120networks"], "seq_scores": [0.9407917261123657, 0.9706122875213623, 0.9618796110153198], "text": " recurrent neural networks", "score": 0.9577612082163492, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27309, "end": 27333, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120optimal", "\u0120visual", "-", "serv", "oing"], "seq_scores": [0.8191513419151306, 0.7384856939315796, 0.9488064050674438, 0.961725652217865, 0.9666178226470947], "text": " optimal visual-servoing", "score": 0.8869573831558227, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27345, "end": 27347, "seq_label": ["B-ReferenceLink"], "seq_token": ["30"], "seq_scores": [0.9933521747589111], "text": "30", "score": 0.9933521747589111, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27354, "end": 27356, "seq_label": ["B-ReferenceLink"], "seq_token": ["31"], "seq_scores": [0.9940164089202881], "text": "31", "score": 0.9940164089202881, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27376, "end": 27401, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120recurrent", "\u0120neural", "\u0120network"], "seq_scores": [0.7749599814414978, 0.979107141494751, 0.978871762752533], "text": " recurrent neural network", "score": 0.9109796285629272, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27403, "end": 27406, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["R", "NN"], "seq_scores": [0.8757696747779846, 0.9830796718597412], "text": "RNN", "score": 0.9294246733188629, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27494, "end": 27513, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120a", "\u0120dynamic", "\u0120internal"], "seq_scores": [0.4911331832408905, 0.6045752763748169, 0.5492741465568542], "text": " a dynamic internal", "score": 0.5483275353908539, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27577, "end": 27606, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120conv", "olution", "al", "\u0120neural", "\u0120network"], "seq_scores": [0.9530322551727295, 0.9723049998283386, 0.9768198132514954, 0.978270411491394, 0.9805002212524414], "text": " convolutional neural network", "score": 0.9721855401992798, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27608, "end": 27619, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["CNN", ")", "\u0120layers"], "seq_scores": [0.6705338954925537, 0.7982093691825867, 0.9060007929801941], "text": "CNN) layers", "score": 0.7915813525517782, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 27696, "end": 27712, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120optimal", "\u0120control"], "seq_scores": [0.4352113902568817, 0.8727415204048157], "text": " optimal control", "score": 0.6539764553308487, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27783, "end": 27803, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120multiple", "\u0120CNN", "\u0120layers"], "seq_scores": [0.651568591594696, 0.7159675359725952, 0.8514368534088135], "text": " multiple CNN layers", "score": 0.7396576603253683, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27816, "end": 27834, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120attention", "\u0120network"], "seq_scores": [0.7208647131919861, 0.9160941243171692], "text": " attention network", "score": 0.8184794187545776, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 27921, "end": 27926, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120CNN", "s"], "seq_scores": [0.9478002190589905, 0.9599252343177795], "text": " CNNs", "score": 0.953862726688385, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 27928, "end": 27930, "seq_label": ["B-ReferenceLink"], "seq_token": ["32"], "seq_scores": [0.9950266480445862], "text": "32", "score": 0.9950266480445862, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27972, "end": 27982, "seq_label": ["B-Method"], "seq_token": ["\u0120attention"], "seq_scores": [0.6744735836982727], "text": " attention", "score": 0.6744735836982727, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 27987, "end": 27990, "seq_label": ["B-Method"], "seq_token": ["\u0120RL"], "seq_scores": [0.8561797142028809], "text": " RL", "score": 0.8561797142028809, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 28021, "end": 28043, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120hand", "-", "crafted", "\u0120features"], "seq_scores": [0.9204441905021667, 0.9493981599807739, 0.956218421459198, 0.940112292766571], "text": " hand-crafted features", "score": 0.9415432661771774, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 28060, "end": 28070, "seq_label": ["B-ModelArchitecture"], "seq_token": ["\u0120attention"], "seq_scores": [0.7017697691917419], "text": " attention", "score": 0.7017697691917419, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28078, "end": 28080, "seq_label": ["B-ReferenceLink"], "seq_token": ["33"], "seq_scores": [0.9955589771270752], "text": "33", "score": 0.9955589771270752, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 28086, "end": 28108, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120hand", "-", "crafted", "\u0120features"], "seq_scores": [0.5458945035934448, 0.8455491065979004, 0.891396701335907, 0.8079917430877686], "text": " hand-crafted features", "score": 0.7727080136537552, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 28202, "end": 28229, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120CNN", "based", "\u0120attention", "\u0120network"], "seq_scores": [0.8702623248100281, 0.9557244777679443, 0.794387698173523, 0.9604658484458923], "text": " CNNbased attention network", "score": 0.8952100872993469, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 28354, "end": 28356, "seq_label": ["B-ReferenceLink"], "seq_token": ["34"], "seq_scores": [0.9959239959716797], "text": "34", "score": 0.9959239959716797, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27079, "end": 27096, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Attention", "\u0120models"], "seq_scores": [0.9708350300788879, 0.9626940488815308], "text": " Attention models", "score": 0.9667645394802094, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27226, "end": 27243, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120attention", "\u0120models"], "seq_scores": [0.9537214040756226, 0.9329146146774292], "text": " attention models", "score": 0.9433180093765259, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27271, "end": 27297, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120recurrent", "\u0120neural", "\u0120networks"], "seq_scores": [0.8854460716247559, 0.7826504707336426, 0.776705801486969], "text": " recurrent neural networks", "score": 0.8149341146151224, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27374, "end": 27401, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120recurrent", "\u0120neural", "\u0120network"], "seq_scores": [0.8064468502998352, 0.8646985292434692, 0.9012817740440369, 0.8789952993392944], "text": " a recurrent neural network", "score": 0.8628556132316589, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27403, "end": 27406, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["R", "NN"], "seq_scores": [0.6443086266517639, 0.8769519925117493], "text": "RNN", "score": 0.7606303095817566, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27667, "end": 27688, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120an", "\u0120approximate", "\u0120model"], "seq_scores": [0.9817360043525696, 0.9804907441139221, 0.9823795557022095], "text": " an approximate model", "score": 0.9815354347229004, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27727, "end": 27740, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120our", "\u0120approach"], "seq_scores": [0.5895700454711914, 0.5930562615394592], "text": " our approach", "score": 0.5913131535053253, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 27747, "end": 27754, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120images"], "seq_scores": [0.9489796161651611], "text": " images", "score": 0.9489796161651611, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 27812, "end": 27834, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120attention", "\u0120network"], "seq_scores": [0.6253766417503357, 0.6683687567710876, 0.5918529629707336], "text": " the attention network", "score": 0.6285327871640524, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 28056, "end": 28076, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["s", "\u0120the", "\u0120attention", "\u0120model"], "seq_scores": [0.5938596725463867, 0.9346050024032593, 0.9535195231437683, 0.9365352392196655], "text": " the attention model", "score": 0.85462985932827, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 28198, "end": 28229, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Our", "\u0120CNN", "based", "\u0120attention", "\u0120network"], "seq_scores": [0.8498991131782532, 0.8762751817703247, 0.8958567976951599, 0.9048507213592529, 0.9043896198272705], "text": " Our CNNbased attention network", "score": 0.8862542867660522, "type": "ScholarlyEntity"}]}, "filename": "10020_2001_00605.json", "id": "10020_2001_00605"}