{"text": "A Repository of Conversational Datasets github.com/PolyAI-LDN/conversational-datasets\n\nAbstract:\nProgress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.\n\n\n1 Introduction\nDialogue systems, sometimes referred to as conversational systems or conversational agents, are useful in a wide array of applications. They are used to assist users in accomplishing well-defined tasks such as finding and/or booking flights and restaurants (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), or to provide tourist information (Henderson et al., 2014c; Budzianowski et al., 2018). They have found applications in entertainment (Fraser et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). Conversational systems can also be used to aid in customer service 1 or to provide the foundation for intelligent virtual assistants such as Amazon Alexa, Google Assistant, or Apple Siri.\nModern approaches to constructing dialogue systems are almost exclusively data-driven, supported by modular or end-to-end machine learning frameworks (Young, 2010; Vinyals and Le, 2015; Wen et al., 2015 Wen et al., , 2017a,b;,b; Mrk\u0161i\u0107 and Vuli\u0107, 2018; Ramadan et al., 2018; Li et al., 2018, inter alia). The research community, as in any machine learning field, benefits from large datasets and standardised evaluation metrics for tracking and comparing different models. However, collecting data to train data-driven dialogue systems has proven notoriously difficult. First, system designers must construct an ontology to define the constrained set of actions and conversations that the system can support (Henderson et al., 2014a,c; Mrk\u0161i\u0107 et al., 2015). Furthermore, task-oriented dialogue data must be labeled with highly domain-specific dialogue annotations (El Asri et al., 2017; Budzianowski et al., 2018). Because of this, such annotated dialogue datasets remain scarce, and limited in both their size and in the number of domains they cover. For instance, the recently published MultiWOZ dataset (Budzianowski et al., 2018) contains a total of 115,424 dialogue turns scattered over 7 target domains. Other standard task-based datasets are typically single-domain and smaller by several orders of magnitude: DSTC2 (Henderson et al., 2014b) contains 23,354 turns, Frames (El Asri et al., 2017) comprises 19,986 turns, and M2M (Shah et al., 2018) spans 14,796 turns.\nAn alternative solution is to leverage larger conversational datasets available online. Such datasets provide natural conversational structure, that is, the inherent context-to-response relationship which is vital for dialogue modeling. In this work, we present a public repository of three large and diverse conversational datasets containing hundreds of millions of conversation examples. Compared to the most popular conversational datasets used in prior work, such as length-restricted Twitter conversations (Ritter et al., 2010) or very technical domain-restricted technical chats from the Ubuntu corpus (Lowe et al., 2015 (Lowe et al., , 2017;; Gunasekara et al., 2019), conversations from the three conversational datasets available in the repository are more nat-ural and diverse. What is more, the datasets are large: for instance, after preprocessing around 3.7B comments from Reddit available in 256M conversational threads, we obtain 727M valid contextresponse pairs. Similarly, the number of valid pairs in the OpenSubtitles dataset is 316 million. To put these numbers into perspective, the frequently used Ubuntu corpus v2.0 comprises around 4M dialogue turns. Furthermore, our Reddit corpus includes 2 more years of data and so is substantially larger than the previous Reddit dataset of Al-Rfou et al. (2016), which spans around 2.1B comments and 133M conversational threads, and is not publicly available.\nBesides the repository of large datasets, another key contribution of this work is the common evaluation framework. We propose applying consistent data filtering and preprocessing to public datasets, and a simple evaluation metric for response selection, which will facilitate direct comparisons between models from different research groups.\nThese large conversational datasets may support modeling across a large spectrum of natural conversational domains. Similar to the recent work on language model pretraining for diverse NLP applications (Howard and Ruder, 2018; Devlin et al., 2018; Lample and Conneau, 2019), we believe that these datasets can be used in future work to pretrain large general-domain conversational models that are then fine-tuned towards specific tasks using much smaller amounts of task-specific conversational data. We hope that the presented repository, containing a set of strong baseline models and standardised modes of evaluation, will provide means and guidance to the development of nextgeneration conversational systems.\nThe repository is available at github.com/ PolyAI-LDN/conversational-datasets.\n\n2 Conversational Dataset Format\nDatasets are stored as Tensorflow record files containing serialized Tensorflow example protocol buffers (Abadi et al., 2015). Figure 1: An illustrative Tensorflow example in a conversational dataset, consisting of a conversational context and an appropriate response. Each string is stored as a bytes feature using its UTF-8 encoding.\nEach Tensorflow example contains a conversational context and a response that goes with that context, see e.g. figure 1. Explicitly, each example contains a number of string features:\n\u2022 A context feature, the most recent text in the conversational context.\n\u2022 A response feature, text that is in direct response to the context.\n\u2022 A number of extra context features, context/0, context/1 etc. going back in time through the conversation. They are named in reverse order so that context/i always refers to the i th most recent extra context, so that no padding needs to be done, and datasets with different numbers of extra contexts can be mixed.\n\u2022 Depending on the dataset, there may be some extra features also included in each example.\nFor instance, in Reddit the author of the context and response are identified using additional features.\n\n3 Datasets\nRather than providing the raw processed data, we provide scripts and instructions to the users to generate the data themselves. This allows for viewing and potentially manipulating the pre-processing and filtering steps. The repository contains instructions for generating datasets with standard parameters split deterministically into train and test portions. These allow for defining reproducible evaluations in research papers. Section 5 presents benchmark results on these standard datasets for a variety of conversational response selection models. Dataset creation scripts are written using Apache Beam and Google Cloud Dataflow (Akidau et al., 2015), which parallelizes the work across many machines. Using the default quotas, the Reddit script Table 1 provides an overview of the Reddit, OpenSubtitles and AmazonQA datasets, and figure 3 in appendix A gives an illustrative example from each.\n\n3.1 Reddit\nReddit is an American social news aggregation website, where users can post links, and take part in discussions on these posts. Reddit is extremely diverse (Schrading et al., 2015; Al-Rfou et al., 2016) : there are more than 300,000 sub-forums (i.e., subreddits) covering various topics of discussion. These threaded discussions, available in a public BigQuery database, provide a large corpus of conversational contexts paired with appropriate responses. Reddit data has been used to create conversational response selection data by Al-Rfou et al. (2016); Cer et al. (2018); Yang et al. (2018). We share code that allows generating datasets from the Reddit data in a reproducible manner: with consistent filtering, processing, and train/test splitting. We also generate data using two more years of data than the previous work, 3.7 billion comments rather than 2.1 billion, giving a final dataset with 176 million more examples.\nReddit conversations are threaded. Each post may have multiple top-level comments, and every comment may have multiple children comments written in response. In processing, each Reddit thread is used to generate a set of examples. Each response comment generates an example, where the context is the linear path of comments that the comment is in response to.\nExamples may be filtered according to the contents of the context and response features. The example is filtered if either feature has more than 128 characters, or fewer than 9 characters, or if its text is set to [deleted] or [removed]. Full details of the filtering are available in the code, and configurable through command-line flags.\nFurther back contexts, from the comment's parent's parent etc., are stored as extra context features. Their texts are trimmed to be at most 128 characters in length, without splitting words apart. This helps to bound the size of an individual example.\nThe train/test split is deterministic based on the thread ID. As long as all the input to the script is held constant (the input tables, filtering thresholds etc.), the resulting datasets should be identical.\nThe data from 2015 to 2018 inclusive consists of 3,680,746,776 comments, in 256,095,216 threads. In total, 727,013,715 Tensorflow examples are created from this data.\n\n3.2 OpenSubtitles\nOpenSubtitles is a growing online collection of subtitles for movies and television shows available in multiple languages. As a starting point, we use the corpus collected by Lison and Tiedemann (2016), originally intended for statistical machine translation. This corpus is regenerated every year, in 62 different languages.\nConsecutive lines in the subtitle data are used to create conversational examples. There is no guarantee that different lines correspond to different speakers, or that consecutive lines belong to the same scene, or even the same show. The data nevertheless contains a lot of interesting examples for modelling the mapping from conversational contexts to responses.\nShort and long lines are filtered, and some text is filtered such as character names and auditory description text. The English 2018 data consists\n\nInput\n\n\nCandidate Responses\nI watched a great movie yesterday.\nNo, it won't rain probably.\nHave you applied for that job yet? It is nominated for the Golden Globe.\nWe are leaving for a ski trip tomorrow.\nIt is extremely easy working with them. I prefer traveling in the springtime.\n\nInput Candidate Responses\nIs that place affordable? Absolutely, call me any time! There is no place like home. The restaurant serves Japanese food. I would say that the prices are reasonable. This was their second warning.\nIt was so unfortunate to concede the goal.\nFigure 2 : Two examples illustrating the conversational response selection task: given the input context sentence, the goal is to identify the relevant response from a large pool of candidate responses.\nof 441,450,449 lines, and generates 316,891,717 examples. The data is split into chunks of 100,000 lines, and each chunk is used either for the train set or the test set.\n\n3.3 AmazonQA\nThis dataset is based on a corpus extracted by Wan and McAuley (2016); McAuley and Yang (2016), who scraped questions and answers from Amazon product pages. This provides a corpus of questionanswer pairs in the e-commerce domain. Some questions may have multiple answers, so one example is generated for each possible answer.\nExamples with very short or long questions or responses are filtered from the data, resulting in a total of 3,689,912 examples. The train/test split is computed deterministically using the product ID.\n\n4 Response Selection Task\nThe conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017).\nThe task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Al-Rfou et al., 2016; Henderson et al., 2017; Lowe et al., 2017; Wu et al., 2017; Cer et al., 2018; Chaudhuri et al., 2018; Du and Black, 2018; Kumar et al., 2018; Liu et al., 2018; Yang et al., 2018; Zhou et al., 2018; Gunasekara et al., 2019; Tao et al., 2019). Models trained to select responses can be used to drive dialogue systems, question-answering systems, and response suggestion systems. The task of response selection provides a powerful signal for learning implicit semantic representations useful for many downstream tasks in natural language understanding (Cer et al., 2018; Yang et al., 2018).\nThe Recall@k metric allows for direct comparison between models. Direct comparisons are much more difficult for generative models, which are typically evaluated using perplexity scores or using human judgement. Perplexity scores are dependent on normalization, tokenization, and choice of vocabulary, while human judgement is expensive and time consuming.\nWhen evaluating conversational response selection models on these datasets, we propose a Recall@k metric termed 1-of-100 accuracy. This is Recall@1 using 99 responses sampled from the test dataset as negatives. This 1-of-100 accuracy metric has been used in previous studies: (Al-Rfou et al., 2016; Henderson et al., 2017; Cer et al., 2018; Kumar et al., 2018; Yang et al., 2018; Gunasekara et al., 2019). While there is no guarantee that the 99 randomly selected negatives will all be bad responses, the metric nevertheless provides a simple summary of model performance that has been shown to correlate with user-driven quality metrics (Henderson et al., 2017). For efficient computation of this metric, batches of 100 (context, response) pairs can be processed such that the other 99 elements in the batch serve as the negative examples. Sections 4.1 and 4.2 present baseline methods of conversational response selection that are implemented in the repository. These baselines are intended to run quickly using a subset of the training data, to give some idea of performance and characteristics of each dataset. Section 4.3 describes a more competitive neural encoder model that is trained on the entire training set.\n\n4.1 Keyword-based Methods\nThe keyword-based baselines use keyword similarity metrics to rank responses given a context. These are typical baselines for information retrieval tasks. The TF-IDF method computes inverse document frequency statistics on the training set, and scores responses using their tf-idf cosine similarity to the context (Manning et al., 2008).\nThe BM25 method builds on top of the tf-idf similarity, applying an adjustment to the term weights (Robertson and Zaragoza, 2009).\n\n4.2 Vector-based Methods\nThe vector-based methods use publicly available neural net embedding models to embed contexts and responses into a vector space. We include the following five embedding models in the evaluation, all of which are available on Tensorflow Hub: BERT-SMALL the deep bidirectional transformer model of Devlin et al. (2018).\nBERT-LARGE a larger deep bidirectional transformer model.\nThere are two vector-based baseline methods, one for each of the above models. The SIM method ranks responses according to their cosine similarity with the context vector. This method relies on pretrained models and does not use the training set at all.\nThe MAP method learns a linear mapping on top of the response vector. The final score of a response with vector y given a context with vector x is the cosine similarity \u2022, \u2022 of the context vector with the mapped response vector:x, (W + \u03b1I) \u2022 y (1)\nwhere W, \u03b1 are learned parameters and I is the identity matrix. This allows learning an arbitrary linear mapping on the context side, while the residual connection gated by \u03b1 makes it easy for the model to interpolate with the SIM baseline. Vectors are L2-normalized before being fed to the MAP method, so that the method is invariant to scaling.\nThe W and \u03b1 parameters are learned on a random sample of 10,000 examples from the training set, using the dot product loss from Henderson et al. (2017). A sweep over learning rate and regularization parameters is performed using a held-out development set. The final learned parameters are used on the evaluation set.\nThe combination of the three embedding models with the two vector-based methods results in the following six baseline methods: USE-SIM, USE-MAP, USE-LARGE-SIM, USE-LARGE-MAP, ELMO-SIM, and ELMO-MAP.\n\n4.3 Encoder Model\nWe also train and evaluate a neural encoder model that maps the context and response through separate sub-networks to a shared vector space, where the final score is a dot-product between a vector representing the context and a vector representing the response as per Henderson et al. (2017); Cer et al. (2018); Kumar et al. (2018); Yang et al. (2018). This model is referred to as POLYAI-ENCODER in the evaluation.\nFull details of the neural structure are given in Henderson et al. (2019). To summarize, the context and response are both separately passed through sub-networks that:\n1. split the text into unigram and bigram features 2. convert unigrams and bigrams to numeric IDs using a vocabulary of known features in conjunction with a hashing strategy for unseen features 3. separately embed the unigrams and bigrams using large embedding matrices 4. separately apply self-attention then reduction over the sequence dimension to the unigram and bigram embeddings 5. combine the unigram and bigram representations, then pass them through several dense hidden layers 6. L2-normalize the final hidden layer to obtain the final vector representation Both sub-networks are trained jointly using the dot-product loss of Henderson et al. (2017), with label smoothing and a learned scaling factor.\n\n5 Evaluation\nAll the methods discussed in section 4 are evaluated on the three standard datasets from section 3,\n\nReddit\nOpenSubtitles AmazonQA  The keyword-based TF-IDF and BM25 are broadly competitive with the vector-based methods, and are particularly strong for AmazonQA, possibly because rare words such as the product name are informative in this domain. Learning a mapping with the MAP method gives a consistent boost in performance over the SIM method, showing the importance of learning the mapping from context to response versus simply relying on similarity. This approach would benefit from more data and a more powerful mapping network, but we have constrained the baselines so that they run quickly on a single computer. The Universal Sentence Encoder model outperforms ELMo in all cases.\nThe POLYAI-ENCODER model significantly outperforms all of the baseline methods. This is not surprising, as it is trained on the entire training set using multiple GPUs for several hours. We welcome other research groups to share their results, and we will be growing the table of results in the repository.\n\n6 Conclusion\nThis paper has introduced a repository of conversational datasets, providing hundreds of millions examples for training and evaluating conversational response selection systems under a standard evaluation framework. Future work will involve introducing more datasets in this format, more competitive baselines, and more benchmark results. We welcome contributions from other research groups in all of these directions. Features with a star are used to compute the deterministic train/test split.\n\nA Appendix\n\n\nFootnotes:\n1: For an overview, see poly-ai.com/blog/towards-aiassisted-customer-support-automation\n\nReferences:\n\n- Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor- rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Mar- tin Wattenberg, Martin Wicke, Yuan Yu, and Xiao- qiang Zheng. 2015. TensorFlow: Large-scale ma- chine learning on heterogeneous systems. Software available from tensorflow.org.- Tyler Akidau, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fern\u00e1ndez-Moctezuma, Reuven Lax, Sam McVeety, Daniel Mills, Frances Perry, Eric Schmidt, and Sam Whittle. 2015. The dataflow model: A practical approach to balancing correctness, latency, and cost in massive-scale, un- bounded, out-of-order data processing. Proceedings of the VLDB Endowment, 8.\n\n- Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun- Hsuan Sung, Brian Strope, and Ray Kurzweil. 2016. Conversational contextual cues: The case of person- alization and history for response ranking. CoRR, abs/1606.00372.\n\n- Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra- madan, and Milica Ga\u0161i\u0107. 2018. MultiWOZ -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Proceedings of EMNLP.\n\n- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con- stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder. CoRR, abs/1803.11175.\n\n- Debanjan Chaudhuri, Agustinus Kristiadi, Jens Lehmann, and Asja Fischer. 2018. Improving response selection in multi-turn dialogue systems by incorporating domain knowledge. In Proceedings of CoNLL.\n\n- Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Recent advances and new frontiers. CoRR, abs/1711.01731.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.\n\n- Wenchao Du and Alan Black. 2018. Data augmenta- tion for neural online chats response selection. In Proceedings of SCAI.\n\n- Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. 2017. Frames: A corpus for adding memory to goal-oriented dialogue systems. In Proceedings of SIGDIAL.\n\n- Ahmed Fadhil and Gianluca Schiavo. 2019. Designing for health chatbots. CoRR, abs/1902.09022.\n\n- Jamie Fraser, Ioannis Papaioannou, and Oliver Lemon. 2018. Spoken conversational AI in video games: Emotional dialogue management increases user en- gagement. In Proceedings of IVA.\n\n- Chulaka Gunasekara, Jonathan K Kummerfeld, Lazaros Polymenakos, and Walter S Lasecki. 2019. DSTC7 task 1: Noetic end-to-end response selection.\n\n- Charles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS Spoken Language Sys- tems Pilot Corpus. In Proceedings of the Workshop on Speech and Natural Language, HLT '90.\n\n- Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun- Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Sanjiv Ku- mar, Balint Miklos, and Ray Kurzweil. 2017. Effi- cient natural language response suggestion for Smart Reply. CoRR, abs/1705.00652.\n\n- Matthew Henderson, Blaise Thomson, and Jason D. Wiliams. 2014a. The Second Dialog State Tracking Challenge. In Proceedings of SIGDIAL, pages 263- 272.\n\n- Matthew Henderson, Blaise Thomson, and Jason Williams. 2014b. The Second Dialog State Track- ing Challenge. In Proceedings of SIGDIAL.\n\n- Matthew Henderson, Blaise Thomson, and Steve Young. 2014c. Word-based dialog state tracking with recurrent neural networks. In Proceedings of SIGDIAL.\n\n- Matthew Henderson, Ivan Vuli\u0107, Daniela Gerz, I\u00f1igo Casanueva, Pawe\u0142 Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrk\u0161i\u0107, and Pei-Hao Su. 2019. Training neural re- sponse selection for task-oriented dialogue systems. In Proceedings of ACL.\n\n- Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL, pages 328-339.\n\n- Michimasa Inaba and Kenichi Takahashi. 2016. Neural utterance ranking model for conversational dialogue systems. In Proceedings of SIGDIAL.\n\n- Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos, Greg Corrado, L\u00e1szl\u00f3 Luk\u00e1cs, Marina Ganea, Peter Young, and Vivek Ramavajjala. 2016. Smart Reply: Automated response suggestion for email. In Pro- ceedings of KDD.\n\n- Girish Kumar, Matthew Henderson, Shannon Chan, Hoang Nguyen, and Lucas Ngoo. 2018. Question- answer selection in user to user marketplace conver- sations. In Proceedings of IWSDS.\n\n- Guillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. CoRR, abs/1901.07291.\n\n- Liliana Laranjo, Adam G. Dunn, Huong Ly Tong, Ah- met Baki Kocaballi, Jessica Chen, Rabia Bashir, Didi Surian, Blanca Gallego, Farah Magrabi, An- nie Y.S. Lau, and Enrico Coiera. 2018. Conver- sational agents in healthcare: A systematic review. Journal of the American Medical Informatics Asso- ciation, 25(9):1248-1258.\n\n- Xiujun Li, Sarah Panda, Jingjing Liu, and Jianfeng Gao. 2018. Microsoft dialogue challenge: Building end- to-end task-completion dialogue systems. CoRR, abs/1807.11125.\n\n- Pierre Lison and J\u00f6rg Tiedemann. 2016. Opensub- titles2016: Extracting large parallel corpora from movie and TV subtitles.\n\n- Bing Liu, Tong Yu, Ian Lane, and Ole J Mengshoel. 2018. Customized nonlinear bandits for online re- sponse selection in neural conversation models. In Proceedings of AAAI.\n\n- Ryan Lowe, Nissan Pow, Iulian Serban, Laurent Char- lin, Chia-Wei Liu, and Joelle Pineau. 2017. Train- ing end-to-end dialogue systems with the Ubuntu di- alogue corpus. Dialogue & Discourse, 8(1).\n\n- Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dia- logue systems. In Proceedings of SIGDIAL.\n\n- Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. 2008. Introduction to Information Retrieval. Cambridge University Press.\n\n- Julian McAuley and Alex Yang. 2016. Addressing complex and subjective product-related queries with customer reviews. In Proceedings of the 25th In- ternational Conference on World Wide Web, WWW '16, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.\n\n- Nikola Mrk\u0161i\u0107 and Ivan Vuli\u0107. 2018. Fully statistical neural belief tracking. In Proceedings of ACL.\n\n- Nikola Mrk\u0161i\u0107, Diarmuid \u00d3 S\u00e9aghdha, Blaise Thom- son, Milica Ga\u0161i\u0107, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2015. Multi- domain dialog state tracking using recurrent neural networks. In Proceedings of ACL, pages 794-799.\n\n- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of NAACL.\n\n- Osman Ramadan, Pawe\u0142 Budzianowski, and Milica Ga- sic. 2018. Large-scale multi-domain belief tracking with knowledge sharing. In ACL.\n\n- Antoine Raux, Brian Langner, Alan W. Black, and Maxine Esk\u00e9nazi. 2003. LET's GO: Improving spo- ken dialog systems for the elderly and non-natives. In Proceedings of EUROSPEECH.\n\n- Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu- pervised modeling of Twitter conversations. In Pro- ceedings of NAACL-HLT.\n\n- Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of EMNLP, Stroudsburg, PA, USA. As- sociation for Computational Linguistics.\n\n- Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and be- yond. Foundations and Trends in Information Re- trieval, 3(4).\n\n- Nicolas Schrading, Cecilia Ovesdotter Alm, Ray Ptucha, and Christopher Homan. 2015. An analysis of domestic abuse discourse on Reddit. In Proceed- ings of EMNLP.\n\n- Iulian Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016. Generative deep neural networks for dialogue: A short review. CoRR, abs/1611.06216.\n\n- Pararth Shah, Dilek Hakkani-T\u00fcr, Bing Liu, and Gokhan T\u00fcr. 2018. Bootstrapping a neural conversa- tional agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. In Proceedings of NAACL-HLT.\n\n- Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu- ral responding machine for short-text conversation. CoRR, abs/1503.02364.\n\n- Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. CoRR, abs/1506.06714.\n\n- Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao, and Rui Yan. 2019. Multi- representation fusion network for multi-turn re- sponse selection in retrieval-based chatbots. In Pro- ceedings of WSDM.\n\n- Oriol Vinyals and Quoc V. Le. 2015. A neural conver- sational model. CoRR, abs/1506.05869.\n\n- Mengting Wan and Julian McAuley. 2016. Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems. In ICDM, pages 489-498.\n\n- Tsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u0107, Pei- Hao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned LSTM-based natural lan- guage generation for spoken dialogue systems. In Proceedings of EMNLP.\n\n- Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and Steve J. Young. 2017a. Latent intention dialogue models. In Proceedings of ICML.\n\n- Tsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017b. A network- based end-to-end trainable task-oriented dialogue system. In Proceedings of EACL.\n\n- Jason D. Williams. 2012. A belief tracking challenge task for spoken dialog systems. In Proceedings of the NAACL HLT Workshop on Future directions and needs in the Spoken Dialog Community.\n\n- Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhou- jun Li. 2017. Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots. In Proceedings of ACL.\n\n- Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learn- ing semantic textual similarity from conversations. In Proceedings of The Workshop on Representation Learning for NLP.\n\n- Steve Young. 2010. Still talking to machines (cog- nitively speaking).\n\n- In Proceedings of INTER- SPEECH.\n\n- Zhou Yu, Ziyu Xu, Alan W Black, and Alexander Rud- nicky. 2016. Strategy and policy learning for non- task-oriented conversational systems. In Proceed- ings of SIGDIAL.\n\n- Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu. 2018. Multi-turn response selection for chatbots with deep attention matching network. In Proceed- ings of ACL.\n\n", "annotations": {"Abstract": [{"begin": 87, "end": 859, "idx": 0}], "Head": [{"begin": 862, "end": 876, "n": "1", "idx": 0}, {"begin": 5700, "end": 5731, "n": "2", "idx": 1}, {"begin": 6910, "end": 6920, "n": "3", "idx": 2}, {"begin": 7823, "end": 7833, "n": "3.1", "idx": 3}, {"begin": 10093, "end": 10110, "n": "3.2", "idx": 4}, {"begin": 10950, "end": 10955, "idx": 5}, {"begin": 10958, "end": 10977, "idx": 6}, {"begin": 11233, "end": 11258, "idx": 7}, {"begin": 11874, "end": 11886, "n": "3.3", "idx": 8}, {"begin": 12415, "end": 12440, "n": "4", "idx": 9}, {"begin": 15597, "end": 15622, "n": "4.1", "idx": 10}, {"begin": 16093, "end": 16117, "n": "4.2", "idx": 11}, {"begin": 17861, "end": 17878, "n": "4.3", "idx": 12}, {"begin": 19176, "end": 19188, "n": "5", "idx": 13}, {"begin": 19290, "end": 19296, "idx": 14}, {"begin": 20287, "end": 20299, "n": "6", "idx": 15}, {"begin": 20797, "end": 20807, "idx": 16}], "ReferenceToBib": [{"begin": 1134, "end": 1157, "target": "#b13", "idx": 0}, {"begin": 1158, "end": 1173, "target": "#b51", "idx": 1}, {"begin": 1174, "end": 1195, "target": "#b9", "idx": 2}, {"begin": 1231, "end": 1256, "target": "#b17", "idx": 3}, {"begin": 1257, "end": 1283, "target": "#b3", "idx": 4}, {"begin": 1331, "end": 1352, "target": "#b11", "idx": 5}, {"begin": 1372, "end": 1391, "target": "#b36", "idx": 6}, {"begin": 1392, "end": 1410, "target": "#b6", "idx": 7}, {"begin": 1427, "end": 1449, "target": "#b24", "idx": 8}, {"begin": 1450, "end": 1475, "target": "#b10", "idx": 9}, {"begin": 1815, "end": 1828, "target": "#b54", "idx": 10}, {"begin": 1829, "end": 1850, "target": "#b46", "idx": 11}, {"begin": 1851, "end": 1867, "target": "#b48", "idx": 12}, {"begin": 1868, "end": 1893, "idx": 13}, {"begin": 1894, "end": 1917, "target": "#b32", "idx": 14}, {"begin": 1918, "end": 1939, "idx": 15}, {"begin": 1940, "end": 1968, "idx": 16}, {"begin": 2373, "end": 2400, "idx": 17}, {"begin": 2401, "end": 2421, "target": "#b33", "idx": 18}, {"begin": 2529, "end": 2551, "target": "#b9", "idx": 19}, {"begin": 2552, "end": 2578, "target": "#b3", "idx": 20}, {"begin": 2771, "end": 2798, "target": "#b3", "idx": 21}, {"begin": 2988, "end": 3012, "target": "#b16", "idx": 22}, {"begin": 3012, "end": 3065, "idx": 23}, {"begin": 3065, "end": 3118, "idx": 24}, {"begin": 3651, "end": 3672, "target": "#b37", "idx": 25}, {"begin": 3748, "end": 3766, "target": "#b29", "idx": 26}, {"begin": 3767, "end": 3789, "target": "#b28", "idx": 27}, {"begin": 3790, "end": 3814, "target": "#b12", "idx": 28}, {"begin": 4443, "end": 4464, "target": "#b2", "idx": 29}, {"begin": 5108, "end": 5132, "target": "#b19", "idx": 30}, {"begin": 5133, "end": 5153, "target": "#b7", "idx": 31}, {"begin": 5154, "end": 5179, "target": "#b23", "idx": 32}, {"begin": 5837, "end": 5857, "target": "#b0", "idx": 33}, {"begin": 7556, "end": 7577, "target": "#b1", "idx": 34}, {"begin": 7990, "end": 8014, "target": "#b40", "idx": 35}, {"begin": 8015, "end": 8036, "target": "#b2", "idx": 36}, {"begin": 8368, "end": 8389, "target": "#b2", "idx": 37}, {"begin": 8391, "end": 8408, "target": "#b4", "idx": 38}, {"begin": 8410, "end": 8428, "target": "#b53", "idx": 39}, {"begin": 10286, "end": 10312, "target": "#b26", "idx": 40}, {"begin": 11934, "end": 11956, "target": "#b47", "idx": 41}, {"begin": 11958, "end": 11981, "target": "#b31", "idx": 42}, {"begin": 12685, "end": 12706, "target": "#b41", "idx": 43}, {"begin": 12707, "end": 12727, "target": "#b38", "idx": 44}, {"begin": 12728, "end": 12749, "target": "#b46", "idx": 45}, {"begin": 12750, "end": 12771, "target": "#b44", "idx": 46}, {"begin": 12772, "end": 12791, "target": "#b43", "idx": 47}, {"begin": 12792, "end": 12812, "target": "#b21", "idx": 48}, {"begin": 12885, "end": 12904, "target": "#b29", "idx": 49}, {"begin": 12905, "end": 12931, "target": "#b20", "idx": 50}, {"begin": 12932, "end": 12948, "target": "#b56", "idx": 51}, {"begin": 12949, "end": 12972, "target": "#b14", "idx": 52}, {"begin": 13345, "end": 13364, "target": "#b29", "idx": 53}, {"begin": 13365, "end": 13391, "target": "#b20", "idx": 54}, {"begin": 13392, "end": 13408, "target": "#b56", "idx": 55}, {"begin": 13409, "end": 13430, "target": "#b2", "idx": 56}, {"begin": 13431, "end": 13454, "target": "#b14", "idx": 57}, {"begin": 13455, "end": 13473, "target": "#b28", "idx": 58}, {"begin": 13474, "end": 13490, "target": "#b52", "idx": 59}, {"begin": 13491, "end": 13508, "target": "#b4", "idx": 60}, {"begin": 13509, "end": 13532, "target": "#b5", "idx": 61}, {"begin": 13533, "end": 13552, "target": "#b8", "idx": 62}, {"begin": 13553, "end": 13572, "target": "#b22", "idx": 63}, {"begin": 13573, "end": 13590, "target": "#b27", "idx": 64}, {"begin": 13591, "end": 13609, "target": "#b53", "idx": 65}, {"begin": 13610, "end": 13628, "target": "#b57", "idx": 66}, {"begin": 13629, "end": 13653, "target": "#b12", "idx": 67}, {"begin": 13654, "end": 13671, "target": "#b45", "idx": 68}, {"begin": 13980, "end": 13998, "target": "#b4", "idx": 69}, {"begin": 13999, "end": 14017, "target": "#b53", "idx": 70}, {"begin": 14651, "end": 14673, "target": "#b2", "idx": 71}, {"begin": 14674, "end": 14697, "target": "#b14", "idx": 72}, {"begin": 14698, "end": 14715, "target": "#b4", "idx": 73}, {"begin": 14716, "end": 14735, "target": "#b22", "idx": 74}, {"begin": 14736, "end": 14754, "target": "#b53", "idx": 75}, {"begin": 14755, "end": 14779, "target": "#b12", "idx": 76}, {"begin": 15013, "end": 15037, "target": "#b14", "idx": 77}, {"begin": 15937, "end": 15959, "target": "#b30", "idx": 78}, {"begin": 16060, "end": 16090, "target": "#b39", "idx": 79}, {"begin": 16414, "end": 16434, "target": "#b7", "idx": 80}, {"begin": 17471, "end": 17494, "target": "#b14", "idx": 81}, {"begin": 18147, "end": 18170, "target": "#b14", "idx": 82}, {"begin": 18172, "end": 18189, "target": "#b4", "idx": 83}, {"begin": 18191, "end": 18210, "target": "#b22", "idx": 84}, {"begin": 18212, "end": 18230, "target": "#b53", "idx": 85}, {"begin": 18345, "end": 18368, "target": "#b18", "idx": 86}, {"begin": 19099, "end": 19122, "target": "#b14", "idx": 87}], "ReferenceToFootnote": [{"begin": 1544, "end": 1545, "target": "#foot_0", "idx": 0}], "SectionFootnote": [{"begin": 20810, "end": 20908, "idx": 0}], "ReferenceString": [{"begin": 20925, "end": 21646, "id": "b0", "idx": 0}, {"begin": 21648, "end": 22016, "id": "b1", "idx": 1}, {"begin": 22020, "end": 22236, "id": "b2", "idx": 2}, {"begin": 22240, "end": 22486, "id": "b3", "idx": 3}, {"begin": 22490, "end": 22741, "id": "b4", "idx": 4}, {"begin": 22745, "end": 22943, "id": "b5", "idx": 5}, {"begin": 22947, "end": 23096, "id": "b6", "idx": 6}, {"begin": 23100, "end": 23277, "id": "b7", "idx": 7}, {"begin": 23281, "end": 23401, "id": "b8", "idx": 8}, {"begin": 23405, "end": 23631, "id": "b9", "idx": 9}, {"begin": 23635, "end": 23728, "id": "b10", "idx": 10}, {"begin": 23732, "end": 23913, "id": "b11", "idx": 11}, {"begin": 23917, "end": 24060, "id": "b12", "idx": 12}, {"begin": 24064, "end": 24254, "id": "b13", "idx": 13}, {"begin": 24258, "end": 24490, "id": "b14", "idx": 14}, {"begin": 24494, "end": 24644, "id": "b15", "idx": 15}, {"begin": 24648, "end": 24782, "id": "b16", "idx": 16}, {"begin": 24786, "end": 24936, "id": "b17", "idx": 17}, {"begin": 24940, "end": 25205, "id": "b18", "idx": 18}, {"begin": 25209, "end": 25334, "id": "b19", "idx": 19}, {"begin": 25338, "end": 25477, "id": "b20", "idx": 20}, {"begin": 25481, "end": 25734, "id": "b21", "idx": 21}, {"begin": 25738, "end": 25917, "id": "b22", "idx": 22}, {"begin": 25921, "end": 26028, "id": "b23", "idx": 23}, {"begin": 26032, "end": 26352, "id": "b24", "idx": 24}, {"begin": 26356, "end": 26524, "id": "b25", "idx": 25}, {"begin": 26528, "end": 26650, "id": "b26", "idx": 26}, {"begin": 26654, "end": 26825, "id": "b27", "idx": 27}, {"begin": 26829, "end": 27026, "id": "b28", "idx": 28}, {"begin": 27030, "end": 27223, "id": "b29", "idx": 29}, {"begin": 27227, "end": 27364, "id": "b30", "idx": 30}, {"begin": 27368, "end": 27671, "id": "b31", "idx": 31}, {"begin": 27675, "end": 27775, "id": "b32", "idx": 32}, {"begin": 27779, "end": 28020, "id": "b33", "idx": 33}, {"begin": 28024, "end": 28211, "id": "b34", "idx": 34}, {"begin": 28215, "end": 28348, "id": "b35", "idx": 35}, {"begin": 28352, "end": 28529, "id": "b36", "idx": 36}, {"begin": 28533, "end": 28662, "id": "b37", "idx": 37}, {"begin": 28666, "end": 28861, "id": "b38", "idx": 38}, {"begin": 28865, "end": 29022, "id": "b39", "idx": 39}, {"begin": 29026, "end": 29187, "id": "b40", "idx": 40}, {"begin": 29191, "end": 29342, "id": "b41", "idx": 41}, {"begin": 29346, "end": 29560, "id": "b42", "idx": 42}, {"begin": 29564, "end": 29689, "id": "b43", "idx": 43}, {"begin": 29693, "end": 29948, "id": "b44", "idx": 44}, {"begin": 29952, "end": 30155, "id": "b45", "idx": 45}, {"begin": 30159, "end": 30249, "id": "b46", "idx": 46}, {"begin": 30253, "end": 30413, "id": "b47", "idx": 47}, {"begin": 30417, "end": 30633, "id": "b48", "idx": 48}, {"begin": 30637, "end": 30764, "id": "b49", "idx": 49}, {"begin": 30768, "end": 30995, "id": "b50", "idx": 50}, {"begin": 30999, "end": 31187, "id": "b51", "idx": 51}, {"begin": 31191, "end": 31385, "id": "b52", "idx": 52}, {"begin": 31389, "end": 31658, "id": "b53", "idx": 53}, {"begin": 31662, "end": 31732, "id": "b54", "idx": 54}, {"begin": 31736, "end": 31768, "id": "b55", "idx": 55}, {"begin": 31772, "end": 31940, "id": "b56", "idx": 56}, {"begin": 31944, "end": 32151, "id": "b57", "idx": 57}], "ReferenceToTable": [{"begin": 7679, "end": 7680, "target": "#tab_1", "idx": 0}], "Footnote": [{"begin": 20821, "end": 20908, "id": "foot_0", "n": "1", "idx": 0}], "Paragraph": [{"begin": 97, "end": 859, "idx": 0}, {"begin": 877, "end": 1664, "idx": 1}, {"begin": 1665, "end": 3138, "idx": 2}, {"begin": 3139, "end": 4562, "idx": 3}, {"begin": 4563, "end": 4905, "idx": 4}, {"begin": 4906, "end": 5619, "idx": 5}, {"begin": 5620, "end": 5698, "idx": 6}, {"begin": 5732, "end": 6067, "idx": 7}, {"begin": 6068, "end": 6251, "idx": 8}, {"begin": 6252, "end": 6324, "idx": 9}, {"begin": 6325, "end": 6394, "idx": 10}, {"begin": 6395, "end": 6711, "idx": 11}, {"begin": 6712, "end": 6803, "idx": 12}, {"begin": 6804, "end": 6908, "idx": 13}, {"begin": 6921, "end": 7821, "idx": 14}, {"begin": 7834, "end": 8763, "idx": 15}, {"begin": 8764, "end": 9123, "idx": 16}, {"begin": 9124, "end": 9463, "idx": 17}, {"begin": 9464, "end": 9715, "idx": 18}, {"begin": 9716, "end": 9924, "idx": 19}, {"begin": 9925, "end": 10091, "idx": 20}, {"begin": 10111, "end": 10436, "idx": 21}, {"begin": 10437, "end": 10801, "idx": 22}, {"begin": 10802, "end": 10948, "idx": 23}, {"begin": 10978, "end": 11012, "idx": 24}, {"begin": 11013, "end": 11040, "idx": 25}, {"begin": 11041, "end": 11113, "idx": 26}, {"begin": 11114, "end": 11153, "idx": 27}, {"begin": 11154, "end": 11231, "idx": 28}, {"begin": 11259, "end": 11455, "idx": 29}, {"begin": 11456, "end": 11498, "idx": 30}, {"begin": 11499, "end": 11701, "idx": 31}, {"begin": 11702, "end": 11872, "idx": 32}, {"begin": 11887, "end": 12212, "idx": 33}, {"begin": 12213, "end": 12413, "idx": 34}, {"begin": 12441, "end": 12973, "idx": 35}, {"begin": 12974, "end": 14018, "idx": 36}, {"begin": 14019, "end": 14374, "idx": 37}, {"begin": 14375, "end": 15595, "idx": 38}, {"begin": 15623, "end": 15960, "idx": 39}, {"begin": 15961, "end": 16091, "idx": 40}, {"begin": 16118, "end": 16435, "idx": 41}, {"begin": 16436, "end": 16493, "idx": 42}, {"begin": 16494, "end": 16747, "idx": 43}, {"begin": 16748, "end": 16976, "idx": 44}, {"begin": 16996, "end": 17342, "idx": 45}, {"begin": 17343, "end": 17660, "idx": 46}, {"begin": 17661, "end": 17859, "idx": 47}, {"begin": 17879, "end": 18294, "idx": 48}, {"begin": 18295, "end": 18462, "idx": 49}, {"begin": 18463, "end": 19174, "idx": 50}, {"begin": 19189, "end": 19288, "idx": 51}, {"begin": 19297, "end": 19978, "idx": 52}, {"begin": 19979, "end": 20285, "idx": 53}, {"begin": 20300, "end": 20795, "idx": 54}], "SectionHeader": [{"begin": 0, "end": 859, "idx": 0}], "SectionReference": [{"begin": 20910, "end": 32153, "idx": 0}], "Sentence": [{"begin": 97, "end": 249, "idx": 0}, {"begin": 250, "end": 471, "idx": 1}, {"begin": 472, "end": 634, "idx": 2}, {"begin": 635, "end": 859, "idx": 3}, {"begin": 877, "end": 1012, "idx": 4}, {"begin": 1013, "end": 1284, "idx": 5}, {"begin": 1285, "end": 1476, "idx": 6}, {"begin": 1477, "end": 1664, "idx": 7}, {"begin": 1665, "end": 1969, "idx": 8}, {"begin": 1970, "end": 2137, "idx": 9}, {"begin": 2138, "end": 2234, "idx": 10}, {"begin": 2235, "end": 2422, "idx": 11}, {"begin": 2423, "end": 2579, "idx": 12}, {"begin": 2580, "end": 2716, "idx": 13}, {"begin": 2717, "end": 2874, "idx": 14}, {"begin": 2875, "end": 3138, "idx": 15}, {"begin": 3139, "end": 3226, "idx": 16}, {"begin": 3227, "end": 3375, "idx": 17}, {"begin": 3376, "end": 3529, "idx": 18}, {"begin": 3530, "end": 3927, "idx": 19}, {"begin": 3928, "end": 4118, "idx": 20}, {"begin": 4119, "end": 4200, "idx": 21}, {"begin": 4201, "end": 4314, "idx": 22}, {"begin": 4315, "end": 4562, "idx": 23}, {"begin": 4563, "end": 4678, "idx": 24}, {"begin": 4679, "end": 4905, "idx": 25}, {"begin": 4906, "end": 5021, "idx": 26}, {"begin": 5022, "end": 5406, "idx": 27}, {"begin": 5407, "end": 5619, "idx": 28}, {"begin": 5620, "end": 5662, "idx": 29}, {"begin": 5663, "end": 5698, "idx": 30}, {"begin": 5732, "end": 5858, "idx": 31}, {"begin": 5859, "end": 6000, "idx": 32}, {"begin": 6001, "end": 6067, "idx": 33}, {"begin": 6068, "end": 6178, "idx": 34}, {"begin": 6179, "end": 6251, "idx": 35}, {"begin": 6252, "end": 6324, "idx": 36}, {"begin": 6325, "end": 6394, "idx": 37}, {"begin": 6395, "end": 6503, "idx": 38}, {"begin": 6504, "end": 6711, "idx": 39}, {"begin": 6712, "end": 6803, "idx": 40}, {"begin": 6804, "end": 6908, "idx": 41}, {"begin": 6921, "end": 7048, "idx": 42}, {"begin": 7049, "end": 7141, "idx": 43}, {"begin": 7142, "end": 7281, "idx": 44}, {"begin": 7282, "end": 7351, "idx": 45}, {"begin": 7352, "end": 7474, "idx": 46}, {"begin": 7475, "end": 7628, "idx": 47}, {"begin": 7629, "end": 7821, "idx": 48}, {"begin": 7834, "end": 7961, "idx": 49}, {"begin": 7962, "end": 8135, "idx": 50}, {"begin": 8136, "end": 8289, "idx": 51}, {"begin": 8290, "end": 8429, "idx": 52}, {"begin": 8430, "end": 8587, "idx": 53}, {"begin": 8588, "end": 8763, "idx": 54}, {"begin": 8764, "end": 8798, "idx": 55}, {"begin": 8799, "end": 8921, "idx": 56}, {"begin": 8922, "end": 8994, "idx": 57}, {"begin": 8995, "end": 9123, "idx": 58}, {"begin": 9124, "end": 9212, "idx": 59}, {"begin": 9213, "end": 9361, "idx": 60}, {"begin": 9362, "end": 9463, "idx": 61}, {"begin": 9464, "end": 9565, "idx": 62}, {"begin": 9566, "end": 9660, "idx": 63}, {"begin": 9661, "end": 9715, "idx": 64}, {"begin": 9716, "end": 9777, "idx": 65}, {"begin": 9778, "end": 9924, "idx": 66}, {"begin": 9925, "end": 10021, "idx": 67}, {"begin": 10022, "end": 10091, "idx": 68}, {"begin": 10111, "end": 10233, "idx": 69}, {"begin": 10234, "end": 10370, "idx": 70}, {"begin": 10371, "end": 10436, "idx": 71}, {"begin": 10437, "end": 10519, "idx": 72}, {"begin": 10520, "end": 10671, "idx": 73}, {"begin": 10672, "end": 10801, "idx": 74}, {"begin": 10802, "end": 10917, "idx": 75}, {"begin": 10918, "end": 10948, "idx": 76}, {"begin": 10978, "end": 11012, "idx": 77}, {"begin": 11013, "end": 11040, "idx": 78}, {"begin": 11041, "end": 11075, "idx": 79}, {"begin": 11076, "end": 11113, "idx": 80}, {"begin": 11114, "end": 11153, "idx": 81}, {"begin": 11154, "end": 11193, "idx": 82}, {"begin": 11194, "end": 11231, "idx": 83}, {"begin": 11259, "end": 11284, "idx": 84}, {"begin": 11285, "end": 11314, "idx": 85}, {"begin": 11315, "end": 11343, "idx": 86}, {"begin": 11344, "end": 11380, "idx": 87}, {"begin": 11381, "end": 11424, "idx": 88}, {"begin": 11425, "end": 11455, "idx": 89}, {"begin": 11456, "end": 11498, "idx": 90}, {"begin": 11499, "end": 11701, "idx": 91}, {"begin": 11702, "end": 11759, "idx": 92}, {"begin": 11760, "end": 11872, "idx": 93}, {"begin": 11887, "end": 12043, "idx": 94}, {"begin": 12044, "end": 12116, "idx": 95}, {"begin": 12117, "end": 12212, "idx": 96}, {"begin": 12213, "end": 12340, "idx": 97}, {"begin": 12341, "end": 12413, "idx": 98}, {"begin": 12441, "end": 12586, "idx": 99}, {"begin": 12587, "end": 12973, "idx": 100}, {"begin": 12974, "end": 13248, "idx": 101}, {"begin": 13249, "end": 13672, "idx": 102}, {"begin": 13673, "end": 13807, "idx": 103}, {"begin": 13808, "end": 14018, "idx": 104}, {"begin": 14019, "end": 14083, "idx": 105}, {"begin": 14084, "end": 14229, "idx": 106}, {"begin": 14230, "end": 14374, "idx": 107}, {"begin": 14375, "end": 14505, "idx": 108}, {"begin": 14506, "end": 14585, "idx": 109}, {"begin": 14586, "end": 14780, "idx": 110}, {"begin": 14781, "end": 15038, "idx": 111}, {"begin": 15039, "end": 15215, "idx": 112}, {"begin": 15216, "end": 15338, "idx": 113}, {"begin": 15339, "end": 15489, "idx": 114}, {"begin": 15490, "end": 15595, "idx": 115}, {"begin": 15623, "end": 15716, "idx": 116}, {"begin": 15717, "end": 15777, "idx": 117}, {"begin": 15778, "end": 15960, "idx": 118}, {"begin": 15961, "end": 16091, "idx": 119}, {"begin": 16118, "end": 16246, "idx": 120}, {"begin": 16247, "end": 16435, "idx": 121}, {"begin": 16436, "end": 16493, "idx": 122}, {"begin": 16494, "end": 16572, "idx": 123}, {"begin": 16573, "end": 16665, "idx": 124}, {"begin": 16666, "end": 16747, "idx": 125}, {"begin": 16748, "end": 16817, "idx": 126}, {"begin": 16818, "end": 16976, "idx": 127}, {"begin": 16996, "end": 17059, "idx": 128}, {"begin": 17060, "end": 17236, "idx": 129}, {"begin": 17237, "end": 17342, "idx": 130}, {"begin": 17343, "end": 17495, "idx": 131}, {"begin": 17496, "end": 17599, "idx": 132}, {"begin": 17600, "end": 17660, "idx": 133}, {"begin": 17661, "end": 17859, "idx": 134}, {"begin": 17879, "end": 18231, "idx": 135}, {"begin": 18232, "end": 18294, "idx": 136}, {"begin": 18295, "end": 18369, "idx": 137}, {"begin": 18370, "end": 18462, "idx": 138}, {"begin": 18463, "end": 19174, "idx": 139}, {"begin": 19189, "end": 19288, "idx": 140}, {"begin": 19297, "end": 19536, "idx": 141}, {"begin": 19537, "end": 19745, "idx": 142}, {"begin": 19746, "end": 19910, "idx": 143}, {"begin": 19911, "end": 19978, "idx": 144}, {"begin": 19979, "end": 20058, "idx": 145}, {"begin": 20059, "end": 20165, "idx": 146}, {"begin": 20166, "end": 20285, "idx": 147}, {"begin": 20300, "end": 20515, "idx": 148}, {"begin": 20516, "end": 20638, "idx": 149}, {"begin": 20639, "end": 20718, "idx": 150}, {"begin": 20719, "end": 20795, "idx": 151}], "ReferenceToFigure": [{"begin": 11506, "end": 11507, "idx": 0}, {"begin": 13140, "end": 13141, "idx": 1}], "Div": [{"begin": 97, "end": 859, "idx": 0}, {"begin": 862, "end": 5698, "idx": 1}, {"begin": 5700, "end": 6908, "idx": 2}, {"begin": 6910, "end": 7821, "idx": 3}, {"begin": 7823, "end": 10091, "idx": 4}, {"begin": 10093, "end": 10948, "idx": 5}, {"begin": 10950, "end": 10956, "idx": 6}, {"begin": 10958, "end": 11231, "idx": 7}, {"begin": 11233, "end": 11872, "idx": 8}, {"begin": 11874, "end": 12413, "idx": 9}, {"begin": 12415, "end": 15595, "idx": 10}, {"begin": 15597, "end": 16091, "idx": 11}, {"begin": 16093, "end": 17859, "idx": 12}, {"begin": 17861, "end": 19174, "idx": 13}, {"begin": 19176, "end": 19288, "idx": 14}, {"begin": 19290, "end": 20285, "idx": 15}, {"begin": 20287, "end": 20795, "idx": 16}, {"begin": 20797, "end": 20808, "idx": 17}], "SectionMain": [{"begin": 859, "end": 20808, "idx": 0}], "ScholarlyEntity": [{"label": "Method", "begin": 108, "end": 125, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Machine", "\u0120Learning"], "seq_scores": [0.8274962902069092, 0.8203393220901489], "text": " Machine Learning", "score": 0.823917806148529, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 405, "end": 439, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.9610193371772766, 0.965440571308136, 0.9738145470619202, 0.9737809896469116], "text": " conversational response selection", "score": 0.9685138612985611, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 694, "end": 728, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120pre", "-", "processing", "\u0120data", "\u0120filtering", "\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.4160699248313904, 0.923019528388977, 0.904922604560852, 0.6045716404914856, 0.9325394034385681, 0.9847589135169983, 0.9890528321266174, 0.9886221289634705, 0.9900292754173279], "text": " conversational response selection", "score": 0.8592873613039652, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 164, "end": 179, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120datasets"], "seq_scores": [0.9802157282829285, 0.9967764019966125], "text": " large datasets", "score": 0.9884960651397705, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 273, "end": 313, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120repository", "\u0120of", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9355027079582214, 0.9582886695861816, 0.8773960471153259, 0.6643808484077454, 0.9982106685638428, 0.9987296462059021], "text": " a repository of conversational datasets", "score": 0.9054180979728699, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 327, "end": 360, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120hundreds", "\u0120of", "\u0120millions", "\u0120of", "\u0120examples"], "seq_scores": [0.9365935921669006, 0.9803873896598816, 0.9929735660552979, 0.9895534515380859, 0.9912419319152832], "text": " hundreds of millions of examples", "score": 0.9781499862670898, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 405, "end": 446, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120models"], "seq_scores": [0.9898895621299744, 0.9962205290794373, 0.9965373277664185, 0.9968948364257812, 0.9949886202812195], "text": " conversational response selection models", "score": 0.9949061751365662, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 539, "end": 561, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120standard", "\u0120datasets"], "seq_scores": [0.994475781917572, 0.9968410730361938, 0.9983237385749817], "text": " the standard datasets", "score": 0.9965468645095825, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 660, "end": 690, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120several", "\u0120competitive", "\u0120bas", "elines"], "seq_scores": [0.5653218030929565, 0.7930523753166199, 0.9587691426277161, 0.9411041140556335], "text": " several competitive baselines", "score": 0.8145618587732315, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 792, "end": 815, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120neural", "\u0120enc", "oder", "\u0120model"], "seq_scores": [0.9893390536308289, 0.990195095539093, 0.997124969959259, 0.9970241189002991, 0.9918761253356934], "text": " a neural encoder model", "score": 0.9931118726730347, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 834, "end": 858, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120entire", "\u0120training", "\u0120set"], "seq_scores": [0.9863243699073792, 0.9902031421661377, 0.9952797889709473, 0.991789698600769], "text": " the entire training set", "score": 0.9908992499113083, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1135, "end": 1156, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "em", "ph", "ill", "\u0120et", "\u0120al", ".,", "\u01201990"], "seq_scores": [0.9982573390007019, 0.9970604777336121, 0.997878909111023, 0.997771143913269, 0.9980448484420776, 0.9979456067085266, 0.9977298378944397, 0.9972929358482361], "text": "Hemphill et al., 1990", "score": 0.9977476373314857, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1157, "end": 1172, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Williams", ",", "\u01202012"], "seq_scores": [0.9958113431930542, 0.9977709054946899, 0.9975317716598511], "text": " Williams, 2012", "score": 0.9970380067825317, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1173, "end": 1194, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120El", "\u0120As", "ri", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9969936609268188, 0.9972347617149353, 0.9981011748313904, 0.9983212351799011, 0.9983246922492981, 0.9981074333190918, 0.9978935122489929], "text": " El Asri et al., 2017", "score": 0.9978537814957755, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1232, "end": 1255, "seq_label": ["I-Task", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120information", "H", "end", "erson", "\u0120et", "\u0120al", ".,", "\u01202014", "c"], "seq_scores": [0.6022739410400391, 0.9985116124153137, 0.9967987537384033, 0.9980545043945312, 0.9981583952903748, 0.9980589747428894, 0.9979144930839539, 0.9978079199790955, 0.9970539808273315], "text": "Henderson et al., 2014c", "score": 0.9538480639457703, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1256, "end": 1282, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bud", "z", "ian", "owski", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9980539083480835, 0.9975252747535706, 0.9980928301811218, 0.9981774091720581, 0.9982485771179199, 0.9983501434326172, 0.9982193112373352, 0.9979502558708191], "text": " Budzianowski et al., 2018", "score": 0.9980772137641907, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1332, "end": 1351, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Fr", "aser", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9983401298522949, 0.997776448726654, 0.9983943104743958, 0.9981666803359985, 0.9979085922241211, 0.9975478053092957], "text": "Fraser et al., 2018", "score": 0.99802232782046, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1353, "end": 1371, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120language", "\u0120learning"], "seq_scores": [0.7742413282394409, 0.7028464674949646], "text": " language learning", "score": 0.7385438978672028, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1373, "end": 1390, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "aux", "\u0120et", "\u0120al", ".,", "\u01202003"], "seq_scores": [0.9984809756278992, 0.9974455833435059, 0.9980753660202026, 0.9978246688842773, 0.9976436495780945, 0.9972777962684631], "text": "Raux et al., 2003", "score": 0.9977913399537405, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1391, "end": 1409, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Chen", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9977352619171143, 0.9977758526802063, 0.9981166124343872, 0.9979532957077026, 0.99783855676651], "text": " Chen et al., 2017", "score": 0.9978839159011841, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1428, "end": 1448, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["L", "aran", "jo", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9985824823379517, 0.9977973699569702, 0.9982484579086304, 0.9983376264572144, 0.9982706308364868, 0.99812251329422, 0.9978461265563965], "text": "Laranjo et al., 2018", "score": 0.9981721724782672, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1449, "end": 1474, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120F", "adh", "il", "\u0120and", "\u0120Sch", "ia", "vo", ",", "\u01202019"], "seq_scores": [0.9981697797775269, 0.9973862767219543, 0.9980487823486328, 0.998222291469574, 0.998275637626648, 0.9984367489814758, 0.9982423782348633, 0.9982895255088806, 0.9981243014335632], "text": " Fadhil and Schiavo, 2019", "score": 0.9981328580114577, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 1526, "end": 1543, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120customer", "\u0120service"], "seq_scores": [0.4947933554649353, 0.5345620512962341], "text": " customer service", "score": 0.5146777033805847, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 1652, "end": 1658, "seq_label": ["B-MLModel"], "seq_token": ["\u0120Apple"], "seq_scores": [0.3842850923538208], "text": " Apple", "score": 0.3842850923538208, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 1764, "end": 1814, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120modular", "\u0120or", "\u0120end", "-", "to", "-", "end", "\u0120machine", "\u0120learning", "\u0120frameworks"], "seq_scores": [0.7262308597564697, 0.8114876747131348, 0.6252084970474243, 0.9090700149536133, 0.8931055068969727, 0.8979395627975464, 0.8695566058158875, 0.8755123019218445, 0.9034742116928101, 0.7926232814788818], "text": " modular or end-to-end machine learning frameworks", "score": 0.8304208517074585, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1816, "end": 1827, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Young", ",", "\u01202010"], "seq_scores": [0.9977491497993469, 0.9970408082008362, 0.996914267539978], "text": "Young, 2010", "score": 0.9972347418467203, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1828, "end": 1849, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120V", "iny", "als", "\u0120and", "\u0120Le", ",", "\u01202015"], "seq_scores": [0.9977928400039673, 0.9978799819946289, 0.998230516910553, 0.9982174038887024, 0.9982315897941589, 0.9980199337005615, 0.997357189655304], "text": " Vinyals and Le, 2015", "score": 0.9979613508496966, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1850, "end": 1867, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Wen", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9977389574050903, 0.9979546070098877, 0.9983502626419067, 0.9981820583343506, 0.9978923201560974], "text": " Wen et al., 2015", "score": 0.9980236411094665, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1867, "end": 1892, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Wen", "\u0120et", "\u0120al", ".,", "\u0120,", "\u01202017", "a", ",", "b", ";", ",", "b"], "seq_scores": [0.9954198598861694, 0.9974491000175476, 0.9980649352073669, 0.9978742599487305, 0.9970001578330994, 0.9978387951850891, 0.992347002029419, 0.9900367259979248, 0.9934079647064209, 0.9878365397453308, 0.9903237819671631, 0.9945603013038635], "text": " Wen et al., , 2017a,b;,b", "score": 0.9943466186523438, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1893, "end": 1916, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mr", "k", "\u00c5\u00a1", "i", "\u00c4\u0129", "\u0120and", "\u0120Vul", "i", "\u00c4\u0129", ",", "\u01202018"], "seq_scores": [0.9982312321662903, 0.9969626069068909, 0.9985055923461914, 0.9986883997917175, 0.9987473487854004, 0.9986866116523743, 0.9984151124954224, 0.9988127946853638, 0.9987389445304871, 0.9987247586250305, 0.9983643889427185], "text": " Mrk\u0161i\u0107 and Vuli\u0107, 2018", "score": 0.9984434355388988, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1917, "end": 1938, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ramadan", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9976603984832764, 0.9983915686607361, 0.9987228512763977, 0.998663067817688, 0.9984896183013916], "text": " Ramadan et al., 2018", "score": 0.9983855009078979, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 1939, "end": 1967, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Li", "\u0120et", "\u0120al", ".,", "\u01202018", ",", "\u0120inter", "\u0120al", "ia"], "seq_scores": [0.9980702996253967, 0.998569130897522, 0.9988269209861755, 0.9988213181495667, 0.9987351298332214, 0.9938661456108093, 0.9717137813568115, 0.9918136596679688, 0.976141631603241], "text": " Li et al., 2018, inter alia", "score": 0.991839779747857, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2374, "end": 2399, "seq_label": ["I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120learning", "H", "end", "erson", "\u0120et", "\u0120al", ".,", "\u01202014", "a", ",", "c"], "seq_scores": [0.5282559394836426, 0.9983037710189819, 0.9972867965698242, 0.9984719157218933, 0.9985740184783936, 0.9985519051551819, 0.9984075427055359, 0.9982243180274963, 0.9970546960830688, 0.992607593536377, 0.9932979941368103], "text": "Henderson et al., 2014a,c", "score": 0.954457862810655, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2400, "end": 2420, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Mr", "k", "\u00c5\u00a1", "i", "\u00c4\u0129", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9981681108474731, 0.9970846772193909, 0.9984764456748962, 0.9986886382102966, 0.9987442493438721, 0.9987775683403015, 0.9988232254981995, 0.9986079335212708, 0.9983160495758057], "text": " Mrk\u0161i\u0107 et al., 2015", "score": 0.9984096553590562, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2530, "end": 2550, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["El", "\u0120As", "ri", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9975812435150146, 0.9975171089172363, 0.9984191656112671, 0.9986733198165894, 0.9986360669136047, 0.9983758926391602, 0.9980491399765015], "text": "El Asri et al., 2017", "score": 0.998178848198482, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2551, "end": 2577, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Bud", "z", "ian", "owski", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9982274174690247, 0.9978333115577698, 0.9985818862915039, 0.998747706413269, 0.9987195730209351, 0.9987553358078003, 0.9985798597335815, 0.9983100891113281], "text": " Budzianowski et al., 2018", "score": 0.9984693974256516, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2753, "end": 2762, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Multi", "W", "O", "Z"], "seq_scores": [0.9955573678016663, 0.9966809153556824, 0.9969016313552856, 0.9963138699531555], "text": " MultiWOZ", "score": 0.9963634461164474, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2772, "end": 2797, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["B", "ud", "z", "ian", "owski", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9983214735984802, 0.9972015619277954, 0.998565137386322, 0.9986692667007446, 0.9987939596176147, 0.9988228678703308, 0.9988238215446472, 0.9985824823379517, 0.9983394145965576], "text": "Budzianowski et al., 2018", "score": 0.9984577761756049, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 2981, "end": 2987, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120D", "ST", "C", "2"], "seq_scores": [0.9945479035377502, 0.9968048334121704, 0.997038722038269, 0.9961212277412415], "text": " DSTC2", "score": 0.9961281716823578, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 2989, "end": 3012, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "end", "erson", "\u0120et", "\u0120al", ".,", "\u01202014", "b"], "seq_scores": [0.9983200430870056, 0.9977116584777832, 0.998599112033844, 0.9987044334411621, 0.998629093170166, 0.9984813332557678, 0.9982435703277588, 0.9980840682983398], "text": "Henderson et al., 2014b", "score": 0.9983466640114784, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3036, "end": 3043, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Frames"], "seq_scores": [0.9820252656936646], "text": " Frames", "score": 0.9820252656936646, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3045, "end": 3065, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["El", "\u0120As", "ri", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9976906776428223, 0.9977362155914307, 0.9986035227775574, 0.9989438652992249, 0.9989007711410522, 0.9986999034881592, 0.9985161423683167], "text": "El Asri et al., 2017", "score": 0.9984415854726519, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3094, "end": 3098, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120M", "2", "M"], "seq_scores": [0.9956002235412598, 0.9965921640396118, 0.9967890977859497], "text": " M2M", "score": 0.9963271617889404, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3100, "end": 3117, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sh", "ah", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9983708262443542, 0.9980568289756775, 0.998801589012146, 0.9987517595291138, 0.9986163377761841, 0.9984322190284729], "text": "Shah et al., 2018", "score": 0.9985049267609915, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2041, "end": 2056, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120datasets"], "seq_scores": [0.9880744814872742, 0.9972846508026123], "text": " large datasets", "score": 0.9926795661449432, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 2119, "end": 2136, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120different", "\u0120models"], "seq_scores": [0.7920840978622437, 0.8753142952919006], "text": " different models", "score": 0.8336991965770721, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2157, "end": 2162, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.6500454545021057], "text": " data", "score": 0.6500454545021057, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2435, "end": 2463, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120task", "-", "oriented", "\u0120dialogue", "\u0120data"], "seq_scores": [0.9814401865005493, 0.9907822012901306, 0.9934602975845337, 0.9852703809738159, 0.9887461066246033], "text": " task-oriented dialogue data", "score": 0.9879398345947266, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2596, "end": 2629, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120such", "\u0120annot", "ated", "\u0120dialogue", "\u0120datasets"], "seq_scores": [0.9637190699577332, 0.8835709095001221, 0.9978086352348328, 0.9971500039100647, 0.9982853531837463], "text": " such annotated dialogue datasets", "score": 0.9681067943572998, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2818, "end": 2841, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120115", ",", "424", "\u0120dialogue", "\u0120turns"], "seq_scores": [0.6358857750892639, 0.9542480111122131, 0.9539554119110107, 0.7465132474899292, 0.7111688256263733], "text": " 115,424 dialogue turns", "score": 0.800354254245758, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 2874, "end": 2909, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Other", "\u0120standard", "\u0120task", "-", "based", "\u0120datasets"], "seq_scores": [0.9859530925750732, 0.9669985771179199, 0.9967262744903564, 0.9976001381874084, 0.9976075887680054, 0.9986485838890076], "text": " Other standard task-based datasets", "score": 0.9905890425046285, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3022, "end": 3035, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012023", ",", "354", "\u0120turns"], "seq_scores": [0.8047868609428406, 0.896953821182251, 0.8693749904632568, 0.652127206325531], "text": " 23,354 turns", "score": 0.8058107197284698, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3076, "end": 3089, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012019", ",", "986", "\u0120turns"], "seq_scores": [0.6919702887535095, 0.8312898278236389, 0.8106447458267212, 0.5152713656425476], "text": " 19,986 turns", "score": 0.7122940570116043, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3124, "end": 3137, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012014", ",", "796", "\u0120turns"], "seq_scores": [0.7465141415596008, 0.8724513053894043, 0.7779185771942139, 0.5740455389022827], "text": " 14,796 turns", "score": 0.7427323907613754, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 3356, "end": 3374, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120dialogue", "\u0120modeling"], "seq_scores": [0.69905686378479, 0.4752914607524872], "text": " dialogue modeling", "score": 0.5871741622686386, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 3628, "end": 3636, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Twitter"], "seq_scores": [0.8302856683731079], "text": " Twitter", "score": 0.8302856683731079, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3652, "end": 3671, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["R", "itter", "\u0120et", "\u0120al", ".,", "\u01202010"], "seq_scores": [0.9980018734931946, 0.9987479448318481, 0.9990218877792358, 0.9989697933197021, 0.9988489151000977, 0.9986717700958252], "text": "Ritter et al., 2010", "score": 0.9987103641033173, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 3733, "end": 3740, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Ubuntu"], "seq_scores": [0.547980785369873], "text": " Ubuntu", "score": 0.547980785369873, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3749, "end": 3766, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Low", "e", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9981645941734314, 0.9985975623130798, 0.9989386200904846, 0.9988191723823547, 0.9986586570739746, 0.9982977509498596], "text": "Lowe et al., 2015", "score": 0.9985793928305308, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3768, "end": 3787, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Low", "e", "\u0120et", "\u0120al", ".,", "\u0120,", "\u01202017"], "seq_scores": [0.9972702860832214, 0.998474657535553, 0.9986856579780579, 0.9987276196479797, 0.9984751343727112, 0.9973415732383728, 0.9979143738746643], "text": "Lowe et al., , 2017", "score": 0.9981270432472229, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 3789, "end": 3813, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Gun", "ase", "k", "ara", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9973301887512207, 0.9987478256225586, 0.9990313053131104, 0.9990692734718323, 0.9991558790206909, 0.9991759657859802, 0.9990813732147217, 0.9989199638366699], "text": " Gunasekara et al., 2019", "score": 0.9988139718770981, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 4025, "end": 4032, "seq_label": ["I-Method", "B-Datasource"], "seq_token": ["processing", "\u0120Reddit"], "seq_scores": [0.5138277411460876, 0.9355958700180054], "text": " Reddit", "score": 0.7247118055820465, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4162, "end": 4176, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Sub", "t", "itles"], "seq_scores": [0.986283540725708, 0.98973548412323, 0.9881502389907837, 0.9852505922317505], "text": " OpenSubtitles", "score": 0.987354964017868, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4259, "end": 4273, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120Ubuntu", "\u0120corpus"], "seq_scores": [0.8999248147010803, 0.6036573648452759], "text": " Ubuntu corpus", "score": 0.7517910897731781, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 4273, "end": 4278, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120v", "2", ".", "0"], "seq_scores": [0.5377298593521118, 0.9857864379882812, 0.9690713286399841, 0.9752664566040039], "text": " v2.0", "score": 0.8669635206460953, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 4331, "end": 4338, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.8473407030105591], "text": " Reddit", "score": 0.8473407030105591, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 4424, "end": 4431, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.8328494429588318], "text": " Reddit", "score": 0.8328494429588318, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 4442, "end": 4465, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Al", "-", "R", "f", "ou", "\u0120et", "\u0120al", ".", "\u0120(", "2016", "),"], "seq_scores": [0.9963962435722351, 0.998051643371582, 0.998637855052948, 0.9992098808288574, 0.9992629885673523, 0.9992896318435669, 0.9993133544921875, 0.9993273019790649, 0.9982498288154602, 0.9985609650611877, 0.9900546073913574], "text": " Al-Rfou et al. (2016),", "score": 0.9978503909977999, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3177, "end": 3208, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120larger", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9902855753898621, 0.9937060475349426, 0.998435914516449, 0.9989032745361328], "text": " larger conversational datasets", "score": 0.9953327029943466, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3226, "end": 3240, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Such", "\u0120datasets"], "seq_scores": [0.9685813784599304, 0.9741353988647461], "text": " Such datasets", "score": 0.9713583886623383, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3400, "end": 3420, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120public", "\u0120repository"], "seq_scores": [0.9384450316429138, 0.9597782492637634, 0.9684798121452332], "text": " a public repository", "score": 0.9555676976839701, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3423, "end": 3471, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120three", "\u0120large", "\u0120and", "\u0120diverse", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.887014627456665, 0.9931893348693848, 0.9978637099266052, 0.9983847141265869, 0.9958884119987488, 0.9985124468803406, 0.9990857839584351], "text": " three large and diverse conversational datasets", "score": 0.9814198613166809, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3482, "end": 3528, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120hundreds", "\u0120of", "\u0120millions", "\u0120of", "\u0120conversation", "\u0120examples"], "seq_scores": [0.8983741402626038, 0.9025596976280212, 0.9625290036201477, 0.9675708413124084, 0.9136359691619873, 0.9692177772521973], "text": " hundreds of millions of conversation examples", "score": 0.9356479048728943, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3541, "end": 3582, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120most", "\u0120popular", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9911981225013733, 0.9951900243759155, 0.9969344139099121, 0.9969030022621155, 0.9987922310829163, 0.9991366267204285], "text": " the most popular conversational datasets", "score": 0.9963590701421102, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3610, "end": 3650, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120length", "-", "restricted", "\u0120Twitter", "\u0120conversations"], "seq_scores": [0.7546096444129944, 0.8055964112281799, 0.8545557260513306, 0.8039429187774658, 0.8229936361312866], "text": " length-restricted Twitter conversations", "score": 0.8083396673202514, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3675, "end": 3724, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120very", "\u0120technical", "\u0120domain", "-", "restricted", "\u0120technical", "\u0120chats"], "seq_scores": [0.8168643712997437, 0.8977758884429932, 0.9262521266937256, 0.9199726581573486, 0.9319400787353516, 0.913304328918457, 0.9095534086227417], "text": " very technical domain-restricted technical chats", "score": 0.9022375515529087, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3834, "end": 3868, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120three", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9756380319595337, 0.9784375429153442, 0.9952402114868164, 0.9982402324676514, 0.9989553689956665], "text": " the three conversational datasets", "score": 0.9893022775650024, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 3941, "end": 3954, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120repository", "\u0120the", "\u0120datasets"], "seq_scores": [0.5429840683937073, 0.9889951944351196, 0.9960722923278809], "text": " the datasets", "score": 0.8426838517189026, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4006, "end": 4020, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01203", ".", "7", "B", "\u0120comments"], "seq_scores": [0.9448090195655823, 0.990878164768219, 0.9928500652313232, 0.9910388588905334, 0.981186032295227], "text": " 3.7B comments", "score": 0.980152428150177, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4045, "end": 4073, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120256", "M", "\u0120convers", "ational", "\u0120threads"], "seq_scores": [0.6781933903694153, 0.7979999780654907, 0.5565670132637024, 0.5739248394966125, 0.6018842458724976], "text": " 256M conversational threads", "score": 0.6417138934135437, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4084, "end": 4117, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01207", "27", "M", "\u0120valid", "\u0120context", "response", "\u0120pairs"], "seq_scores": [0.9792566299438477, 0.9928098917007446, 0.9948756098747253, 0.995894193649292, 0.9953104853630066, 0.9931368827819824, 0.9905851483345032], "text": " 727M valid contextresponse pairs", "score": 0.9916955488068717, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4327, "end": 4345, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120our", "\u0120Reddit", "\u0120corpus"], "seq_scores": [0.9701129198074341, 0.961193859577179, 0.9843421578407288], "text": " our Reddit corpus", "score": 0.9718829790751139, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4411, "end": 4439, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120data", "\u0120the", "\u0120previous", "\u0120Reddit", "\u0120dataset"], "seq_scores": [0.4426684081554413, 0.9704071283340454, 0.9852575659751892, 0.9583275318145752, 0.9886243343353271], "text": " the previous Reddit dataset", "score": 0.8690569937229157, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4484, "end": 4498, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01202", ".", "1", "B", "\u0120comments"], "seq_scores": [0.8992977738380432, 0.9267008304595947, 0.9450057148933411, 0.9265843629837036, 0.9524572491645813], "text": " 2.1B comments", "score": 0.9300091862678528, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4502, "end": 4530, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120133", "M", "\u0120convers", "ational", "\u0120threads"], "seq_scores": [0.6415364146232605, 0.7927609086036682, 0.5028557181358337, 0.5754168629646301, 0.6239010691642761], "text": " 133M conversational threads", "score": 0.6272941946983337, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4698, "end": 4724, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120consistent", "\u0120data", "\u0120filtering"], "seq_scores": [0.7169015407562256, 0.5262373685836792, 0.9688286781311035], "text": " consistent data filtering", "score": 0.7373225291570028, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4728, "end": 4742, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120pre", "processing"], "seq_scores": [0.761139452457428, 0.9341012835502625], "text": " preprocessing", "score": 0.8476203680038452, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 4797, "end": 4816, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120response", "\u0120selection"], "seq_scores": [0.5912303924560547, 0.631723165512085], "text": " response selection", "score": 0.6114767789840698, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4588, "end": 4603, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120large", "\u0120datasets"], "seq_scores": [0.9863507151603699, 0.9984216690063477], "text": " large datasets", "score": 0.9923861920833588, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4745, "end": 4761, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120public", "\u0120datasets"], "seq_scores": [0.9946073889732361, 0.9980770349502563], "text": " public datasets", "score": 0.9963422119617462, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 4866, "end": 4873, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9231387376785278], "text": " models", "score": 0.9231387376785278, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5051, "end": 5078, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120language", "\u0120model", "\u0120pret", "raining"], "seq_scores": [0.9560418128967285, 0.9747132062911987, 0.9747672080993652, 0.9750294089317322], "text": " language model pretraining", "score": 0.9701379090547562, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5109, "end": 5131, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Howard", "\u0120and", "\u0120Rud", "er", ",", "\u01202018"], "seq_scores": [0.9975209832191467, 0.9979477524757385, 0.9980326294898987, 0.9980917572975159, 0.9977149963378906, 0.9972569346427917], "text": "Howard and Ruder, 2018", "score": 0.9977608422438303, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5132, "end": 5152, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Dev", "lin", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9979264736175537, 0.9977121353149414, 0.9979453682899475, 0.9979686141014099, 0.9976837635040283, 0.9974892139434814], "text": " Devlin et al., 2018", "score": 0.997787594795227, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5153, "end": 5178, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120L", "ample", "\u0120and", "\u0120Con", "neau", ",", "\u01202019"], "seq_scores": [0.9978690147399902, 0.9975569248199463, 0.9980000853538513, 0.9981285929679871, 0.9981701374053955, 0.9977697134017944, 0.9976882934570312], "text": " Lample and Conneau, 2019", "score": 0.9978832517351423, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5241, "end": 5250, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120pret", "rain"], "seq_scores": [0.8306151032447815, 0.7573028206825256], "text": " pretrain", "score": 0.7939589619636536, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 4906, "end": 4941, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["These", "\u0120large", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9687234163284302, 0.9429394602775574, 0.9977396726608276, 0.9985859394073486, 0.9990712404251099], "text": "These large conversational datasets", "score": 0.9814119458198547, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5196, "end": 5211, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.987297773361206, 0.9953206181526184], "text": " these datasets", "score": 0.9913091957569122, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5250, "end": 5293, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120large", "\u0120general", "-", "domain", "\u0120convers", "ational", "\u0120models"], "seq_scores": [0.993158757686615, 0.9967880249023438, 0.997620165348053, 0.9979385733604431, 0.9979566335678101, 0.9982486963272095, 0.9982811212539673], "text": " large general-domain conversational models", "score": 0.9971417103494916, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5347, "end": 5405, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120much", "\u0120smaller", "\u0120amounts", "\u0120of", "\u0120task", "-", "specific", "\u0120convers", "ational", "\u0120data"], "seq_scores": [0.9821175932884216, 0.9932199716567993, 0.9964269995689392, 0.9907227158546448, 0.8981305956840515, 0.9971510767936707, 0.9980880618095398, 0.9935101866722107, 0.9973994493484497, 0.9980257749557495], "text": " much smaller amounts of task-specific conversational data", "score": 0.9844792425632477, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 5456, "end": 5488, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120presented", "\u0120repository", "\u0120a", "\u0120set", "\u0120of", "\u0120strong", "\u0120baseline", "\u0120models"], "seq_scores": [0.657281219959259, 0.5930578112602234, 0.9778483510017395, 0.9705571532249451, 0.9725094437599182, 0.9794384241104126, 0.996649444103241, 0.9960706233978271], "text": " a set of strong baseline models", "score": 0.8929265588521957, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5674, "end": 5688, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["con", "vers", "ational"], "seq_scores": [0.7473716735839844, 0.38869136571884155, 0.8131961822509766], "text": "conversational", "score": 0.6497530738512675, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5754, "end": 5765, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120T", "ensor", "flow"], "seq_scores": [0.6691311001777649, 0.7105851173400879, 0.6416930556297302], "text": " Tensorflow", "score": 0.6738030910491943, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 5838, "end": 5856, "seq_label": ["I-Method", "I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["ensor", "flow", "Ab", "adi", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.7429714798927307, 0.6720179319381714, 0.9980694651603699, 0.9954994320869446, 0.9963812232017517, 0.9962072372436523, 0.9945825934410095, 0.9926890730857849], "text": "Abadi et al., 2015", "score": 0.9235523045063019, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 5884, "end": 5895, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120T", "ensor", "flow"], "seq_scores": [0.8289006352424622, 0.9383260011672974, 0.9017695188522339], "text": " Tensorflow", "score": 0.8896653850873312, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5732, "end": 5740, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["Dat", "as", "ets"], "seq_scores": [0.9909279942512512, 0.9961062073707581, 0.995518684387207], "text": "Datasets", "score": 0.9941842953364054, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 5906, "end": 5931, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120convers", "ational", "\u0120dataset"], "seq_scores": [0.9886061549186707, 0.9948626160621643, 0.9974294304847717, 0.9981560111045837], "text": " a conversational dataset", "score": 0.9947635531425476, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 6072, "end": 6083, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120T", "ensor", "flow"], "seq_scores": [0.8159969449043274, 0.8194202780723572, 0.8095048069953918], "text": " Tensorflow", "score": 0.8149740099906921, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6272, "end": 6293, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120most", "\u0120recent", "\u0120text"], "seq_scores": [0.8195452690124512, 0.8990588188171387, 0.8780571222305298, 0.8762123584747314], "text": " the most recent text", "score": 0.8682183921337128, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6647, "end": 6656, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9913630485534668], "text": " datasets", "score": 0.9913630485534668, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6726, "end": 6738, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120dataset"], "seq_scores": [0.9724186062812805, 0.961725115776062], "text": " the dataset", "score": 0.9670718610286713, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 6820, "end": 6827, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.7785896062850952], "text": " Reddit", "score": 0.7785896062850952, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7105, "end": 7124, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120pre", "-", "processing", "\u0120and"], "seq_scores": [0.4084475636482239, 0.8088764548301697, 0.7750140428543091, 0.5054455399513245], "text": " pre-processing and", "score": 0.6244459003210068, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 7432, "end": 7466, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.7500747442245483, 0.6024101972579956, 0.6921467185020447, 0.6679595708847046], "text": " conversational response selection", "score": 0.6781478077173233, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7517, "end": 7529, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120Apache", "\u0120Beam"], "seq_scores": [0.933355987071991, 0.9613890647888184], "text": " Apache Beam", "score": 0.9473725259304047, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 7533, "end": 7555, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Google", "\u0120Cloud", "\u0120Data", "flow"], "seq_scores": [0.9661259651184082, 0.9816131591796875, 0.9794384241104126, 0.984423816204071], "text": " Google Cloud Dataflow", "score": 0.9779003411531448, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 7557, "end": 7576, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ak", "id", "au", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9912761449813843, 0.9730168581008911, 0.9826003909111023, 0.9877211451530457, 0.9844030141830444, 0.9710610508918762, 0.967089056968689], "text": "Akidau et al., 2015", "score": 0.9795953801700047, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7658, "end": 7665, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.7333242893218994], "text": " Reddit", "score": 0.7333242893218994, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7708, "end": 7715, "seq_label": ["B-Dataset"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.8191578388214111], "text": " Reddit", "score": 0.8191578388214111, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7716, "end": 7730, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Open", "Sub", "t", "itles"], "seq_scores": [0.994140088558197, 0.9963065385818481, 0.9957636594772339, 0.9955090284347534], "text": " OpenSubtitles", "score": 0.9954298287630081, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 7734, "end": 7743, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Amazon", "Q", "A"], "seq_scores": [0.9921784400939941, 0.9960427284240723, 0.9962750673294067], "text": " AmazonQA", "score": 0.9948320786158243, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 6942, "end": 6965, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120raw", "\u0120processed", "\u0120data"], "seq_scores": [0.9035202264785767, 0.9815710186958313, 0.9898616075515747, 0.9840149283409119], "text": " the raw processed data", "score": 0.9647419452667236, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7027, "end": 7036, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.9313600063323975, 0.9420689940452576], "text": " the data", "score": 0.9367145001888275, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7193, "end": 7202, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9693045616149902], "text": " datasets", "score": 0.9693045616149902, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7256, "end": 7280, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120train", "\u0120and", "\u0120test", "\u0120portions"], "seq_scores": [0.9444310069084167, 0.9471819400787354, 0.9735638499259949, 0.9578086733818054], "text": " train and test portions", "score": 0.9557463675737381, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7391, "end": 7415, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120standard", "\u0120datasets"], "seq_scores": [0.9896669387817383, 0.982183575630188, 0.9981866478919983], "text": " these standard datasets", "score": 0.9900123874346415, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 7419, "end": 7473, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120variety", "\u0120of", "\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120models"], "seq_scores": [0.939355194568634, 0.9250646233558655, 0.9323073625564575, 0.9416055083274841, 0.995415210723877, 0.9961989521980286, 0.9962841868400574, 0.9945218563079834], "text": " a variety of conversational response selection models", "score": 0.9650941118597984, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 7474, "end": 7482, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Dat", "as", "et"], "seq_scores": [0.8195816278457642, 0.760199248790741, 0.5308461785316467], "text": " Dataset", "score": 0.7035423517227173, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7834, "end": 7840, "seq_label": ["B-Datasource"], "seq_token": ["Reddit"], "seq_scores": [0.935985803604126], "text": "Reddit", "score": 0.935985803604126, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 7961, "end": 7968, "seq_label": ["I-Datasource", "B-Datasource"], "seq_token": ["\u0120news", "\u0120Reddit"], "seq_scores": [0.4853774607181549, 0.9308307766914368], "text": " Reddit", "score": 0.7081041187047958, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 7991, "end": 8013, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Sch", "r", "ading", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9974429607391357, 0.9987909197807312, 0.9988765120506287, 0.9990167617797852, 0.9990187883377075, 0.9988322854042053, 0.9985575079917908], "text": "Schrading et al., 2015", "score": 0.998647962297712, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8014, "end": 8035, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Al", "-", "R", "f", "ou", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.99779212474823, 0.9989076852798462, 0.9988908171653748, 0.9990311861038208, 0.9989951252937317, 0.9989933371543884, 0.999083399772644, 0.9989266991615295, 0.998651921749115], "text": " Al-Rfou et al., 2016", "score": 0.9988080329365201, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8084, "end": 8095, "seq_label": ["B-Datasource"], "seq_token": ["\u0120subreddits"], "seq_scores": [0.5322154760360718], "text": " subreddits", "score": 0.5322154760360718, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8185, "end": 8203, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Big", "Query", "\u0120database"], "seq_scores": [0.5889831185340881, 0.7916846871376038, 0.6687981486320496], "text": " BigQuery database", "score": 0.6831553181012472, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8289, "end": 8296, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.923238217830658], "text": " Reddit", "score": 0.923238217830658, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 8325, "end": 8359, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.8288630247116089, 0.7952402830123901, 0.7928364276885986, 0.8179957866668701], "text": " conversational response selection", "score": 0.8087338805198669, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8367, "end": 8390, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Al", "-", "R", "f", "ou", "\u0120et", "\u0120al", ".", "\u0120(", "2016", ");"], "seq_scores": [0.9973145127296448, 0.9986683130264282, 0.9989218711853027, 0.9991546869277954, 0.9991165995597839, 0.9990973472595215, 0.9991506338119507, 0.9989235997200012, 0.9975100755691528, 0.99842369556427, 0.9761016964912415], "text": " Al-Rfou et al. (2016);", "score": 0.9965802756222811, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8390, "end": 8409, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Cer", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ");"], "seq_scores": [0.9972014427185059, 0.9990299940109253, 0.9991747736930847, 0.9991183876991272, 0.9986106157302856, 0.9985803365707397, 0.9530818462371826], "text": " Cer et al. (2018);", "score": 0.9921139138085502, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 8409, "end": 8429, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ")."], "seq_scores": [0.997675359249115, 0.9990215301513672, 0.9991973042488098, 0.999024510383606, 0.9982361793518066, 0.998467743396759, 0.9909895062446594], "text": " Yang et al. (2018).", "score": 0.9975160190037319, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8484, "end": 8491, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.9297494888305664], "text": " Reddit", "score": 0.9297494888305664, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8527, "end": 8548, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120consistent", "\u0120filtering"], "seq_scores": [0.8186360001564026, 0.9272030591964722], "text": " consistent filtering", "score": 0.8729195296764374, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 8565, "end": 8586, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120train", "/", "test", "\u0120splitting"], "seq_scores": [0.7683759331703186, 0.7699966430664062, 0.8491283655166626, 0.8239580392837524], "text": " train/test splitting", "score": 0.802864745259285, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8212, "end": 8230, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": [",", "000", "-", "forums", "\u0120threaded", "\u0120discussions", "\u0120a", "\u0120large", "\u0120corpus", "\u0120of"], "seq_scores": [0.5246250033378601, 0.5737365484237671, 0.5450602173805237, 0.53181391954422, 0.6173079013824463, 0.7229960560798645, 0.9905120730400085, 0.9942629933357239, 0.9967249035835266, 0.6197266578674316], "text": " a large corpus of", "score": 0.7116766273975372, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8230, "end": 8254, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120convers", "ational", "\u0120contexts"], "seq_scores": [0.43377554416656494, 0.87774258852005, 0.9133120775222778], "text": " conversational contexts", "score": 0.741610070069631, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8266, "end": 8288, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120appropriate", "\u0120responses"], "seq_scores": [0.60036301612854, 0.9092212319374084], "text": " appropriate responses", "score": 0.7547921240329742, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8289, "end": 8301, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120Reddit", "\u0120data"], "seq_scores": [0.7949720621109009, 0.7710550427436829], "text": " Reddit data", "score": 0.7830135524272919, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8325, "end": 8364, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120data"], "seq_scores": [0.9926024079322815, 0.9966044425964355, 0.9978513717651367, 0.9976904392242432, 0.9957636594772339], "text": " conversational response selection data", "score": 0.9961024641990661, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8466, "end": 8475, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120datasets"], "seq_scores": [0.9708877205848694], "text": " datasets", "score": 0.9708877205848694, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8565, "end": 8576, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120train", "/", "test"], "seq_scores": [0.5516681671142578, 0.6930422186851501, 0.7436282634735107], "text": " train/test", "score": 0.6627795497576395, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8604, "end": 8609, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120data"], "seq_scores": [0.7358173727989197], "text": " data", "score": 0.7358173727989197, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8615, "end": 8619, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120two"], "seq_scores": [0.7108367085456848], "text": " two", "score": 0.7108367085456848, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8662, "end": 8683, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120years", "\u0120data", "\u01203", ".", "7", "\u0120billion", "\u0120comments"], "seq_scores": [0.7409235239028931, 0.648230791091919, 0.9727544188499451, 0.9888882040977478, 0.9933099746704102, 0.9925242066383362, 0.9892567992210388], "text": " 3.7 billion comments", "score": 0.90369827406747, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8695, "end": 8707, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01202", ".", "1", "\u0120billion"], "seq_scores": [0.9260994791984558, 0.9919747710227966, 0.9946613907814026, 0.9908073544502258], "text": " 2.1 billion", "score": 0.9758857488632202, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8715, "end": 8731, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120final", "\u0120dataset"], "seq_scores": [0.9927157759666443, 0.9945549964904785, 0.9952858090400696], "text": " a final dataset", "score": 0.9941855271657308, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8736, "end": 8762, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120176", "\u0120million", "\u0120more", "\u0120examples"], "seq_scores": [0.9135470986366272, 0.9890933036804199, 0.9878684282302856, 0.9854152202606201], "text": " 176 million more examples", "score": 0.9689810127019882, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8764, "end": 8770, "seq_label": ["B-Datasource"], "seq_token": ["Reddit"], "seq_scores": [0.8033490777015686], "text": "Reddit", "score": 0.8033490777015686, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 8941, "end": 8948, "seq_label": ["B-Datasource"], "seq_token": ["\u0120Reddit"], "seq_scores": [0.7192438244819641], "text": " Reddit", "score": 0.7192438244819641, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 8975, "end": 8993, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120set", "\u0120of", "\u0120examples"], "seq_scores": [0.7651842832565308, 0.7121022939682007, 0.5507428050041199, 0.5502507090568542], "text": " a set of examples", "score": 0.6445700228214264, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9716, "end": 9736, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["The", "\u0120train", "/", "test", "\u0120split"], "seq_scores": [0.7883565425872803, 0.9302894473075867, 0.9542297720909119, 0.9723177552223206, 0.76590496301651], "text": "The train/test split", "score": 0.8822196960449219, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9880, "end": 9903, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120resulting", "\u0120datasets"], "seq_scores": [0.9734853506088257, 0.9760264754295349, 0.9928176403045654], "text": " the resulting datasets", "score": 0.9807764887809753, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9925, "end": 9933, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["The", "\u0120data"], "seq_scores": [0.9756246209144592, 0.9814977645874023], "text": "The data", "score": 0.9785611927509308, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 9973, "end": 9996, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01203", ",", "680", ",", "7", "46", ",", "776", "\u0120comments"], "seq_scores": [0.9380362033843994, 0.9840818643569946, 0.9886922836303711, 0.99063640832901, 0.9928237199783325, 0.993084192276001, 0.9923885464668274, 0.9918181896209717, 0.9755134582519531], "text": " 3,680,746,776 comments", "score": 0.9830083184772067, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10000, "end": 10020, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120256", ",", "09", "5", ",", "216", "\u0120threads"], "seq_scores": [0.9026671648025513, 0.9582433700561523, 0.961777925491333, 0.9715988039970398, 0.9843772649765015, 0.9840238094329834, 0.9592368602752686], "text": " 256,095,216 threads", "score": 0.9602750284331185, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10031, "end": 10063, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01207", "27", ",", "013", ",", "7", "15", "\u0120T", "ensor", "flow", "\u0120examples"], "seq_scores": [0.6601917743682861, 0.6450832486152649, 0.8063269853591919, 0.7271264791488647, 0.889502763748169, 0.9071071743965149, 0.9513301849365234, 0.9169216156005859, 0.9437426924705505, 0.9259651303291321, 0.9424614310264587], "text": " 727,013,715 Tensorflow examples", "score": 0.8468872254545038, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10080, "end": 10090, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120this", "\u0120data"], "seq_scores": [0.7609003782272339, 0.7147640585899353], "text": " this data", "score": 0.7378322184085846, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10111, "end": 10124, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Open", "Sub", "t", "itles"], "seq_scores": [0.9401862621307373, 0.972358763217926, 0.9679573774337769, 0.9727190136909485], "text": "OpenSubtitles", "score": 0.9633053541183472, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 10285, "end": 10313, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120L", "ison", "\u0120and", "\u0120T", "ied", "em", "ann", "\u0120(", "2016", "),"], "seq_scores": [0.9940800070762634, 0.997914731502533, 0.9987381100654602, 0.9984859824180603, 0.9989244341850281, 0.998833954334259, 0.9990052580833435, 0.9972633123397827, 0.9976645708084106, 0.9815248250961304], "text": " Lison and Tiedemann (2016),", "score": 0.9962435185909271, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 10337, "end": 10369, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120statistical", "\u0120machine", "\u0120translation"], "seq_scores": [0.9661687016487122, 0.9701959490776062, 0.9876842498779297], "text": " statistical machine translation", "score": 0.9746829668680826, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10127, "end": 10155, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120growing", "\u0120online", "\u0120collection"], "seq_scores": [0.9643343091011047, 0.986480176448822, 0.9899817109107971, 0.9873868823051453], "text": " a growing online collection", "score": 0.9820457696914673, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10158, "end": 10168, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120subtitles"], "seq_scores": [0.6390225887298584], "text": " subtitles", "score": 0.6390225887298584, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10261, "end": 10272, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120corpus"], "seq_scores": [0.9741637706756592, 0.983367919921875], "text": " the corpus", "score": 0.9787658452987671, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10370, "end": 10382, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120This", "\u0120corpus"], "seq_scores": [0.9809442758560181, 0.9907220602035522], "text": " This corpus", "score": 0.9858331680297852, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 10736, "end": 10758, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120modelling", "\u0120the", "\u0120mapping"], "seq_scores": [0.5243050456047058, 0.40619879961013794, 0.5139816999435425], "text": " modelling the mapping", "score": 0.4814951817194621, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10457, "end": 10475, "seq_label": ["I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["secut", "ive", "\u0120lines", "\u0120the", "\u0120subtitle", "\u0120data"], "seq_scores": [0.5545310974121094, 0.6635459661483765, 0.8717324137687683, 0.9513104557991028, 0.9906893968582153, 0.9841561913490295], "text": " the subtitle data", "score": 0.8359942535559336, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10671, "end": 10680, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120data"], "seq_scores": [0.9817502498626709, 0.9854418039321899], "text": " The data", "score": 0.9835960268974304, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 10921, "end": 10934, "seq_label": ["B-Dataset", "I-Dataset"], "seq_token": ["\u0120English", "\u01202018"], "seq_scores": [0.9805363416671753, 0.9758287072181702], "text": " English 2018", "score": 0.9781825244426727, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 10917, "end": 10939, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120English", "\u01202018", "\u0120data"], "seq_scores": [0.7946303486824036, 0.9120767712593079, 0.9679719805717468, 0.9471451640129089], "text": " The English 2018 data", "score": 0.9054560661315918, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 11539, "end": 11573, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.9673854112625122, 0.990159809589386, 0.9877169728279114, 0.9907752871513367], "text": " conversational response selection", "score": 0.9840093702077866, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11704, "end": 11722, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01204", "41", ",", "450", ",", "449", "\u0120lines"], "seq_scores": [0.9269450306892395, 0.9519287943840027, 0.9716669917106628, 0.9588054418563843, 0.9740334749221802, 0.9370508193969727, 0.9597036242485046], "text": " 441,450,449 lines", "score": 0.9543048824582782, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11737, "end": 11758, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120316", ",", "89", "1", ",", "7", "17", "\u0120examples"], "seq_scores": [0.9376718401908875, 0.9939804077148438, 0.9960662722587585, 0.9967065453529358, 0.9975186586380005, 0.9973821043968201, 0.9974440336227417, 0.9949786067008972], "text": " 316,891,717 examples", "score": 0.9889685586094856, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11759, "end": 11768, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120data"], "seq_scores": [0.9745548963546753, 0.9802541136741638], "text": " The data", "score": 0.9774045050144196, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11782, "end": 11789, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120chunks"], "seq_scores": [0.6604291796684265], "text": " chunks", "score": 0.6604291796684265, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11792, "end": 11806, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120100", ",", "000", "\u0120lines"], "seq_scores": [0.8261284828186035, 0.9669451713562012, 0.9721634387969971, 0.9752566814422607], "text": " 100,000 lines", "score": 0.9351234436035156, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11841, "end": 11855, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120train", "\u0120set"], "seq_scores": [0.9852494597434998, 0.9927576780319214, 0.9950042366981506], "text": " the train set", "score": 0.9910037914911906, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11858, "end": 11871, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120test", "\u0120set"], "seq_scores": [0.9808846712112427, 0.9956877827644348, 0.9951403141021729], "text": " the test set", "score": 0.9905709226926168, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11933, "end": 11957, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Wan", "\u0120and", "\u0120McA", "u", "ley", "\u0120(", "2016", ");"], "seq_scores": [0.9966412782669067, 0.9989939332008362, 0.9989249110221863, 0.9991083741188049, 0.9990562796592712, 0.9982497096061707, 0.9986549615859985, 0.9936172962188721], "text": " Wan and McAuley (2016);", "score": 0.9979058429598808, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 11957, "end": 11982, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120McA", "u", "ley", "\u0120and", "\u0120Yang", "\u0120(", "2016", "),"], "seq_scores": [0.9956888556480408, 0.9989557266235352, 0.9991480112075806, 0.9991647005081177, 0.9991000890731812, 0.998589813709259, 0.9986203908920288, 0.995879054069519], "text": " McAuley and Yang (2016),", "score": 0.9981433302164078, "type": "ScholarlyEntity"}, {"label": "Datasource", "begin": 12021, "end": 12042, "seq_label": ["B-Datasource", "I-Datasource", "I-Datasource"], "seq_token": ["\u0120Amazon", "\u0120product", "\u0120pages"], "seq_scores": [0.8673347234725952, 0.6557672619819641, 0.5846078991889954], "text": " Amazon product pages", "score": 0.7025699615478516, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11887, "end": 11899, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["This", "\u0120dataset"], "seq_scores": [0.9925199747085571, 0.9970398545265198], "text": "This dataset", "score": 0.9947799146175385, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11911, "end": 11920, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120corpus"], "seq_scores": [0.9742575883865356, 0.988227903842926], "text": " a corpus", "score": 0.9812427461147308, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 11994, "end": 12004, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120questions"], "seq_scores": [0.9441437721252441], "text": " questions", "score": 0.9441437721252441, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12057, "end": 12066, "seq_label": ["I-DatasetGeneric", "B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120answers", "\u0120a", "\u0120corpus"], "seq_scores": [0.5467663407325745, 0.9885638952255249, 0.992159366607666], "text": " a corpus", "score": 0.8424965341885885, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12069, "end": 12090, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120question", "answer", "\u0120pairs"], "seq_scores": [0.8772454857826233, 0.9895589351654053, 0.9869387149810791], "text": " questionanswer pairs", "score": 0.9512477119763693, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12286, "end": 12295, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120data"], "seq_scores": [0.964331865310669, 0.9631275534629822], "text": " the data", "score": 0.9637297093868256, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12309, "end": 12320, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120total", "\u0120of"], "seq_scores": [0.9698165655136108, 0.7424753308296204, 0.7787373065948486], "text": " a total of", "score": 0.8303430676460266, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12320, "end": 12339, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u01203", ",", "689", ",", "9", "12", "\u0120examples"], "seq_scores": [0.7022122740745544, 0.9961913824081421, 0.9975710511207581, 0.9971319437026978, 0.9983853101730347, 0.998172402381897, 0.9971545934677124], "text": " 3,689,912 examples", "score": 0.9552598510469709, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12340, "end": 12361, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120The", "\u0120train", "/", "test", "\u0120split"], "seq_scores": [0.569722056388855, 0.8306068778038025, 0.861706018447876, 0.9170367121696472, 0.550654411315918], "text": " The train/test split", "score": 0.7459452152252197, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 12660, "end": 12684, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response"], "seq_scores": [0.8928465247154236, 0.9102608561515808, 0.8958023190498352], "text": " conversational response", "score": 0.8996365666389465, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12686, "end": 12705, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Ser", "ban", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9978072047233582, 0.9975515007972717, 0.9976145029067993, 0.997369647026062, 0.9969902038574219, 0.9962325692176819], "text": "Serban et al., 2016", "score": 0.9972609380880991, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12706, "end": 12726, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120R", "itter", "\u0120et", "\u0120al", ".,", "\u01202011"], "seq_scores": [0.9977071285247803, 0.9969015121459961, 0.9974184036254883, 0.9973229765892029, 0.9970512390136719, 0.9965417981147766], "text": " Ritter et al., 2011", "score": 0.9971571763356527, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12727, "end": 12748, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120V", "iny", "als", "\u0120and", "\u0120Le", ",", "\u01202015"], "seq_scores": [0.9977771639823914, 0.9971514344215393, 0.9973936080932617, 0.9976539015769958, 0.997660756111145, 0.9974665641784668, 0.9966962337493896], "text": " Vinyals and Le, 2015", "score": 0.9973999517304557, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12749, "end": 12770, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120S", "ordon", "i", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9977443218231201, 0.9972034692764282, 0.9977655410766602, 0.99772709608078, 0.9976876974105835, 0.9974241256713867, 0.9969115853309631], "text": " Sordoni et al., 2015", "score": 0.9974948338099888, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12771, "end": 12790, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Shang", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9972133040428162, 0.9974046349525452, 0.997822642326355, 0.9976959824562073, 0.997393012046814], "text": " Shang et al., 2015", "score": 0.9975059151649475, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12791, "end": 12811, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120K", "ann", "an", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9978067278862, 0.9977656602859497, 0.9982219338417053, 0.9980625510215759, 0.9979902505874634, 0.9978025555610657, 0.9975449442863464], "text": " Kannan et al., 2016", "score": 0.9978849462100438, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 12850, "end": 12884, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.9510751366615295, 0.9832875728607178, 0.9863359928131104, 0.9838466644287109], "text": " conversational response selection", "score": 0.9761363416910172, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12886, "end": 12903, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Low", "e", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9983938336372375, 0.9980705380439758, 0.9983539581298828, 0.9981377124786377, 0.9977376461029053, 0.9971611499786377], "text": "Lowe et al., 2015", "score": 0.9979758063952128, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12904, "end": 12930, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120In", "aba", "\u0120and", "\u0120Tak", "ah", "ashi", ",", "\u01202016"], "seq_scores": [0.9979604482650757, 0.9974379539489746, 0.9978491067886353, 0.9979839324951172, 0.9980623126029968, 0.9979183077812195, 0.9977409839630127, 0.9972884654998779], "text": " Inaba and Takahashi, 2016", "score": 0.9977801889181137, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12931, "end": 12947, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yu", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.997235119342804, 0.9974769949913025, 0.9980815649032593, 0.9980138540267944, 0.9978221654891968], "text": " Yu et al., 2016", "score": 0.9977259397506714, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 12948, "end": 12971, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9975460171699524, 0.9979609251022339, 0.9982292056083679, 0.9981239438056946, 0.9981184005737305], "text": " Henderson et al., 2017", "score": 0.9979956984519959, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12441, "end": 12468, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["The", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9922245740890503, 0.9869634509086609, 0.9981611371040344, 0.9987818598747253], "text": "The conversational datasets", "score": 0.9940327554941177, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12538, "end": 12558, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120variety", "\u0120of", "\u0120models"], "seq_scores": [0.9571427702903748, 0.9144995212554932, 0.9328442215919495, 0.9852895140647888], "text": " a variety of models", "score": 0.9474440068006516, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 12600, "end": 12613, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120datasets"], "seq_scores": [0.9953176975250244, 0.9974004030227661], "text": " the datasets", "score": 0.9963590502738953, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 12639, "end": 12657, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120gener", "ative", "\u0120models"], "seq_scores": [0.9851434826850891, 0.9915178418159485, 0.9920057654380798], "text": " generative models", "score": 0.9895556966463724, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 12985, "end": 13019, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.976153552532196, 0.9918819069862366, 0.9918467998504639, 0.9930515289306641], "text": " conversational response selection", "score": 0.9882334470748901, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13214, "end": 13236, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120information", "\u0120retrieval"], "seq_scores": [0.5116109848022461, 0.5213579535484314], "text": " information retrieval", "score": 0.5164844691753387, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13346, "end": 13363, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Low", "e", "\u0120et", "\u0120al", ".,", "\u01202015"], "seq_scores": [0.9983412027359009, 0.9986355900764465, 0.9988223910331726, 0.9987216591835022, 0.9985371828079224, 0.9981688261032104], "text": "Lowe et al., 2015", "score": 0.9985378086566925, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13364, "end": 13390, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120In", "aba", "\u0120and", "\u0120Tak", "ah", "ashi", ",", "\u01202016"], "seq_scores": [0.9978197813034058, 0.9983304142951965, 0.9985117316246033, 0.9985491633415222, 0.9985981583595276, 0.9985243678092957, 0.9984999895095825, 0.9981104135513306], "text": " Inaba and Takahashi, 2016", "score": 0.998368002474308, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13391, "end": 13407, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yu", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9976229071617126, 0.9983270764350891, 0.9986814856529236, 0.9985700845718384, 0.9983736276626587], "text": " Yu et al., 2016", "score": 0.9983150362968445, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13408, "end": 13429, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Al", "-", "R", "f", "ou", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9977365732192993, 0.9984544515609741, 0.9985582232475281, 0.9987074136734009, 0.9986806511878967, 0.9987609386444092, 0.9988328814506531, 0.998687207698822, 0.9983389377593994], "text": " Al-Rfou et al., 2016", "score": 0.9985285864935981, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13430, "end": 13453, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9975560903549194, 0.9983994364738464, 0.9986783862113953, 0.9985639452934265, 0.9983939528465271], "text": " Henderson et al., 2017", "score": 0.998318362236023, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13454, "end": 13472, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Lowe", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9976072311401367, 0.9983774423599243, 0.998621940612793, 0.9985089898109436, 0.9983812570571899], "text": " Lowe et al., 2017", "score": 0.9982993721961975, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13473, "end": 13489, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Wu", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9975858926773071, 0.9983457326889038, 0.9986926913261414, 0.9985913634300232, 0.998441755771637], "text": " Wu et al., 2017", "score": 0.9983314871788025, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13490, "end": 13507, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Cer", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9976049661636353, 0.9984017014503479, 0.9986323714256287, 0.9986054301261902, 0.9984641075134277], "text": " Cer et al., 2018", "score": 0.9983417153358459, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13508, "end": 13531, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Ch", "aud", "h", "uri", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9980965256690979, 0.9985843896865845, 0.9989047050476074, 0.9988129138946533, 0.9988155364990234, 0.9988238215446472, 0.998753547668457, 0.9985221028327942], "text": " Chaudhuri et al., 2018", "score": 0.9986641928553581, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13532, "end": 13551, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Du", "\u0120and", "\u0120Black", ",", "\u01202018"], "seq_scores": [0.9976323843002319, 0.9985141158103943, 0.9984922409057617, 0.9987433552742004, 0.9985030889511108], "text": " Du and Black, 2018", "score": 0.9983770370483398, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13552, "end": 13571, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kumar", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9974908828735352, 0.998336136341095, 0.9987205266952515, 0.9986662864685059, 0.9985054731369019], "text": " Kumar et al., 2018", "score": 0.9983438611030578, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13572, "end": 13589, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Liu", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.997479259967804, 0.9983745813369751, 0.9987183809280396, 0.9986516833305359, 0.9985222220420837], "text": " Liu et al., 2018", "score": 0.9983492255210876, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13590, "end": 13608, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9974966645240784, 0.9983273148536682, 0.9987291693687439, 0.9986841082572937, 0.9985111355781555], "text": " Yang et al., 2018", "score": 0.9983496785163879, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13609, "end": 13627, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Zhou", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9975082874298096, 0.9983657002449036, 0.9987325072288513, 0.9986833930015564, 0.9985337257385254], "text": " Zhou et al., 2018", "score": 0.9983647227287292, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13628, "end": 13652, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Gun", "ase", "k", "ara", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9978139400482178, 0.9984253644943237, 0.9987667798995972, 0.9986658096313477, 0.9987576007843018, 0.9987767338752747, 0.9986972212791443, 0.9985145926475525], "text": " Gunasekara et al., 2019", "score": 0.9985522553324699, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13653, "end": 13670, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Tao", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9973087310791016, 0.9984486103057861, 0.9987462759017944, 0.9986826777458191, 0.9985989928245544], "text": " Tao et al., 2019", "score": 0.9983570575714111, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13728, "end": 13737, "seq_label": ["B-Task"], "seq_token": ["\u0120dialogue"], "seq_scores": [0.6359326839447021], "text": " dialogue", "score": 0.6359326839447021, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13746, "end": 13773, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Task", "I-Method"], "seq_token": ["\u0120question", "-", "ans", "w", "ering", "\u0120systems"], "seq_scores": [0.8133789896965027, 0.6141748428344727, 0.5565579533576965, 0.4986518621444702, 0.5081376433372498, 0.577754020690918], "text": " question-answering systems", "score": 0.5947758853435516, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13778, "end": 13806, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120response", "\u0120suggestion", "\u0120systems"], "seq_scores": [0.8768218159675598, 0.857426643371582, 0.3561684489250183], "text": " response suggestion systems", "score": 0.6968056360880533, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13819, "end": 13838, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120response", "\u0120selection"], "seq_scores": [0.9661741256713867, 0.9832248687744141], "text": " response selection", "score": 0.9746994972229004, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 13869, "end": 13878, "seq_label": ["B-Method"], "seq_token": ["\u0120learning"], "seq_scores": [0.829339325428009], "text": " learning", "score": 0.829339325428009, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 13878, "end": 13912, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120implicit", "\u0120semantic", "\u0120representations"], "seq_scores": [0.44002410769462585, 0.9412959218025208, 0.9198422431945801], "text": " implicit semantic representations", "score": 0.7670540908972422, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 13948, "end": 13979, "seq_label": ["B-Task", "I-Task", "I-Task"], "seq_token": ["\u0120natural", "\u0120language", "\u0120understanding"], "seq_scores": [0.8671503663063049, 0.9035528302192688, 0.9183942675590515], "text": " natural language understanding", "score": 0.8963658213615417, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13981, "end": 13997, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["C", "er", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9984025359153748, 0.9979524612426758, 0.9985663294792175, 0.9984763264656067, 0.9983667731285095, 0.9980921149253845], "text": "Cer et al., 2018", "score": 0.9983094235261282, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 13998, "end": 14016, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.997766375541687, 0.9982494115829468, 0.9986035227775574, 0.9985621571540833, 0.9984627962112427], "text": " Yang et al., 2018", "score": 0.9983288526535035, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13142, "end": 13154, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120Such", "\u0120models"], "seq_scores": [0.9804537296295166, 0.9793287515640259], "text": " Such models", "score": 0.9798912405967712, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 13672, "end": 13679, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120Models"], "seq_scores": [0.9470975995063782], "text": " Models", "score": 0.9470975995063782, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14212, "end": 14228, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120judgement"], "seq_scores": [0.8631666898727417, 0.8930535912513733], "text": " human judgement", "score": 0.8781101405620575, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14264, "end": 14278, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120normal", "ization"], "seq_scores": [0.8648887872695923, 0.9329232573509216], "text": " normalization", "score": 0.898906022310257, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14279, "end": 14292, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120token", "ization"], "seq_scores": [0.8243653774261475, 0.9429478049278259], "text": " tokenization", "score": 0.8836565911769867, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 14325, "end": 14341, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120human", "\u0120judgement"], "seq_scores": [0.916141152381897, 0.9049950242042542], "text": " human judgement", "score": 0.9105680882930756, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14075, "end": 14082, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120models"], "seq_scores": [0.9292452335357666], "text": " models", "score": 0.9292452335357666, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14130, "end": 14148, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120gener", "ative", "\u0120models"], "seq_scores": [0.9914871454238892, 0.9958330988883972, 0.9945308566093445], "text": " generative models", "score": 0.993950366973877, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 14390, "end": 14424, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.8783041834831238, 0.7897553443908691, 0.8555369973182678, 0.8535596132278442], "text": " conversational response selection", "score": 0.8442890346050262, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14652, "end": 14672, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Al", "-", "R", "f", "ou", "\u0120et", "\u0120al", ".,", "\u01202016"], "seq_scores": [0.9983262419700623, 0.998379111289978, 0.9986151456832886, 0.9988746047019958, 0.9988883137702942, 0.9988893866539001, 0.9989708662033081, 0.9987898468971252, 0.9983463287353516], "text": "Al-Rfou et al., 2016", "score": 0.9986755384339226, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14673, "end": 14696, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.9980984330177307, 0.9985514283180237, 0.9987541437149048, 0.998610258102417, 0.9983828067779541], "text": " Henderson et al., 2017", "score": 0.9984794139862061, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14697, "end": 14714, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Cer", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9979006052017212, 0.9984752535820007, 0.9986944794654846, 0.9986351132392883, 0.9984129667282104], "text": " Cer et al., 2018", "score": 0.9984236836433411, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14715, "end": 14734, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kumar", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9980886578559875, 0.9984519481658936, 0.9987733960151672, 0.9987102746963501, 0.9984862804412842], "text": " Kumar et al., 2018", "score": 0.9985021114349365, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14735, "end": 14753, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".,", "\u01202018"], "seq_scores": [0.9980892539024353, 0.9985334873199463, 0.9987830519676208, 0.9987242817878723, 0.9985464215278625], "text": " Yang et al., 2018", "score": 0.9985352993011475, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 14754, "end": 14778, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Gun", "ase", "k", "ara", "\u0120et", "\u0120al", ".,", "\u01202019"], "seq_scores": [0.9982823133468628, 0.9985641837120056, 0.998993456363678, 0.9989672899246216, 0.9989884495735168, 0.9989678859710693, 0.9988607168197632, 0.9987241625785828], "text": " Gunasekara et al., 2019", "score": 0.9987935572862625, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 15014, "end": 15036, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["H", "end", "erson", "\u0120et", "\u0120al", ".,", "\u01202017"], "seq_scores": [0.998406708240509, 0.9972707629203796, 0.9984533786773682, 0.9986341595649719, 0.998508632183075, 0.9980729818344116, 0.997525155544281], "text": "Henderson et al., 2017", "score": 0.9981245398521423, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 15264, "end": 15298, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection"], "seq_scores": [0.9554929733276367, 0.9524612426757812, 0.953332245349884, 0.9549449682235718], "text": " conversational response selection", "score": 0.9540578573942184, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 14390, "end": 14431, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120models"], "seq_scores": [0.9765797257423401, 0.992201566696167, 0.993786096572876, 0.9954226613044739, 0.9932928085327148], "text": " conversational response selection models", "score": 0.9902565717697144, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14434, "end": 14449, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120these", "\u0120datasets"], "seq_scores": [0.9897339940071106, 0.995697021484375], "text": " these datasets", "score": 0.9927155077457428, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14528, "end": 14541, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u012099", "\u0120responses"], "seq_scores": [0.9500557780265808, 0.9468902945518494], "text": " 99 responses", "score": 0.9484730362892151, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14554, "end": 14571, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120test", "\u0120dataset"], "seq_scores": [0.9804193377494812, 0.9973403811454773, 0.997462272644043], "text": " the test dataset", "score": 0.9917406638463339, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14574, "end": 14584, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120negatives"], "seq_scores": [0.8923256397247314], "text": " negatives", "score": 0.8923256397247314, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14813, "end": 14848, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u012099", "\u0120randomly", "\u0120selected", "\u0120negatives"], "seq_scores": [0.7335212826728821, 0.7212221026420593, 0.8114707469940186, 0.8399910926818848, 0.9186767339706421], "text": " the 99 randomly selected negatives", "score": 0.8049763917922974, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 14860, "end": 14874, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120bad", "\u0120responses"], "seq_scores": [0.6618263721466064, 0.8411697149276733], "text": " bad responses", "score": 0.7514980435371399, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15091, "end": 15121, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120100", "\u0120(", "context", ",", "\u0120response", ")", "\u0120pairs"], "seq_scores": [0.5928410887718201, 0.8606800436973572, 0.8682992458343506, 0.8997818827629089, 0.8639426231384277, 0.8898781538009644, 0.7946927547454834], "text": " 100 (context, response) pairs", "score": 0.8243022561073303, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15192, "end": 15214, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120negative", "\u0120examples"], "seq_scores": [0.9131048917770386, 0.8860445022583008, 0.7887434363365173], "text": " the negative examples", "score": 0.8626309434572855, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15244, "end": 15261, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120baseline", "\u0120methods"], "seq_scores": [0.7715714573860168, 0.5673249363899231], "text": " baseline methods", "score": 0.66944819688797, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15338, "end": 15354, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120These", "\u0120bas", "elines"], "seq_scores": [0.8995252251625061, 0.9256731271743774, 0.9523014426231384], "text": " These baselines", "score": 0.925833264986674, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15388, "end": 15418, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120subset", "\u0120of", "\u0120the", "\u0120training", "\u0120data"], "seq_scores": [0.9597517848014832, 0.9705759286880493, 0.9093778729438782, 0.8731892108917236, 0.9968434572219849, 0.9962027668952942], "text": " a subset of the training data", "score": 0.9509901702404022, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15475, "end": 15488, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120each", "\u0120dataset"], "seq_scores": [0.9734400510787964, 0.9876500964164734], "text": " each dataset", "score": 0.9805450737476349, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15511, "end": 15551, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120more", "\u0120competitive", "\u0120neural", "\u0120enc", "oder", "\u0120model"], "seq_scores": [0.9886300563812256, 0.9920185208320618, 0.9942057728767395, 0.9925020933151245, 0.9976136684417725, 0.9974550604820251, 0.9937188625335693], "text": " a more competitive neural encoder model", "score": 0.9937348621232169, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15570, "end": 15594, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120entire", "\u0120training", "\u0120set"], "seq_scores": [0.9885661005973816, 0.9958142638206482, 0.9969954490661621, 0.99620121717453], "text": " the entire training set", "score": 0.9943942576646805, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15654, "end": 15681, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120keyword", "\u0120similarity", "\u0120metrics"], "seq_scores": [0.7200880646705627, 0.8122071027755737, 0.6541348695755005], "text": " keyword similarity metrics", "score": 0.7288100123405457, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 15748, "end": 15770, "seq_label": ["B-Task", "I-Task"], "seq_token": ["\u0120information", "\u0120retrieval"], "seq_scores": [0.9324698448181152, 0.8940057754516602], "text": " information retrieval", "score": 0.9132378101348877, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15781, "end": 15788, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120TF", "-", "ID", "F"], "seq_scores": [0.6241855621337891, 0.9914714694023132, 0.9926619529724121, 0.9903678894042969], "text": " TF-IDF", "score": 0.8996717184782028, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15804, "end": 15831, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120inverse", "\u0120document", "\u0120frequency"], "seq_scores": [0.5672592520713806, 0.5830210447311401, 0.5683825612068176], "text": " inverse document frequency", "score": 0.5728876193364462, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15896, "end": 15921, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120tf", "-", "id", "f", "\u0120cos", "ine", "\u0120similarity"], "seq_scores": [0.4911535084247589, 0.8975154161453247, 0.9232248067855835, 0.9332177042961121, 0.908287763595581, 0.875378429889679, 0.7761844992637634], "text": " tf-idf cosine similarity", "score": 0.8292803040572575, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 15938, "end": 15958, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["Man", "ning", "\u0120et", "\u0120al", ".,", "\u01202008"], "seq_scores": [0.9977208971977234, 0.9940819144248962, 0.9963290095329285, 0.99581378698349, 0.9945585131645203, 0.9919902682304382], "text": "Manning et al., 2008", "score": 0.9950823982556661, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15623, "end": 15650, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["The", "\u0120keyword", "-", "based", "\u0120bas", "elines"], "seq_scores": [0.988669216632843, 0.9747247695922852, 0.9966059923171997, 0.9972057938575745, 0.9972628355026245, 0.9958729147911072], "text": "The keyword-based baselines", "score": 0.9917235871156057, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15689, "end": 15699, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120responses"], "seq_scores": [0.7692934274673462], "text": " responses", "score": 0.7692934274673462, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 15726, "end": 15744, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120typical", "\u0120bas", "elines"], "seq_scores": [0.9296256303787231, 0.9315212965011597, 0.9805110096931458], "text": " typical baselines", "score": 0.9472193121910095, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15845, "end": 15862, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.9910587668418884, 0.9956503510475159, 0.9951364398002625], "text": " the training set", "score": 0.9939485192298889, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 15874, "end": 15884, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120responses"], "seq_scores": [0.8078722953796387], "text": " responses", "score": 0.8078722953796387, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15964, "end": 15976, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120BM", "25", "\u0120method"], "seq_scores": [0.655771017074585, 0.9860709309577942, 0.5854752659797668], "text": " BM25 method", "score": 0.742439071337382, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 15993, "end": 16015, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120tf", "-", "id", "f", "\u0120similarity"], "seq_scores": [0.8941233158111572, 0.6341702342033386, 0.9851470589637756, 0.9880185127258301, 0.9877553582191467, 0.9724459052085876], "text": " the tf-idf similarity", "score": 0.910276730855306, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16025, "end": 16039, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120an", "\u0120adjustment"], "seq_scores": [0.59980309009552, 0.5892564654350281], "text": " an adjustment", "score": 0.594529777765274, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 16061, "end": 16089, "seq_label": ["I-Method", "B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120term", "Roberts", "on", "\u0120and", "\u0120Zar", "ago", "za", ",", "\u01202009"], "seq_scores": [0.5272071361541748, 0.9969424605369568, 0.9883531928062439, 0.9945993423461914, 0.9923169612884521, 0.9943134784698486, 0.9932678937911987, 0.9942060112953186, 0.9920187592506409], "text": "Robertson and Zaragoza, 2009", "score": 0.9414694706598917, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16342, "end": 16357, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120T", "ensor", "flow", "\u0120Hub"], "seq_scores": [0.7860444784164429, 0.7760348320007324, 0.8136054873466492, 0.6919275522232056], "text": " Tensorflow Hub", "score": 0.7669030874967575, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 16358, "end": 16369, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120B", "ERT", "-", "SM", "ALL"], "seq_scores": [0.9947287440299988, 0.9964219331741333, 0.9966138005256653, 0.9967708587646484, 0.9952355027198792], "text": " BERT-SMALL", "score": 0.995954167842865, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 16373, "end": 16410, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120bid", "irection", "al", "\u0120transformer", "\u0120model"], "seq_scores": [0.8028968572616577, 0.9262528419494629, 0.9497777223587036, 0.9447585940361023, 0.9437342286109924, 0.7459025979042053], "text": " deep bidirectional transformer model", "score": 0.8855538070201874, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 16413, "end": 16435, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Dev", "lin", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ")."], "seq_scores": [0.9951826930046082, 0.9972555041313171, 0.9978770017623901, 0.9981585144996643, 0.9976673722267151, 0.9968783855438232, 0.9978405237197876, 0.9775731563568115], "text": " Devlin et al. (2018).", "score": 0.9948041439056396, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16146, "end": 16193, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120vector", "-", "\u0120publicly", "\u0120available", "\u0120neural", "\u0120net", "\u0120embed", "ding", "\u0120models"], "seq_scores": [0.5050210356712341, 0.5510385036468506, 0.9731482267379761, 0.9870948195457458, 0.9765457510948181, 0.9963585734367371, 0.996614396572113, 0.9966979026794434, 0.9949106574058533], "text": " publicly available neural net embedding models", "score": 0.8863810963100858, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16202, "end": 16211, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120contexts"], "seq_scores": [0.7911168336868286], "text": " contexts", "score": 0.7911168336868286, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 16436, "end": 16446, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["BER", "T", "-", "L", "AR", "GE"], "seq_scores": [0.9776110649108887, 0.9881287217140198, 0.9850475788116455, 0.9846217632293701, 0.9799627065658569, 0.9628809094429016], "text": "BERT-LARGE", "score": 0.9797087907791138, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 16455, "end": 16492, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120deep", "\u0120bid", "irection", "al", "\u0120transformer", "\u0120model"], "seq_scores": [0.4397660195827484, 0.9360936880111694, 0.9504448175430298, 0.9495988488197327, 0.9366750717163086, 0.5650996565818787], "text": " deep bidirectional transformer model", "score": 0.7962796837091446, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16446, "end": 16492, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120larger", "\u0120deep", "\u0120bid", "irection", "al", "\u0120transformer", "\u0120model"], "seq_scores": [0.8075283765792847, 0.8406708240509033, 0.9915764927864075, 0.9932729005813599, 0.9926671385765076, 0.9918050765991211, 0.9937756061553955, 0.9905011653900146], "text": " a larger deep bidirectional transformer model", "score": 0.9502246975898743, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16576, "end": 16580, "seq_label": ["B-Method"], "seq_token": ["\u0120SIM"], "seq_scores": [0.5771216750144958], "text": " SIM", "score": 0.5771216750144958, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16503, "end": 16537, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120two", "\u0120vector", "-", "based", "\u0120baseline", "\u0120methods"], "seq_scores": [0.8103092908859253, 0.9319140911102295, 0.9774670004844666, 0.9774755239486694, 0.9713658690452576, 0.9135450124740601], "text": " two vector-based baseline methods", "score": 0.9303461313247681, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16554, "end": 16571, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120above", "\u0120models"], "seq_scores": [0.9753771424293518, 0.9925082325935364, 0.9901797771453857], "text": " the above models", "score": 0.9860217173894247, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16593, "end": 16603, "seq_label": ["B-DatasetGeneric"], "seq_token": ["\u0120responses"], "seq_scores": [0.5335683822631836], "text": " responses", "score": 0.5335683822631836, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16665, "end": 16677, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120method"], "seq_scores": [0.5070075988769531, 0.6929926872253418], "text": " This method", "score": 0.6000001430511475, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 16687, "end": 16705, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120pret", "rained", "\u0120models"], "seq_scores": [0.9908402562141418, 0.9959279894828796, 0.9954820871353149], "text": " pretrained models", "score": 0.9940834442774454, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 16722, "end": 16739, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.9920519590377808, 0.9940683841705322, 0.9946081638336182], "text": " the training set", "score": 0.993576169013977, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 16748, "end": 16762, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["The", "\u0120MAP", "\u0120method"], "seq_scores": [0.7801293730735779, 0.819040834903717, 0.885229766368866], "text": "The MAP method", "score": 0.8281333247820536, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17071, "end": 17108, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120learning", "\u0120an", "\u0120arbitrary", "\u0120linear", "\u0120mapping"], "seq_scores": [0.7210910320281982, 0.5385369062423706, 0.9182624816894531, 0.9123042225837708, 0.8894920349121094], "text": " learning an arbitrary linear mapping", "score": 0.7959373354911804, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17282, "end": 17297, "seq_label": ["I-Method", "I-Method", "B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120residual", "\u0120connection", "\u0120the", "\u0120MAP", "\u0120method"], "seq_scores": [0.6225751638412476, 0.528550922870636, 0.7671194672584534, 0.7546865344047546, 0.684375524520874], "text": " the MAP method", "score": 0.6714615225791931, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17188, "end": 17198, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120model"], "seq_scores": [0.9831693172454834, 0.9864077568054199], "text": " the model", "score": 0.9847885370254517, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17306, "end": 17317, "seq_label": ["I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120SIM", "\u0120the", "\u0120method"], "seq_scores": [0.5817020535469055, 0.7268081307411194, 0.6959115266799927], "text": " the method", "score": 0.6681405703226725, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17444, "end": 17465, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120dot", "\u0120product", "\u0120loss"], "seq_scores": [0.82382732629776, 0.8890255689620972, 0.9067041277885437, 0.9055950045585632], "text": " the dot product loss", "score": 0.881288006901741, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 17470, "end": 17495, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".", "\u0120(", "2017", ")."], "seq_scores": [0.9952194094657898, 0.9979422688484192, 0.9986905455589294, 0.9983306527137756, 0.9967897534370422, 0.9972471594810486, 0.989897608757019], "text": " Henderson et al. (2017).", "score": 0.9963024854660034, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17380, "end": 17415, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120random", "\u0120sample", "\u0120of", "\u012010", ",", "000", "\u0120examples"], "seq_scores": [0.9564335346221924, 0.8740929365158081, 0.8873156905174255, 0.8026870489120483, 0.5579220056533813, 0.9919731616973877, 0.993457019329071, 0.9913512468338013], "text": " a random sample of 10,000 examples", "score": 0.8819040805101395, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17420, "end": 17437, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120training", "\u0120set"], "seq_scores": [0.8465207815170288, 0.9979337453842163, 0.9974838495254517], "text": " the training set", "score": 0.9473127921422323, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17571, "end": 17598, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120held", "-", "out", "\u0120development", "\u0120set"], "seq_scores": [0.9897522926330566, 0.9911174774169922, 0.99094557762146, 0.9932959675788879, 0.9930214285850525, 0.9930578470230103], "text": " a held-out development set", "score": 0.9918650984764099, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 17640, "end": 17659, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120evaluation", "\u0120set"], "seq_scores": [0.9913908243179321, 0.9948163628578186, 0.9934754967689514], "text": " the evaluation set", "score": 0.9932275613149008, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17787, "end": 17795, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120embed", "ding", "\u0120vector", "-", "based", "\u0120methods", "\u0120USE", "-", "SIM"], "seq_scores": [0.6717125773429871, 0.7013785243034363, 0.5258945822715759, 0.6414527297019958, 0.6774051189422607, 0.5823383331298828, 0.7861321568489075, 0.782663881778717, 0.8052495121955872], "text": " USE-SIM", "score": 0.6860252685017056, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17796, "end": 17804, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120USE", "-", "MAP"], "seq_scores": [0.8661519289016724, 0.8364128470420837, 0.8626141548156738], "text": " USE-MAP", "score": 0.8550596435864767, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17805, "end": 17819, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120USE", "-", "L", "AR", "GE", "-", "SIM"], "seq_scores": [0.8370622992515564, 0.7342877388000488, 0.7469345331192017, 0.7526206374168396, 0.7221598029136658, 0.7356805205345154, 0.705088198184967], "text": " USE-LARGE-SIM", "score": 0.7476905328886849, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17820, "end": 17834, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120USE", "-", "L", "AR", "GE", "-", "MAP"], "seq_scores": [0.8202471137046814, 0.7027952671051025, 0.7235541343688965, 0.7157149910926819, 0.6927456855773926, 0.7117527723312378, 0.7038763165473938], "text": " USE-LARGE-MAP", "score": 0.7243837543896267, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17835, "end": 17844, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "MO", "-", "SIM"], "seq_scores": [0.7584273815155029, 0.5723326206207275, 0.6554335951805115, 0.6269100308418274], "text": " ELMO-SIM", "score": 0.6532759070396423, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 17849, "end": 17858, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "MO", "-", "MAP"], "seq_scores": [0.8195368647575378, 0.7433416843414307, 0.7844365239143372, 0.7782730460166931], "text": " ELMO-MAP", "score": 0.7813970297574997, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17679, "end": 17706, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120three", "\u0120embed", "ding", "\u0120models"], "seq_scores": [0.9435787200927734, 0.9771058559417725, 0.9921283721923828, 0.9935985803604126, 0.9928224682807922], "text": " the three embedding models", "score": 0.9798467993736267, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17751, "end": 17786, "seq_label": ["I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120vector", "-", "based", "\u0120methods", "\u0120the", "\u0120following", "\u0120six", "\u0120baseline", "\u0120methods"], "seq_scores": [0.6275376081466675, 0.6478102803230286, 0.6345331072807312, 0.5372704863548279, 0.710932195186615, 0.7594956755638123, 0.7818728089332581, 0.9293003082275391, 0.8740618228912354], "text": " the following six baseline methods", "score": 0.7225349214341905, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 17886, "end": 17892, "seq_label": ["B-Method"], "seq_token": ["\u0120train"], "seq_scores": [0.5958828330039978], "text": " train", "score": 0.5958828330039978, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 17907, "end": 17922, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120neural", "\u0120enc", "oder"], "seq_scores": [0.7062664031982422, 0.553699791431427, 0.7756737470626831], "text": " neural encoder", "score": 0.6785466472307841, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 17971, "end": 17993, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120separate", "\u0120sub", "-", "net", "works"], "seq_scores": [0.5293805003166199, 0.562637984752655, 0.780868411064148, 0.8251267075538635, 0.7877431511878967], "text": " separate sub-networks", "score": 0.6971513509750367, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18146, "end": 18171, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".", "\u0120(", "2017", ");"], "seq_scores": [0.9969329833984375, 0.9988294243812561, 0.999086856842041, 0.9987699389457703, 0.9981825351715088, 0.9985461235046387, 0.9312140345573425], "text": " Henderson et al. (2017);", "score": 0.9887945566858564, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18171, "end": 18190, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Cer", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ");"], "seq_scores": [0.9969210624694824, 0.9986016154289246, 0.9989088773727417, 0.9987125396728516, 0.9980713725090027, 0.9984332919120789, 0.7946652770042419], "text": " Cer et al. (2018);", "score": 0.969187719481332, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18190, "end": 18211, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Kumar", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ");"], "seq_scores": [0.9971653819084167, 0.9984440207481384, 0.9988467693328857, 0.9985792636871338, 0.9979633092880249, 0.9983816146850586, 0.7530298829078674], "text": " Kumar et al. (2018);", "score": 0.9632014632225037, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18211, "end": 18231, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Yang", "\u0120et", "\u0120al", ".", "\u0120(", "2018", ")."], "seq_scores": [0.9973627924919128, 0.9984734654426575, 0.9988264441490173, 0.9983365535736084, 0.9970533847808838, 0.9983434677124023, 0.9659748673439026], "text": " Yang et al. (2018).", "score": 0.9934815679277692, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 18260, "end": 18275, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120POL", "Y", "AI", "-", "ENC", "OD", "ER"], "seq_scores": [0.9839510917663574, 0.98363196849823, 0.988694965839386, 0.9905952215194702, 0.9926160573959351, 0.992968738079071, 0.991581380367279], "text": " POLYAI-ENCODER", "score": 0.9891484890665326, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 17905, "end": 17928, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120a", "\u0120neural", "\u0120enc", "oder", "\u0120model"], "seq_scores": [0.9874672293663025, 0.991132915019989, 0.9950610995292664, 0.9953640699386597, 0.986377477645874], "text": " a neural encoder model", "score": 0.9910805583000183, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18231, "end": 18242, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120This", "\u0120model"], "seq_scores": [0.9639586210250854, 0.9773091673851013], "text": " This model", "score": 0.9706338942050934, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 18344, "end": 18369, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".", "\u0120(", "2019", ")."], "seq_scores": [0.9962269067764282, 0.9982998967170715, 0.9986345171928406, 0.9982908368110657, 0.9973427653312683, 0.9980071187019348, 0.9496833086013794], "text": " Henderson et al. (2019).", "score": 0.990926478590284, "type": "ScholarlyEntity"}, {"label": "ModelArchitecture", "begin": 18443, "end": 18456, "seq_label": ["B-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture", "I-ModelArchitecture"], "seq_token": ["\u0120sub", "-", "net", "works"], "seq_scores": [0.8079381585121155, 0.8481483459472656, 0.8816035985946655, 0.872835099697113], "text": " sub-networks", "score": 0.8526313006877899, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 18310, "end": 18331, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120neural", "\u0120structure"], "seq_scores": [0.7177860736846924, 0.6785194873809814, 0.654451310634613], "text": " the neural structure", "score": 0.6835856239000956, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18752, "end": 18767, "seq_label": ["I-Method", "B-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120strategy", "\u0120self", "-", "att", "ention"], "seq_scores": [0.5042902827262878, 0.665524423122406, 0.6911632418632507, 0.6972897052764893, 0.7279231548309326], "text": " self-attention", "score": 0.6572381615638733, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 18817, "end": 18847, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120un", "ig", "ram", "\u0120and", "\u0120big", "ram", "\u0120embed", "d", "ings"], "seq_scores": [0.3918556272983551, 0.7516197562217712, 0.7895808219909668, 0.7037380933761597, 0.8097459077835083, 0.8083000183105469, 0.7725146412849426, 0.7116290926933289, 0.6799829006195068], "text": " unigram and bigram embeddings", "score": 0.7132185399532318, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19074, "end": 19095, "seq_label": ["I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["ig", "ram", "\u0120and", "\u0120big", "ram", "\u0120representations", "\u0120the", "\u0120dot", "-", "product", "\u0120loss"], "seq_scores": [0.5574050545692444, 0.5843941569328308, 0.5410141348838806, 0.6227195262908936, 0.5508189797401428, 0.5375029444694519, 0.8547090888023376, 0.9079594016075134, 0.9396509528160095, 0.9401274919509888, 0.9318390488624573], "text": " the dot-product loss", "score": 0.7243764346296137, "type": "ScholarlyEntity"}, {"label": "ReferenceLink", "begin": 19098, "end": 19123, "seq_label": ["B-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink", "I-ReferenceLink"], "seq_token": ["\u0120Henderson", "\u0120et", "\u0120al", ".", "\u0120(", "2017", "),"], "seq_scores": [0.9953033924102783, 0.9964624047279358, 0.9980375170707703, 0.9970085024833679, 0.9936181306838989, 0.9951735138893127, 0.9937585592269897], "text": " Henderson et al. (2017),", "score": 0.9956231457846505, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19128, "end": 19144, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120label", "\u0120smoot", "hing"], "seq_scores": [0.9537195563316345, 0.9788460731506348, 0.9745960235595703], "text": " label smoothing", "score": 0.9690538843472799, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19244, "end": 19272, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120three", "\u0120standard", "\u0120datasets"], "seq_scores": [0.9848909974098206, 0.9266777038574219, 0.9966555833816528, 0.9988030195236206], "text": " the three standard datasets", "score": 0.976756826043129, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19297, "end": 19310, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["Open", "Sub", "t", "itles"], "seq_scores": [0.9764372110366821, 0.975881814956665, 0.9688632488250732, 0.9653326869010925], "text": "OpenSubtitles", "score": 0.9716287404298782, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19310, "end": 19319, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Amazon", "Q", "A"], "seq_scores": [0.9891514778137207, 0.9942003488540649, 0.9943363070487976], "text": " AmazonQA", "score": 0.9925627112388611, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19320, "end": 19345, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120The", "\u0120keyword", "-", "based", "\u0120TF", "-", "ID", "F"], "seq_scores": [0.7594878077507019, 0.7172526121139526, 0.9494233131408691, 0.9463481307029724, 0.9425181150436401, 0.9693806171417236, 0.9705402851104736, 0.9616163969039917], "text": " The keyword-based TF-IDF", "score": 0.9020709097385406, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19349, "end": 19354, "seq_label": ["B-Method", "I-Method"], "seq_token": ["\u0120BM", "25"], "seq_scores": [0.8417316675186157, 0.9218438863754272], "text": " BM25", "score": 0.8817877769470215, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19383, "end": 19408, "seq_label": ["B-Method", "I-Method", "I-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120vector", "-", "based", "\u0120methods"], "seq_scores": [0.5541115403175354, 0.6651687026023865, 0.8129046559333801, 0.8352939486503601, 0.7168601155281067], "text": " the vector-based methods", "score": 0.7168677926063538, "type": "ScholarlyEntity"}, {"label": "Dataset", "begin": 19441, "end": 19450, "seq_label": ["B-Dataset", "I-Dataset", "I-Dataset"], "seq_token": ["\u0120Amazon", "Q", "A"], "seq_scores": [0.9891982674598694, 0.9933764338493347, 0.9946119785308838], "text": " AmazonQA", "score": 0.9923955599466959, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19536, "end": 19555, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120Learning", "\u0120a", "\u0120mapping"], "seq_scores": [0.8380975723266602, 0.8048564195632935, 0.9082193374633789], "text": " Learning a mapping", "score": 0.8503911097844442, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19560, "end": 19575, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120MAP", "\u0120method"], "seq_scores": [0.8879505395889282, 0.8641767501831055, 0.7517617344856262], "text": " the MAP method", "score": 0.8346296747525533, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19620, "end": 19635, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120the", "\u0120SIM", "\u0120method"], "seq_scores": [0.8037521839141846, 0.7398803234100342, 0.6589641571044922], "text": " the SIM method", "score": 0.7341988881429037, "type": "ScholarlyEntity"}, {"label": "Method", "begin": 19662, "end": 19683, "seq_label": ["B-Method", "I-Method", "I-Method"], "seq_token": ["\u0120learning", "\u0120the", "\u0120mapping"], "seq_scores": [0.7069059610366821, 0.5159962177276611, 0.7486559748649597], "text": " learning the mapping", "score": 0.6571860512097677, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19914, "end": 19941, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120Universal", "\u0120Sent", "ence", "\u0120Enc", "oder"], "seq_scores": [0.9470580220222473, 0.9798997044563293, 0.9774408936500549, 0.9744284749031067, 0.9685482382774353], "text": " Universal Sentence Encoder", "score": 0.9694750666618347, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19959, "end": 19964, "seq_label": ["B-MLModel", "I-MLModel"], "seq_token": ["\u0120EL", "Mo"], "seq_scores": [0.9902745485305786, 0.9905872941017151], "text": " ELMo", "score": 0.9904309213161469, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 19778, "end": 19788, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120more", "\u0120data"], "seq_scores": [0.8170225024223328, 0.7943040728569031], "text": " more data", "score": 0.8056632876396179, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19849, "end": 19863, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120bas", "elines"], "seq_scores": [0.7975974082946777, 0.8789319396018982, 0.8039873838424683], "text": " the baselines", "score": 0.8268389105796814, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19910, "end": 19914, "seq_label": ["B-MLModelGeneric"], "seq_token": ["\u0120The"], "seq_scores": [0.595496654510498], "text": " The", "score": 0.595496654510498, "type": "ScholarlyEntity"}, {"label": "MLModel", "begin": 19982, "end": 19997, "seq_label": ["B-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel", "I-MLModel"], "seq_token": ["\u0120POL", "Y", "AI", "-", "ENC", "OD", "ER"], "seq_scores": [0.9907063841819763, 0.9938825368881226, 0.9948830604553223, 0.9949354529380798, 0.994629979133606, 0.9939045310020447, 0.9923227429389954], "text": " POLYAI-ENCODER", "score": 0.9936092410768781, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 19979, "end": 20003, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["The", "\u0120POL", "Y", "AI", "-", "ENC", "OD", "ER", "\u0120model"], "seq_scores": [0.8371084928512573, 0.8092294335365295, 0.8350533843040466, 0.7693739533424377, 0.6998573541641235, 0.6344732642173767, 0.5337763428688049, 0.5387166738510132, 0.6999160647392273], "text": "The POLYAI-ENCODER model", "score": 0.7063894404305352, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20036, "end": 20057, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120the", "\u0120baseline", "\u0120methods"], "seq_scores": [0.8690025806427002, 0.9749097228050232, 0.9527955055236816], "text": " the baseline methods", "score": 0.9322359363238016, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20102, "end": 20126, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120the", "\u0120entire", "\u0120training", "\u0120set"], "seq_scores": [0.985654890537262, 0.9924889802932739, 0.9960920214653015, 0.9964724779129028], "text": " the entire training set", "score": 0.9926770925521851, "type": "ScholarlyEntity"}, {"label": "Task", "begin": 20434, "end": 20476, "seq_label": ["B-Task", "I-Task", "I-Task", "I-Task", "I-Method"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120systems"], "seq_scores": [0.6030487418174744, 0.6194562911987305, 0.6638301610946655, 0.664465606212616, 0.4441085755825043], "text": " conversational response selection systems", "score": 0.5989818751811982, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20325, "end": 20365, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120a", "\u0120repository", "\u0120of", "\u0120convers", "ational", "\u0120datasets"], "seq_scores": [0.9880792498588562, 0.9901738166809082, 0.953739583492279, 0.889455258846283, 0.9983556866645813, 0.9989796280860901], "text": " a repository of conversational datasets", "score": 0.9697972039381663, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20376, "end": 20406, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120hundreds", "\u0120of", "\u0120millions", "\u0120examples"], "seq_scores": [0.9303945899009705, 0.932854175567627, 0.972246527671814, 0.8961424231529236], "text": " hundreds of millions examples", "score": 0.9329094290733337, "type": "ScholarlyEntity"}, {"label": "MLModelGeneric", "begin": 20434, "end": 20476, "seq_label": ["B-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric", "I-MLModelGeneric"], "seq_token": ["\u0120convers", "ational", "\u0120response", "\u0120selection", "\u0120systems"], "seq_scores": [0.6172165870666504, 0.6883268356323242, 0.7940340042114258, 0.8142067790031433, 0.7223235964775085], "text": " conversational response selection systems", "score": 0.7272215604782104, "type": "ScholarlyEntity"}, {"label": "DatasetGeneric", "begin": 20552, "end": 20566, "seq_label": ["B-DatasetGeneric", "I-DatasetGeneric"], "seq_token": ["\u0120more", "\u0120datasets"], "seq_scores": [0.9857813715934753, 0.9951554536819458], "text": " more datasets", "score": 0.9904684126377106, "type": "ScholarlyEntity"}]}, "filename": "00010_1904_06472.json", "id": "00010_1904_06472"}