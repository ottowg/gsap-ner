{"text": "MAAD: A Model and Dataset for \"Attended Awareness\" in Driving\n\nAbstract:\nWe propose a computational model to estimate a person's attended awareness of their environment. We define \"attended awareness\" to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of. Our model takes as input scene information in the form of a video and noisy gaze estimates, and outputs visual saliency, a refined gaze estimate and an estimate of the person's attended awareness. In order to test our model, we capture a new dataset with a high-precision gaze tracker including 24.5 hours of gaze sequences from 23 subjects attending to videos of driving scenes. The dataset also contains thirdparty annotations of the subjects' attended awareness based on observations of their scan path. Our results show that our model is able to reasonably estimate attended awareness in a controlled setting, and in the future could potentially be extended to real egocentric driving data to help enable more effective ahead-of-time warnings in safety systems and thereby augment driver performance. We also demonstrate our model's effectiveness on the tasks of saliency, gaze calibration and denoising, using both our dataset and an existing saliency dataset. We make our model and dataset available at https://github.com/ToyotaResearchInstitute/att-aware/.\n\nMain:\n\n\n\n1 Introduction\nWe define \"attended awareness\" to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of. While attended awareness is difficult to objectively measure, in certain situations such as driving, it is possible to infer, at least to some useful degree. Driving instructors routinely assess a driver's behavior based on their estimated attended awareness of a given driving scene, and provide real-time feedback to ensure safety. In the context of human-machine interaction, inferring human attended awareness in a given scenario is valuable for machines to facilitate seamless interaction and effective interventions. Human gaze information can be used by machines for this purpose and many models have been developed to relate scene understanding and overt attention as saliency estimates [9, 12, 17, 24, 42, 43] and objectness measures [1, 8]. However, further exploration of the link between visual attention and processes of scene understanding and decision making has been constrained by a limited ability to reason about such cognitive processes, which is difficult in general contexts [51, 56, 68].\nIn this work, we propose a model to estimate attended awareness as a spatial heatmap. The input to our model is a video of the scene under observation, and a noisy estimate of the person's gaze. Our model allows us to (i) compute visual saliency for the scene, (ii) leverage this to refine the noisy gaze estimate, and (iii) combine this information over time to infer the person's attended awareness. The construction of our model is driven by axiomatic considerations that define an image translation network from the scene image to the saliency and awareness heat maps. Within a larger human-machine interaction scenario, the model becomes useful as an inference engine for human awareness enabling seamless interaction (for e.g., a person operating a semi-autonomous car or a smart wheelchair). We adopt Figure 1 : Model for Attended Awareness in Driving (MAAD) overview. Our model takes as input a video of a scene, as seen by a person performing a task in the scene, along with noisily registered ego-centric gaze sequences from that person. The model estimates (i) a saliency heat map, (ii) a refined gaze estimate, and (iii) an estimate of the subject's attended awareness of the scene. We evaluate our model using a unique annotated dataset of third-person estimates of a driver's attended awareness. By explicitly estimating a person's attended awareness from noisy measurements of their gaze, MAAD can improve human-machine interactions. In the driving example, such interactions might include safety warnings in situations where attended awareness is deemed insufficient. a data-driven approach (as opposed to more limited-scope analytical models [31]) to allow for scalable and more comprehensive modeling of attended awareness that can potentially rely on supervision from multiple sources (such as resulting actions/self-reporting).\nContributions 1) We propose a learned model that affords estimation of attended awareness based on noisy gaze estimates and scene video over time. 2) We further demonstrate how the model affords saliency estimation as well as the refinement of a noisy gaze signal. 3) We present a new dataset that explores the gaze and perceived attended awareness of subjects as they observe a variety of driving and cognitive task conditions. While the dataset is captured via a proxy hazard awareness task rather than through real or simulated driving, it serves as a useful starting point to study visual saliency and awareness in driving scenes.\n\n2 Related Work\nOur work builds on prior work in visual saliency estimation, situational awareness, and driving-specific exploration. We briefly summarize related work from these areas.\nVisual saliency. Vision scientists have long sought to model and understand the mechanisms behind our allocation of attention in a visual scene [68]. Visual salience is a function of many factors, including the spatio-temporal nature of the stimulus itself as well as its relationship to neighboring stimuli and the nature of the visual system perceiving it [25]. The evolution of computational approaches to estimating visual image salience has mirrored that of other popular computer vision problems, with largely hand-engineered models-designed to mirror certain bottom-up and top-down attention processes in the human visual system (e.g.  [10, 25])-giving way to supervised, data-driven approaches (e.g.  [24, 26]). Unlike image saliency, video saliency models consider spatial as well as temporal information to detect objects of interest in a dynamic way.\nSpatio-temporal saliency estimation opens inquiry into how processes of visual attention, situational awareness and task-related decision making are connected. Previous attempts have been made to computationally model situational awareness (see e.g.  [3, 39, 64]). Our approach to modeling situational awareness is unique in that we try to explicitly estimate the parts of the visual scene to which a person has attended using a spatio-temporal model for gaze and scene understanding. The three stages of forming situational awareness consists of perception, comprehension and projection [21] and in our work we focused primarily on perception. We aim to model, from noisy observations of a person's scan path, the set of objects and scene structures which that person is likely to have attended to, and therefore might be better able to incorporate into their future decision-making. While we note that peripheral vision alone can achieve visual awareness in many settings [63], we focus on objects of fixation, since we are concerned primarily with estimating when drivers fail to notice potential driving hazards, which are known to strongly induce fixations [18].\nData-Driven Saliency Modeling and Datasets. Data-driven approaches to image and video saliency rely on state-ofthe-art deep learning architectures: CNNs [12, 24, 42], GANs [43] and LSTMs [2, 17, 60], and single image inputs through to multi-stream inputs incorporating video, along with optical flow, depth or semantic segmentation estimates [34, 41] and even additional modalities such as audio [58]. In our work, similar to [40], we adopt a 3D CNN-based approach, due to its simplicity and success on other video understanding tasks such as action recognition. During training, our model takes as input a video sequence with associated scan paths from a gaze tracker, along with optical flow estimates. The video sequence is processed using a 3D convolutional encoder/decoder, and information from gaze, and optic flow is transformed and injected during the decoding stage. The model outputs two heat maps: a gaze probability density heat map, p G , which provides a clean estimate of the noisy gaze input, and an attended awareness heat map, M A , which provides an estimate of the driver attended awareness, both at the final frame of the input video sequence. For further details, see Section 3.2.\nWhile the majority of saliency datasets explore image and video saliency under controlled viewing conditions (e.g.  [33, 38, 60]), in recent years, numerous ego-centric video gaze datasets have been developed in which subjects perform tasks as varied as cooking and meal preparation [35], playing video games [10] and driving [41], in parallel to developments in applications of saliency (see, e.g.  [53, 47, 13]. On the other hand in the driving domain, in-lab data collection procedures have been extensively adopted as they have the advantages of high accuracy, repeatability and the ability to focus on rare scenarios such as critical situations and accidents [66, 6, 4, 5], where understanding human perception can inform safety systems approaches.\nOur dataset uses an in-lab data collection paradigm, however it differs from prior work for several reasons. Firstly and most notably, we capture multiple subjects observing the same visual stimuli under different cognitive task modifiers. Our dataset therefore allows for reasoning about the effect of different cognitive task modifiers on the visual gaze patterns, given identical visual input. Secondly, we provide annotations for third party estimates of a subject's attended awareness, based on observations of their scan path. For this purpose, we devise a novel reproducible annotation scheme. Finally, as our dataset is gathered using a high precision gaze tracker with a chin rest, the precision of the scan paths is extremely high when compared to that of eye-glass gaze tracking ego-centric datasets such as [41].\nDriving specific applications. Driving is a predominantly visual task. The first studies into driver attention and eye scanning patterns date back over half a century [52]. Since then, certain driving behaviors have been well established and modelled, such as the \"perceptual narrowing\" effect in which drivers increasingly fixate on the road ahead as task demands increase (e.g. through greater speed, increased traffic or lack of familiarity with a route) [22], or the benefits of understanding driver attention when predicting a driver's future intent [20, 65]. However, to the best of our knowledge, no models exist with the purpose of quantitatively estimating a driver's spatial awareness of a scene. In recent years, works such as [41, 66] have used high-precision gaze trackers to create video gaze datasets both in the car and in the lab, allowing for data-driven approaches to modelling driver attention. While we make use of the road-facing data from [41] in our experiments, our model differs in one key respect. Rather than estimating visual saliency from video alone, we demonstrate how, given access to a noisy gaze estimate of a driver it is possible to simultaneously estimate scene saliency, a denoised gaze signal and an estimate of the driver's overall awareness of the scene.\n\n3 Method\nThe computational model we propose is guided by several assumptions related to human attended awareness and its relation to gaze patterns (Section 3.1). These assumptions are implemented through a mixture of explicit objectives and behavioral regularizations (Section 3.3).\n\n3.1 Assumptions and Supervisory Cues\nWe use several assumptions about gaze patterns and attended awareness to define the priors in training our model:\n\u2022 A1 Saliency: Gaze tends to focus on specific regions [9], both object and stuff [32].\n\u2022 A2 Attended awareness: People tend to become aware of the objects they look at [54, 14]. Their attention is, however, limited in its capacity.\n\u2022 A3 Awareness decay: Awareness of an object can decrease (due to forgetting), or increase (when looking at an object) at different rates [48].\n\u2022 A4 Regularity of gaze, awareness: Gaze and awareness maps should be regular and smooth unless otherwise warranted [11, 27].\n\u2022 A5 Awareness and motion: As an observer moves through a dynamic scene, their awareness moves along with objects and regions in the scene and exhibits temporal persistence.  [36, 7].\n\n3.2 Model\nWe define the Model for Attended Awareness in Driving (MAAD), shown in Figure 2, as a fully convolutional image encoding-decoding network with shortcut links such as U-Net [49]. Sensor images are encoded and decoded into a latent feature map, M (x, t), from which two convolutional modules emit the estimated gaze distribution p G (x, t), and the attended awareness image M A (x, t).\nThe gaze distribution is normalized as a probability density function (via a softmax operator). We note that p G (x, t) is a unified notation for gaze probability maps with and without a noisy gaze input from an external gaze tracker. In the absence of a noisy gaze input, p G (x, t) is a probabilistic form of saliency. In the rest of the paper we use p G (x, t) to denote both forms to simplify the notation. The individual modules in the decoder are fed additional information: the (noisy) driver gaze measurement over time in the form of Voronoi maps, and optical flow maps [41] encoded as two feature layers for horizontal and vertical motion.\n\n3.3 Loss Function Design\nAt training time, the gaze and attended awareness maps are used to compute several supervisory and regularization terms, whose design is guided by the assumptions in Section 3.1.\n\n3.3.1 Supervisory Terms\nGaze Prediction. We want p G to predict a subject's gaze as accurately as possible. This is encouraged via the primary data term:L G = \u2212 t x\u2208Xg(t) log p G (x, t),\nwhere X g (t) are the 2D ground truth gaze points at time t.\nPerceived Awareness. We include a supervisory source for attended awareness estimation. This term surrogates awareness estimation training by an attended awareness estimate. One approach to obtain an attended awareness estimate is to provide a person with the gaze estimate of the driver overlaid on the road scene and query how aware the driver is of specific locations in the image at particular points in time. This is further described in Section 4. The cost term reads:L ATT = (x,t)\u2208labeled (M A (x, t) \u2212 L A (x, t)) 2 ,\nwhere the summation is over all annotated samples in location x at time t, and L A denotes the annotated measure of awareness in the range [0, 1] as described in Section 4.2.\nAwareness of Attended Objects. Based on (A2), we add a term that encourages awareness to be high when a person is gazing at a scene location:L AA = t x\u2208Xg(t) (M A (x, t) \u2212 1) 2 .\n(3)\n\n3.3.2 Regularization Terms\nSpatial Smoothness. Based on (A4-5), we added regularity terms to both the awareness and gaze maps:L S,\u2022 = |\u2207\u03c6| 2 |\u2207I| 2 + dx,\nwhere \u03c6 is M A and p G for L S,A and L S,G respectively and the integral is computed over all pixels. This regularization is a variant of anisotropic diffusion [45, 44] with cross-diffusivity based on the scene image I.\nTemporal Smoothness. Based on (A3-5), we apply also temporal regularization for awareness:\nIn order to make the map temporally consistent with respect to the locations and not just object boundaries, we use a smoothness / decay term based on the image optical flow:L T = x,t f wOF (M A (x + v OF (x), t + 1), M A (x, t)),f wOF (a, b) = c 1 ((a \u2212 w OF b) + ) + c 2 ((a \u2212 w OF b) \u2212 ) 2 ,\nwhere v OF (x) is the optical flow computed on the input images and w OF < 1 is a weight factor that is close to 1. () + and () \u2212 denote positive and negative change respectively in awareness values when going from t to t + 1.\nParticularly, f wOF (a, b) is set to be an asymmetric loss function that penalizes awareness that increases instantaneously when attending to an object less compared to awareness that decreases rapidly via forgetting. This is accomplished by having the forgetting term, () \u2212 , to be quadratic.\nAwareness Decay. Based on (A3), we expect the level of awareness to decay over time, which we model via:L DEC = x,t (1 \u2212 DEC ) M A (x, t) \u2212M A (x, t + 1) 2 ,\nwhere DEC is a decay factor.\nAttention Capacity. Based on (A2), we expect that the cognitive resources available to the driver do not change over time. This assumption captures the fact that the overall awareness should be similar between frames on average.L CAP = t x M A (x, t) \u2212 x M A (x, t + 1) 2 . ()\nBlock-level consistency. We denote by M A (x, t; t 1 ) the awareness estimate at (x, t) that is emitted based on the training snippet started at t 1 , similarly for p G . We define a consistency term [50] between consecutive estimates via the loss:L CON,\u2022 = t1,t x \u03c6(x, t; t 1 ) \u2212 \u03c6(x, t; t 1 + 1) 2 , ()\nwhere \u03c6 is M A , p G for L CON,A , L CON,G respectively. L CON,\u2022 helps to minimize the difference between the predictions of multiple passes of samples at the same timestamps through the network. The overall training loss is a linear combination of the previously described loss terms. See supplementary material for additional details.\n\n3.4 Training Procedure\nWe trained our model using PyTorch 1.7 using NV100 GPUs. The training was carried out using video input subsequences of length 4 seconds, sampled at 5Hz with frames resized to 240\u00d7135. Our model was trained using Adam optimizer [30] with a learning rate of 5 \u00d7 10 \u22123 and a batch size of 4. The model weights were updated after the gradients were aggregated for a fixed number of steps in order to reduce the variance in the loss. The batch aggregation size was set at 8. The first two layers of the encoder (kept frozen during training) are taken from a pretrained Resnet18 implementation in Torchvision [37]. Later spatio-temporal encoding is done by three layers of separable 3D convolution modules denoted as S3D in Figure 2 [67]. To provide optical flow estimates, we used RAFT pre-trained on FlyingChairs and FlyingThings datasets [57].\nThe decoder consists of stacked decoder units each of which receives input from three sources a) side-channel information b) skip connections from the encoder layer and c) the output of the previous decoder unit, when available. Each decoder unit consists of two submodules: 1) The skip connections are first processed via an S3D module whose output is then concatenated (channel-wise) with the side-channel information and the output of the previous decoder unit. This concatenated input is processed by another S3D module followed by bilinear upsampling that brings the output to the proper resolution for the next decoder unit.\nThe decoder emits a latent map M which is subsequently processed by two separate convolutional models to emit a gaze probability map, p G and an awareness heatmap denoted as a M A . The softmax in the gaze convolutional module ensures that the gaze probability map is a valid probability distribution. More details regarding network architecture and training to be found in supplementary material.\n\n4 Dataset Description\nOur complete dataset comprises approximately 24.5 hours of gaze tracking data captured via multiple exposures from different subjects to 6.2 hours of road-facing video drawn from the DR(eye)VE dataset [41]. We concentrate our gaze capture on repeated exposures of downtown (as opposed to highway and rural) driving scenes (77%) and daylight scenes (90%), since these contain the most diverse visual scenarios. While the original DR(eye)VE dataset captured and registered gaze to the road-facing video using a driver head-mounted eye tracker and feature-based matching for homography estimation, accumulating significant measurement errors, we opted for an in-lab experiment. In-lab experiment offers several advantages such as higher accuracy and repeatability across subjects and cognitive task conditions. Furthermore, models trained on in-lab data has already been shown to be effective when tested on inthe-wild data [66]. We measure gaze to an extremely high precision (0.15 The subjects all carried US driving licenses and had at least two years of driving experience. Their primary task was to monitor the driving scene as a safety driver might monitor an autonomous vehicle. While not a perfect substitute for incar driving data collection, this primary task allowed for the capture of many of the characteristics of attentive driving behavior. In order to explore the effect of the cognitive task difference (vs. in-car data) on the gaze and awareness estimates, subjects viewed the video under different cognitive task modifiers, as detailed in Section 4.1 (data collected with non-null cognitive task modifiers comprise 30% of total captured gaze data). Around 45% of video stimuli were watched more than once, of which 11% (40 minutes) was observed by 16 or more subjects.\n\n4.1 Cognitive Task Modifiers\nAlthough our dataset is collected in an in-lab experiment, we are still interested in subjects' behavior under a different task (in-car driving, engaged as well as distracted). We therefore included in our experiments several secondary cognitive task modifiers to artificially alter participant behavior in a way which might mimic certain variations in the behavior of drivers in real conditions. We aimed at modifiers that affected visual search patterns, but did not explicitly instruct the participants toward specific search targets. These modifiers affect the visual patterns by either changing the goal of visual search or by changing the scene appearance. The cognitive task modifiers were as follows:\n1. Null condition: The subjects were given the task of supervising the driving of the car, looking for and flagging possible risky events or obstacles.\n2. Blurred condition: Same as 1, but stimulus videos were blurred with a Gaussian kernel corresponding to N deg of visual field, making scene understanding and therefore the supervisory task harder, and affecting the visual search pattern.\n3. Vertical-flip condition: Same as 1, but stimulus videos were flipped upside down, making scene understanding and therefore the supervisory task counter-intuitive, and affecting the visual search pattern.\n4. Road-only condition: Same as 1, but subjects were asked to only fixate on road structure and not on dynamic obstacles such as cars and people.\n\n5.\nReading-text condition: Same as 1, but stimulus videos were overlaid with snippets of text of approximately even length at random image locations for P seconds at an average interval of Q seconds. Subjects were asked to read each text snippet while supervising the driving. Here we overlay a sub-sampled one second gaze history per subject for a given frame in a video sequence. From observation of the scan paths of subjects, it is evident that some are aware and fixate strongly on the risk in the scene (e.g. crossing vehicles or pedestrians), while others fixate fleetingly or, due to the presence of cognitive task modifiers, are unaware of the risk.\n\n4.2 Annotations\nWe annotated 53,983 sequences of approximately 10 seconds sampled randomly from within the data for attended awareness. While inferring attended awareness is difficult and subjective, and probing the subject's working memory directly is impossible, we devised a reproducible annotation scheme to explore a third person's estimate of a person's attended awareness. Our annotation protocol leverages the fact that humans are able to develop a theory of mind of other peoples' mental states from cues inherent in eye gaze [19]. While the labels provided (as in any manually annotated dataset) are imperfect and have certain limitations, they are an important first step towards data-driven modeling of awareness. Annotators watched a video snippet where the subject's gaze was marked by two circles centered at the gaze point. One circle (green) size was set to the diameter of a person's central foveal vision area (2 degrees) at the viewing distance. Another circle (red) was set to a diameter four times the foveal vision circle. At the end of the video snippet, a random location was chosen and the annotators were asked whether they believe the subject has attended to that location on a scale between 1 and 5 (1-no, definitely not aware, 5-yes, definitely aware). Three different sampling types (object, edges, and non-objects) were used for sampling the final location. The annotations were linearly transformed to [0, 1] in L A and provided a supervisory signal that the network (awareness predictor) tried to match. Annotators were asked whether the cursor corresponded to a well-defined object, whether they would expect to be aware of the location if they were driving and how surprised they by the appearance/behavior of the highlighted region. The annotations had good coverage across awareness levels and contained sufficient number of examples of both highly aware as well as unaware examples. Figure 4 shows frames from an example annotation video. More annotation details and statistics in supp. material.\n\n5 Results\nWe now demonstrate the results from our model on several tasks of interest such as saliency, gaze refinement and attended awareness estimation. Our model, while applied here to a dataset which isn't strictly egocentric, could be straightforwardly extended to a true egocentric setting.\n\n5.1 Saliency\nIn order to confirm that our model is capable of estimating visual saliency, we trained it on the DR(eye)VE dataset images and splits [41]. As our approach assumes individual gaze samples per frame as the primary supervisory cue, we sampled gaze points from the fixation maps provided in the original dataset for every frame. We generated p G , and compared it against the ground truth gaze map. For the standard measures of KL, cross-correlation and information gain (see [41] for details), we obtained 1.734, 0.565, and \u22120.0002 respectively, comparing favorably to other algorithms tested on that dataset, such as [16, 61].\n\n5.2 Gaze refinement\nIn the following experiments, we show how our model is able to correct imperfect gaze estimates. In driving, driver gaze estimates can be obtained using camera-based driver monitoring systems (DMS). However, these estimates can be highly noisy due to the inherent process noise introduced during gaze tracking, or biased due to calibration errors in the DMS [28]. Our hypothesis is that knowledge of where the person might look in the scene can help the model refine noisy and biased (miscalibrated) coarse estimates. We describe two experiments that were conducted to address these typical modes of failure. For all experiments herein, we trained our model with a training dataset encompassing 8 urban day-time videos from our dataset with the highest subject and task coverage and adopted a fixed 80%/20% non-overlapping training/test split.\n\n5.2.1 Gaze Denoising\nFor this experiment the gaze input to the gaze transform module is corrupted by noise, mimicking imperfect knowledge of a person's gaze. We use our model to try to identify the correct gaze point taking scene saliency into consideration. We use a spatially-varying additive white Gaussian noise model, where the input to the network x noisy is computed according to [29] :x (i) noisy (t) =x (i) true (t) + \u03b7 (i) , \u03b7 (i) \u223c N (0, (\u03c3 (i) ) 2 ),\u03c3 (i) = max(\u03c3 (i) n , w * |x (i) \u2212 x (i) 0 |)\nwhere \u03b7 is the additive noise and the standard deviation \u03c3 (i) increases as we get further from the center of the image along each coordinate x (i) \u2208 {x (1) , x (2) }. Our network denoises the gaze input by leveraging scene saliency information as encoded in the network. Fusing the noisy gaze location allows us to surpass the capability of a pure-saliency based model. The latter merely finds saliency image peaks that are close to the noisy gaze location and would be the straightforward way of incorporating saliency information. We also compare to an approach that relates gaze to the nearest object. In all cases we use a meanshift approach [15] to find nearby objects or peaks, with a standard deviation given by \u03c3 n (H 2 + W 2 ), where H and W are the dimensions of the map. The results are summarized in Table 1 and demonstrate significant improvement with MAAD. Table 1 : Mean absolute error (in pixels) of noisy gaze recovery based on object attention: meanshift into nearby objects (OBJ), meanshift according to pure saliency map (SAL), and meanshift correction based on the MAAD gaze map, for different noise levels. Our approach improved upon other alternatives over a wide variety of input noise levels, far beyond the noise level at train time (\u03c3 n = 0.03).\n\n5.2.2 Gaze Recalibration\nWe model imperfect calibration as an affine transformation. For this experiment, the DMS gaze input, x noisy , to the gaze transform module is given by: where T correct , T corrupt are both 2D affine transforms. We model the correcting transform, T correct , as an MLP with one hidden layer. The corruption transform is created by adding element-wise zero-mean Gaussian noise with standard deviation \u03c3 2 n to the transformation matrix and vector of an identity transform. We show the reduction in average error after calibration in Table 2. By leveraging saliency information, we are able to naturally compensate for the calibration errors using the model.x noisy (t) = T correct (T corrupt (x))\n\n5.3 Driver Attended Awareness\nIn this experiment, we measure the model's ability to infer attended awareness. We do so by measuring the model's agreement with annotated third person estimate of attended awareness. We compare our approach to the following alternative of filtered gaze (FG) using a spatio-temporal Gaussian filter. We convolve each of the past gaze samples with a spatial Gaussian and utilize optic flow to temporally propagate the gaze information to the subsequent frames and aggregate them to form an awareness estimate. The optic flow mimics the subject's notion of object permanence under motion, and the spatial Gaussian account for subjective uncertainty accumulated over time as well as track limitations with optic flow. The results are given in Table 3.\n\n5.4 Ablation Experiments\nWe performed a series of leave-one out ablations to investigate the impact of the various cost terms on the attended awareness estimation task. Both L AT T and L AA are crucial for more accurate awareness estimation (Table 4).\n\nAblation\nAwareness\n\n6 Discussion and Conclusion\nWe have introduced a new model which uses imperfect gaze information and visual saliency to reason about perceived attended awareness within a single model. To train and validate our model, we generated a dataset which includes both high-accuracy gaze tracks as well as third person annotations for estimated attended awareness. MAAD can easily be extended to work with multi-image streams or scene graphs. Although our subjects viewed pre-collected video stimuli as opposed to being part of a true egocentric vision-action loop, one advantage is that we could acquire multiple observations of the same video, enabling the study of distributions rather than single scanpaths of attention for any given video. Our dataset can be compared in many ways to the related egocentric driving dataset from [41].\nExtending our work and model to study subjects who are in control of real vehicles is a topic for future work.  6 : Detailed structure of the S3D convolution modules used in the Decoder Units. The spatial Conv3d in the S3D modules uses kernel size of 1\u00d73\u00d73 and a stride length of 1. Similarly, the Conv3D responsible for temporal processing relies on a kernel of 3\u00d71\u00d71. A replication pad of size 1 is applied to the skip connection input to ensure that the output can be concatenated channel-wise to the other side-channel input and the previous decoder unit output.Encoder S3D ID Structure S3D 1 S3D(in=128, out=256) S3D 2 S3D(in=256, out=512) S3D 3 S3D(in=512, out=512)\n\n7.1.2 Gaze Transform Module\nThe gaze transform module consists of a single layer MLP whose output is encoded as a multi-channel Voronoi map which then is provided as a side channel input to the decoder units. The number of gaze points used per frame (for supervision as well as the side channel information) is fixed to be 3. The side-channel gaze input was corrupted by a spatially varying zero mean Gaussian white noise with \u03c3 = 0.0347, to account for the uncertainty due to both the foveal center location and eye tracker error; both treated as two Gaussian independent sources. Each Voronoi channel encodes a particular distance related feature, such as dx, dy, dx 2 , dy 2 , dxdy, dx 2 + dy 2 . Additionally, we also provide a bit to encode whether a particular instance of the gaze input is dropped out (as a result of the dropout applied during training) and also whether the gaze value is a valid input or not (to indicate NaNs that occur in the gaze data primarily due to eye blinks and tracker error). The total number of channels for the gaze side information is 8.\n\n7.1.3 Optic Flow Module\nThe optic flow is provided as a 2-channel input, where the channels encode the flow in the horizontal and vertical direction respectively. We apply an adaptive average pool operator on the optic flow input to match the resolution of the decoder unit.\n\n7.1.4 Decoder Unit\nEach Decoder Unit (DU) can receive up to three sources of input, 1) the skip connections from the encoder, 2) the side channel information (gaze information, and optic flow) and 3) the output of the previous decoder unit, when available.\nAll S3D modules (for skip modules as well as side-channel+previous output modules) in each of the Decoder Unit uses a S3D unit with a kernel size of 1\u00d73\u00d73 for spatial and 3\u00d71\u00d71 for temporal processing. The input to each of the spatial and the temporal modules in the S3D uses a replication pad of size 1. An InstanceNorm3D and a ReLU nonlinearity is applied after the spatial and temporal processing.\nThe output of the skip connection module is concatenated channel-wise to the side-channel information and the output of the previous decoder unit. The concatenated input is processed by another S3D module finally undergoes a bilinear upsampling to match the resolution size of the next decoder unit. The output of last decoder unit (DU1) undergoes a final bilinear upsampling stage to match the resolution of the size of model input (240\u00d7135). The detailed structure of all the decoder units in the decoder is presented in Table 8.Encoder S3D ID Structure S3D 1 S3D(in=128, out=256) S3D 2 S3D(in=256, out=512) S3D 3 S3D(in=512, out=512)\nTable 7 : Detailed structure of the 3D convolution modules used in the Decoder Units. The spatial Conv3d in the encoder S3D modules uses kernel size of 1\u00d73\u00d73 and a stride length of 1. Similarly, the Conv3D responsible for temporal processing relies on a kernel of 3\u00d71\u00d71. A replication pad of size 1 is applied to the input before being processed by each of the Conv3D modules.Decoder Unit Id Skip Module Concatenated Module DU5 NA S3D(in=266, out=128) DU4 NA S3D(in=138, out=64) DU3 NA S3D(in=74, out=32) DU2 S3D(in=128, out=128) S3D(in=170, out=16) DU1 S3D(in=64, out=64) S3D(in=90, out=16)\nTable 8 : Detailed structure of the S3D convolution modules used in the Decoder Units. The spatial Conv3d in the S3D modules uses kernel size of 1\u00d73\u00d73 and a stride length of 1. Similarly, the Conv3D responsible for temporal processing relies on a kernel of 3\u00d71\u00d71. A replication pad of size 1 is applied to the skip connection input to ensure that the output can be concatenated channel-wise to the other side-channel input and the previous decoder unit output.\nThe total number of channels from the side information is 10 (Voronoi gaze maps=8, and optic flow=2). In general, the following relationship holds for the feature sizes:n concat,DU (l) in = n skip,DU (l) out + n concat,DU\nwhere n in and n out are the number of input and output features respectively and l \u2208 [1, 2] denotes the decoder unit id. For DU5, n in = n encoder,postproc out + 10.\n\n7.1.5 Gaze and Awareness Convolutional Modules\nThe output of the decoder is processed using a Conv2D with kernel operator of size 5\u00d75 and 6 output features to generate a feature map M . The 1D gaze heatmap (p G ) is produced from M by a Conv1D operator with a kernel size of 1 followed by softmax operator to ensure that the heatmap is a valid probability distribution. Likewise, the awareness heatmap (M A ) is generated from M by another Conv1D operator with a kernel size of 1 followed by a sigmoid operator to ensure that each pixel value remains between 0 and 1. Note that, the awareness map is NOT a probability distribution.\n\n7.2 Cost Weights and Parameters\nTable 9 contains all the parameters and coefficients used for model training. These coefficients were chosen so that the relative magnitudes of the different supervisory terms were comparable. The regularization terms are roughly an order of magnitude lower than the supervisory cues. The gaze and awareness supervision costs are computed only on valid gaze points (gaze points that are not NaNs).\n\n8 Annotation Dataset Details\nTable 10 shows the breakdown of the labelled set. Table 11 contains information regarding the time of the day and the weather condition for all the 8 video sequences (from the Dr(Eye)ve dataset) we used for MAAD model training.\nWe randomly sampled approximately 10s clips from these 8 videos from within the data we collected for third-party attended awareness annotation. The gaze data was overlaid on the video clip and in the last frame of the clip a random location was chosen and marked with a red cross. This random location was chosen equi-probably from objects, edges or anywhere in the image. After the annotators watched the video, they were asked whether they believed the subject had attended to the location marked with the red cross. More specifically, the annotators answered the following questions: Annotations reflect the variability in awareness of locations under certain cognitive modifiers, including conditions where we expect reduced awareness of annotated locations (e.g. reading text and road-only conditions).\u03b1 G 1.\n\u2022 Do you think the driver is aware of the object/area? (red cross; must be near the green circle at some point in the video, not being near at the end of the video is fine, if it is close and moving along with the object, we want a human judgment of someone who has the extra knowledge and is focusing on this) a) Yes, definitely aware b) Yes, probably aware c) Very unsure d) No, probably not aware e) No, definitely not aware\n\u2022 Is the red cursor on a well-defined object such as a car or person? (not well defined: exit, piece of road, something you cannot put a boundary around. If it is part of an object, then it is still well defined. For example, building is not well defined because it's a large area and cannot be separated from the ground) a) Yes b) No c) Unsure.\n\n9 Examples of Calibration Optimization\nFigure 9 shows more examples of how the network successfully corrects a miscalibrated side-channel gaze input. In each of the examples in the figure, before correction the miscalibrated gaze distorts the heatmap and pulls it away from the ground truth gaze. As the networks learns the correction transform (for this experiment, the correction transform was learned by training the network on the test split that was used during the original model training phase), it corrects for the miscalibration and the heatmap begins to align closely with the ground truth gaze. Note that, during the optimization procedure for learning the correction the weights of the entire network except that of T correct are kept frozen.\n\n10 Visualization of Denoising Mean Shift Traces\nThe meanshift algorithm is a procedure for locating the local maxima-the modes-of a density function. For the gaze denoising experiment, we perform the exact same meanshift procedure on three different density maps a) the gaze-conditioned saliency map, b) pure saliency map and c) the mask image that encodes the objects in the scene. Figure 10 shows different examples of the traces of the mean shift procedure on the mask image, gaze map without and with side channel gaze. In general, we see that when mean shift is performed on the gaze-conditioned saliency maps the resulting mode is closer to the ground truth (right-most column in Figure 10).\n\n11 Ablation Experiments\nWe performed a set of leave-N -out ablations to investigate the impact of different regularization terms on the network's ability to estimate attended awareness. Table 12 shows the mean squared error in the awareness estimation for different ablations that we tested.\nRegularization for stability: One of the key functions of the regularization terms is to provide stability during training. In our ablation experiments we found that ablating the attention capacity regularization term (L ACAP ) in general, resulted in training instability and in truncated training runs despite seemingly comparable (and at times better) awareness estimation scores to the full model.\nWe also experimented with a different network architecture in which the S3D modules in the decoder units were replaced with standard Conv3D modules. Due to the larger number of parameters for Conv3D modules, the number of layers in the encoder and decoder were reduced to 4. For these architectures, we found that including the spatial regularization for the gaze map (L S\u2212G ) was critical for stability during training. In general, from our ablation experiments we recommend that for both the S3D and non-S3D versions of the model, the spatial regularization (L S\u2212G ) and the attention capacity (L ACAP ) cost terms should be added to improve training stability.\nL DEC ablation: Although removing the decay term, (L DEC ), resulted in better awareness estimation scores (row 2, Table 12, this was due to the fact that without L DEC the awareness heatmap was no longer spatially localized as shown in Figure 8 essentially resulting in over-estimation of attended awareness. Over-estimation of driver awareness (model falsely predicting that the driver is aware of something when they are not) can lead to undesirable consequences when used in safety warning systems in autonomous vehicles. Additionally, utilizing L DEC also accelerated the convergence of the model during training.\n12 Influence of Cognitive Task Modifiers\nDuring the dataset collection procedure we opted for a high-accuracy gaze tracker. However, this raises a question about the effect of the cognitive task modifier in a passive observation experiment. In order to investigate the impact of cognitive task modifiers as a latent factor that could influence awareness estimation accuracy, we trained MAAD exclusively on training data collected under the 'null condition'. This model was then evaluated on the data collected under the remaining cognitive task modifier conditions.\nFrom Table 13 we can see that a model that was trained exclusively on null condition data performed worse on the other task modifiers compared to the full model. However, as shown in Table 14, the null condition model still did considerably better than the spatio-temporal Gaussian baseline (FG) with optic flow. These results indicate that the model is sensitive to the cognitive task that the subject is executing. Future work will explore ways to disentangle this latent factor within the network capabilities.\n13 Examples of Gaze and Awareness Maps   Right: Gaze map with side channel noisy gaze (our approach). The meanshift sequences is shown as green polylines.\nThe starting point (the noisy gaze) of the sequence is indicated using a green crosshair. The ground truth gaze is denoted as white cross on the images. Our approach with noisy side channel gaze outperforms the object-based and pure saliency-based approaches.\n\nFootnotes:\n\nReferences:\n\n- Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. Measuring the objectness of image windows. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):2189-2202, 2012.- Loris Bazzani, Hugo Larochelle, and Lorenzo Torresani. Recurrent mixture density network for spatiotemporal visual attention. In International Conference on Learning Representations, 2017.\n\n- Thierry Bellet, Pierre Mayenobe, Jean-Charles Bornard, Dominique Gruyer, and Bernard Claverie. A compu- tational model of the car driver interfaced with a simulation platform for future virtual human centred design applications: Cosmo-sivic. Engineering Applications of Artificial Intelligence, 25(7):1488-1504, 2012.\n\n- Tao Deng, Hongmei Yan, Long Qin, Thuyen Ngo, and BS Manjunath. How do drivers allocate their poten- tial attention? driving fixation prediction via convolutional neural networks. IEEE Transactions on Intelligent Transportation Systems, 21(5):2146-2154, 2019.\n\n- Sonia Baee, Erfan Pakdamanian, Inki Kim, Lu Feng, Vicente Ordonez, and Laura Barnes. Medirl: Predict- ing the visual attention of drivers via maximum entropy deep inverse reinforcement learning. arXiv preprint arXiv:1912.07773, 2019.\n\n- Jianwu Fang, Dingxin Yan, Jiahuan Qiao, and Jianru Xue. Dada: A large-scale benchmark and model for driver attention prediction in accidental scenarios. arXiv preprint arXiv:1912.12148, 2019.\n\n- Paul J Boon, Jan Theeuwes, and Artem V Belopolsky. Updating visual-spatial working memory during object movement. Vision Research, 94:51-57, 2014.\n\n- Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark. IEEE Transac- tions on Image Processing, 24(12):5706-5722, 2015.\n\n- Ali Borji and Laurent Itti. State-of-the-art in visual attention modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):185-207, 2012.\n\n- Ali Borji, Dicky N Sihite, and Laurent Itti. Probabilistic learning of task-specific visual attention. In IEEE Conference on Computer Vision and Pattern Recognition, pages 470-477. IEEE, 2012.\n\n- Timothy F Brady and Joshua B Tenenbaum. A probabilistic model of visual working memory: Incorporating higher order regularities into working memory capacity estimates. Psychological Review, 120(1):85, 2013.\n\n- Neil DB Bruce, Christopher Catton, and Sasa Janjic. A deeper look at saliency: Feature contrast, semantics, and beyond. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 516-524, 2016.\n\n- Dario Cazzato, Marco Leo, Cosimo Distante, and Holger Voos. When i look into your eyes: A survey on computer vision contributions for human gaze estimation and tracking. Sensors, 20(13):3739, 2020.\n\n- Marvin M Chun. Visual working memory as visual attention sustained internally over time. Neuropsychologia, 49(6):1407-1409, 2011.\n\n- Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 24(5):603-619, 2002.\n\n- Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. A deep multi-level network for saliency prediction. In International Conference on Pattern Recognition, pages 3488-3493. IEEE, 2016.\n\n- Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. Predicting human eye fixations via an lstm-based saliency attentive model. IEEE Transactions on Image Processing, 27(10):5142-5154, 2018.\n\n- David Crundall, Peter Chapman, Steven Trawley, Lyn Collins, Editha Van Loon, Ben Andrews, and Geoffrey Underwood. Some hazards are more attractive than others: Drivers of varying experience respond differently to different types of hazard. Accident Analysis & Prevention, 45:600-609, 2012.\n\n- Saara Khalid, Jason C Deska, and Kurt Hugenberg. The eyes are the windows to the mind: Direct eye gaze triggers the ascription of others' minds. Personality and Social Psychology Bulletin, 42(12):1666-1677, 2016.\n\n- A. Doshi and M. M. Trivedi. On the roles of eye gaze and head dynamics in predicting driver's intent to change lanes. IEEE Transactions on Intelligent Transportation Systems, 10(3):453-462, 2009.\n\n- Mica R Endsley. Toward a theory of situation awareness in dynamic systems. Human Factors, 37(1):32-64, 1995.\n\n- Johan Engstr\u00f6m, Emma Johansson, and Joakim \u00d6stlund. Effects of visual and cognitive load in real and simulated motorway driving. Transportation Research Part F: Traffic Psychology and Behaviour, 8(2):97-120, 2005.\n\n- John M Findlay, John M Findlay, Iain D Gilchrist, et al. Active vision: The psychology of looking and seeing. Number 37 in Oxford Psychology. Oxford University Press, 2003.\n\n- Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. Salicon: Reducing the semantic gap in saliency pre- diction by adapting deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 262-270, 2015.\n\n- Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254-1259, 1998.\n\n- Tilke Judd, Krista Ehinger, Fr\u00e9do Durand, and Antonio Torralba. Learning to predict where humans look. In 2009 IEEE 12th International Conference on Computer Vision,, pages 2106-2113. IEEE, 2009.\n\n- Daniel Kaiser, Timo Stein, and Marius V Peelen. Real-world spatial regularities affect visual working memory for objects. Psychonomic Bulletin & Review, 22(6):1784-1790, 2015.\n\n- Anuradha Kar and Peter Corcoran. Performance evaluation strategies for eye gaze estimation systems with quantitative metrics and visualizations. Sensors, 18(9):3151, 2018.\n\n- Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Matusik, and Antonio Torralba. Gaze360: Physically unconstrained gaze estimation in the wild. In Proceedings of the IEEE/CVF International Conference on Com- puter Vision, pages 6912-6921, 2019.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\n\n- Katja Kircher and Christer Ahlstr\u00f6m. The driver distraction detection algorithm AttenD. In Trent W. Victor Michael A. Regan, John D. Lee, editor, Driver Distraction and Inattention, pages 327-348. Ashgate, 2013.\n\n- Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9404-9413, 2019.\n\n- Matthias K\u00fcmmerer, Zoya Bylinskii, Tilke Judd, Ali Borji, Laurent Itti, Fr\u00e9do Durand, Aude Oliva, and Antonio Torralba. MIT/T\u00fcbingen Saliency Benchmark. https://saliency.tuebingen.ai/.\n\n- Congyan Lang, Tam V Nguyen, Harish Katti, Karthik Yadati, Mohan Kankanhalli, and Shuicheng Yan. Depth matters: Influence of depth cues on visual saliency. In European Conference on Computer Vision, pages 101-115. Springer, 2012.\n\n- Yin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 287-295, 2015.\n\n- Tal Makovski and Yuhong V Jiang. The role of visual working memory in attentive tracking of unique objects. Journal of Experimental Psychology: Human Perception and Performance, 35(6):1687, 2009.\n\n- S\u00e9bastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM International Conference on Multimedia, MM '10, page 1485-1488, New York, NY, USA, 2010. Association for Computing Machinery.\n\n- Stefan Mathe and Cristian Sminchisescu. Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(7):1408-1424, 2014.\n\n- Jason McCarley, Christopher Wickens, Juliana Goh, and William Horrey. A computational model of attention/si- tuation awareness. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 46, 09 2002.\n\n- Kyle Min and Jason J Corso. Tased-net: Temporally-aggregating spatial encoder-decoder network for video saliency detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 2394-2403, 2019.\n\n- Andrea Palazzi, Davide Abati, Francesco Solera, Rita Cucchiara, et al. Predicting the driver's focus of attention: the dr (eye) ve project. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(7):1720-1733, 2018.\n\n- Junting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Kevin McGuinness, and Noel E O'Connor. Shallow and deep con- volutional networks for saliency prediction. In IEEE Conference on Computer Vision and Pattern Recognition, pages 598-606, 2016.\n\n- Junting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Cristian Canton Ferrer, Jordi Torres, Kevin McGuinness, and Noel E OConnor. Salgan: Visual saliency prediction with adversarial networks. In CVPR Scene Understanding Workshop (SUNw), 2017.\n\n- Sylvain Paris and Fr\u00e9do Durand. A fast approximation of the bilateral filter using a signal processing approach. International Journal of Computer Vision, 81(1):24-52, 2009.\n\n- Pietro Perona and Jitendra Malik. Scale-space and edge detection using anisotropic diffusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(7):629-639, 1990.\n\n- M Press. Situation awareness: Let's get serious about the clue-bird. Unpublished manuscript, 1986.\n\n- Shafin Rahman, Sejuti Rahman, Omar Shahid, Md Tahmeed Abdullah, and Jubair Ahmed Sourov. Classifying eye-tracking data using saliency maps. arXiv, 2020.\n\n- Timothy J Ricker and Nelson Cowan. Loss of visual working memory within seconds: The combined use of re- freshable and non-refreshable features. Journal of Experimental Psychology: Learning, Memory, and Cognition, 36(6):1355, 2010.\n\n- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234-241. Springer, 2015.\n\n- Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In NeurIPS, 2016.\n\n- Dario D Salvucci. An integrated model of eye movements and visual encoding. Cognitive Systems Research, 1(4):201-220, 2001.\n\n- JW Senders, AB Kristofferson, WH Levison, CW Dietrich, and JL Ward. The attentional demand of automobile driving. Highway Research Record, 1526(195), 1967.\n\n- Yusuke Sugano, Yasuyuki Matsushita, and Yoichi Sato. Calibration-free gaze sensing using saliency maps. In Proceedings of the Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, pages 2667- 2674, 2010.\n\n- Benjamin W Tatler. Characterising the visual buffer: Real-world evidence for overwriting early in each fixation. Perception, 30(8):993-1006, 2001.\n\n- Benjamin W. Tatler, Mary M. Hayhoe, Michael F. Land, and Dana H. Ballard. Eye guidance in natural vision: Reinterpreting salience. Journal of Vision, 11(5):5-5, 2011.\n\n- Benjamin W Tatler, Nicholas J Wade, Hoi Kwan, John M Findlay, and Boris M Velichkovsky. Yarbus, eye movements, and vision. i-Perception, 1(1):7-27, 2010.\n\n- Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. arXiv preprint arXiv:2003.12039, 2020.\n\n- Antigoni Tsiami, Petros Koutras, and Petros Maragos. Stavis: Spatio-temporal audiovisual saliency network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\n- Edward K Vogel, Geoffrey F Woodman, and Steven J Luck. Storage of features, conjunctions, and objects in visual working memory. Journal of Experimental Psychology: Human Perception and Performance, 27(1):92, 2001.\n\n- Wenguan Wang, Jianbing Shen, and Haibin Ling. A deep network solution for attention and aesthetics aware photo cropping. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(7):1531-1544, 2018.\n\n- Wenguan Wang, Jianbing Shen, and Ling Shao. Consistent video saliency using local gradient flow optimization and global refinement. IEEE Transactions on Image Processing, 24(11):4185-4196, 2015.\n\n- Mary E Wheeler and Anne M Treisman. Binding in short-term visual memory. Journal of Experimental Psy- chology: General, 131(1):48, 2002.\n\n- Benjamin Wolfe, Jonathan Dobres, Ruth Rosenholtz, and Bryan Reimer. More than the useful field: Considering peripheral vision in driving. Applied Ergonomics, 65:316-325, 2017.\n\n- Bertram Wortelen, Martin Baumann, and Andreas L\u00fcdtke. Dynamic simulation and prediction of drivers' atten- tion distribution. Transportation Research part F: Traffic Psychology and Behaviour, 21:278-294, 2013.\n\n- Min Wu, Tyron Louw, Morteza Lahijanian, Wenjie Ruan, Xiaowei Huang, Natasha Merat, and Marta Kwiatkowska. Gaze-based intention anticipation over driving manoeuvres in semi-autonomous vehicles. In International Conference on Intelligent Robots and Systems, 2019.\n\n- Ye Xia, Danqing Zhang, Jinkyu Kim, Ken Nakayama, Karl Zipser, and David Whitney. Predicting driver attention in critical situations. pages 658-674. Springer, 2018.\n\n- Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 305-321, 2018.\n\n- Alfred L Yarbus. Eye Movements and Vision. Plenum Press, 1967.\n\n", "annotations": {"ReferenceToTable": [{"begin": 28331, "end": 28332, "idx": 0}, {"begin": 28390, "end": 28391, "idx": 1}, {"begin": 29350, "end": 29351, "target": "#tab_1", "idx": 2}, {"begin": 30285, "end": 30286, "target": "#tab_2", "idx": 3}, {"begin": 30537, "end": 30538, "target": "#tab_3", "idx": 4}, {"begin": 31505, "end": 31506, "idx": 5}, {"begin": 34607, "end": 34608, "idx": 6}, {"begin": 34721, "end": 34722, "idx": 7}, {"begin": 35313, "end": 35314, "idx": 8}, {"begin": 36829, "end": 36830, "target": "#tab_5", "idx": 9}, {"begin": 37257, "end": 37259, "target": "#tab_6", "idx": 10}, {"begin": 37307, "end": 37309, "target": "#tab_8", "idx": 11}, {"begin": 40716, "end": 40718, "target": "#tab_9", "idx": 12}, {"begin": 42003, "end": 42005, "target": "#tab_9", "idx": 13}, {"begin": 43078, "end": 43080, "target": "#tab_10", "idx": 14}, {"begin": 43256, "end": 43258, "target": "#tab_3", "idx": 15}], "SectionMain": [{"begin": 1429, "end": 43995, "idx": 0}], "SectionReference": [{"begin": 44009, "end": 57405, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1429, "idx": 0}], "Div": [{"begin": 73, "end": 1421, "idx": 0}, {"begin": 1432, "end": 5128, "idx": 1}, {"begin": 5130, "end": 11423, "idx": 2}, {"begin": 11425, "end": 11707, "idx": 3}, {"begin": 11709, "end": 12546, "idx": 4}, {"begin": 12548, "end": 13590, "idx": 5}, {"begin": 13592, "end": 13795, "idx": 6}, {"begin": 13797, "end": 14928, "idx": 7}, {"begin": 14930, "end": 17316, "idx": 8}, {"begin": 17318, "end": 19211, "idx": 9}, {"begin": 19213, "end": 21019, "idx": 10}, {"begin": 21021, "end": 22503, "idx": 11}, {"begin": 22505, "end": 23163, "idx": 12}, {"begin": 23165, "end": 25200, "idx": 13}, {"begin": 25202, "end": 25497, "idx": 14}, {"begin": 25499, "end": 26137, "idx": 15}, {"begin": 26139, "end": 27002, "idx": 16}, {"begin": 27004, "end": 28785, "idx": 17}, {"begin": 28787, "end": 29507, "idx": 18}, {"begin": 29509, "end": 30287, "idx": 19}, {"begin": 30289, "end": 30540, "idx": 20}, {"begin": 30542, "end": 30560, "idx": 21}, {"begin": 30562, "end": 32064, "idx": 22}, {"begin": 32066, "end": 33142, "idx": 23}, {"begin": 33144, "end": 33418, "idx": 24}, {"begin": 33420, "end": 36156, "idx": 25}, {"begin": 36158, "end": 36789, "idx": 26}, {"begin": 36791, "end": 37220, "idx": 27}, {"begin": 37222, "end": 39067, "idx": 28}, {"begin": 39069, "end": 39823, "idx": 29}, {"begin": 39825, "end": 40522, "idx": 30}, {"begin": 40524, "end": 43995, "idx": 31}], "Head": [{"begin": 1432, "end": 1446, "n": "1", "idx": 0}, {"begin": 5130, "end": 5144, "n": "2", "idx": 1}, {"begin": 11425, "end": 11433, "n": "3", "idx": 2}, {"begin": 11709, "end": 11745, "n": "3.1", "idx": 3}, {"begin": 12548, "end": 12557, "n": "3.2", "idx": 4}, {"begin": 13592, "end": 13616, "n": "3.3", "idx": 5}, {"begin": 13797, "end": 13820, "n": "3.3.1", "idx": 6}, {"begin": 14930, "end": 14956, "n": "3.3.2", "idx": 7}, {"begin": 17318, "end": 17340, "n": "3.4", "idx": 8}, {"begin": 19213, "end": 19234, "n": "4", "idx": 9}, {"begin": 21021, "end": 21049, "n": "4.1", "idx": 10}, {"begin": 22505, "end": 22507, "idx": 11}, {"begin": 23165, "end": 23180, "n": "4.2", "idx": 12}, {"begin": 25202, "end": 25211, "n": "5", "idx": 13}, {"begin": 25499, "end": 25511, "n": "5.1", "idx": 14}, {"begin": 26139, "end": 26158, "n": "5.2", "idx": 15}, {"begin": 27004, "end": 27024, "n": "5.2.1", "idx": 16}, {"begin": 28787, "end": 28811, "n": "5.2.2", "idx": 17}, {"begin": 29509, "end": 29538, "n": "5.3", "idx": 18}, {"begin": 30289, "end": 30313, "n": "5.4", "idx": 19}, {"begin": 30542, "end": 30550, "idx": 20}, {"begin": 30562, "end": 30589, "n": "6", "idx": 21}, {"begin": 32066, "end": 32093, "n": "7.1.2", "idx": 22}, {"begin": 33144, "end": 33167, "n": "7.1.3", "idx": 23}, {"begin": 33420, "end": 33438, "n": "7.1.4", "idx": 24}, {"begin": 36158, "end": 36204, "n": "7.1.5", "idx": 25}, {"begin": 36791, "end": 36822, "n": "7.2", "idx": 26}, {"begin": 37222, "end": 37250, "n": "8", "idx": 27}, {"begin": 39069, "end": 39107, "n": "9", "idx": 28}, {"begin": 39825, "end": 39872, "n": "10", "idx": 29}, {"begin": 40524, "end": 40547, "n": "11", "idx": 30}], "Paragraph": [{"begin": 73, "end": 1421, "idx": 0}, {"begin": 1447, "end": 2645, "idx": 1}, {"begin": 2646, "end": 4493, "idx": 2}, {"begin": 4494, "end": 5128, "idx": 3}, {"begin": 5145, "end": 5314, "idx": 4}, {"begin": 5315, "end": 6176, "idx": 5}, {"begin": 6177, "end": 7344, "idx": 6}, {"begin": 7345, "end": 8547, "idx": 7}, {"begin": 8548, "end": 9301, "idx": 8}, {"begin": 9302, "end": 10126, "idx": 9}, {"begin": 10127, "end": 11423, "idx": 10}, {"begin": 11434, "end": 11707, "idx": 11}, {"begin": 11746, "end": 11859, "idx": 12}, {"begin": 11860, "end": 11947, "idx": 13}, {"begin": 11948, "end": 12092, "idx": 14}, {"begin": 12093, "end": 12236, "idx": 15}, {"begin": 12237, "end": 12362, "idx": 16}, {"begin": 12363, "end": 12546, "idx": 17}, {"begin": 12558, "end": 12941, "idx": 18}, {"begin": 12942, "end": 13590, "idx": 19}, {"begin": 13617, "end": 13795, "idx": 20}, {"begin": 13821, "end": 13950, "idx": 21}, {"begin": 13984, "end": 14044, "idx": 22}, {"begin": 14045, "end": 14519, "idx": 23}, {"begin": 14571, "end": 14745, "idx": 24}, {"begin": 14746, "end": 14887, "idx": 25}, {"begin": 14925, "end": 14928, "idx": 26}, {"begin": 14957, "end": 15056, "idx": 27}, {"begin": 15084, "end": 15303, "idx": 28}, {"begin": 15304, "end": 15394, "idx": 29}, {"begin": 15395, "end": 15569, "idx": 30}, {"begin": 15690, "end": 15916, "idx": 31}, {"begin": 15917, "end": 16210, "idx": 32}, {"begin": 16211, "end": 16315, "idx": 33}, {"begin": 16369, "end": 16397, "idx": 34}, {"begin": 16398, "end": 16626, "idx": 35}, {"begin": 16675, "end": 16923, "idx": 36}, {"begin": 16980, "end": 17316, "idx": 37}, {"begin": 17341, "end": 18182, "idx": 38}, {"begin": 18183, "end": 18813, "idx": 39}, {"begin": 18814, "end": 19211, "idx": 40}, {"begin": 19235, "end": 21019, "idx": 41}, {"begin": 21050, "end": 21758, "idx": 42}, {"begin": 21759, "end": 21910, "idx": 43}, {"begin": 21911, "end": 22150, "idx": 44}, {"begin": 22151, "end": 22357, "idx": 45}, {"begin": 22358, "end": 22503, "idx": 46}, {"begin": 22508, "end": 23163, "idx": 47}, {"begin": 23181, "end": 25200, "idx": 48}, {"begin": 25212, "end": 25497, "idx": 49}, {"begin": 25512, "end": 26137, "idx": 50}, {"begin": 26159, "end": 27002, "idx": 51}, {"begin": 27025, "end": 27397, "idx": 52}, {"begin": 27512, "end": 28785, "idx": 53}, {"begin": 28812, "end": 29468, "idx": 54}, {"begin": 29539, "end": 30287, "idx": 55}, {"begin": 30314, "end": 30540, "idx": 56}, {"begin": 30551, "end": 30560, "idx": 57}, {"begin": 30590, "end": 31392, "idx": 58}, {"begin": 31393, "end": 31959, "idx": 59}, {"begin": 32094, "end": 33142, "idx": 60}, {"begin": 33168, "end": 33418, "idx": 61}, {"begin": 33439, "end": 33676, "idx": 62}, {"begin": 33677, "end": 34077, "idx": 63}, {"begin": 34078, "end": 34609, "idx": 64}, {"begin": 34715, "end": 35091, "idx": 65}, {"begin": 35307, "end": 35767, "idx": 66}, {"begin": 35768, "end": 35937, "idx": 67}, {"begin": 35990, "end": 36156, "idx": 68}, {"begin": 36205, "end": 36789, "idx": 69}, {"begin": 36823, "end": 37220, "idx": 70}, {"begin": 37251, "end": 37478, "idx": 71}, {"begin": 37479, "end": 38287, "idx": 72}, {"begin": 38294, "end": 38721, "idx": 73}, {"begin": 38722, "end": 39067, "idx": 74}, {"begin": 39108, "end": 39823, "idx": 75}, {"begin": 39873, "end": 40522, "idx": 76}, {"begin": 40548, "end": 40815, "idx": 77}, {"begin": 40816, "end": 41217, "idx": 78}, {"begin": 41218, "end": 41881, "idx": 79}, {"begin": 41882, "end": 42500, "idx": 80}, {"begin": 42501, "end": 42541, "idx": 81}, {"begin": 42542, "end": 43066, "idx": 82}, {"begin": 43067, "end": 43580, "idx": 83}, {"begin": 43581, "end": 43735, "idx": 84}, {"begin": 43736, "end": 43995, "idx": 85}], "ReferenceToBib": [{"begin": 2330, "end": 2333, "target": "#b8", "idx": 0}, {"begin": 2334, "end": 2337, "target": "#b11", "idx": 1}, {"begin": 2338, "end": 2341, "target": "#b16", "idx": 2}, {"begin": 2342, "end": 2345, "target": "#b23", "idx": 3}, {"begin": 2346, "end": 2349, "target": "#b41", "idx": 4}, {"begin": 2350, "end": 2353, "target": "#b42", "idx": 5}, {"begin": 2378, "end": 2381, "target": "#b0", "idx": 6}, {"begin": 2382, "end": 2384, "target": "#b7", "idx": 7}, {"begin": 2632, "end": 2636, "target": "#b50", "idx": 8}, {"begin": 2637, "end": 2640, "target": "#b55", "idx": 9}, {"begin": 2641, "end": 2644, "target": "#b67", "idx": 10}, {"begin": 4305, "end": 4309, "target": "#b30", "idx": 11}, {"begin": 5459, "end": 5463, "target": "#b67", "idx": 12}, {"begin": 5673, "end": 5677, "target": "#b24", "idx": 13}, {"begin": 5958, "end": 5962, "target": "#b9", "idx": 14}, {"begin": 5963, "end": 5966, "target": "#b24", "idx": 15}, {"begin": 6024, "end": 6028, "target": "#b23", "idx": 16}, {"begin": 6029, "end": 6032, "target": "#b25", "idx": 17}, {"begin": 6428, "end": 6431, "target": "#b2", "idx": 18}, {"begin": 6432, "end": 6435, "target": "#b38", "idx": 19}, {"begin": 6436, "end": 6439, "target": "#b63", "idx": 20}, {"begin": 6765, "end": 6769, "target": "#b20", "idx": 21}, {"begin": 7151, "end": 7155, "target": "#b62", "idx": 22}, {"begin": 7339, "end": 7343, "target": "#b17", "idx": 23}, {"begin": 7498, "end": 7502, "target": "#b11", "idx": 24}, {"begin": 7503, "end": 7506, "target": "#b23", "idx": 25}, {"begin": 7507, "end": 7510, "target": "#b41", "idx": 26}, {"begin": 7517, "end": 7521, "target": "#b42", "idx": 27}, {"begin": 7532, "end": 7535, "target": "#b1", "idx": 28}, {"begin": 7536, "end": 7539, "target": "#b16", "idx": 29}, {"begin": 7540, "end": 7543, "target": "#b59", "idx": 30}, {"begin": 7687, "end": 7691, "target": "#b33", "idx": 31}, {"begin": 7692, "end": 7695, "target": "#b40", "idx": 32}, {"begin": 7741, "end": 7745, "target": "#b57", "idx": 33}, {"begin": 7771, "end": 7775, "target": "#b39", "idx": 34}, {"begin": 8664, "end": 8668, "target": "#b32", "idx": 35}, {"begin": 8669, "end": 8672, "target": "#b37", "idx": 36}, {"begin": 8673, "end": 8676, "target": "#b59", "idx": 37}, {"begin": 8831, "end": 8835, "target": "#b34", "idx": 38}, {"begin": 8857, "end": 8861, "target": "#b9", "idx": 39}, {"begin": 8874, "end": 8878, "target": "#b40", "idx": 40}, {"begin": 8948, "end": 8952, "target": "#b52", "idx": 41}, {"begin": 8953, "end": 8956, "target": "#b46", "idx": 42}, {"begin": 8957, "end": 8960, "target": "#b12", "idx": 43}, {"begin": 9212, "end": 9216, "target": "#b65", "idx": 44}, {"begin": 9217, "end": 9219, "target": "#b5", "idx": 45}, {"begin": 9220, "end": 9222, "target": "#b3", "idx": 46}, {"begin": 9223, "end": 9225, "target": "#b4", "idx": 47}, {"begin": 10121, "end": 10125, "target": "#b40", "idx": 48}, {"begin": 10294, "end": 10298, "target": "#b51", "idx": 49}, {"begin": 10585, "end": 10589, "target": "#b21", "idx": 50}, {"begin": 10682, "end": 10686, "target": "#b19", "idx": 51}, {"begin": 10687, "end": 10690, "target": "#b64", "idx": 52}, {"begin": 10865, "end": 10869, "target": "#b40", "idx": 53}, {"begin": 10870, "end": 10873, "target": "#b65", "idx": 54}, {"begin": 11089, "end": 11093, "target": "#b40", "idx": 55}, {"begin": 11915, "end": 11918, "target": "#b8", "idx": 56}, {"begin": 11942, "end": 11946, "target": "#b31", "idx": 57}, {"begin": 12029, "end": 12033, "target": "#b53", "idx": 58}, {"begin": 12034, "end": 12037, "target": "#b13", "idx": 59}, {"begin": 12231, "end": 12235, "target": "#b47", "idx": 60}, {"begin": 12353, "end": 12357, "target": "#b10", "idx": 61}, {"begin": 12358, "end": 12361, "target": "#b26", "idx": 62}, {"begin": 12538, "end": 12542, "target": "#b35", "idx": 63}, {"begin": 12543, "end": 12545, "target": "#b6", "idx": 64}, {"begin": 12730, "end": 12734, "target": "#b48", "idx": 65}, {"begin": 13520, "end": 13524, "target": "#b40", "idx": 66}, {"begin": 15244, "end": 15248, "target": "#b44", "idx": 67}, {"begin": 15249, "end": 15252, "target": "#b43", "idx": 68}, {"begin": 16875, "end": 16879, "target": "#b49", "idx": 69}, {"begin": 17569, "end": 17573, "target": "#b29", "idx": 70}, {"begin": 17945, "end": 17949, "target": "#b36", "idx": 71}, {"begin": 18069, "end": 18073, "target": "#b66", "idx": 72}, {"begin": 18177, "end": 18181, "target": "#b56", "idx": 73}, {"begin": 19436, "end": 19440, "target": "#b40", "idx": 74}, {"begin": 20156, "end": 20160, "target": "#b65", "idx": 75}, {"begin": 23700, "end": 23704, "target": "#b18", "idx": 76}, {"begin": 25646, "end": 25650, "target": "#b40", "idx": 77}, {"begin": 25985, "end": 25989, "target": "#b40", "idx": 78}, {"begin": 26128, "end": 26132, "target": "#b15", "idx": 79}, {"begin": 26133, "end": 26136, "target": "#b60", "idx": 80}, {"begin": 26517, "end": 26521, "target": "#b27", "idx": 81}, {"begin": 27391, "end": 27395, "target": "#b28", "idx": 82}, {"begin": 28159, "end": 28163, "target": "#b14", "idx": 83}, {"begin": 31387, "end": 31391, "target": "#b40", "idx": 84}], "Sentence": [{"begin": 73, "end": 169, "idx": 0}, {"begin": 170, "end": 357, "idx": 1}, {"begin": 358, "end": 554, "idx": 2}, {"begin": 555, "end": 737, "idx": 3}, {"begin": 738, "end": 864, "idx": 4}, {"begin": 865, "end": 1162, "idx": 5}, {"begin": 1163, "end": 1323, "idx": 6}, {"begin": 1324, "end": 1421, "idx": 7}, {"begin": 1447, "end": 1634, "idx": 8}, {"begin": 1635, "end": 1792, "idx": 9}, {"begin": 1793, "end": 1968, "idx": 10}, {"begin": 1969, "end": 2157, "idx": 11}, {"begin": 2158, "end": 2385, "idx": 12}, {"begin": 2386, "end": 2645, "idx": 13}, {"begin": 2646, "end": 2731, "idx": 14}, {"begin": 2732, "end": 2840, "idx": 15}, {"begin": 2841, "end": 3047, "idx": 16}, {"begin": 3048, "end": 3218, "idx": 17}, {"begin": 3219, "end": 3444, "idx": 18}, {"begin": 3445, "end": 3521, "idx": 19}, {"begin": 3522, "end": 3693, "idx": 20}, {"begin": 3694, "end": 3840, "idx": 21}, {"begin": 3841, "end": 3955, "idx": 22}, {"begin": 3956, "end": 4094, "idx": 23}, {"begin": 4095, "end": 4229, "idx": 24}, {"begin": 4230, "end": 4493, "idx": 25}, {"begin": 4494, "end": 4640, "idx": 26}, {"begin": 4641, "end": 4758, "idx": 27}, {"begin": 4759, "end": 4922, "idx": 28}, {"begin": 4923, "end": 5128, "idx": 29}, {"begin": 5145, "end": 5262, "idx": 30}, {"begin": 5263, "end": 5314, "idx": 31}, {"begin": 5315, "end": 5331, "idx": 32}, {"begin": 5332, "end": 5464, "idx": 33}, {"begin": 5465, "end": 5678, "idx": 34}, {"begin": 5679, "end": 5956, "idx": 35}, {"begin": 5957, "end": 6022, "idx": 36}, {"begin": 6023, "end": 6034, "idx": 37}, {"begin": 6035, "end": 6176, "idx": 38}, {"begin": 6177, "end": 6336, "idx": 39}, {"begin": 6337, "end": 6426, "idx": 40}, {"begin": 6427, "end": 6441, "idx": 41}, {"begin": 6442, "end": 6661, "idx": 42}, {"begin": 6662, "end": 6821, "idx": 43}, {"begin": 6822, "end": 7061, "idx": 44}, {"begin": 7062, "end": 7344, "idx": 45}, {"begin": 7345, "end": 7388, "idx": 46}, {"begin": 7389, "end": 7746, "idx": 47}, {"begin": 7747, "end": 7907, "idx": 48}, {"begin": 7908, "end": 8049, "idx": 49}, {"begin": 8050, "end": 8220, "idx": 50}, {"begin": 8221, "end": 8509, "idx": 51}, {"begin": 8510, "end": 8547, "idx": 52}, {"begin": 8548, "end": 8662, "idx": 53}, {"begin": 8663, "end": 8946, "idx": 54}, {"begin": 8947, "end": 8961, "idx": 55}, {"begin": 8962, "end": 9301, "idx": 56}, {"begin": 9302, "end": 9410, "idx": 57}, {"begin": 9411, "end": 9541, "idx": 58}, {"begin": 9542, "end": 9698, "idx": 59}, {"begin": 9699, "end": 9834, "idx": 60}, {"begin": 9835, "end": 9902, "idx": 61}, {"begin": 9903, "end": 10126, "idx": 62}, {"begin": 10127, "end": 10157, "idx": 63}, {"begin": 10158, "end": 10197, "idx": 64}, {"begin": 10198, "end": 10299, "idx": 65}, {"begin": 10300, "end": 10691, "idx": 66}, {"begin": 10692, "end": 10833, "idx": 67}, {"begin": 10834, "end": 11041, "idx": 68}, {"begin": 11042, "end": 11151, "idx": 69}, {"begin": 11152, "end": 11423, "idx": 70}, {"begin": 11434, "end": 11586, "idx": 71}, {"begin": 11587, "end": 11707, "idx": 72}, {"begin": 11746, "end": 11859, "idx": 73}, {"begin": 11860, "end": 11947, "idx": 74}, {"begin": 11948, "end": 12038, "idx": 75}, {"begin": 12039, "end": 12092, "idx": 76}, {"begin": 12093, "end": 12236, "idx": 77}, {"begin": 12237, "end": 12362, "idx": 78}, {"begin": 12363, "end": 12536, "idx": 79}, {"begin": 12537, "end": 12546, "idx": 80}, {"begin": 12558, "end": 12735, "idx": 81}, {"begin": 12736, "end": 12941, "idx": 82}, {"begin": 12942, "end": 13037, "idx": 83}, {"begin": 13038, "end": 13176, "idx": 84}, {"begin": 13177, "end": 13262, "idx": 85}, {"begin": 13263, "end": 13352, "idx": 86}, {"begin": 13353, "end": 13590, "idx": 87}, {"begin": 13617, "end": 13795, "idx": 88}, {"begin": 13821, "end": 13837, "idx": 89}, {"begin": 13838, "end": 13904, "idx": 90}, {"begin": 13905, "end": 13950, "idx": 91}, {"begin": 13984, "end": 14044, "idx": 92}, {"begin": 14045, "end": 14065, "idx": 93}, {"begin": 14066, "end": 14132, "idx": 94}, {"begin": 14133, "end": 14218, "idx": 95}, {"begin": 14219, "end": 14458, "idx": 96}, {"begin": 14459, "end": 14519, "idx": 97}, {"begin": 14571, "end": 14745, "idx": 98}, {"begin": 14746, "end": 14776, "idx": 99}, {"begin": 14777, "end": 14887, "idx": 100}, {"begin": 14925, "end": 14928, "idx": 101}, {"begin": 14957, "end": 14976, "idx": 102}, {"begin": 14977, "end": 15056, "idx": 103}, {"begin": 15084, "end": 15185, "idx": 104}, {"begin": 15186, "end": 15303, "idx": 105}, {"begin": 15304, "end": 15324, "idx": 106}, {"begin": 15325, "end": 15394, "idx": 107}, {"begin": 15395, "end": 15569, "idx": 108}, {"begin": 15690, "end": 15916, "idx": 109}, {"begin": 15917, "end": 16134, "idx": 110}, {"begin": 16135, "end": 16210, "idx": 111}, {"begin": 16211, "end": 16227, "idx": 112}, {"begin": 16228, "end": 16315, "idx": 113}, {"begin": 16369, "end": 16397, "idx": 114}, {"begin": 16398, "end": 16417, "idx": 115}, {"begin": 16418, "end": 16520, "idx": 116}, {"begin": 16521, "end": 16626, "idx": 117}, {"begin": 16675, "end": 16699, "idx": 118}, {"begin": 16700, "end": 16845, "idx": 119}, {"begin": 16846, "end": 16923, "idx": 120}, {"begin": 16980, "end": 17036, "idx": 121}, {"begin": 17037, "end": 17175, "idx": 122}, {"begin": 17176, "end": 17265, "idx": 123}, {"begin": 17266, "end": 17316, "idx": 124}, {"begin": 17341, "end": 17397, "idx": 125}, {"begin": 17398, "end": 17525, "idx": 126}, {"begin": 17526, "end": 17770, "idx": 127}, {"begin": 17771, "end": 17950, "idx": 128}, {"begin": 17951, "end": 18074, "idx": 129}, {"begin": 18075, "end": 18182, "idx": 130}, {"begin": 18183, "end": 18411, "idx": 131}, {"begin": 18412, "end": 18647, "idx": 132}, {"begin": 18648, "end": 18813, "idx": 133}, {"begin": 18814, "end": 18995, "idx": 134}, {"begin": 18996, "end": 19115, "idx": 135}, {"begin": 19116, "end": 19211, "idx": 136}, {"begin": 19235, "end": 19441, "idx": 137}, {"begin": 19442, "end": 19644, "idx": 138}, {"begin": 19645, "end": 19909, "idx": 139}, {"begin": 19910, "end": 20042, "idx": 140}, {"begin": 20043, "end": 20161, "idx": 141}, {"begin": 20162, "end": 20309, "idx": 142}, {"begin": 20310, "end": 20417, "idx": 143}, {"begin": 20418, "end": 20587, "idx": 144}, {"begin": 20588, "end": 20656, "idx": 145}, {"begin": 20657, "end": 20899, "idx": 146}, {"begin": 20900, "end": 21019, "idx": 147}, {"begin": 21050, "end": 21226, "idx": 148}, {"begin": 21227, "end": 21446, "idx": 149}, {"begin": 21447, "end": 21587, "idx": 150}, {"begin": 21588, "end": 21712, "idx": 151}, {"begin": 21713, "end": 21758, "idx": 152}, {"begin": 21759, "end": 21910, "idx": 153}, {"begin": 21911, "end": 22150, "idx": 154}, {"begin": 22151, "end": 22357, "idx": 155}, {"begin": 22358, "end": 22503, "idx": 156}, {"begin": 22508, "end": 22704, "idx": 157}, {"begin": 22705, "end": 22781, "idx": 158}, {"begin": 22782, "end": 22886, "idx": 159}, {"begin": 22887, "end": 23019, "idx": 160}, {"begin": 23020, "end": 23163, "idx": 161}, {"begin": 23181, "end": 23300, "idx": 162}, {"begin": 23301, "end": 23544, "idx": 163}, {"begin": 23545, "end": 23705, "idx": 164}, {"begin": 23706, "end": 23890, "idx": 165}, {"begin": 23891, "end": 24004, "idx": 166}, {"begin": 24005, "end": 24130, "idx": 167}, {"begin": 24131, "end": 24210, "idx": 168}, {"begin": 24211, "end": 24447, "idx": 169}, {"begin": 24448, "end": 24554, "idx": 170}, {"begin": 24555, "end": 24702, "idx": 171}, {"begin": 24703, "end": 24934, "idx": 172}, {"begin": 24935, "end": 25086, "idx": 173}, {"begin": 25087, "end": 25142, "idx": 174}, {"begin": 25143, "end": 25190, "idx": 175}, {"begin": 25191, "end": 25200, "idx": 176}, {"begin": 25212, "end": 25355, "idx": 177}, {"begin": 25356, "end": 25497, "idx": 178}, {"begin": 25512, "end": 25651, "idx": 179}, {"begin": 25652, "end": 25837, "idx": 180}, {"begin": 25838, "end": 25907, "idx": 181}, {"begin": 25908, "end": 26137, "idx": 182}, {"begin": 26159, "end": 26255, "idx": 183}, {"begin": 26256, "end": 26357, "idx": 184}, {"begin": 26358, "end": 26522, "idx": 185}, {"begin": 26523, "end": 26676, "idx": 186}, {"begin": 26677, "end": 26767, "idx": 187}, {"begin": 26768, "end": 27002, "idx": 188}, {"begin": 27025, "end": 27161, "idx": 189}, {"begin": 27162, "end": 27262, "idx": 190}, {"begin": 27263, "end": 27397, "idx": 191}, {"begin": 27512, "end": 27679, "idx": 192}, {"begin": 27680, "end": 27783, "idx": 193}, {"begin": 27784, "end": 27882, "idx": 194}, {"begin": 27883, "end": 28045, "idx": 195}, {"begin": 28046, "end": 28117, "idx": 196}, {"begin": 28118, "end": 28294, "idx": 197}, {"begin": 28295, "end": 28383, "idx": 198}, {"begin": 28384, "end": 28641, "idx": 199}, {"begin": 28642, "end": 28785, "idx": 200}, {"begin": 28812, "end": 28871, "idx": 201}, {"begin": 28872, "end": 29023, "idx": 202}, {"begin": 29024, "end": 29103, "idx": 203}, {"begin": 29104, "end": 29283, "idx": 204}, {"begin": 29284, "end": 29352, "idx": 205}, {"begin": 29353, "end": 29468, "idx": 206}, {"begin": 29539, "end": 29618, "idx": 207}, {"begin": 29619, "end": 29722, "idx": 208}, {"begin": 29723, "end": 29838, "idx": 209}, {"begin": 29839, "end": 30047, "idx": 210}, {"begin": 30048, "end": 30253, "idx": 211}, {"begin": 30254, "end": 30287, "idx": 212}, {"begin": 30314, "end": 30457, "idx": 213}, {"begin": 30458, "end": 30540, "idx": 214}, {"begin": 30551, "end": 30560, "idx": 215}, {"begin": 30590, "end": 30746, "idx": 216}, {"begin": 30747, "end": 30918, "idx": 217}, {"begin": 30919, "end": 30996, "idx": 218}, {"begin": 30997, "end": 31298, "idx": 219}, {"begin": 31299, "end": 31392, "idx": 220}, {"begin": 31393, "end": 31503, "idx": 221}, {"begin": 31504, "end": 31585, "idx": 222}, {"begin": 31586, "end": 31675, "idx": 223}, {"begin": 31676, "end": 31762, "idx": 224}, {"begin": 31763, "end": 31959, "idx": 225}, {"begin": 32094, "end": 32274, "idx": 226}, {"begin": 32275, "end": 32391, "idx": 227}, {"begin": 32392, "end": 32647, "idx": 228}, {"begin": 32648, "end": 32765, "idx": 229}, {"begin": 32766, "end": 33077, "idx": 230}, {"begin": 33078, "end": 33142, "idx": 231}, {"begin": 33168, "end": 33306, "idx": 232}, {"begin": 33307, "end": 33418, "idx": 233}, {"begin": 33439, "end": 33676, "idx": 234}, {"begin": 33677, "end": 33878, "idx": 235}, {"begin": 33879, "end": 33981, "idx": 236}, {"begin": 33982, "end": 34077, "idx": 237}, {"begin": 34078, "end": 34224, "idx": 238}, {"begin": 34225, "end": 34377, "idx": 239}, {"begin": 34378, "end": 34521, "idx": 240}, {"begin": 34522, "end": 34609, "idx": 241}, {"begin": 34715, "end": 34800, "idx": 242}, {"begin": 34801, "end": 34898, "idx": 243}, {"begin": 34899, "end": 34985, "idx": 244}, {"begin": 34986, "end": 35091, "idx": 245}, {"begin": 35307, "end": 35393, "idx": 246}, {"begin": 35394, "end": 35483, "idx": 247}, {"begin": 35484, "end": 35570, "idx": 248}, {"begin": 35571, "end": 35767, "idx": 249}, {"begin": 35768, "end": 35869, "idx": 250}, {"begin": 35870, "end": 35937, "idx": 251}, {"begin": 35990, "end": 36111, "idx": 252}, {"begin": 36112, "end": 36156, "idx": 253}, {"begin": 36205, "end": 36343, "idx": 254}, {"begin": 36344, "end": 36527, "idx": 255}, {"begin": 36528, "end": 36725, "idx": 256}, {"begin": 36726, "end": 36789, "idx": 257}, {"begin": 36823, "end": 36900, "idx": 258}, {"begin": 36901, "end": 37015, "idx": 259}, {"begin": 37016, "end": 37107, "idx": 260}, {"begin": 37108, "end": 37220, "idx": 261}, {"begin": 37251, "end": 37300, "idx": 262}, {"begin": 37301, "end": 37478, "idx": 263}, {"begin": 37479, "end": 37623, "idx": 264}, {"begin": 37624, "end": 37760, "idx": 265}, {"begin": 37761, "end": 37852, "idx": 266}, {"begin": 37853, "end": 37998, "idx": 267}, {"begin": 37999, "end": 38247, "idx": 268}, {"begin": 38248, "end": 38287, "idx": 269}, {"begin": 38294, "end": 38348, "idx": 270}, {"begin": 38349, "end": 38721, "idx": 271}, {"begin": 38722, "end": 38791, "idx": 272}, {"begin": 38792, "end": 38875, "idx": 273}, {"begin": 38876, "end": 38934, "idx": 274}, {"begin": 38935, "end": 39067, "idx": 275}, {"begin": 39108, "end": 39218, "idx": 276}, {"begin": 39219, "end": 39365, "idx": 277}, {"begin": 39366, "end": 39674, "idx": 278}, {"begin": 39675, "end": 39823, "idx": 279}, {"begin": 39873, "end": 39974, "idx": 280}, {"begin": 39975, "end": 40207, "idx": 281}, {"begin": 40208, "end": 40348, "idx": 282}, {"begin": 40349, "end": 40522, "idx": 283}, {"begin": 40548, "end": 40709, "idx": 284}, {"begin": 40710, "end": 40815, "idx": 285}, {"begin": 40816, "end": 40939, "idx": 286}, {"begin": 40940, "end": 41217, "idx": 287}, {"begin": 41218, "end": 41366, "idx": 288}, {"begin": 41367, "end": 41638, "idx": 289}, {"begin": 41639, "end": 41881, "idx": 290}, {"begin": 41882, "end": 42191, "idx": 291}, {"begin": 42192, "end": 42407, "idx": 292}, {"begin": 42408, "end": 42500, "idx": 293}, {"begin": 42501, "end": 42541, "idx": 294}, {"begin": 42542, "end": 42624, "idx": 295}, {"begin": 42625, "end": 42741, "idx": 296}, {"begin": 42742, "end": 42958, "idx": 297}, {"begin": 42959, "end": 43066, "idx": 298}, {"begin": 43067, "end": 43228, "idx": 299}, {"begin": 43229, "end": 43379, "idx": 300}, {"begin": 43380, "end": 43483, "idx": 301}, {"begin": 43484, "end": 43580, "idx": 302}, {"begin": 43581, "end": 43682, "idx": 303}, {"begin": 43683, "end": 43735, "idx": 304}, {"begin": 43736, "end": 43825, "idx": 305}, {"begin": 43826, "end": 43888, "idx": 306}, {"begin": 43889, "end": 43995, "idx": 307}], "ReferenceToFigure": [{"begin": 3461, "end": 3462, "idx": 0}, {"begin": 12636, "end": 12637, "target": "#fig_0", "idx": 1}, {"begin": 25094, "end": 25095, "target": "#fig_2", "idx": 2}, {"begin": 39115, "end": 39116, "idx": 3}, {"begin": 40215, "end": 40217, "target": "#fig_9", "idx": 4}, {"begin": 40518, "end": 40520, "target": "#fig_9", "idx": 5}, {"begin": 42126, "end": 42127, "target": "#fig_8", "idx": 6}], "Abstract": [{"begin": 63, "end": 1421, "idx": 0}], "SectionFootnote": [{"begin": 43997, "end": 44007, "idx": 0}], "ReferenceString": [{"begin": 44024, "end": 44208, "id": "b0", "idx": 0}, {"begin": 44210, "end": 44398, "id": "b1", "idx": 1}, {"begin": 44402, "end": 44719, "id": "b2", "idx": 2}, {"begin": 44723, "end": 44981, "id": "b3", "idx": 3}, {"begin": 44985, "end": 45218, "id": "b4", "idx": 4}, {"begin": 45222, "end": 45413, "id": "b5", "idx": 5}, {"begin": 45417, "end": 45563, "id": "b6", "idx": 6}, {"begin": 45567, "end": 45724, "id": "b7", "idx": 7}, {"begin": 45728, "end": 45887, "id": "b8", "idx": 8}, {"begin": 45891, "end": 46083, "id": "b9", "idx": 9}, {"begin": 46087, "end": 46293, "id": "b10", "idx": 10}, {"begin": 46297, "end": 46519, "id": "b11", "idx": 11}, {"begin": 46523, "end": 46720, "id": "b12", "idx": 12}, {"begin": 46724, "end": 46853, "id": "b13", "idx": 13}, {"begin": 46857, "end": 47036, "id": "b14", "idx": 14}, {"begin": 47040, "end": 47242, "id": "b15", "idx": 15}, {"begin": 47246, "end": 47453, "id": "b16", "idx": 16}, {"begin": 47457, "end": 47746, "id": "b17", "idx": 17}, {"begin": 47750, "end": 47962, "id": "b18", "idx": 18}, {"begin": 47966, "end": 48161, "id": "b19", "idx": 19}, {"begin": 48165, "end": 48273, "id": "b20", "idx": 20}, {"begin": 48277, "end": 48490, "id": "b21", "idx": 21}, {"begin": 48494, "end": 48666, "id": "b22", "idx": 22}, {"begin": 48670, "end": 48908, "id": "b23", "idx": 23}, {"begin": 48912, "end": 49115, "id": "b24", "idx": 24}, {"begin": 49119, "end": 49314, "id": "b25", "idx": 25}, {"begin": 49318, "end": 49493, "id": "b26", "idx": 26}, {"begin": 49497, "end": 49668, "id": "b27", "idx": 27}, {"begin": 49672, "end": 49921, "id": "b28", "idx": 28}, {"begin": 49925, "end": 50063, "id": "b29", "idx": 29}, {"begin": 50067, "end": 50278, "id": "b30", "idx": 30}, {"begin": 50282, "end": 50494, "id": "b31", "idx": 31}, {"begin": 50498, "end": 50682, "id": "b32", "idx": 32}, {"begin": 50686, "end": 50914, "id": "b33", "idx": 33}, {"begin": 50918, "end": 51090, "id": "b34", "idx": 34}, {"begin": 51094, "end": 51289, "id": "b35", "idx": 35}, {"begin": 51293, "end": 51535, "id": "b36", "idx": 36}, {"begin": 51539, "end": 51758, "id": "b37", "idx": 37}, {"begin": 51762, "end": 51974, "id": "b38", "idx": 38}, {"begin": 51978, "end": 52196, "id": "b39", "idx": 39}, {"begin": 52200, "end": 52426, "id": "b40", "idx": 40}, {"begin": 52430, "end": 52667, "id": "b41", "idx": 41}, {"begin": 52671, "end": 52907, "id": "b42", "idx": 42}, {"begin": 52911, "end": 53084, "id": "b43", "idx": 43}, {"begin": 53088, "end": 53266, "id": "b44", "idx": 44}, {"begin": 53270, "end": 53368, "id": "b45", "idx": 45}, {"begin": 53372, "end": 53524, "id": "b46", "idx": 46}, {"begin": 53528, "end": 53759, "id": "b47", "idx": 47}, {"begin": 53763, "end": 54001, "id": "b48", "idx": 48}, {"begin": 54005, "end": 54176, "id": "b49", "idx": 49}, {"begin": 54180, "end": 54303, "id": "b50", "idx": 50}, {"begin": 54307, "end": 54462, "id": "b51", "idx": 51}, {"begin": 54466, "end": 54688, "id": "b52", "idx": 52}, {"begin": 54692, "end": 54838, "id": "b53", "idx": 53}, {"begin": 54842, "end": 55008, "id": "b54", "idx": 54}, {"begin": 55012, "end": 55165, "id": "b55", "idx": 55}, {"begin": 55169, "end": 55295, "id": "b56", "idx": 56}, {"begin": 55299, "end": 55490, "id": "b57", "idx": 57}, {"begin": 55494, "end": 55707, "id": "b58", "idx": 58}, {"begin": 55711, "end": 55918, "id": "b59", "idx": 59}, {"begin": 55922, "end": 56116, "id": "b60", "idx": 60}, {"begin": 56120, "end": 56256, "id": "b61", "idx": 61}, {"begin": 56260, "end": 56435, "id": "b62", "idx": 62}, {"begin": 56439, "end": 56648, "id": "b63", "idx": 63}, {"begin": 56652, "end": 56913, "id": "b64", "idx": 64}, {"begin": 56917, "end": 57080, "id": "b65", "idx": 65}, {"begin": 57084, "end": 57337, "id": "b66", "idx": 66}, {"begin": 57341, "end": 57403, "id": "b67", "idx": 67}]}}