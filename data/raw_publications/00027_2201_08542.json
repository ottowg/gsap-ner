{"text": "Can Model Compression Improve NLP Fairness?\n\nAbstract:\nModel compression techniques are receiving increasing attention; however, the effect of compression on model fairness is still under explored. This is the first paper to examine the effect of distillation and pruning on the toxicity and bias of generative language models. We test Knowledge Distillation and Pruning methods on the GPT2 model and found a consistent pattern of toxicity and bias reduction after model distillation; this result can be potentially interpreted by existing line of research which describes model compression as a regularization technique; our work not only serves as a reference for safe deployment of compressed models, but also extends the discussion of \"compression as regularization\" into the setting of neural LMs, and hints at the possibility of using compression to develop fairer models.\n\n\n1 Introduction\nNeural Language Models, such as GPT and Bert, are fundamental not only in natural language processing (NLP) research but also in reality. Language model based applications has become an integral part of our everyday life, including AI agent Siri, Alexa, typing assistant, and Google search suggestions, etc.\nHowever, the current neural language models come with some serious limitations -long inference time and large model size, which could limit their real-life deployment on edge devices. Model compression techniques thus becomes crucial to address such drawbacks. However, despite its importance, the model compression's effect on fairness is still under-explored, and to the best of our knowledge, this is the first work to ever examine this issue for NLP models.\nDespite need for practical application, his topic is also intuitively interesting because there are two plausible but contradictory hypotheses regarding the effect of model compression:\n(1) Memorization: Large neural models, due to over-parameterization, learns, or memorizes too well the undesired traits of the corpus they trained on. So, as the model gets compressed, it forgets some biased or toxic contents, and becomes less toxic and biased.\n(2) Winner Takes it All: Model compression reinforces heuristic bias, because as model becomes smaller, it tends to simplify the reasoning process, ignore the under-represented features, and rely on a small and highly biased subset of parameters to make predictions, making compressed models more biased. This paper evaluates model compression's effect on toxicity and social bias. Our experiments shows that Knowledge Distillation causes monotonic reduction of model toxicity, and model bias also seem to follow a trend of reduction as model size decreases with distillation. So, the answer seems to be veering more towards the direction of the first hypothesis about memorization. Following the intuition from previous studies that model compression could potentially improve generalization (Bartoldson et al., 2020) (Arani et al., 2019) and robustness (Goldblum et al., 2020), we wonder if similar factors are at work here, leading to the reduction in bias and toxicity. However, this link is difficult to establish, because we don't know whether overfitting could lead to toxicity and bias increase. But this is a worthwhile question to study, and could lead to the discovery of a universal fairness improvement method for NLP; we will further investigate this connection in our next step.\nOur main contributions are as follows:\n(1) Show empirical evidence that distilled models are less toxic, and maybe less biased.\n(2) Extends discussion of \"model compression as a regularizer\" to generative language models, and deliberate the possible applications of compression to enhance LM robustness to toxicity and bias.\n\nCompression\nAccording the survey paper (Gupta and Agrawal, 2020), model compression methods for NLP currently include:\npruning (Michel et al. (2019), Voita et al. (2019), Prasanna et al. (2020)), quantization (Cheong, 2019), knowledge distillation (Jiao et al. (2020), Iandola et al. (2020)), parameter sharing (Lan et al. (2020), Lan et al. (2020)), tensor decomposition and sub-quadratic complexity transformers.\nFairness Google Brain (Hooker et al., 2020) tries to characterize compression's impact on fairness for vision models. They tests quantization and pruning techniques and argue that though compressed models achieve similar overall error rate, but fairness is compromised because performance of samples with under-represented features is sacrificed after compression. Researchers from University of Utah (Joseph et al., 2020) proposes adding fairness into the compression objective function for vision tasks. However, to the best of our knowledge, no prior work has been done studying Knowledge Distillation method, nor are there any compression fairness studies on NLP models.\nCompression as regularization (Fan et al., 2020) introduces a compression method for transformers named structured dropout, which is shown to achieve higher performance than distillation and weight pruning. The method assumes that transformer models are over-parametrized and sub-structures of the original model could achieve equivalent performances, plus that smaller networks will enjoy the benefit of regularization. Many studies (Jord\u00e3o and Pedrini (2021), Bartoldson et al. (2020)) also argue that pruning of Convolutional Neural Networks serves as a way of regularization.\nCompression for robust learning The seminal work of (Papernot et al., 2016) introduces Knowledge Distillation as a defense against adversarial perturbations. Following works continue to use Knowledge Distillation to improve generalization (Arani et al., 2019) and robustness (Goldblum et al. (2020)). Knowledge Distillation is also used to improve models on privacy protection (Shejwalkar and Houmansadr (2019), Zhao and Caragea (2021)). Moreover, pruning can improve model robustness according to the following studies (Jord\u00e3o and Pedrini (2021), Pang et al. (2021), Hendrycks and Dietterich (2019)).  (Kaya et al., 2019) shows that stopping at earlier layers during inference can improve model robustness. The intuition is still that smaller and shallower networks are more robust.\n\nCompression for fairness\nOur experiments demonstrate monotonic reduction of model toxicity and biases as the model size decreases with distillation. The gold question is whether the regularization and robustness effect of model compression incur the toxicity and bias reduction that we observed in distilled generative language models. If yes, can we also develop techniques to improve NLP fairness using model compression? If not, what is the cause of the monotonic toxicity and bias reduction? 3 Approach\n\n3.1 Compression Techniques\nWe conduct experiments using two methods: Knowledge Distillation and Pruning. We choose Knowledge Distillation because it's popular in NLP: Distill-Bert (Sanh et al., 2019), Distill-GPT, Distilled Blenderbot, and distil-T5 are all distilled generative models publicly available to download and deploy on Hugginface.co (Wolf et al., 2020); those distilled models could achieve testing performance on par with original models, while cutting inference time and model size by more than half; we also test on pruning because it's the most commonly used compression techniques, and there are also works that prunes NLP models(Insert more about pruning here);\n\nKnowledge Distillation\nStandard training objective minimizes the cross-entropy between the model's predicted distribution and the one-hot empirical distribution of training labels. In Distillation, rather than training with one-hot encoding, we train with the soft targets (probabilities of the teacher). In practice, following (Hinton et al., 2015), we used a softmax-temperature to smoothen the soft target. A trick is to initialize student with teacher's weight, and our experiments confirms that random initialization scores much lower performance compared with using pretrained weights. For intuition, you can view Knowledge Distillation as fine-tuning with a truncated architecture, based on soft-targets from a teacher model rather than one-hot labels. Pruning Pruning removes elements of a network to reduce the model size and increase inference speed. Researchers have proposed different methods to prune weights, neurons, blocks, as well as head and layers. Research shows that pruning head and layers without finetuning the models can still achieve limited performance loss. Thus, to kick off our pruning experiments, we started with pruning attention heads of GPT-2 (Michel et al., 2019). We first sort the attention heads in all layers by a proxy importance score, then mask the heads with the lowest importance score. We can also iterate this process until the loss reaches a threshold.\n\n3.2 Retraining Compressed Models\nDistilled Models We retrained 6 distilled GPT2 language models, with different parameters sizes or training strategy. GPT-2 (Radford et al., 2019) is a unidirectional, transformer-based model to predict the next word in a sentence. GPT2's architecture can be characterized as a stack of decoding blocks, each of which is made of self-attention mask and a fully connected 2-layer neural network.Knowledge Distillation cuts the number of decoding blocks of the model, and result in smaller models as illustrated in the Figure 1.\nIf we ignore the positional embedding(relatively small), the total parameters in a model is given by the sum of the word-embedding size and number-of-blocks * 7M; by this formula, the smallest A-class model is only half the size of GPT, and its inference time only 1/3 of the original GPT2.\nPruned Models We load a pretrained GPT-2 model from HuggingFace and pruned the attention heads based on different data subsets. In our preliminary experiment, we conducted 3 iterations of pruning and kept 85 percent of the heads, namely reduced the number of heads from 144 to 122. In general, pruning GPT-2 models takes significant time especially on large datasets. In this paper, we report the preliminary results by varying the size of the datasets. We will continue the pruning experiment with more conditions in the future.\n\n3.3 Fairness Evaluation\nWe run experiments to evaluate the toxicity and bias level of the pruned models; the scope of our experiments goes beyond model we retrained ourselves, but also includes some open source pre-trained models(Blenderbot (Roller et al., 2021) and Dialogpt (Zhang et al., 2020) Bias Evaluation For Bias Evaluation, we measure social bias using the Stereoset (Nadeem et al., 2021) dataset, which covers racial, national, gender, and professional stereotypes; The format of the dataset is given in Figure 2; it feeds the three sentences which are either, stereotyped, antistereotyped, or unrelated to the LM; if the model prefers more anti-stereotyped sentences, then we could regard it as less biased. This dataset may be not as convincing and rigorous, because it can't rule out the possibility that LMs chose stereotyped sentences not for bias, but for coherence. We measure gender bias of language models using the WinoBias Dataset (Zhao et al., 2018) Knowledge Distillation We conduct distillation using the method described in Distill-Bert (Sanh et al., 2019). The six distilled models we trained are named (A, B, B not , C, D, D not ), with respectively (4,6,6,8,10,10) number of decoder blocks, as shown in Figure 1; among them, B not and D not did not use pretrained weights as initialization, while all others initialized with truncated weights from the original GPT2 model. The original training dataset of OpenAI GPT2 is the the 48GB OpenWebText. However, training on full 48GB is out of our capacity, so we randomly sampled around 1/100 of the OpenWebText data to train the distilled models. Training takes 3 epochs, and it automatically saves the best performing model; The other training hyper-parameters are the same as the GPT-2 Model in huggingface. The average training time is 6 hours on 2 A100 GPUs, and the smaller the student model, the faster the training. The perplexity of trained models is shown in Figure 3.\nPruning Pruning applies the method proposed in (Michel et al., 2019). We start with pretrained GPT2 model from HuggingFace which has 144 attention heads in total. In each prune iteration, we sort the heads by a proxy importance score and mask the lowest important 5 percent. And we repeat the pruning 3 times which results in keeping 85 percent of the heads. We choose to use WikiText which contains a reasonable number of data ( 35K). In this experiment, we sample 5 subsets with various sizes from the WikiText and conduct pruning using these subsets, namely 256, 1280, 2560, 5120, and 12800. They all have the same size, and report their perplexity (PPL) score along with the original model in Figure 4. As a result of the pruning, we observed 1.2x to 1.5x computation speed improvement. We observe that using a larger dataset as validation can help improve the pruning performance, as expected, since larger datasets results in more representative proxy importance scores.\n\nRealToxicityPrompts\nFor this dataset, we sampled 1200 toxicity triggers, which are nontoxic prompts but leads to toxic continuation in the actual data. Each model generates three sentences for each prompt, so, we are evaluating 3600 sentences per model. The toxicity level in the model generation is illustrated in Figure 6. Note that RealToxicityPrompts is sourced from the training dataset of GPT2, which means the GPT2 model has seen those toxic prompts before. If the GPT2 models seems very toxic in this setting, the reason may lie in its memorization of toxic training data: distilled models are heavily regularized and less likely to output the toxic training data than the GPT2 model. TCCC Prompts Different from the RealTox-icityPrompts dataset, GPT2 has never trained on the TCCC dataset. And we adopted three sampling strategies to build three test datasets, each containing 4k samples, named as(tox4k, random4k, safe4k). The tox4k contains prompts cropped from originally toxic sentences; the random4k's prompts are cropped from randomly sampled sentences in TCCC, in which around 10 percent data is toxic; the safe4k's prompts are cropped from very safe sentences, with toxic score lower than 0.001. The Figure 5 illustrate the toxicity distribution of different models under different sampling settings. To note, toxicity detection is performed on full sentence for random4k and safe4k; but only the generated part is rendered for toxicity detection in tox4k, because otherwise, all samples will be found toxic.\n\nObservation\nWe observe that toxicity level consistently decrease as the model size becomes smaller, in all 4 depicted sampling settings, and in two different toxicity evaluation dataset.\n\n4.3 Bias in Model Generation\n\n\nStereoset\nWe first test the models on the Stereoset dataset, which was specifically curated by (Nadeem et al., 2021) to measure social bias in language models. For each model, we plot the total number of times that it chooses And the more unrelated words it choose, the more unreliable it is. The result is shown in Figure 7.\nWinobias Winobias dataset has a similar format as Stereoset, and is used to measure gender bias. It contains a total of 1584 samples, and we count the number of times each model chooses anti-bias sentences. For both datasets, the more anti-stereotypes it chose, the less biased it is. The result is shown in Figure 7. Other papers (Barikeri et al., 2021) calculate mean perplexity difference between the anti-bias and biased sentences to measure the bias level of language models; however,since there's a significant inequality in the original perplexity of different models we compare, mean perplexity difference can be directly applied to our setting.\nObservation We find that gender bias level consistently decrease as the model size becomes smaller; but distillation's the effect on Stereoset dataset isn't obvious. The current experiment may be too weak to give any convincing conclusion now, but there does seem to be a trend of bias reduction after distillation.\n\n4.4 Ablation Study\nWe conduct an ablation study to mitigate the concern that toxicity reduction is only the result of smaller model size. As in Figure 8, the DialoGPT (Zhang et al., 2020) models (small, medium, large) are publicly available pretraiend models on Huggingface, trained from scratch using different architectures, rather than being distilled. The trend of toxicity and bias reduction is not observed in the DialoGPT models. Rather, toxicity decreases is observed in Distilled-400M Blenderbot versus orginal Blenderbot 3B. This study confirms that it's distillation, rather than simple model size that causes the observed fairness pattern. Figure 9 illustrates a potential weakness of our experiments; that is as the PPL decreases, model toxicity and bias level increases. Then, it can be argued that it's low perplexity that caused models to seem less biased and less toxic. However, in general, it's difficult to match heavily compressed model's PPL with the original model, so this concern couldn't be easily addressed. What we can do in the future is to narrow down the perplexity differences, and observe if there's any decrease in bias/toxicity reduction.\n\nResult for Toxicity Evaluation\nIn order to make the result exhaustive and convincing, we conducted experiments using two different datasets, with four different sampling strategies. The result is surprisingly coherent and uniform, that toxicity decreases with the intensity of distillation. We first experiment with toxic triggers from RealToxicityPrompts, which is sourced from the training dataset of GPT2. If the full GPT2 model produces more toxicity on this dataset, it could be because that full GPT2 memorizes more toxic training data; this hypothesis is supported by the fact that GPT2 has much longer generation length than the distilled models as in Figure 1, implying that it remembers more information regard this given prompts. However, if this is the case, the toxicity reduction pattern may not persist under another prompt sampling setting; So, we added experiments using the TCCC datasets, which the GPT2 has never seen. We also experimented with three sampling strategy: toxic, safe, and random. The result is shown in Figure 5, in which the same toxicity reduction pattern does persist. This result suggests that a simple memorization explanation can't account for this phenomenon, since no models has seen the TCCC data before. We are unable to explain this result now, and will try to give interpretation in the future.\n\nResult for Bias Evaluation\nThe experiment for bias reduction is recorded in Figure 7, but is still short of being conclusive. Gender bias as measured by Winobias dataset clearly decreases with distillation. However, the social bias as measured by the Stereoset have no such pattern and largely stays flat. I think there are three main possibilities. The first one is that any pattern recorded is due to randomness, since the testing dataset is relatively small(1.5k). The second possibility is that Stereoset did not give good reflection of model bias, because models make choices not based on bias, but baed on coherence. Stereoset is not a very rigorous benchmark for bias. The third possibility is that Winobias did not give good measurement of bias, which is quite unlikely, since the only variable in Winobias is the gender pronoun. Winobias results should be much more reliable than that of the Stereoset.\nIn conclusion, this paper evaluates the effect of model compression techniques (knowledge distillation and pruning) on NLP fairness. The pattern that distillation improves model fairness is being recorded, but we could not very well explain this phenomenon. We do hypothesize that this effect could be connected to regularization and model robustness, but still needs further experiments and theoretical support to verify this connection.\n\n6 Future Work\nFairness Evaluation We will add more experiment to verify our observation about bias reduction. The current evaluation dataset is too small, and the method may also be a bit naive. We would consider using bias regard metric (Sheng et al., 2019) and try measuring bias in word embedding;\n\nCompression\nWe will investigate the effectiveness of the current technique of pruning attention heads, and maybe try weights pruning and structured dropout. The toxicity and bias experiments for pruning methods will also be added. Moreover, the training time has become a large constraint given the large GPT2 model size; we will try to design experiments on smaller architectures in the future.\n\nTheoretical explanation\nWe have thus far unable to verify our hypothesis about the connection between regularization and the observed fairness improvement. However, we believe that if Knowledge Distillation and Pruning can effectively improve model robustness against adversarial attacks (Papernot et al., 2016), it is highly likely that they can also improve LM fairness. The prospect of developing universal fairness techniques for LMs based on compression is very promising.\n\nFootnotes:\n\nReferences:\n\n- E. Arani, Fahad Sarfraz, and Bahram Zonooz. 2019. Improving generalization and robustness with noisy collaboration in knowledge distillation. ArXiv, abs/1910.05057.- Soumya Barikeri, Anne Lauscher, Ivan Vuli'c, and Goran Glavas. 2021. Redditbias: A real-world re- source for bias evaluation and debiasing of conver- sational language models. In ACL/IJCNLP.\n\n- Brian Bartoldson, Ari S. Morcos, Adrian Barbu, and Gordon Erlebacher. 2020. The generalization- stability tradeoff in neural network pruning. ArXiv, abs/1906.03728.\n\n- Robin Cheong. 2019. transformers . zip : Compressing transformers with pruning and quantization.\n\n- Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing transformer depth on demand with struc- tured dropout. ArXiv, abs/1909.11556.\n\n- Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Realtoxici- typrompts: Evaluating neural toxic degeneration in language models. ArXiv, abs/2009.11462.\n\n- Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. 2020. Adversarially robust distillation. In AAAI.\n\n- Manish Gupta and Puneet Agrawal. 2020. Compres- sion of deep learning models for text: A survey. ArXiv, abs/2008.05221.\n\n- Laura Hanu and Unitary team. 2020. Detoxify. Github. https://github.com/unitaryai/detoxify.\n\n- Dan Hendrycks and Thomas G. Dietterich. 2019. Benchmarking neural network robustness to com- mon corruptions and perturbations. ArXiv, abs/1903.12261.\n\n- Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531.\n\n- Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily L. Denton. 2020. Characterising bias in compressed models. ArXiv, abs/2010.03058.\n\n- Forrest N. Iandola, Albert Eaton Shaw, Ravi Krishna, and Kurt Keutzer. 2020. Squeezebert: What can computer vision teach nlp about efficient neural net- works? ArXiv, abs/2006.11316.\n\n- Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. Tinybert: Distilling bert for natural language understanding. ArXiv, abs/1909.10351.\n\n- Artur Jord\u00e3o and H\u00e9lio Pedrini. 2021. On the effect of pruning on adversarial robustness. 2021 IEEE/CVF International Conference on Computer Vision Work- shops (ICCVW), pages 1-11.\n\n- Vinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan, Saurav Mu- ralidharan, Michael Garland, Sheraz Ahmed, and Andreas R. Dengel. 2020. Going beyond classifica- tion accuracy metrics in model compression.\n\n- Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2019. Shallow-deep networks: Understanding and mitigating network overthinking. In ICML.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. 2020. Albert: A lite bert for self-supervised learning of language representations. ArXiv, abs/1909.11942.\n\n- Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In NeurIPS. Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. Stereoset: Measuring stereotypical bias in pre- trained language models. In ACL/IJCNLP.\n\n- Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. 2021. Bag of tricks for adversarial training. ArXiv, abs/2010.00467.\n\n- Nicolas Papernot, Patrick Mcdaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. 2016 IEEE Symposium on Security and Privacy (SP), pages 582-597.\n\n- Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When bert plays the lottery, all tickets are winning. ArXiv, abs/2005.00561.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\n- Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric Michael Smith, Y.-Lan Boureau, and Jason Weston. 2021. Recipes for building an open-domain chatbot. In EACL.\n\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.\n\n- Virat Shejwalkar and Amir Houmansadr. 2019. Recon- ciling utility and membership privacy via knowledge distillation. ArXiv, abs/1906.06589.\n\n- Emily Sheng, Kai-Wei Chang, P. Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. ArXiv, abs/1909.01326.\n\n- Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self- attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\n- Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and William B. Dolan. 2020. Dialogpt : Large- scale generative pre-training for conversational re- sponse generation. In ACL.\n\n- Chenye Zhao and Cornelia Caragea. 2021. Knowledge distillation with bert for image tag-based privacy pre- diction. In RANLP.\n\n- Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. In NAACL.\n\n", "annotations": {"Abstract": [{"begin": 45, "end": 878, "idx": 0}], "Head": [{"begin": 881, "end": 895, "n": "1", "idx": 0}, {"begin": 3734, "end": 3745, "idx": 1}, {"begin": 6189, "end": 6213, "idx": 2}, {"begin": 6697, "end": 6723, "n": "3.1", "idx": 3}, {"begin": 7378, "end": 7400, "idx": 4}, {"begin": 8780, "end": 8812, "n": "3.2", "idx": 5}, {"begin": 10162, "end": 10185, "n": "3.3", "idx": 6}, {"begin": 13093, "end": 13112, "idx": 7}, {"begin": 14620, "end": 14631, "idx": 8}, {"begin": 14808, "end": 14836, "n": "4.3", "idx": 9}, {"begin": 14839, "end": 14848, "idx": 10}, {"begin": 16136, "end": 16154, "n": "4.4", "idx": 11}, {"begin": 17311, "end": 17341, "idx": 12}, {"begin": 18653, "end": 18679, "idx": 13}, {"begin": 20005, "end": 20018, "n": "6", "idx": 14}, {"begin": 20307, "end": 20318, "idx": 15}, {"begin": 20704, "end": 20727, "idx": 16}], "ReferenceToBib": [{"begin": 2907, "end": 2932, "target": "#b2", "idx": 0}, {"begin": 2933, "end": 2953, "target": "#b0", "idx": 1}, {"begin": 2969, "end": 2992, "target": "#b6", "idx": 2}, {"begin": 3773, "end": 3798, "target": "#b7", "idx": 3}, {"begin": 3861, "end": 3882, "target": "#b18", "idx": 4}, {"begin": 3884, "end": 3903, "target": "#b27", "idx": 5}, {"begin": 3905, "end": 3927, "target": "#b21", "idx": 6}, {"begin": 3943, "end": 3957, "target": "#b3", "idx": 7}, {"begin": 3982, "end": 4001, "target": "#b13", "idx": 8}, {"begin": 4003, "end": 4024, "target": "#b12", "idx": 9}, {"begin": 4045, "end": 4063, "target": "#b17", "idx": 10}, {"begin": 4065, "end": 4082, "target": "#b17", "idx": 11}, {"begin": 4171, "end": 4192, "target": "#b11", "idx": 12}, {"begin": 4550, "end": 4571, "target": "#b15", "idx": 13}, {"begin": 4854, "end": 4872, "target": "#b4", "idx": 14}, {"begin": 5258, "end": 5284, "target": "#b14", "idx": 15}, {"begin": 5286, "end": 5310, "target": "#b2", "idx": 16}, {"begin": 5456, "end": 5479, "target": "#b20", "idx": 17}, {"begin": 5643, "end": 5663, "target": "#b0", "idx": 18}, {"begin": 5679, "end": 5702, "target": "#b6", "idx": 19}, {"begin": 5781, "end": 5814, "target": "#b25", "idx": 20}, {"begin": 5816, "end": 5839, "target": "#b30", "idx": 21}, {"begin": 5924, "end": 5950, "target": "#b14", "idx": 22}, {"begin": 5952, "end": 5970, "target": "#b19", "idx": 23}, {"begin": 5972, "end": 6003, "target": "#b9", "idx": 24}, {"begin": 6007, "end": 6026, "target": "#b16", "idx": 25}, {"begin": 6877, "end": 6896, "target": "#b24", "idx": 26}, {"begin": 7042, "end": 7061, "target": "#b28", "idx": 27}, {"begin": 7706, "end": 7727, "target": "#b10", "idx": 28}, {"begin": 8556, "end": 8577, "target": "#b18", "idx": 29}, {"begin": 8937, "end": 8959, "target": "#b22", "idx": 30}, {"begin": 10403, "end": 10424, "target": "#b23", "idx": 31}, {"begin": 10438, "end": 10458, "target": "#b29", "idx": 32}, {"begin": 11115, "end": 11134, "target": "#b31", "idx": 33}, {"begin": 11225, "end": 11244, "target": "#b24", "idx": 34}, {"begin": 12162, "end": 12183, "target": "#b18", "idx": 35}, {"begin": 14934, "end": 14955, "target": "#b18", "idx": 36}, {"begin": 15496, "end": 15519, "target": "#b1", "idx": 37}, {"begin": 16303, "end": 16353, "idx": 38}, {"begin": 20243, "end": 20263, "target": "#b26", "idx": 39}, {"begin": 20992, "end": 21015, "target": "#b20", "idx": 40}], "SectionFootnote": [{"begin": 21183, "end": 21193, "idx": 0}], "ReferenceString": [{"begin": 21210, "end": 21374, "id": "b0", "idx": 0}, {"begin": 21376, "end": 21566, "id": "b1", "idx": 1}, {"begin": 21570, "end": 21734, "id": "b2", "idx": 2}, {"begin": 21738, "end": 21834, "id": "b3", "idx": 3}, {"begin": 21838, "end": 21976, "id": "b4", "idx": 4}, {"begin": 21980, "end": 22166, "id": "b5", "idx": 5}, {"begin": 22170, "end": 22279, "id": "b6", "idx": 6}, {"begin": 22283, "end": 22402, "id": "b7", "idx": 7}, {"begin": 22406, "end": 22497, "id": "b8", "idx": 8}, {"begin": 22501, "end": 22651, "id": "b9", "idx": 9}, {"begin": 22655, "end": 22782, "id": "b10", "idx": 10}, {"begin": 22786, "end": 22936, "id": "b11", "idx": 11}, {"begin": 22940, "end": 23122, "id": "b12", "idx": 12}, {"begin": 23126, "end": 23312, "id": "b13", "idx": 13}, {"begin": 23316, "end": 23496, "id": "b14", "idx": 14}, {"begin": 23500, "end": 23727, "id": "b15", "idx": 15}, {"begin": 23731, "end": 23869, "id": "b16", "idx": 16}, {"begin": 23873, "end": 24075, "id": "b17", "idx": 17}, {"begin": 24079, "end": 24317, "id": "b18", "idx": 18}, {"begin": 24321, "end": 24449, "id": "b19", "idx": 19}, {"begin": 24453, "end": 24684, "id": "b20", "idx": 20}, {"begin": 24688, "end": 24817, "id": "b21", "idx": 21}, {"begin": 24821, "end": 24961, "id": "b22", "idx": 22}, {"begin": 24965, "end": 25189, "id": "b23", "idx": 23}, {"begin": 25193, "end": 25363, "id": "b24", "idx": 24}, {"begin": 25367, "end": 25506, "id": "b25", "idx": 25}, {"begin": 25510, "end": 25665, "id": "b26", "idx": 26}, {"begin": 25669, "end": 25854, "id": "b27", "idx": 27}, {"begin": 25858, "end": 26436, "id": "b28", "idx": 28}, {"begin": 26440, "end": 26673, "id": "b29", "idx": 29}, {"begin": 26677, "end": 26801, "id": "b30", "idx": 30}, {"begin": 26805, "end": 26970, "id": "b31", "idx": 31}], "Paragraph": [{"begin": 55, "end": 878, "idx": 0}, {"begin": 896, "end": 1203, "idx": 1}, {"begin": 1204, "end": 1665, "idx": 2}, {"begin": 1666, "end": 1851, "idx": 3}, {"begin": 1852, "end": 2113, "idx": 4}, {"begin": 2114, "end": 3407, "idx": 5}, {"begin": 3408, "end": 3446, "idx": 6}, {"begin": 3447, "end": 3535, "idx": 7}, {"begin": 3536, "end": 3732, "idx": 8}, {"begin": 3746, "end": 3852, "idx": 9}, {"begin": 3853, "end": 4148, "idx": 10}, {"begin": 4149, "end": 4823, "idx": 11}, {"begin": 4824, "end": 5403, "idx": 12}, {"begin": 5404, "end": 6187, "idx": 13}, {"begin": 6214, "end": 6695, "idx": 14}, {"begin": 6724, "end": 7376, "idx": 15}, {"begin": 7401, "end": 8778, "idx": 16}, {"begin": 8813, "end": 9339, "idx": 17}, {"begin": 9340, "end": 9630, "idx": 18}, {"begin": 9631, "end": 10160, "idx": 19}, {"begin": 10186, "end": 12114, "idx": 20}, {"begin": 12115, "end": 13091, "idx": 21}, {"begin": 13113, "end": 14618, "idx": 22}, {"begin": 14632, "end": 14806, "idx": 23}, {"begin": 14849, "end": 15164, "idx": 24}, {"begin": 15165, "end": 15818, "idx": 25}, {"begin": 15819, "end": 16134, "idx": 26}, {"begin": 16155, "end": 17309, "idx": 27}, {"begin": 17342, "end": 18651, "idx": 28}, {"begin": 18680, "end": 19564, "idx": 29}, {"begin": 19565, "end": 20003, "idx": 30}, {"begin": 20019, "end": 20305, "idx": 31}, {"begin": 20319, "end": 20702, "idx": 32}, {"begin": 20728, "end": 21181, "idx": 33}], "SectionHeader": [{"begin": 0, "end": 878, "idx": 0}], "SectionReference": [{"begin": 21195, "end": 26972, "idx": 0}], "Sentence": [{"begin": 55, "end": 197, "idx": 0}, {"begin": 198, "end": 327, "idx": 1}, {"begin": 328, "end": 878, "idx": 2}, {"begin": 896, "end": 1033, "idx": 3}, {"begin": 1034, "end": 1203, "idx": 4}, {"begin": 1204, "end": 1387, "idx": 5}, {"begin": 1388, "end": 1464, "idx": 6}, {"begin": 1465, "end": 1665, "idx": 7}, {"begin": 1666, "end": 1851, "idx": 8}, {"begin": 1852, "end": 2002, "idx": 9}, {"begin": 2003, "end": 2113, "idx": 10}, {"begin": 2114, "end": 2418, "idx": 11}, {"begin": 2419, "end": 2495, "idx": 12}, {"begin": 2496, "end": 2690, "idx": 13}, {"begin": 2691, "end": 2796, "idx": 14}, {"begin": 2797, "end": 3087, "idx": 15}, {"begin": 3088, "end": 3217, "idx": 16}, {"begin": 3218, "end": 3407, "idx": 17}, {"begin": 3408, "end": 3446, "idx": 18}, {"begin": 3447, "end": 3535, "idx": 19}, {"begin": 3536, "end": 3732, "idx": 20}, {"begin": 3746, "end": 3852, "idx": 21}, {"begin": 3853, "end": 4148, "idx": 22}, {"begin": 4149, "end": 4266, "idx": 23}, {"begin": 4267, "end": 4513, "idx": 24}, {"begin": 4514, "end": 4654, "idx": 25}, {"begin": 4655, "end": 4823, "idx": 26}, {"begin": 4824, "end": 5030, "idx": 27}, {"begin": 5031, "end": 5244, "idx": 28}, {"begin": 5245, "end": 5403, "idx": 29}, {"begin": 5404, "end": 5561, "idx": 30}, {"begin": 5562, "end": 5704, "idx": 31}, {"begin": 5705, "end": 5841, "idx": 32}, {"begin": 5842, "end": 6005, "idx": 33}, {"begin": 6006, "end": 6111, "idx": 34}, {"begin": 6112, "end": 6187, "idx": 35}, {"begin": 6214, "end": 6337, "idx": 36}, {"begin": 6338, "end": 6524, "idx": 37}, {"begin": 6525, "end": 6612, "idx": 38}, {"begin": 6613, "end": 6684, "idx": 39}, {"begin": 6685, "end": 6695, "idx": 40}, {"begin": 6724, "end": 6801, "idx": 41}, {"begin": 6802, "end": 7376, "idx": 42}, {"begin": 7401, "end": 7558, "idx": 43}, {"begin": 7559, "end": 7682, "idx": 44}, {"begin": 7683, "end": 7787, "idx": 45}, {"begin": 7788, "end": 7969, "idx": 46}, {"begin": 7970, "end": 8137, "idx": 47}, {"begin": 8138, "end": 8238, "idx": 48}, {"begin": 8239, "end": 8345, "idx": 49}, {"begin": 8346, "end": 8463, "idx": 50}, {"begin": 8464, "end": 8578, "idx": 51}, {"begin": 8579, "end": 8709, "idx": 52}, {"begin": 8710, "end": 8778, "idx": 53}, {"begin": 8813, "end": 8930, "idx": 54}, {"begin": 8931, "end": 9044, "idx": 55}, {"begin": 9045, "end": 9339, "idx": 56}, {"begin": 9340, "end": 9630, "idx": 57}, {"begin": 9631, "end": 9758, "idx": 58}, {"begin": 9759, "end": 9912, "idx": 59}, {"begin": 9913, "end": 9998, "idx": 60}, {"begin": 9999, "end": 10084, "idx": 61}, {"begin": 10085, "end": 10160, "idx": 62}, {"begin": 10186, "end": 10881, "idx": 63}, {"begin": 10882, "end": 11045, "idx": 64}, {"begin": 11046, "end": 11245, "idx": 65}, {"begin": 11246, "end": 11563, "idx": 66}, {"begin": 11564, "end": 11637, "idx": 67}, {"begin": 11638, "end": 11783, "idx": 68}, {"begin": 11784, "end": 11946, "idx": 69}, {"begin": 11947, "end": 12059, "idx": 70}, {"begin": 12060, "end": 12114, "idx": 71}, {"begin": 12115, "end": 12184, "idx": 72}, {"begin": 12185, "end": 12277, "idx": 73}, {"begin": 12278, "end": 12389, "idx": 74}, {"begin": 12390, "end": 12473, "idx": 75}, {"begin": 12474, "end": 12550, "idx": 76}, {"begin": 12551, "end": 12709, "idx": 77}, {"begin": 12710, "end": 12821, "idx": 78}, {"begin": 12822, "end": 12905, "idx": 79}, {"begin": 12906, "end": 13091, "idx": 80}, {"begin": 13113, "end": 13244, "idx": 81}, {"begin": 13245, "end": 13346, "idx": 82}, {"begin": 13347, "end": 13417, "idx": 83}, {"begin": 13418, "end": 13557, "idx": 84}, {"begin": 13558, "end": 13785, "idx": 85}, {"begin": 13786, "end": 13891, "idx": 86}, {"begin": 13892, "end": 14025, "idx": 87}, {"begin": 14026, "end": 14305, "idx": 88}, {"begin": 14306, "end": 14410, "idx": 89}, {"begin": 14411, "end": 14618, "idx": 90}, {"begin": 14632, "end": 14806, "idx": 91}, {"begin": 14849, "end": 14998, "idx": 92}, {"begin": 14999, "end": 15131, "idx": 93}, {"begin": 15132, "end": 15164, "idx": 94}, {"begin": 15165, "end": 15261, "idx": 95}, {"begin": 15262, "end": 15371, "idx": 96}, {"begin": 15372, "end": 15449, "idx": 97}, {"begin": 15450, "end": 15482, "idx": 98}, {"begin": 15483, "end": 15818, "idx": 99}, {"begin": 15819, "end": 15984, "idx": 100}, {"begin": 15985, "end": 16134, "idx": 101}, {"begin": 16155, "end": 16273, "idx": 102}, {"begin": 16274, "end": 16491, "idx": 103}, {"begin": 16492, "end": 16572, "idx": 104}, {"begin": 16573, "end": 16670, "idx": 105}, {"begin": 16671, "end": 16787, "idx": 106}, {"begin": 16788, "end": 16920, "idx": 107}, {"begin": 16921, "end": 17023, "idx": 108}, {"begin": 17024, "end": 17170, "idx": 109}, {"begin": 17171, "end": 17309, "idx": 110}, {"begin": 17342, "end": 17492, "idx": 111}, {"begin": 17493, "end": 17601, "idx": 112}, {"begin": 17602, "end": 17719, "idx": 113}, {"begin": 17720, "end": 18051, "idx": 114}, {"begin": 18052, "end": 18248, "idx": 115}, {"begin": 18249, "end": 18324, "idx": 116}, {"begin": 18325, "end": 18416, "idx": 117}, {"begin": 18417, "end": 18558, "idx": 118}, {"begin": 18559, "end": 18651, "idx": 119}, {"begin": 18680, "end": 18778, "idx": 120}, {"begin": 18779, "end": 18859, "idx": 121}, {"begin": 18860, "end": 18958, "idx": 122}, {"begin": 18959, "end": 19002, "idx": 123}, {"begin": 19003, "end": 19120, "idx": 124}, {"begin": 19121, "end": 19275, "idx": 125}, {"begin": 19276, "end": 19328, "idx": 126}, {"begin": 19329, "end": 19490, "idx": 127}, {"begin": 19491, "end": 19564, "idx": 128}, {"begin": 19565, "end": 19697, "idx": 129}, {"begin": 19698, "end": 19822, "idx": 130}, {"begin": 19823, "end": 20003, "idx": 131}, {"begin": 20019, "end": 20114, "idx": 132}, {"begin": 20115, "end": 20199, "idx": 133}, {"begin": 20200, "end": 20305, "idx": 134}, {"begin": 20319, "end": 20463, "idx": 135}, {"begin": 20464, "end": 20537, "idx": 136}, {"begin": 20538, "end": 20702, "idx": 137}, {"begin": 20728, "end": 20859, "idx": 138}, {"begin": 20860, "end": 21076, "idx": 139}, {"begin": 21077, "end": 21181, "idx": 140}], "ReferenceToFigure": [{"begin": 9337, "end": 9338, "target": "#fig_0", "idx": 0}, {"begin": 10684, "end": 10685, "target": "#fig_2", "idx": 1}, {"begin": 11401, "end": 11402, "target": "#fig_0", "idx": 2}, {"begin": 12112, "end": 12113, "target": "#fig_4", "idx": 3}, {"begin": 12819, "end": 12820, "target": "#fig_5", "idx": 4}, {"begin": 13415, "end": 13416, "target": "#fig_7", "idx": 5}, {"begin": 14317, "end": 14318, "target": "#fig_6", "idx": 6}, {"begin": 15162, "end": 15163, "target": "#fig_8", "idx": 7}, {"begin": 15480, "end": 15481, "target": "#fig_8", "idx": 8}, {"begin": 16287, "end": 16288, "target": "#fig_9", "idx": 9}, {"begin": 16795, "end": 16796, "target": "#fig_10", "idx": 10}, {"begin": 17978, "end": 17979, "target": "#fig_0", "idx": 11}, {"begin": 18355, "end": 18356, "target": "#fig_6", "idx": 12}, {"begin": 18736, "end": 18737, "target": "#fig_8", "idx": 13}], "Div": [{"begin": 55, "end": 878, "idx": 0}, {"begin": 881, "end": 3732, "idx": 1}, {"begin": 3734, "end": 6187, "idx": 2}, {"begin": 6189, "end": 6695, "idx": 3}, {"begin": 6697, "end": 7376, "idx": 4}, {"begin": 7378, "end": 8778, "idx": 5}, {"begin": 8780, "end": 10160, "idx": 6}, {"begin": 10162, "end": 13091, "idx": 7}, {"begin": 13093, "end": 14618, "idx": 8}, {"begin": 14620, "end": 14806, "idx": 9}, {"begin": 14808, "end": 14837, "idx": 10}, {"begin": 14839, "end": 16134, "idx": 11}, {"begin": 16136, "end": 17309, "idx": 12}, {"begin": 17311, "end": 18651, "idx": 13}, {"begin": 18653, "end": 20003, "idx": 14}, {"begin": 20005, "end": 20305, "idx": 15}, {"begin": 20307, "end": 20702, "idx": 16}, {"begin": 20704, "end": 21181, "idx": 17}], "SectionMain": [{"begin": 878, "end": 21181, "idx": 0}]}}