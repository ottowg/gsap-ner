{"text": "Adversarially-regularized mixed effects deep learning (ARMED) models improve interpretability, performance, and generalization on clustered (non-iid) data\n\nAbstract:\nNatural science datasets frequently violate assumptions of independence. Samples may be clustered (e.g. by study site, subject, or experimental batch), leading to spurious associations, poor model fitting, and confounded analyses. While largely unaddressed in deep learning, this problem has been handled in the statistics community through mixed effects models, which separate cluster-invariant fixed effects from cluster-specific random effects. We propose a general-purpose framework for Adversarially-Regularized Mixed Effects Deep learning (ARMED) models through non-intrusive additions to existing neural networks: 1) an adversarial classifier constraining the original model to learn only cluster-invariant features, 2) a random effects subnetwork capturing cluster-specific features, and 3) an approach to apply random effects to clusters unseen during training. We apply ARMED to dense, convolutional, and autoencoder neural networks on 4 applications including simulated nonlinear data, dementia prognosis and diagnosis, and live-cell image analysis. Compared to prior techniques, ARMED models better distinguish confounded from true associations in simulations and learn more biologically plausible features in clinical applications. They can also quantify inter-cluster variance and visualize cluster effects in data. Finally, ARMED improves accuracy on data from clusters seen during training (up to 28% vs. conventional models) and generalization to unseen clusters (up to 9% vs. conventional models).\n\nMain:\n\n\n\n1 INTRODUCTION\nI N predictive modeling, one often assumes that data is independent and identically distributed (iid), such that no samples are correlated or interdependent. However, this assumption is frequently violated in the natural sciences when samples are clustered. For example, many multisite neurological studies acquire cognitive scores using a different human rater at each site, which are subject to interrater differences [1], [2], [3]. As a result, these measurements have inherent intra-site correlation and inter-site variability. Another example is medical imaging, such as magnetic resonance imaging (MRI), where differences in imaging protocol and scanner hardware lead to substantial site effects in multi-site studies [4], [5]. Clustering also occurs in biological data, such as when measurements are collected across different experimental batches [6] or tissue samples [7], and in environmental data collected across locations [8].\nIf not properly handled in analysis, the cluster effects of\n\u2022 The authors were with the Lyda Hill Department of Bioinformatics, University of Texas Southwestern Medical Center, Dallas, TX, 75390. E-mail: kevin3.nguyen@utsouthwestern.edu, albert.montillo@utsouthwestern.edu\n\u2022 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).\nAs such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how to apply/ADNI Acknowledgement List.pdf non-iid data can lead to erroneous conclusions. The so-called Simpson's paradox occurs when an association between two variables appears, disappears, or even reverses when analysis is performed at the population level vs. when analysis is stratified by cluster, indicating a confounding effect. This situation can lead to Type I (false positive) or Type II (false negative) findings in many situations, including clinical studies [9], proteomics [7], and economics [10]. Despite these consequences, the machine learning community has generally ignored the problems underlying noniid data. Meanwhile, the traditional statistics community has addressed clustered data with mixed effects models, which learn a combination of fixed and random effects. The most common of these is the linear mixed effects (LME) model, which builds upon the basic linear regression model. Suppose that we have data X \u2208 R n\u00d7p with n samples and p independent variables (features), originating from c clusters, and a dependent variable (target) y \u2208 R n\u00d71 . We can define the following LME regression model; for a sample i = 1, 2, ..., n originating from cluster j = 1, 2, ..., c we have: \u0177i = \u03b2 0 + x i,1 \u03b2 1 + ... + x i,p \u03b2 p +u j,0 + x i,1 u j,1 + ... + x i,p u j,p + i (1)= \u03b2 0 + x i \u03b2 + u j,0 + x i u j + i\nwhere \u0177i is the predicted target, x i = [x i,1 , ..., x i,p ] is the p-dimensional feature vector of the i th sample from X, and\n\nPREPRINT\nThe fixed effect intercept \u03b2 0 and slopes \u03b2 = [\u03b2 1 , ..., \u03b2 p ] are cluster-invariant and apply globally to all samples. The random effects weights include the intercept u j,0 and slopes u j = [u j,1 , ..., u j,p ], whose values are specific to each cluster j. The random effect weight values are assumed to follow a random distribution, most often a multivariate normal distribution with mean 0, i.e. u \u223c N (0, \u03c3). Consequently, the random effect weights u can be interpreted as clusterspecific offsets from the fixed effect weights \u03b2. The LME model separates the variance explained by global associations from the inter-cluster variance, controls for correlated samples, and improves weight estimates [8], [11]. Unfortunately, proper handling of mixed effects in deep learning, delivering all of these gains, has gone unanswered. In this work, we describe how appropriate handling of mixed effects can address the inadequacies of deep learning models when applied to clustered data.\n\n1.1 Related work\nPrevious deep learning approaches for clustered data have key limitations. A naive but prevalent strategy is to insert cluster information into the model as an additional, onehot encoded covariate [12]. This increases data dimensionality, which may cause overfitting with a high number of clusters c [12], [13], and it entangles the cluster-invariant and cluster-specific features within the model weights, hampering model interpretation. Domain adaptation techniques train a model on a source domain (i.e. cluster), then adapt it in a subsequent training step to a target domain [14]. This yields an adapted model for each target domain but not a single unified model. It also does not scale easily to many domains or separate domain-invariant from domainspecific features, which also limits interpretability. Domain generalization techniques address some of these weaknesses by producing a single generalized model agnostic to domain differences. Earlier approaches used gradient reversal layers, which modify backpropagation to maximize domain invariance [15], [16]. Other methods use meta-learning to guide gradient descent in a direction that reduces the loss for all domains [17], [18]. However, these involve second-order optimization which vastly increase computational cost. A third category of domain generalization methods uses an adversarial classifier [19], [20]. The adversarial classifier learns to classify domains from the latent features of the main model, while the main model learns features that maximize domain classification error. The common limitation of all domain generalization techniques is that they produce a model that has only learned the domain-invariant features (fixed effects), while domain-specific information (random effects), are discarded. Our proposed framework captures this ignored information in a separate random effects subnetwork, while an adversarially-regularized subnetwork captures global fixed effects. We show that this adds predictive value and allows users to understand more about cluster variance in their data.\nTo date, there have been three prior approaches to incorporate mixed effects into deep learning. Xiong et al. proposed MeNet, a mixed effects convolutional neural network (CNN), for a gaze estimation dataset containing repeated images per subject [21]. While improving accuracy, the method requires an expensive expectation-maximization algorithm with inversion of large covariance matrices (n j \u00d7n j where n j is the number of samples within each cluster). MeNet also only models random slopes and not intercepts. Next, Tran et al. proposed DeepGLMM, a mixed effects approach for dense feedforward neural networks (DFNNs) using Bayesian deep learning and variational inference for more efficient training [22]. Though theoretically capable of modeling both random slopes and intercepts, their applications only used models with random intercepts. Their experiments also lacked comparisons with other deep learning methods. Finally, Simchoni et al. proposed LMMNN, a mixed effects approach for both DFNNs and CNNs, and demonstrated a performance benefit across multiple applications. However, LMMNN is trained using expensive covariance matrix inversions, and their real-world applications use only random intercepts [13].\nThere are several common limitations across the MeNet, DeepGLMM, and LMMNN approaches. These methods prioritize the improvement of predictive performance and ignore the additional interpretability afforded by mixed effects, such as quantification and visualization of intercluster variance. They also lack explicit guidance of the fixed effects to be cluster-invariant, so their resilience to confounded associations is unclear. Additionally, none of these works demonstrate models with both random slopes and intercepts or unsupervised learning models, such as autoencoders. Lastly, there are no specific recommendations for applying these models to new data that does not originate from the same clusters seen during training, which limits real-world utility where data from new clusters is frequently encountered.\n\n1.2 Contributions\nWe propose an Adversarially-Regularized Mixed Effects Deep learning (ARMED) framework that generalizes across model archetypes and alleviates the shortcomings of the previous approaches. This framework contains three components that can be readily added to a conventional deep learning model with minimal modification of the existing architecture. First, inspired by domain generalization, we employ an adversarial classifier to regularize the model to learn cluster-invariant fixed effects. We show through simulations that this improves the separation of cluster-specific, potentially confounded features from cluster-invariant features. Second, we introduce a Bayesian random effects subnetwork to learn the cluster-specific features, and we demonstrate how it can quantify and visualize the variance across clusters. Third, we add another classifier which infers random effects for so-called \"unseen cluster\" data, where samples originate outside the clusters seen during training. We demonstrate the advantages of our framework across 4 test cases using DFNNs, CNNs, and convolutional autoencoders, including simulations and three biomedical examples. In each case, we achieve not only the separation and identification of fixed and random effects, but also better predictive performance on data from seen clusters and better generalization to unseen clusters.\n\nZ-predictor for unseen clusters\n\n\nFixed effect prediction\n\n\nMixed effect prediction Cluster membership\n\n\nInput data\nFig. 1. The ARMED framework for a generic neural network. The conventional model (blue area) predicts \u0177F from the data sample x. Cluster membership of the sample is one-hot encoded into z. The fixed effects subnetwork (blue + gray areas) is constructed by adding an adversarial classifier (gray area) to predict cluster membership \u1e91. The original model is penalized through the generalization loss for learning features that allow cluster membership prediction. The random effects subnetwork (orange area) uses Bayesian layers to learn cluster-specific weights, dependent on z, that follow zero-mean multivariate normal distributions. These weights can be formulated as nonlinear slopes multiplied by the fixed effects latent representation h F (X; \u03b2), linear slopes multiplied by X, and/or intercepts. The fixed and random effects are combined with a mixing function m(...). For prediction on data from clusters unseen during training, z is inferred with a classifier (Z-predictor) trained on data from seen clusters.\n\n2 METHODS\nIn general, a conventional feed-forward neural network computes a nonlinear transformation of the data X through its layers (Fig. 1, blue area). We denote the output of the penultimate layer as h(X, \u03b2) \u2208 R n\u00d7q , where \u03b2 contains all learned weights up to and including this layer and q is the number of neurons. For a typical regression or classification task, a final linear or softmax output layer o then transforms h(X, \u03b2) into the final prediction output \u0177:\u0177 = o(h(X, \u03b2))\nDuring training, the model finds the weights, \u03b2, which minimize a given loss function quantifying the error for the predictive task, L e (y, \u0177).\nTo encode cluster membership information for a dataset with n samples and c clusters, we introduce a one-hot encoded design matrix Z \u2208 R n\u00d7c , where Z i,j = 1 if sample i belongs to cluster j and Z i,j = 0 otherwise. The following sections present a description of the ARMED framework components, agnostic to model architecture. These components include the fixed effects subnetwork h F , including a conventional neural network and an adversarial classifier a that together learn cluster-invariant features, the random effects subnetwork h R for learning Z-dependent clusterspecific features, the mixing function m that combines the fixed and random effects for prediction, and the Z-predictor used to apply random effects to new clusters.\n\n2.1 Fixed effects subnetwork\nFirst, we add an adversarial classifier (Fig. 1, gray area) to the conventional model (Fig. 1, blue area) to enforce the learning of cluster-invariant fixed effects, creating the Fixed effects subnetwork h F (X; \u03b2). This is based on the adversarial learning technique for domain generalization [19], [20]. For a neural network with L layers, letH F (X; \u03b2 F ) = [h F,1 (X; \u03b2 F,1 ), ..., h F,L (X; \u03b2 F,L )]\nrepresent the collected outputs of each layer, where \u03b2 F,l contains the weights up to the lth layer. We define an adversarial classifier a which predicts a sample's cluster membership from these layer outputs, \u1e90 = a(H F (X; \u03b2 F ); \u03b2 A ), where \u03b2 A contains the weights for this adversary. The adversary is trained to minimize the categorical cross-entropy loss:L CCE (Z, \u1e90) = \u2212 1 n n i=1 c j=1 Z i,j log( \u1e90i,j ) + (1 \u2212 Z i,j ) log(1 \u2212 \u1e90i,j )\nMeanwhile, the main model is penalized for learning features that allow the adversary to predict cluster membership. It must maximize this cross-entropy, which we call the cluster generalization loss. The resulting training objective of the fixed effects subnetwork isL e (y, \u0177F ) \u2212 \u03bb g L CCE (Z, \u1e90)\nwhere the hyperparameter \u03bb g controls the weight of the generalization loss. We use \u0177F to denote the prediction output of this fixed effects subnetwork.\n\n2.2 Random effects subnetwork\nWe next define a second subnetwork to learn the Random effects, h R (X; U (Z)) with cluster-specific weights U (Z) (Fig. 1, orange area). The cluster-specific values for each individual weight u(Z) in U (Z) are assumed to follow a normal distribution with mean 0, i.e. u(Z) \u223c N (0, \u03c3) where \u03c3 represents the inter-cluster variance of each weight. Collectively, \u03a3 contains the inter-cluster variance for all weights in U (Z). We implement these weights using a Bayesian formulation. We specify a zero-mean normal prior distribution for each weight p(U ) \u223c N (0, \u03c3 p ) with the fixed prior variance \u03c3 p as a global hyperparameter. The posterior distribution p(U |X) is then learned through variational inference, which reframes Bayesian modeling as an optimization problem that can be efficiently handled through gradient descent [23], [24]. The objective of variational inference is to learn a surrogate posterior q(U ), here a multivariate normal distribution, which closely approximates the true posterior p(U |X), where \"closeness\" is measured by the Kullback-Leibler (KL) divergence:D KL (q(U )||p(U |X)) = q(U ) log q(U ) p(U |X) dU\nMinimizing D KL (q(U )||p(U |X)) directly is impossible because computing the posterior through Bayes Rule, p(U |X) = p(X|U )p(U ) p(X)\n, involves the intractable marginalization p(X). Instead, variational inference maximizes the Evidence Lower Bound (ELBO) which contains fully tractable and differentiable quantities:ELBO = E q [log p(X|U )] \u2212 D KL (q(U )||p(U ))\nwhere the first right-hand term is the log-likelihood and the second term is the KL divergence between the surrogate posterior and the prior. For gradient descent, we minimize the negative ELBO and let L e (y, \u0177) represent the first term, i.e. the negative log-likelihood loss. This yields the following objective:\nL e (y, \u0177)+ \u03bb K D KL (q(U )||p(U ))\nwith the hyperparameter \u03bb K controlling the strength of the regularization to the prior. Note that our method based on variational inference does not require expensive inversions of covariance matrices as in MeNet and LMMNN [13], [21].\nThe architecture of this subnetwork will depend on the types of random effects to be modeled. Nonlinear random effects slopes can be modeled as weights multiplied by the fixed effects latent representation h F (X; \u03b2):h R,nlin (x i ; u nlin (z i )) = h F (x i ; \u03b2) u nlin (z i )\nwhere z i and x i are the rows in Z and X for the ith sample and u nlin (z i ) \u2208 R q\u00d71 returns the slopes for cluster z i , q being the number of output neurons of h F (x i ; \u03b2). A random intercept is modeled simply as a weight:h R,int (u(z i )) = u int (z i )\nwhere u int (z i ) is a scalar value. Additionally, for tabular data, we can model linear random effects slopes multiplied directly with X, which allows each slope to be interpreted directly with respect to a corresponding input variable:h R,lin (x i ; u lin (z i )) = x i u lin (z i )\nwhere u lin (z i ) \u2208 R p\u00d71 returns the slopes for cluster z i . The random effects subnetwork outputs the sum of these random effects:h R (x i ; U (z i )) = h R,nlin (x i ; u nlin (z i )) +h R,lin (x i ; u lin (z i ))\n+h R,int (u int (z i ))\nThese three cases will apply to most models with a dense penultimate layer producing a vector-form h F (X; \u03b2). For models such as autoencoders, we describe in the Supplemental Materials how random effects can be readily applied across multiple convolutional layers (Section 3.1.3, Fig. S4).\n\n2.3 Combining fixed and random effects\nWe construct the final ARMED model by combining the outputs of the fixed effects and random effects subnetworks.\nIn the linear model of Eq. 1, random and fixed effects were combined through addition. For greater flexibility here, we substitute the addition in Eq. 1 with a more general mixing function m(...).\u0177M = m(h F (X; \u03b2), h R (X; U (Z)))\nFor example, in the following binary classification applications, we use a nonlinear analog of Eq. 1. We add h R (X; U (Z)) to the logit of \u0177F (equal to h F (x i ; \u03b2) \u03b2 L where \u03b2 L are the weights of the output layer), then apply the sigmoid activation function:\u0177M = sigmoid h F (x i ; \u03b2) \u03b2 L + h R (x i ; U (z i ))\nThe objective function is obtained by combining Eq. 2 and Eq. 3:\nL e (y, \u0177M ) + \u03bb F L e (y, \u0177F )\u2212\u03bb g L CCE (Z, \u1e90) + \u03bb K D KL (q(U )||p(U ))\nThe second term ensures that the fixed effect subnetwork will still be capable of prediction on its own so that the fixed effect features will be meaningful in later analyses. The loss weight \u03bb F < 1 balances the fixed effect error with the mixed effect error L e (y, \u0177M ). ARMED includes these hyperparameters: the generalization loss weight \u03bb g , the KL divergence weight \u03bb K , the fixed effect prediction error weight \u03bb F , and the prior distribution variance \u03c3 p . Usage of linear vs. nonlinear slopes must also be considered. In practice, we find that these can be easily tuned for model performance using standard hyperparameter optimization approaches, such as random search or Bayesian optimization, and appropriate cross-validation.\n\n2.4 Prediction on unseen clusters\nThe previous mixed effects deep learning approaches provide no method for using the learned random effects when predicting on data not from clusters seen during training, i.e. not included in Z [13], [21], [22]. The authors of LMMNN propose to use only the fixed effects of their model on unseen clusters [13]. While the learned fixed effects, by definition, represent population-average associations, new data is not necessarily free of cluster effects and performance may be improved by fully utilizing the learned random effects. We propose to infer Z for unseen cluster data using a classifier we call the Z-predictor. We train this classifier to predict Z from X on the data from seen clusters, then use it to infer Z for data from unseen clusters. The unthresholded softmax predictions from the classifier provide a weighted combination of seen clusters that are most similar to each unseen cluster sample. In our applications, the Z-predictor uses the same architecture as the adversarial classifier.\n\n2.5 Applications\n\n\n2.5.1 Applications of ARMED to dense feedforward neural networks\nOur first architectural application of ARMED is to a dense feedforward neural network (DFNN), which is suited to tabular data such as clinical measurements or pre-engineered image features. We describe the specifics of the ARMED-DFNN architecture in Fig. S1 and the Supplemental Materials 3.1.1.\n\nSpiral classification simulations:\nFirst, we evaluated the ARMED-DFNN on a simulated classification problem where cluster effects can be controlled, model-learned information can be compared to ground truth, and known confounded features can be added. The simulations are built upon the well-known spiral classification problem, where points must be classified into one of two spirals based on their coordinates x 1 and x 2 [25]. We simulated a nonlinear random effect by dividing the points into 10 clusters and randomly varied the spiral radius across clusters (Fig. S2). There were 3 variations of this simulation: 1) spiral radii varied across clusters, 2) spiral radii varied across clusters and spiral labels were inverted in half of the clusters (a more severe random effect), and 3) spiral radii varied across clusters and 2 known confounded probe features x 3 and x 4 were added. These probes created a spurious association between cluster and label but were not associated with the underlying spiral functions. Further details on these simulations can be found in the Supplemental Materials 3.2. Because we have defined the random effects to be nonlinear, we used an ARMED-DFNN architecture with nonlinear random slopes (Eq. 4) and a random intercept (Eq. 5).\nTo test the ability of the fixed effects subnetwork to correctly downweight these confounded probes, we measured feature importance by computing the gradient of the model output with respect to the input features [26], [27]. Features with larger gradient magnitudes are more important in forming the model output. We compared the importance of each confounded probe (x 3 and x 4 ) to that of the least important true feature (x 1 or x 2 ).\n\nMild cognitive impairment conversion prediction:\nFor a complementary real-world application, the ARMED-DFNN was used to predict the future development of full Alzheimer's Disease (AD) in subjects with mild cognitive impairment (MCI). MCI is an early stage of cognitive decline that may progress to dementia. Our target was to distinguish progressive MCI (pMCI), where a subject converts to AD within 24 months of baseline observation, from stable MCI (sMCI), where the subject does not convert within 24 months. We used data from the Alzheimer's Disease Neuroimaging Initiative, which includes baseline demographic information, cognitive scores, neuroimaging measurements, and biomarker measurements, as well as longitudinal diagnoses for each participant, acquired with informed consent and institutional review board approval (Supplemental Materials 3.3.1). The training dataset came from the largest 20 study sites, and we used site as the random effect cluster. Inter-site variance has been shown to affect cognitive scores, which are sensitive to judgments by human raters, and neuroimaging, which is sensitive to MRI scanner parameters [1], [3], [28]. We held out the remaining 34 sites to evaluate model performance on sites unseen during training. Performance metrics included area under the receiver operating characteristic curve (AUROC), balanced accuracy, sensitivity, and specificity. For this application, we used an architecture with linear random slopes (Eq. 6) and a random intercept (Eq. 5). These were chosen to allow direct interpretation of the learned random slopes and inter-site variance for each input feature.\nAs with the spiral simulations, we subsequently added simulated confounded probe features to test how well each model could downweight known confounded features. We generated 5 confounded probes that were nonlinearly asso-ciated with site and with the probability of being labeled pMCI but had no real biological relevance (Supplemental Materials 3.3.1). We then compared how highly each model ranked the probes based on feature importance (gradient magnitudes).\n\n2.5.2 Application of ARMED to convolutional neural networks\nWe next applied our approach to a convolutional neural network (CNN), another important deep learning archetype, creating an ARMED-CNN capable of learning nonlinear random slopes and random intercepts. Architecture details are described in Fig. 3 and Supplemental Materials 3.1.2.\nWe applied the ARMED-CNN to the classification of AD vs. cognitively normal (CN) structural MRI, with study site as the random effect cluster. We acquired T1-weighted MRI from 12 sites in the ADNI dataset (inclusion criteria and preprocessing details are in Supplemental Materials 3.3.2). These 12 sites were selected to emphasize the confounding site effect, where sites using General Electric MRI scanners had a greater proportion of AD subjects compared to sites using Philips or Siemens scanners (Table S1). The remaining 51 sites were held out to evaluate performance on sites unseen during training. We extracted a two-dimensional coronal slice through the hippocampi from each image. Performance metrics included AUROC, balanced accuracy, sensitivity, and specificity.\n\n2.5.3 Application of ARMED to autoencoders\nTo demonstrate our framework on unsupervised learning models, we developed a mixed effects autoencoder. Our fourth application was the melanoma live-cell image compression and phenotypic classification problem described in [6]. In this work, the authors used a convolutional autoencoder to compress the images into a vector latent representation, then trained a classifier to label cells as having either high or low metastatic efficiency. They revealed that batch effects are prominent in this dataset, due to discrepancies between image batches acquired across different days, and that the latent representations strongly segregated by batch. The dataset is described further in Supplemental Materials 3.4. The training data from the melanoma cell image dataset contained images acquired over 13 days (batches), and the remaining 11 days were held out as unseen batches.\nWe extended their autoencoder architecture by connecting the metastatic efficiency classifier directly to the autoencoder and training the autoencoder-classifier (AEC) end-toend. We then applied our ARMED framework to create an ARMED-AEC, containing a fixed effects subnetwork that produces batch-invariant latent representations and a random effects subnetwork that learns how the batch effects alter image appearance (Fig. S4). Our hypothesis was that the modeling of mixed effects would improve classification performance over the base AEC. This architecture is described in Supplemental Materials 3.1.3.\nIn addition to evaluating the reconstruction error (MSE) and phenotype prediction performance (AUROC), we also measured how strongly each model's latent representations clustered by batch. We computed the Davies-Bouldin (DB) score, where lower values indicate stronger clustering [29], and the Calinksi-Harabasz (CH) score, where higher values indicate stronger clustering [30]. Consequently, we desire a higher DB score and lower CH score to achieve batchinvariant latent representations.\n\n2.6 Compared methods and ablation tests\nIn each application, we compared the proposed mixed effects model with the following approaches. First, we tested a conventional neural network where the cluster membership Z is disregarded and data is assumed to be iid. Second, for the DFNN and CNN, we tried the \"cluster input\" approach of treating the one-hot cluster membership Z as a categorical covariate, i.e. an additional model input. For the DFNN, Z was simply concatenated to X. For the CNN, Z was concatenated to flattened output of the last convolutional layer, before the dense hidden layer. When evaluating on unseen clusters, we used the inferred Z from the Z-predictor. Third, we also compared to meta-learning domain generalization (MLDG) [17]. However, due to the high computational cost of second-order gradients in MLDG (training took 10 times longer than the conventional DFNN) and poor performance, we dropped the MLDG comparison for the other applications, after the spiral simulation application. Fourth, we tested a domain adversarial (DA) neural network, i.e. the fixed effect subnetwork by itself. Despite regularization to learn only fixed effects, it does not model any cluster-specific random effects. Finally, for the DFNN and CNN, we tested MeNet [21] and LMMNN [13]. For the autoencoder, only the proposed ARMED approach has a suitable adaptation.\nAdditionally, we performed two ablation tests of the proposed mixed effects approach. We first trained the ARMED models without the adversarial classifier (\"w/o Adv.\") to test the necessity of the generalization loss to learn fixed effects. Additionally, we evaluated the model using randomly-assigned cluster memberships in Z (\"randomized Z\"). For data from seen clusters, this tested whether the model truly learned cluster-specific effects. For data from unseen clusters, this tested the impact of using the Zpredictor to infer cluster membership.\n\n3 RESULTS\n\n\n3.1 Spiral classification simulations\nThe classification accuracy of each model, with 10-fold cross-validation, is presented in Table 1. In simulation 1 (random cluster-specific radii distributed around 1), the ARMED-DFNN outperformed all other models and had statistically significantly higher accuracy than the secondbest model, MeNet (78.8% vs. 77.4%, p = 0.003 in paired T-test). It was also uniquely able to learn appropriate cluster-specific decision boundaries that scaled in size with the cluster-specific spiral radii (Fig. 2). For example, cluster 1 (left column) has the smallest ground truth radius (green dashed line), and while cluster 2 (middle column) has the largest true radius, and the ARMED-DFNN uniquely learned this difference. In simulation 2 (greater inter-cluster variance, spiral labels inverted in half), only the cluster input DFNN, MeNet, and ARMED-DFNN achieved accuracy substantially higher than chance (50%), with 67.1%, 53.3%, and 65.0% respectively. The cluster input DFNN and ARMED-DFNN statistically significantly outperformed MeNet (p 0.001), but did not differ significantly from each other at p < 0.05. In simulation 3 (confounded probe features added), the cluster input ranked first (76.4%), followed by the ARMED-DFNN (74.5%) and MeNet (73.0%). However, the ARMED-DFNN more effectively downweighted the 2 confounded features compared to the true features (T-statistic = 12.631 and 18.173) compared to the cluster input DFNN (T-statistic = 5.346 and 5.042) and MeNet (T-statistic = 7.923 and 4.541) (Table 2). The conventional and meta-learning models placed greater importance on the confounded than the true features.\nIn ablation tests, removing adversarial regularization non-significantly improved the accuracy of the ARMED-DFNN in simulations 1 and 2, but decreased accuracy in simulation 3. It also worsened the separation of confounded and true features in simulation 3 (T = 1.390 and 0.343). Using randomly assigned cluster memberships in Z uniformly decreased performance, confirming that the ARMED-DFNN learned necessary cluster-specific information.\n\n3.2 MCI conversion prediction\nThe performance of each model in classifying pMCI vs. sMCI, over 10\u00d710 nested cross-validation folds, is compared in Table 3. On study sites seen during training, the ARMED-DFNN outperformed all other models in AUROC, accuracy, and specificity (Table 3, top). The AUROC of the ARMED-DFNN was statistically significantly higher than that of the second-best model, the conventional DFNN (0.926 vs. 0.884, p = 0.048). On held-out study sites unseen during training, the ARMED-DFNN again outperformed all other models in AUROC, accuracy, and specificity (Table 3, bottom). The AUROC of the ARMED-DFNN was statistically significantly higher than that of the second-best conventional DFNN (0.837 vs. 0.806, p = 0.007). The DA-DFNN performed the poorest on both seen and unseen sites, with AUROC of 0.811 and 0.723 respectively.\nRemoving the adversarial regularization of the ARMED-DFNN reduced AUROC (0.926 to 0.919) and accuracy (81.9% to 81.4%) on seen sites and accuracy (75.6% to 73.5%) and sensitivity (72.4% to 65.4%) on unseen sites. On seen sites, randomizing the site assignments reduced all metrics, including AUROC from 0.926 to 0.889. On unseen sites, using random instead of inferred site assignments also reduced all metrics including sensitivity from 72.4% to 69.8%.\nWe examined the feature importance ranking, based on the fixed effects subnetwork, and learned site-specific random slopes, based on the random effects subnetwork, of the ARMED-DFNN (Fig. 3). Demographic features including, race, ethnicity, and marital status had especially low intersite variance. Cognitive scores such as the Clinical Dementia Rating Sum of Boxes (CDR-SB) and and Mini Mental State Exam (MMSE) had especially high inter-site variance. These results are further discussed in Section 4.3.1. Feature importance rankings for all 6 DFNNs are presented in Fig. S6. We also examined the site-specific random intercepts of the ARMED-DFNN and found they correlated strongly with the percentage of pMCI subjects at each site (Pearson's r = 0.860, p < 10 \u22125 ), indicating the random intercepts captured the variability in class balance across sites, a major source of confounding effect. When simulated confounded probe features were added, the ARMED-DFNN ranked these probes the lowest. The 10 highest ranked features for each model are shown in Fig.\n\n3.3 AD diagnosis\nThe cross-validated performance of each model in classifying brain MRIs as CN vs. AD is presented in  LMMNN had the lowest AUROC on the unseen sites, indicating poor generalization. Without adversarial regularization, the performance of the ARMED-CNN increased slightly on unseen sites (mean AUROC 0.645 to 0.655) but decreased on seen sites (mean AUROC 0.900 to 0.816).\nRandomizing the site membership for seen sites drastically reduced all metrics, including mean AUROC from 0.900 to 0.585. On unseen sites, randomizing instead of inferring site membership reduced mean AUROC from 0.645 to 0.551.\nGradient-weighted Class Activation Mapping (Grad-CAM) visualizations from each model revealed differences in the features learned (Fig. 5) [31]. The conventional, cluster input, MeNet, and LMMNN CNNs attributed more weight to regions in the edges of each image, near the periphery of the brain. However, the DA-CNN emphasized medial brain areas, including the hippocampi and surrounding parahippocampal gyri. For the ARMED-CNN, we produced Grad-CAMs using the fixed effects subnetwork, which contains  the learned cluster-invariant features. Like the DA-CNN, the ARMED-CNN also emphasized medial brain areas but gave additional weight to the superior regions including the lateral ventricles. Furthermore, we created separate Grad-CAMs to visualize the distinct site-specific features learned by the ARMED-CNN random effects subnetwork (Fig. S7), which involved the image periphery for some sites and more medial areas for others.\n\n3.4 Cell image compression and classification\nThe performance of each AEC model in compressing and classifying melanoma live-cell images is presented in Table 5. For computational efficiency, the pre-trained and frozen DA-AEC was reused as the fixed effects subnetwork of the ARMED-AEC. For the ablation test without adversarial regularization (\"w/o Adv.\"), the pre-trained conventional AEC was reused as the fixed effects subnetwork. Confidence intervals were computed using DeLong's method [32]. On seen batches, (Table 5, first column group) the ARMED-AEC had the highest performance in classifying metastatic efficiency (AUROC 0.869), statistically significantly outperforming the second-best model, the conventional AEC (p < 0.001), and it had the lowest reconstruction error (MSE 0.0012). On unseen batches (Table 5, second column group), the ARMED-AEC again showed the best classification performance (AUROC 0.789). This classification performance was statistically significantly higher than the second-best model, the conventional AEC (p < 0.001). All models had similar reconstruction error (MSE 0.0024) on unseen batches. Examining each AEC's latent representations, the DA-AEC and ARMED-AEC (using the DA-AEC as its fixed effects subnetwork) exhibited much less batch effect contamination. Compared  to the conventional AEC, the DB score improved from 8.885 to 43.009 (484% relative increase) and the CH score improved from 545.9 to 20.4 (96% relative decrease).\nIn the ablation tests, removing the domain adversarial regularization of the fixed effects subnetwork in the ARMED-AEC (using the conventional AEC as the fixed effects subnetwork) slightly increased classification AUROC on seen batches (0.876 vs. 0.869) and on unseen batches (0.791 vs. 0.789). However, this came at the expense of greater batch contamination of the latent representations (DB score 8.885 and CH score 545.9). When cluster assignments were randomized instead of using the true cluster assignments on seen batches, reconstruction MSE worsened from 0.0012 to 0.0018 and classification AUROC decreased from 0.869 to 0.732. On unseen batches, randomized instead of Zpredictor-inferred cluster assignments reduced classification AUROC from 0.789 to 0.712.\nTo visualize the random effects learned by the ARMED-AEC, we generated image reconstructions from the random effects subnetwork with various learned batch-specific effects applied (Fig. 6). These simulate the appearance of an image if it had been acquired within different batches. We compared these with the image reconstructions from the fixed effects subnetwork, where batch effects have been removed. Some batches showed stronger specular highlights (e.g. batches 2 and 5), while others had greater contrast in the cell periphery (e.g. batches 1 and 3).\n\n4 DISCUSSION\n\n\n4.1 General observations\nOur experiments across four applications illustrate the three critical contributions of ARMED. First, we demonstrated that the fixed effects subnetwork of ARMED models assigns feature importance more appropriately than the compared models. In the spiral simulations, the ARMED-DFNN most strongly separated the true and confounded features by feature importance. The conventional and MLDG models erroneously placed greater importance on the confounded probes than the true features, while the cluster input, DA-DFNN, MeNet, and LMMNN models downweighted the confounded probes to a lesser degree than the ARMED-DFNN. In MCI conversion prediction with simulated confounded probes, the ARMED-DFNN ranked the probes statistically significantly lower than any other model, including the DA-DFNN and MeNet. In contrast, the conventional, cluster input, and LMMNN models were most sensitive to the probes. In AD diagnosis, Grad-CAM visualizations showed that the ARMED-CNN highlighted more biologically plausible brain regions than the conventional, cluster input, MeNet, and LMMNN CNN's, which is further discussed in Section 4.3.2.\nSecond, we demonstrated the ability of ARMED to visualize random effects learned by the random effects subnetwork. In MCI conversion prediction, we quantified the learned inter-site variance of the random slopes for each feature. This allowed us to identify which features are most contaminated by site effects, and we discuss these below (Section 4.3.1). In AD diagnosis, we visualized site-specific differences in Grad-CAMs. Finally, in the cell imaging application, we generated image reconstructions showing the impact of learned batch effects.\nThird, ARMED typically outperforms the compared nonmixed effects methods and outperforms or matches the other mixed effects methods. In the spiral simulations, the ARMED-DFNN had either the best or second-best accuracy, while being more discriminative between true and confounded features and learning the most cluster-appropriate decision boundaries. In the MCI conversion application, the ARMED-DFNN outperformed all other methods on both data from seen and unseen sites. In AD diagnosis, the ARMED-CNN performed similarly to MeNet and LMMNN methods on seen sites and competed favorably with the DA-CNN on unseen sites. Meanwhile, MeNet and LMMNN generalized poorly to unseen sites. In the cell imaging application, the ARMED-AEC had the best reconstruction error on data from seen batches, the best metastatic efficiency classification on both seen and unseen batches, and substantially reduced batch effects in its latent representations compared to the conventional AEC. In ablation tests, we found that ARMED models without DA often performed similarly to or non-significantly better than the full model with DA, but their fixed effects subnetworks were more sensitive to confounded probe features. Therefore, we recommend always using the full ARMED model with DA, as any small performance increase comes at the cost of confound susceptibility. We also found that randomizing cluster assignment reduced performance on seen clusters, confirming that the ARMED models had learned clusterspecific information in the random effects subnetworks. Similarly, performance on unseen clusters decreased when using randomized cluster assignment instead of using the Zpredictor to infer cluster membership. This indicates that the Z-predictor is needed to fully exploit the learned random effects when predicting on data from unseen clusters.\nThough we have focused on biomedical data in this work, we anticipate that our approach will be of use to any case where data is non-iid and subject to random effects. Given its flexible and modular nature, the ARMED framework should apply readily to other architecture types besides the three demonstrated here.\n\n4.2 Comparison to prior work\nA common approach to handling clustered data is to include the cluster membership, which is an unordered categorical variable, as additional one-hot encoded covariates in X [12]. This approach is unable to disentangle the cluster-specific random effects and cluster-independent fixed effects, and we found it was more sensitive to simulated confounded probes than ARMED models. We also found inferior performance vs. ARMED, likely due to the high cardinality of the added features which can lead to overfitting [13], [33]. For example, the MCI conversion application had 20 sites and 37 input features, meaning that to add cluster membership to X would increase the width of X by 35%. ARMED is better suited to handling this high-cardinality information by modeling clustering as a random effect, which imposes a normal distribution prior.\nA more recent approach to handling differences across clusters is domain adversarial learning. We showed that DA does improve generalization to data from unseen clusters. However, ARMED improves upon DA, adding a random effects subnetwork to capture the cluster-specific information that DA discards, which results in better performance on clusters seen during training. Using the Z-predictor, this cluster-specific information can also be used when pre-dicting on data from unseen clusters, allowing ARMED to outperform DA on unseen clusters as well.\nThis work remedies key weaknesses in previous approaches to incorporate mixed effects into deep learning. We described specific random effects architectures for random intercepts, linear slopes, and/or nonlinear slopes. This allows greater flexibility than DeepGLMM and LMMNN, which only learn random intercepts, and MeNet, which only learns nonlinear random slopes [13], [21], [22]. Another key improvement was adversarial regularization of the fixed effects subnetwork to learn generalizable, clusteragnostic information. In our experiments with simulated confounders, this allowed ARMED models to appropriately upweight nonconfounded features and downweight confounded features, while MeNet and LMMNN, lacking adversarial regularization, were susceptible to the spurious confounded features. Next, we demonstrated interpretation and visualization of the learned random effects, which was not explored in these previous works. Finally, we evaluated ARMED models on data from clusters unseen during training and provided a method to infer cluster membership and apply learned random effects on this data. The previous works lacked such a method, meaning that the learned random effects cannot be utilized on new data. This is a major limitation for practical applications, where a deployed model may need to be applied to data from a new cluster, such as a new clinical site or patient.\n\n4.3 Application-specific discussions\n\n\n4.3.1 MCI conversion prediction\nThe ARMED-DFNN quantifies the inter-site variance of the learned random slope for each feature (Fig. 3). We found that demographic features such as race and ethnicity had the lowest inter-site variance, which in unsurprising as the association between these features and MCI conversion should not be sensitive to measurement differences across sites. Certain cognitive measurements, however, had distinctly high inter-site variance. The CDR-SB score had the highest variance, which concurs with a previous report that CDR-SB has suboptimal inter-rater reliability in early dementia patients, such as those with MCI [34]. MMSE had the second highest variance in our ARMED-DFNN, again agreeing with previous findings of low inter-rater reliability [35].\nThough we intentionally held out a large portion of the ADNI dataset to evaluate our models on unseen sites, our ARMED-DFNN performed similarly to or better than several published results on predicting 24-month MCI conversion in ADNI using deep learning. Lee et al. achieved 80% accuracy compared to the 81.9% of our ARMED-DFNN [36]. Shi et al. and Lian et al. achieved AUROC of 0.816 and 0.793, respectively, compared to our 0.926 [37], [38]. Note that neither of these studies held out entire study sites for evaluation, and our AUROC on unseen sites (0.837) still exceeded their results on seen sites.\n\n4.3.2 AD diagnosis\nThe Grad-CAMs of the DA-CNN and ARMED-CNN appropriately emphasized the importance of medial brain regions including the hippocampus and surrounding medial temporal lobe, which are involved in AD-related brain atrophy (Fig. 5) [39], [40], [41]. The ARMED-CNN Grad-CAMs also indicated the importance of the lateral ventricles, where enlargement has been connected to AD [40], [42]. The incorporation of these additional structures likely contributed to the better performance of the ARMED-CNN (AUROC 0.900) vs. the DA-CNN (AUROC 0.823). Meanwhile, the conventional, cluster input, MeNet, and LMMNN models relied highly on likely spurious features in the image periphery. Such features appear to be related to site effects on imaging, since the random effects of the ARMED-CNN affect similar peripheral areas (Fig. S7).\nThe performance of our ARMED-CNN compares favorably to previous models using 2D MRI to diagnose AD in the ADNI dataset. We achieved 88.7% accuracy, while Kang et al. report 90.4% and Ebrahimi et al. report 87.5% [43], [44]. However, we trained on a fraction of the total ADNI data that these reports used, holding out the rest for evaluation of models on unseen sites. Consequently, our work focuses on comparisons across architectures, not with previous studies.\n\n4.3.3 Cell image compression and classification\nWe compare our ARMED-AEC results to the previous analysis published by Zaritsky et al. [6]. While they discussed the batch effect present in the latent representations produced by their autoencoder, their methods did not explicitly suppress this batch effect. In contrast, our proposed ARMED-AEC reduced the batch effect in the latent representations by 484% based on the DB score, compared to an unmodified AEC. We also improved classification AUROC to 0.876 compared to their reported 0.723, though this may be partially due to the direct incorporation of the phenotype classifier into the autoencoder, while Zaritsky et al. trained their classifier separately from their autoencoder.\n\n4.4 Limitations\nMixed effects models generally require the presence of several clusters to accurately estimate the random effect distributions; with <4 clusters, LME models provide less of an advantage over generalized linear regression [8], [11]. Consequently, we suggest some caution when using our method for data with fewer than 4 clusters. Additionally, our method applies to datasets with a single level of random effects, but there are often cases with multiple levels of random effects, such as when multiple observations are collected per subject who are then clustered by study site. We plan to extend our methodology to such multi-level cases in future work. Finally, a practical limitation of ARMED is the additional complexity, which may increase training time by approximately 1.5-2x. However, we note that other methods have even greater computation cost, such as metalearning domain generalization (MLDG) which uses secondorder optimization and MeNet and LMMNN which involve expensive matrix inversions [13], [17], [21].\n\n5 CONCLUSION\nOur proposed approach uses powerful mixed effects techniques from traditional statistics to improve the interpretability, reliability, and performance of deep learning models on non-iid data. ARMED models separately learn random effects and fixed effects in distinct subnetworks, with the fixed effects subnetwork more appropriately assigning feature importance with resilience to confounding effects, helping to avoid Type I and Type II errors. In biomedical applications, this allows better hypothesis formation and prevents waste of resources in following up biased or confounded results. Meanwhile, the random effects subnetwork allows users to understand the cluster effects in their data, which can inform future research. For example, clinical study organizers could prioritize measurements with less inter-site variance in future studies. Besides these benefits, ARMED also increases predictive performance on clustered data, including better generalization to clusters unseen during training. Given these advantages demonstrated across multiple model architectures and applications, we broadly recommend the ARMED framework to deep learning practitioners dealing with non-iid data. We make our code available at tinyurl.com/ARMEDCode.\nAdversarially-regularized mixed effects deep learning (ARMED) models improve interpretability, performance, and generalization on clustered (non-iid ) data: Supplemental Materials\n\nFixed effect prediction\n\n\nMixed effect prediction\nFigure S3 : ARMED convolutional neural network (ARMED-CNN). For the conventional CNN (blue area), the penultimate dense layer produces a latent representation h F (X, \u03b2) of the data X. A final dense layer with weights \u03b2 L produces the classification prediction \u0177F . To create the fixed effects subnetwork, an adversarial classifier (gray area) is added that learns to predict the sample's cluster \u1e91 from the outputs of the layers of the CNN. The generalization loss penalizes the CNN for learning features that allow the adversarial classifier to predict cluster \u1e91. The random effects subnetwork (orange area) learns normally-distributed weights U (Z) \u223c N (0, \u03a3) dependent on cluster \u1e91, including scalars (nonlinear slopes) applied to h F (X, \u03b2) and a bias. These are combined with the fixed effects subnetwork output to yield the mixed effects-based prediction \u0177M . Note: for illustration, not all CNN layers are shown.\n\nCluster membership Input image\n\n\nFixed effect reconstruction\n\n\nRandom effect reconstruction\n\n\nMixed effects phenotype classification\n\n\nFixed effect phenotype classification\nFigure S4 : ARMED autoencoder-classifier (ARMED-AEC). A conventional autoencoder-classifier (blue area) contains an encoder that compresses the input image X into a latent representation e F (X; \u03b2), an auxiliary classifier that predicts the phenotype label \u0177F , and a decoder that reconstructs the image XF . The fixed effects subnetwork combines this conventional model with an adversarial classifier (gray area), which penalizes the encoder through the generalization loss for learning features predictive of cluster membership \u1e91A . The decoder shares weights with the encoder and so receives the same adversarial guidance. Meanwhile, the random effects subnetwork (orange areas) is a parallel autoencoder using random effects convolution blocks with cluster-dependent weights (Fig. S5). Its encoder compresses the image into a latent representation e R (X; U (z)) containing information predictive of cluster \u1e91L . Its decoder then reconstructs an image XR that is also contains information predictive of cluster \u1e91I . A learned combination of the two latent representations yields the mixed effects-based phenotype prediction \u0177M . Note: for illustration, not all autoencoder layers are shown. Our base DFNN architecture for both the spiral classification and the MCI conversion prediction problems contained 3 hidden dense layers with 4 neurons each (Fig. S1, blue area). Each was followed by a ReLU activation function. For binary classification, the output layer was a single-neuron dense layer with a sigmoid activation. To construct the ARMED-DFNN, we began by introducing an adversarial classifier containing 3 hidden dense layers with 8, 8, and 4 neurons, respectively (conceptualized in Fig. S1, gray area). ReLU activations were used for the hidden layers, and the output layer was a softmax layer for multi-class classification. The random effects subnetwork (Fig. S1, orange area) of the ARMED-DFNN varied by problem. For the spiral classification problem where the random effect was nonlinear by design, we included an 4-dimensional cluster-specific nonlinear slope (main text Eq. 4), which was multiplied with the output of the last hidden dense layer in the main (fixed effects) subnetwork, and a cluster-specific intercept (main text Eq. 5):\n\nConvolution\n\n\nLMMNN ARMED-DFNN\nh R (x i ; U (z i )) = h R,nlin (x i ; u nlin (z i )) + h R,int (u int (z i ))For the MCI conversion prediction problem, we included a p-dimensional cluster-specific linear slope (main text Eq. 6), which was multiplied by the input X with p features, and a cluster-specific intercept (main text Eq. 5):h R (x i ; U (z i )) = h R,lin (x i ; u lin (z i )) + h R,int (u int (z i ))\nTo combine the fixed (h F (x i ; \u03b2)) and random effects (h R (x i ; U (z i ))) subnetwork outputs and obtained the mixed effects prediction \u0177M,i , we used the following additive mixing function:\u0177M,i = m (h F (x i ; \u03b2), h R (x i ; U (z i ))) = sigmoid h F (x i ; \u03b2) \u03b2 L + h R (x i ; U (z i ))\nwhere \u03b2 L contains the weights of the output layer in the fixed effects subnetwork. The final training objective for the ARMED-DFNN isL BCE (y, \u0177M ) + \u03bb F L BCE (y, \u0177F ) + \u03bb K D KL (q(U )||p(U )) \u2212 \u03bb g L CCE (Z, \u1e90)\nwhere L BCE is the binary crossentropy loss:L BCE (y, \u0177) = \u2212 1 n n i=1 y i log(\u0177 i ) + (1 \u2212 y i ) log(1 \u2212 \u0177i )\nThe Z-predictor model, which infers the cluster membership matrix Z of unseen sites, used the same architecture as the adversarial classifier. All DFNN models were trained with the Adam optimizer with a learning rate of 0.001 for 50 epochs, with early stopping based on validation loss.\n\n3.1.2 Convolutional neural network (CNN)\nThe base CNN used for AD vs. CN classification contained 7 blocks, each comprising a two-dimensional convolutional layer with 3 \u00d7 3 kernels, batch normalization, and a PReLU activation function (Fig. S3, blue area). A 2 \u00d7 2 max-pooling layer was used between each block. The output of the last convolutional block was flattened and fed into a 512-neuron dense layer. Here, h F (X; \u03b2) is defined as the output of this hidden dense layer. The output layer was a single-neuron dense layer with a sigmoid activation. To form the fixed effects subnetwork of the ARMED-CNN, we first added an adversarial classifier (Fig. S3, gray area). We used an architecture similar to the base CNN, but the output layer was replaced by a softmax layer for multi-class classification. Intermediate outputs from each layer in the original CNN were sent into the adversarial classifier at the layer with the corresponding shape. Next, the random effects subnetwork (Fig. S3, orange area) consisted of a cluster-specific bias and 512-dimensional cluster-specific scalars (nonlinear slopes, akin to Eq. 1), which was multiplied with h F (X; \u03b2). We used the mixing function in Eq. 3 to combine the fixed and random effects subnetwork outputs. The loss function was the same as in Eq. 4. The Z-predictor model used the same architecture as the adversarial classifier and all models were trained with the Nadam optimizer with a learning rate of 0.0001 for 20 epochs.\nNguyen & Montillo: ARMED Models for Clustered Data Supplemental Materials\n\n3.1.3 Autoencoder-classifier (AEC)\nWe began with the autoencoder architecture used in [1] (Fig. S4, blue area), which contains an encoder to compress an image into a 56-dimensional latent representation e F (X; \u03b2). A decoder then reconstructs the image XF . The encoder contained 6 blocks, each with a convolutional layer with 4 \u00d7 4 kernels and 2 \u00d7 2 striding, batch normalization, and PReLU activation. The output of the last convolutional block was flattened and fed into a 56-neuron dense layer with produced the compressed latent representation. The decoder architecture was symmetric and replaced convolutional layers with transposed convolutional layers. To simultaneously perform classification, we introduced an auxiliary classifier subnetwork which predicts the cell phenotype \u0177F , i.e. high vs. low metastatic efficiency, from the latent representation. The auxiliary classifier took the encoder's latent representation as input and contained a 32-neuron hidden layer and a sigmoid output layer.\nTo create the fixed effects subnetwork, we added an adversarial classifier to predict each image's cluster \u1e90A from the layer activations of the encoder (Fig. S4, gray area). Through the generalization loss, the autoencoder is penalized for learning features that allow accurate cluster prediction. The encoder and decoder weights are tied so that this penalty affects both modules of the autoencoder. The adversary used the same architecture as the encoder, with the addition of a final softmax output layer.\nBecause image-level batch effects are believed to pervade all levels of the feature hierarchy, including lower level pixel-level features as well as higher-level morphology features, we construct a second, mirrored autoencoder as the random effects subnetwork (Fig. S4, orange area). In this second autoencoder, called the RE-AEC, random effect features are learned at every layer using random effects convolution (RECON) blocks (Fig. S5), which were inspired by style transfer networks [2, 3]. RECON blocks replaced the batch normalization layers of the original autoencoder. After a convolutional layer, these blocks apply instance normalization. In contrast to batch normalization which normalizes each mini-batch to zero mean and unit variance, instance normalization normalizes each sample independently [3]. Afterward, a cluster-specific random scale and bias is applied to each feature map, effectively rescaling and shifting the convolution outputs in a cluster-dependent manner.\nThe RE-AEC produces a cluster effect-laden latent representation e R (X; U (Z)) and reconstruction XR . To further enforce the learning of cluster-specific features in the RE-AEC, we added two more classifiers to predict cluster membership from the latent representation, \u1e90L , and from the image reconstruction, \u1e90I . By minimizing the prediction error of these two classifiers, the RE-AEC is encouraged to produce latent representations and reconstructions characterizing the cluster effects.\nTo create the ARMED-AEC, we combined the fixed effects subnetwork (the domain adversarial AEC, denoted as DA-AEC) with the RE-AEC. Specifically, we concatenated the latent representations e F (X; \u03b2) and e R (X; U (Z)) into a single vector and used this as input to a separate DFNN classifier trained to produce a mixed effects-based phenotype prediction \u0177M . In other words, the mixing function was \u0177M,i = m (h F (x i ; \u03b2), h R (x i ; U (z i ))) = m([h F (x i ; \u03b2), h R (x i ; U (z i )]) where m(...) represents this new classifier using the concatenated latent representations as input. The combined training objective of the ARMED-AEC was The first line in Eq. 5 contains the image reconstruction loss for the fixed and random effects subnetworks. Here, we use the mean squared error (MSE) between the input X and the reconstruction X. The second line contains the phenotype classification loss (binary crossentropy) between the true and predicted phenotype labels y and \u0177. The third line is the KL divergence for Bayesian layers. The fourth line contains the categorical crossentropy between the true cluster label Z and the cluster labels predicted by the latentcluster classifier \u1e90L and the image-cluster classifier \u1e90I . Finally, the last line is the cluster generalization loss which encourages the fixed effects subnetwork to learn features that prevent the adversary from predicting cluster labels \u1e90A .\nWe used an additional CNN classifier analogous to the adversarial classifier as the Z-predictor. All AEC models were trained with the Nadam optimizer with a learning rate of 0.0001 for 20 epochs, with early stopping based on reconstruction mean squared error on validation data.\n\n3.2 Spiral classification simulations\nThe spiral simulations generated two spirals, labeled y = 0 and y = 1 and separated in phase by \u03c0 radians (see Fig. S2 and Eq. 6). We randomly generated n = 10, 000 points along the spirals at arc lengths between 0 and 2\u03c0. Consequently, the measured features for the i th point are its two-dimensional coordinates x i,1 and x i,2 and the label is y i . We then added Gaussian noise \u03b7 \u223c N (0, 0.1) to X to increase the classification challenge.\nx i,1 = \u2212 r j t i 2\u03c0 cos(t i \u2212 \u03c6) + \u03b7\nx i,2 = r j t i 2\u03c0 sin(t i \u2212 \u03c6) + \u03b7 t \u2208 R n\u00d71 \u223c Uniform(0, 2\u03c0)\u03c6 = 0, y = 0 \u03c0, y = 1\nTo simulate random effects, we divided the points evenly into c = 10 clusters and varied the spiral radius r j for each cluster j. In simulation 1, we sampled the cluster-specific radii from a normal distribution centered at 1, r j \u223c N (1, 0.3) (Fig. S2a). In simulation 2, we increased the severity of the random perturbations by sampling the cluster-specific radii from a normal distribution centered at 0, r j \u223c N (0, 1.0), which causes the spirals to be inverted in half of the clusters (Fig. S2b). In simulation 3, we added two new features x 3 and x 4 which were confounded, i.e. they were associated with spiral label, y, but uncorrelated with the underlying spiral function. To simulate confounded variables, we first sampled a random variable \u03c1 j \u223c N (0, 0.1) for each cluster. We let \u03c1 j determine the class balance for each cluster, e.g. in cluster 3 with \u03c1 3 = 0.2, 20% of samples belong to class y = 0 and 80% belong to class y = 1. We then generated two additional data features as a function of \u03c1 j (Eq. 7), thereby creating a confounded relationship between x and y that is unrelated to the true underlying spiral function (Fig. S2c).\nx i,3 \u223c N (\u03c1 j , 0.2)\nx i,4 \u223c N (\u03c1 j , 0.2) (7)\n\n3.3 Alzheimer's Disease and mild cognitive impairment clincal and neuroimaging datasets\nData used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (https://adni.loni.usc.edu). The ADNI was launched in 2003 as a publicprivate partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).\n\n3.3.1 MCI conversion prediction dataset\nThe DFNN models for the classification of progressive vs. stable MCI (pMCI vs. sMCI) were trained on the curated \"ADNIMERGE\" dataset provided by ADNI. This dataset contains a selection of key variables from the ADNI study. We selected the subjects with the MCI diagnosis at the baseline visit and who had a 24-month follow-up visit. We then selected the 20 sites with the most subjects and held out the remaining 34 sites as the \"unseen site\" testing data. Subjects were labeled pMCI if their 24-month diagnosis changed\n\nFootnotes:\nNguyen & Montillo: ARMED Models for Clustered Data Supplemental Materials\n\nReferences:\n\n- D. J. Connor and M. N. Sabbagh, \"Administration and scoring variance on the ADAS-Cog,\" Journal of Alzheimer's Disease, vol. 15, no. 3, pp. 461-464, 2008.- E. Kozora et al., \"Effects of examiner error on neuropsychological test results in a multi-site study,\" The Clinical Neuropsychologist, vol. 22, no. 6, pp. 977-988, 2008.\n\n- K. Schafer, S. de Santi, and L. S. Schneider, \"Errors in ADAS-cog administration and scoring may undermine clinical trials results,\" Current Alzheimer Research, vol. 8, no. 4, pp. 373-376, 2011.\n\n- F. Kruggel, J. Turner, and L. T. Muftuler, \"Impact of scanner hard- ware and imaging protocol on image quality and compartment volume precision in the ADNI cohort,\" NeuroImage, vol. 49, no. 3, pp. 2123-2133, 2010.\n\n- C. Wachinger et al., \"Quantifying Confounding Bias in Neu- roimaging Datasets with Causal Inference,\" in Medical Image Com- puting and Computer-Assisted Intervention -MICCAI 2019, D. Shen, Ed., vol. 11767. Cham, Switzerland: Springer, 2019, pp. 484-492.\n\n- A. Zaritsky et al., \"Interpretable deep learning uncovers cellular properties in label-free live cell images that are predictive of highly metastatic melanoma,\" Cell Systems, vol. 12, no. 7, pp. 733-747.e6, 2021.\n\n- A. Franks, E. Airoldi, and N. Slavov, \"Post-transcriptional regu- lation across human tissues,\" PLoS Computational Biology, vol. 13, no. 5, p. e1005535, 2017.\n\n- A. Gelman and J. Hill, Data Analysis Using Regression and Multi- level/Hierarchical Models 1st Edition. Cambridge, UK: Cambridge University Press, 2007.\n\n- G. B. Holt, \"Potential Simpson's Paradox in Multicenter Study of Intraperitoneal Chemotherapy for Ovarian Cancer,\" Journal of Clinical Oncology, vol. 34, no. 9, p. 1016, 2016.\n\n- C. H. Wagner, \"Simpson's Paradox in Real Life,\" The American Statistician, vol. 36, no. 1, pp. 46-48, 1982.\n\n- X. A. Harrison et al., \"A brief introduction to mixed effects mod- elling and multi-model inference in ecology,\" PeerJ, vol. 6, p. e4794, 2018.\n\n- J. T. Hancock and T. M. Khoshgoftaar, \"Survey on categorical data for neural networks,\" Journal of Big Data, vol. 7, no. 1, 2020.\n\n- G. Simchoni and S. Rosset, \"Using Random Effects to Account for High-Cardinality Categorical Features and Repeated Measures in Deep Neural Networks,\" in Advances in Neural Information Processing Systems, A. Beygelzimer et al., Eds., 2021.\n\n- M. Wang and W. Deng, \"Deep visual domain adaptation: A survey,\" Neurocomputing, vol. 312, pp. 135-153, 2018.\n\n- Y. Ganin et al., \"Domain-Adversarial Training of Neural Net- works,\" Journal of Machine Learning Research, vol. 17, 2016.\n\n- T.-Y. Liu et al., \"Bridging the Generalization Gap: Training Robust Models on Confounded Biological Data,\" in Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, 2018.\n\n- D. Li et al., \"Learning to generalize: Meta-learning for domain generalization,\" in Thirty-Second AAAI Conference on Artificial Intel- ligence. Association for the Advancement of Artificial Intelligence, 2018.\n\n- Q. Liu, Q. Dou, and P.-A. Heng, \"Shape-Aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains,\" in Medical Image Computing and Computer-Assisted Intervention - MICCAI 2020, A. Martel et al., Eds. Cham, Switzerland: Springer, 2020, pp. 475-485.\n\n- E. Tzeng et al., \"Adversarial Discriminative Domain Adaptation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2962-2971.\n\n- K. Kamnitsas et al., \"Unsupervised Domain Adaptation in Brain Lesion Segmentation with Adversarial Networks,\" in Information processing in medical imaging, M. Niethammer, Ed. Cham: Springer, 2017, pp. 597-609.\n\n- Y. Xiong, H. J. Kim, and V. Singh, \"Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation,\" in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019, pp. 7743-7752.\n\n- M.-N. Tran et al., \"Bayesian Deep Net GLM and GLMM,\" Journal of Computational and Graphical Statistics, vol. 29, no. 1, pp. 97-113, 2020.\n\n- D. P. Kingma and M. Welling, \"Auto-Encoding Variational Bayes,\" arXiv:1312.6114, 2013.\n\n- D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, \"Variational Infer- ence: A Review for Statisticians,\" Journal of the American Statistical Association, vol. 112, no. 518, pp. 859-877, 2017.\n\n- K. J. Lang and M. J. Witbrock, \"Learning to Tell Two Spirals Apart,\" The 1988 Connectionist Models Summer School, pp. 52-59, 1988.\n\n- Y. Dimopoulos, P. Bourret, and S. Lek, \"Use of some sensitivity criteria for choosing networks with good generalization ability,\" Neural Processing Letters, vol. 2, no. 6, pp. 1-4, 1995.\n\n- J. D. Olden, M. K. Joy, and R. G. Death, \"An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data,\" Ecological Modelling, vol. 178, no. 3-4, pp. 389-397, 2004.\n\n- E. Thibeau-Sutre et al., \"MRI field strength predicts Alzheimer's disease: a case example of bias in the ADNI data set,\" in IEEE International Symposium on Biomedical Imaging (ISBI). [Piscataway, New Jersey]: IEEE, 2022.\n\n- D. L. Davies and D. W. Bouldin, \"A Cluster Separation Measure,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-1, no. 2, pp. 224-227, 1979.\n\n- T. Calinski and J. Harabasz, \"A dendrite method for cluster analysis,\" Communications in Statistics -Theory and Methods, vol. 3, no. 1, pp. 1-27, 1974.\n\n- R. R. Selvaraju et al., \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization,\" International Journal of Computer Vision, vol. 128, no. 2, pp. 336-359, 2020.\n\n- E. R. DeLong, D. M. DeLong, and D. L. Clarke-Pearson, \"Compar- ing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach,\" Biometrics, vol. 44, no. 3, p. 837, 1988.\n\n- A. Rao, J. M. Monteiro, and J. Mourao-Miranda, \"Predictive mod- elling using neuroimaging data in the presence of confounds,\" NeuroImage, vol. 150, pp. 23-49, 2017.\n\n- K. Rockwood et al., \"Interrater reliability of the Clinical Dementia Rating in a multicenter trial,\" Journal of the American Geriatrics Society, vol. 48, no. 5, pp. 558-559, 2000.\n\n- P. Bowie, T. Branton, and J. Holmes, \"Should the Mini Mental State Examination be used to monitor dementia treatments?\" The Lancet, vol. 354, no. 9189, pp. 1527-1528, 1999.\n\n- G. Lee et al., \"Predicting Alzheimer's disease progression using multi-modal deep learning approach,\" Scientific Reports, vol. 9, no. 1, p. 1952, 2019.\n\n- H. Shi et al., \"Early diagnosis of Alzheimer's disease on ADNI data using novel longitudinal score based on functional principal component analysis,\" Journal of Medical Imaging, vol. 8, no. 2, p. 024502, 2021.\n\n- C. Lian et al., \"Attention-Guided Hybrid Network for Dementia Diagnosis With Structural MR Images,\" IEEE Transactions on Cy- bernetics, 2020.\n\n- J. M. Schott et al., \"Measuring atrophy in Alzheimer disease: a serial MRI study over 6 and 12 months,\" Neurology, vol. 65, no. 1, pp. 119-124, 2005.\n\n- L. Ferrarini et al., \"Shape differences of the brain ventricles in Alzheimer's disease,\" NeuroImage, vol. 32, no. 3, pp. 1060-1069, 2006.\n\n- B. Dubois et al., \"Advancing research diagnostic criteria for Alzheimer's disease: the IWG-2 criteria,\" The Lancet Neurology, vol. 13, no. 6, pp. 614-629, 2014.\n\n- L. G. Apostolova et al., \"Hippocampal atrophy and ventricular enlargement in normal aging, mild cognitive impairment (MCI), and Alzheimer Disease,\" Alzheimer disease and associated disorders, vol. 26, no. 1, pp. 17-27, 2012.\n\n- W. Kang et al., \"Multi-model and multi-slice ensemble learn- ing architecture based on 2D convolutional neural networks for Alzheimer's disease diagnosis,\" Computers in Biology and Medicine, vol. 136, p. 104678, 2021.\n\n- A. Ebrahimi and S. Luo, \"Convolutional neural networks for Alzheimer's disease detection on MRI images,\" Journal of Medical Imaging, vol. 8, no. 2, p. 024503, 2021.\n\n- A. Zaritsky, A. R. Jamieson, E. S. Welf, A. Nevarez, J. Cillay, U. Eskiocak, B. L. Cantarel, and G. Danuser, \"Interpretable deep learning uncovers cellular properties in label-free live cell images that are predictive of highly metastatic melanoma,\" Cell Systems, vol. 12, no. 7, pp. 733-747.e6, 2021.\n\n- V. Dumoulin, J. Shlens, and M. Kudlur, \"A Learned Representation For Artistic Style,\" in International Conference on Learning Representations, 2017. [Online]. Available: https://arxiv.org/pdf/1610.07629.pdf\n\n- D. Ulyanov, A. Vedaldi, and V. Lempitsky, \"Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6924-6932.\n\n- O. Lucena, R. Souza, L. Rittner, R. Frayne, and R. Lotufo, \"Convolutional neural networks for skull-stripping in brain MR imaging using silver standard masks,\" Artificial Intelligence in Medicine, vol. 98, pp. 48-58, 2019. [Online]. Available: https://pubmed.ncbi.nlm.nih.gov/31521252/\n\n- B. B. Avants, N. J. Tustison, G. Song, P. A. Cook, A. Klein, and J. C. Gee, \"A reproducible evaluation of ANTs similarity metric performance in brain image registration,\" NeuroImage, vol. 54, no. 3, pp. 2033-2044, 2011.\n\n- N. J. Tustison, P. A. Cook, A. Klein, G. Song, S. R. Das, J. T. Duda, B. M. Kandel, N. van Strien, J. R. Stone, J. C. Gee, and B. B. Avants, \"Large-scale evaluation of ants and freesurfer cortical thickness measurements,\" NeuroImage, vol. 99, pp. 166-179, 2014.\n\n- V. S. Fonov, A. C. Evans, R. C. McKinstry, C. R. Almli, and D. L. Collins, \"Unbiased nonlinear average age-appropriate brain templates from birth to adulthood,\" NeuroImage, vol. 47, p. S102, 2009.\n\n", "annotations": {"ReferenceToTable": [{"begin": 26300, "end": 26302, "target": "#tab_6", "idx": 0}, {"begin": 30655, "end": 30656, "target": "#tab_0", "idx": 1}, {"begin": 32068, "end": 32069, "target": "#tab_1", "idx": 2}, {"begin": 32777, "end": 32778, "target": "#tab_3", "idx": 3}, {"begin": 32905, "end": 32906, "target": "#tab_3", "idx": 4}, {"begin": 33211, "end": 33212, "target": "#tab_3", "idx": 5}, {"begin": 36698, "end": 36699, "target": "#tab_5", "idx": 6}, {"begin": 37061, "end": 37062, "target": "#tab_5", "idx": 7}, {"begin": 37359, "end": 37360, "target": "#tab_5", "idx": 8}], "SectionMain": [{"begin": 1689, "end": 65960, "idx": 0}], "SectionReference": [{"begin": 66048, "end": 75778, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1689, "idx": 0}], "Div": [{"begin": 166, "end": 1681, "idx": 0}, {"begin": 1692, "end": 4806, "idx": 1}, {"begin": 4808, "end": 5801, "idx": 2}, {"begin": 5803, "end": 9930, "idx": 3}, {"begin": 9932, "end": 11315, "idx": 4}, {"begin": 11317, "end": 11349, "idx": 5}, {"begin": 11351, "end": 11375, "idx": 6}, {"begin": 11377, "end": 11420, "idx": 7}, {"begin": 11422, "end": 12451, "idx": 8}, {"begin": 12453, "end": 13824, "idx": 9}, {"begin": 13826, "end": 15154, "idx": 10}, {"begin": 15156, "end": 18633, "idx": 11}, {"begin": 18635, "end": 20215, "idx": 12}, {"begin": 20217, "end": 21258, "idx": 13}, {"begin": 21260, "end": 21277, "idx": 14}, {"begin": 21279, "end": 21639, "idx": 15}, {"begin": 21641, "end": 23350, "idx": 16}, {"begin": 23352, "end": 25450, "idx": 17}, {"begin": 25452, "end": 26568, "idx": 18}, {"begin": 26570, "end": 28583, "idx": 19}, {"begin": 28585, "end": 30507, "idx": 20}, {"begin": 30509, "end": 30519, "idx": 21}, {"begin": 30521, "end": 32622, "idx": 22}, {"begin": 32624, "end": 34989, "idx": 23}, {"begin": 34991, "end": 36537, "idx": 24}, {"begin": 36539, "end": 39338, "idx": 25}, {"begin": 39340, "end": 39353, "idx": 26}, {"begin": 39355, "end": 43205, "idx": 27}, {"begin": 43207, "end": 46015, "idx": 28}, {"begin": 46017, "end": 46054, "idx": 29}, {"begin": 46056, "end": 47444, "idx": 30}, {"begin": 47446, "end": 48745, "idx": 31}, {"begin": 48747, "end": 49481, "idx": 32}, {"begin": 49483, "end": 50519, "idx": 33}, {"begin": 50521, "end": 51957, "idx": 34}, {"begin": 51959, "end": 51983, "idx": 35}, {"begin": 51985, "end": 52929, "idx": 36}, {"begin": 52931, "end": 52962, "idx": 37}, {"begin": 52964, "end": 52992, "idx": 38}, {"begin": 52994, "end": 53023, "idx": 39}, {"begin": 53025, "end": 53064, "idx": 40}, {"begin": 53066, "end": 55361, "idx": 41}, {"begin": 55363, "end": 55375, "idx": 42}, {"begin": 55377, "end": 56677, "idx": 43}, {"begin": 56679, "end": 58233, "idx": 44}, {"begin": 58235, "end": 62920, "idx": 45}, {"begin": 62922, "end": 64724, "idx": 46}, {"begin": 64726, "end": 65399, "idx": 47}, {"begin": 65401, "end": 65960, "idx": 48}], "Head": [{"begin": 1692, "end": 1706, "n": "1", "idx": 0}, {"begin": 4808, "end": 4816, "idx": 1}, {"begin": 5803, "end": 5819, "n": "1.1", "idx": 2}, {"begin": 9932, "end": 9949, "n": "1.2", "idx": 3}, {"begin": 11317, "end": 11348, "idx": 4}, {"begin": 11351, "end": 11374, "idx": 5}, {"begin": 11377, "end": 11419, "idx": 6}, {"begin": 11422, "end": 11432, "idx": 7}, {"begin": 12453, "end": 12462, "n": "2", "idx": 8}, {"begin": 13826, "end": 13854, "n": "2.1", "idx": 9}, {"begin": 15156, "end": 15185, "n": "2.2", "idx": 10}, {"begin": 18635, "end": 18673, "n": "2.3", "idx": 11}, {"begin": 20217, "end": 20250, "n": "2.4", "idx": 12}, {"begin": 21260, "end": 21276, "n": "2.5", "idx": 13}, {"begin": 21279, "end": 21343, "n": "2.5.1", "idx": 14}, {"begin": 21641, "end": 21675, "idx": 15}, {"begin": 23352, "end": 23400, "idx": 16}, {"begin": 25452, "end": 25511, "n": "2.5.2", "idx": 17}, {"begin": 26570, "end": 26612, "n": "2.5.3", "idx": 18}, {"begin": 28585, "end": 28624, "n": "2.6", "idx": 19}, {"begin": 30509, "end": 30518, "n": "3", "idx": 20}, {"begin": 30521, "end": 30558, "n": "3.1", "idx": 21}, {"begin": 32624, "end": 32653, "n": "3.2", "idx": 22}, {"begin": 34991, "end": 35007, "n": "3.3", "idx": 23}, {"begin": 36539, "end": 36584, "n": "3.4", "idx": 24}, {"begin": 39340, "end": 39352, "n": "4", "idx": 25}, {"begin": 39355, "end": 39379, "n": "4.1", "idx": 26}, {"begin": 43207, "end": 43235, "n": "4.2", "idx": 27}, {"begin": 46017, "end": 46053, "n": "4.3", "idx": 28}, {"begin": 46056, "end": 46087, "n": "4.3.1", "idx": 29}, {"begin": 47446, "end": 47464, "n": "4.3.2", "idx": 30}, {"begin": 48747, "end": 48794, "n": "4.3.3", "idx": 31}, {"begin": 49483, "end": 49498, "n": "4.4", "idx": 32}, {"begin": 50521, "end": 50533, "n": "5", "idx": 33}, {"begin": 51959, "end": 51982, "idx": 34}, {"begin": 51985, "end": 52008, "idx": 35}, {"begin": 52931, "end": 52961, "idx": 36}, {"begin": 52964, "end": 52991, "idx": 37}, {"begin": 52994, "end": 53022, "idx": 38}, {"begin": 53025, "end": 53063, "idx": 39}, {"begin": 53066, "end": 53103, "idx": 40}, {"begin": 55363, "end": 55374, "idx": 41}, {"begin": 55377, "end": 55393, "idx": 42}, {"begin": 56679, "end": 56719, "n": "3.1.2", "idx": 43}, {"begin": 58235, "end": 58269, "n": "3.1.3", "idx": 44}, {"begin": 62922, "end": 62959, "n": "3.2", "idx": 45}, {"begin": 64726, "end": 64813, "n": "3.3", "idx": 46}, {"begin": 65401, "end": 65440, "n": "3.3.1", "idx": 47}], "Paragraph": [{"begin": 166, "end": 1681, "idx": 0}, {"begin": 1707, "end": 2646, "idx": 1}, {"begin": 2647, "end": 2706, "idx": 2}, {"begin": 2707, "end": 2919, "idx": 3}, {"begin": 2920, "end": 3066, "idx": 4}, {"begin": 3067, "end": 4642, "idx": 5}, {"begin": 4678, "end": 4806, "idx": 6}, {"begin": 4817, "end": 5801, "idx": 7}, {"begin": 5820, "end": 7890, "idx": 8}, {"begin": 7891, "end": 9113, "idx": 9}, {"begin": 9114, "end": 9930, "idx": 10}, {"begin": 9950, "end": 11315, "idx": 11}, {"begin": 11433, "end": 12451, "idx": 12}, {"begin": 12463, "end": 12924, "idx": 13}, {"begin": 12939, "end": 13083, "idx": 14}, {"begin": 13084, "end": 13824, "idx": 15}, {"begin": 13855, "end": 14200, "idx": 16}, {"begin": 14260, "end": 14621, "idx": 17}, {"begin": 14702, "end": 14970, "idx": 18}, {"begin": 15002, "end": 15154, "idx": 19}, {"begin": 15186, "end": 16272, "idx": 20}, {"begin": 16323, "end": 16458, "idx": 21}, {"begin": 16459, "end": 16642, "idx": 22}, {"begin": 16689, "end": 17003, "idx": 23}, {"begin": 17004, "end": 17014, "idx": 24}, {"begin": 17040, "end": 17275, "idx": 25}, {"begin": 17276, "end": 17493, "idx": 26}, {"begin": 17554, "end": 17782, "idx": 27}, {"begin": 17815, "end": 18053, "idx": 28}, {"begin": 18101, "end": 18235, "idx": 29}, {"begin": 18319, "end": 18342, "idx": 30}, {"begin": 18343, "end": 18633, "idx": 31}, {"begin": 18674, "end": 18786, "idx": 32}, {"begin": 18787, "end": 18983, "idx": 33}, {"begin": 19018, "end": 19280, "idx": 34}, {"begin": 19334, "end": 19398, "idx": 35}, {"begin": 19399, "end": 19430, "idx": 36}, {"begin": 19474, "end": 20215, "idx": 37}, {"begin": 20251, "end": 21258, "idx": 38}, {"begin": 21344, "end": 21639, "idx": 39}, {"begin": 21676, "end": 22910, "idx": 40}, {"begin": 22911, "end": 23350, "idx": 41}, {"begin": 23401, "end": 24987, "idx": 42}, {"begin": 24988, "end": 25450, "idx": 43}, {"begin": 25512, "end": 25792, "idx": 44}, {"begin": 25793, "end": 26568, "idx": 45}, {"begin": 26613, "end": 27485, "idx": 46}, {"begin": 27486, "end": 28093, "idx": 47}, {"begin": 28094, "end": 28583, "idx": 48}, {"begin": 28625, "end": 29956, "idx": 49}, {"begin": 29957, "end": 30507, "idx": 50}, {"begin": 30559, "end": 32181, "idx": 51}, {"begin": 32182, "end": 32622, "idx": 52}, {"begin": 32654, "end": 33475, "idx": 53}, {"begin": 33476, "end": 33929, "idx": 54}, {"begin": 33930, "end": 34989, "idx": 55}, {"begin": 35008, "end": 35378, "idx": 56}, {"begin": 35379, "end": 35606, "idx": 57}, {"begin": 35607, "end": 36537, "idx": 58}, {"begin": 36585, "end": 38012, "idx": 59}, {"begin": 38013, "end": 38780, "idx": 60}, {"begin": 38781, "end": 39338, "idx": 61}, {"begin": 39380, "end": 40505, "idx": 62}, {"begin": 40506, "end": 41054, "idx": 63}, {"begin": 41055, "end": 42892, "idx": 64}, {"begin": 42893, "end": 43205, "idx": 65}, {"begin": 43236, "end": 44075, "idx": 66}, {"begin": 44076, "end": 44627, "idx": 67}, {"begin": 44628, "end": 46015, "idx": 68}, {"begin": 46088, "end": 46839, "idx": 69}, {"begin": 46840, "end": 47444, "idx": 70}, {"begin": 47465, "end": 48281, "idx": 71}, {"begin": 48282, "end": 48745, "idx": 72}, {"begin": 48795, "end": 49481, "idx": 73}, {"begin": 49499, "end": 50519, "idx": 74}, {"begin": 50534, "end": 51777, "idx": 75}, {"begin": 51778, "end": 51957, "idx": 76}, {"begin": 52009, "end": 52929, "idx": 77}, {"begin": 53104, "end": 55361, "idx": 78}, {"begin": 55472, "end": 55696, "idx": 79}, {"begin": 55773, "end": 55967, "idx": 80}, {"begin": 56065, "end": 56199, "idx": 81}, {"begin": 56280, "end": 56324, "idx": 82}, {"begin": 56391, "end": 56677, "idx": 83}, {"begin": 56720, "end": 58159, "idx": 84}, {"begin": 58160, "end": 58233, "idx": 85}, {"begin": 58270, "end": 59240, "idx": 86}, {"begin": 59241, "end": 59749, "idx": 87}, {"begin": 59750, "end": 60737, "idx": 88}, {"begin": 60738, "end": 61230, "idx": 89}, {"begin": 61231, "end": 62641, "idx": 90}, {"begin": 62642, "end": 62920, "idx": 91}, {"begin": 62960, "end": 63403, "idx": 92}, {"begin": 63404, "end": 63441, "idx": 93}, {"begin": 63442, "end": 63504, "idx": 94}, {"begin": 63526, "end": 64676, "idx": 95}, {"begin": 64677, "end": 64698, "idx": 96}, {"begin": 64699, "end": 64724, "idx": 97}, {"begin": 64814, "end": 65399, "idx": 98}, {"begin": 65441, "end": 65960, "idx": 99}], "ReferenceToBib": [{"begin": 2127, "end": 2130, "idx": 0}, {"begin": 2132, "end": 2135, "idx": 1}, {"begin": 2137, "end": 2140, "idx": 2}, {"begin": 2431, "end": 2434, "idx": 3}, {"begin": 2436, "end": 2439, "idx": 4}, {"begin": 2562, "end": 2565, "idx": 5}, {"begin": 2584, "end": 2587, "idx": 6}, {"begin": 2642, "end": 2645, "target": "#b7", "idx": 7}, {"begin": 3821, "end": 3824, "target": "#b8", "idx": 8}, {"begin": 3837, "end": 3840, "idx": 9}, {"begin": 3856, "end": 3860, "target": "#b9", "idx": 10}, {"begin": 5520, "end": 5523, "target": "#b7", "idx": 11}, {"begin": 5525, "end": 5529, "target": "#b10", "idx": 12}, {"begin": 6017, "end": 6021, "target": "#b11", "idx": 13}, {"begin": 6120, "end": 6124, "target": "#b11", "idx": 14}, {"begin": 6126, "end": 6130, "target": "#b12", "idx": 15}, {"begin": 6400, "end": 6404, "target": "#b13", "idx": 16}, {"begin": 6878, "end": 6882, "target": "#b14", "idx": 17}, {"begin": 6884, "end": 6888, "target": "#b15", "idx": 18}, {"begin": 7001, "end": 7005, "target": "#b16", "idx": 19}, {"begin": 7007, "end": 7011, "target": "#b17", "idx": 20}, {"begin": 7185, "end": 7189, "target": "#b18", "idx": 21}, {"begin": 7191, "end": 7195, "target": "#b19", "idx": 22}, {"begin": 8138, "end": 8142, "target": "#b20", "idx": 23}, {"begin": 8597, "end": 8601, "target": "#b21", "idx": 24}, {"begin": 9108, "end": 9112, "target": "#b12", "idx": 25}, {"begin": 14149, "end": 14153, "target": "#b18", "idx": 26}, {"begin": 14155, "end": 14159, "target": "#b19", "idx": 27}, {"begin": 16014, "end": 16018, "target": "#b22", "idx": 28}, {"begin": 16020, "end": 16024, "target": "#b23", "idx": 29}, {"begin": 17264, "end": 17268, "target": "#b12", "idx": 30}, {"begin": 17270, "end": 17274, "target": "#b20", "idx": 31}, {"begin": 20445, "end": 20449, "target": "#b12", "idx": 32}, {"begin": 20451, "end": 20455, "target": "#b20", "idx": 33}, {"begin": 20457, "end": 20461, "target": "#b21", "idx": 34}, {"begin": 20556, "end": 20560, "target": "#b12", "idx": 35}, {"begin": 22065, "end": 22069, "target": "#b24", "idx": 36}, {"begin": 23124, "end": 23128, "target": "#b25", "idx": 37}, {"begin": 23130, "end": 23134, "target": "#b26", "idx": 38}, {"begin": 24494, "end": 24497, "idx": 39}, {"begin": 24499, "end": 24502, "idx": 40}, {"begin": 24504, "end": 24508, "target": "#b27", "idx": 41}, {"begin": 26836, "end": 26839, "idx": 42}, {"begin": 28374, "end": 28378, "target": "#b28", "idx": 43}, {"begin": 28467, "end": 28471, "target": "#b29", "idx": 44}, {"begin": 29332, "end": 29336, "target": "#b16", "idx": 45}, {"begin": 29855, "end": 29859, "target": "#b20", "idx": 46}, {"begin": 29870, "end": 29874, "target": "#b12", "idx": 47}, {"begin": 35746, "end": 35750, "target": "#b30", "idx": 48}, {"begin": 37031, "end": 37035, "target": "#b31", "idx": 49}, {"begin": 43409, "end": 43413, "target": "#b11", "idx": 50}, {"begin": 43747, "end": 43751, "target": "#b12", "idx": 51}, {"begin": 43753, "end": 43757, "target": "#b32", "idx": 52}, {"begin": 44994, "end": 44998, "target": "#b12", "idx": 53}, {"begin": 45000, "end": 45004, "target": "#b20", "idx": 54}, {"begin": 45006, "end": 45010, "target": "#b21", "idx": 55}, {"begin": 46703, "end": 46707, "target": "#b33", "idx": 56}, {"begin": 46834, "end": 46838, "target": "#b34", "idx": 57}, {"begin": 47168, "end": 47172, "target": "#b35", "idx": 58}, {"begin": 47272, "end": 47276, "target": "#b36", "idx": 59}, {"begin": 47278, "end": 47282, "target": "#b37", "idx": 60}, {"begin": 47691, "end": 47695, "target": "#b38", "idx": 61}, {"begin": 47697, "end": 47701, "target": "#b39", "idx": 62}, {"begin": 47703, "end": 47707, "target": "#b40", "idx": 63}, {"begin": 47833, "end": 47837, "target": "#b39", "idx": 64}, {"begin": 47839, "end": 47843, "target": "#b41", "idx": 65}, {"begin": 48494, "end": 48498, "target": "#b42", "idx": 66}, {"begin": 48500, "end": 48504, "target": "#b43", "idx": 67}, {"begin": 48882, "end": 48885, "idx": 68}, {"begin": 49720, "end": 49723, "target": "#b7", "idx": 69}, {"begin": 49725, "end": 49729, "target": "#b10", "idx": 70}, {"begin": 50502, "end": 50506, "target": "#b12", "idx": 71}, {"begin": 50508, "end": 50512, "target": "#b16", "idx": 72}, {"begin": 50514, "end": 50518, "target": "#b20", "idx": 73}, {"begin": 58321, "end": 58324, "idx": 74}, {"begin": 60237, "end": 60240, "idx": 75}, {"begin": 60241, "end": 60243, "idx": 76}, {"begin": 60559, "end": 60562, "idx": 77}], "ReferenceString": [{"begin": 66063, "end": 66216, "id": "b0", "idx": 0}, {"begin": 66218, "end": 66388, "id": "b1", "idx": 1}, {"begin": 66392, "end": 66586, "id": "b2", "idx": 2}, {"begin": 66590, "end": 66803, "id": "b3", "idx": 3}, {"begin": 66807, "end": 67060, "id": "b4", "idx": 4}, {"begin": 67064, "end": 67276, "id": "b5", "idx": 5}, {"begin": 67280, "end": 67438, "id": "b6", "idx": 6}, {"begin": 67442, "end": 67594, "id": "b7", "idx": 7}, {"begin": 67598, "end": 67773, "id": "b8", "idx": 8}, {"begin": 67777, "end": 67884, "id": "b9", "idx": 9}, {"begin": 67888, "end": 68031, "id": "b10", "idx": 10}, {"begin": 68035, "end": 68164, "id": "b11", "idx": 11}, {"begin": 68168, "end": 68406, "id": "b12", "idx": 12}, {"begin": 68410, "end": 68518, "id": "b13", "idx": 13}, {"begin": 68522, "end": 68643, "id": "b14", "idx": 14}, {"begin": 68647, "end": 68881, "id": "b15", "idx": 15}, {"begin": 68885, "end": 69094, "id": "b16", "idx": 16}, {"begin": 69098, "end": 69367, "id": "b17", "idx": 17}, {"begin": 69371, "end": 69545, "id": "b18", "idx": 18}, {"begin": 69549, "end": 69758, "id": "b19", "idx": 19}, {"begin": 69762, "end": 69967, "id": "b20", "idx": 20}, {"begin": 69971, "end": 70108, "id": "b21", "idx": 21}, {"begin": 70112, "end": 70198, "id": "b22", "idx": 22}, {"begin": 70202, "end": 70391, "id": "b23", "idx": 23}, {"begin": 70395, "end": 70525, "id": "b24", "idx": 24}, {"begin": 70529, "end": 70715, "id": "b25", "idx": 25}, {"begin": 70719, "end": 70943, "id": "b26", "idx": 26}, {"begin": 70947, "end": 71167, "id": "b27", "idx": 27}, {"begin": 71171, "end": 71337, "id": "b28", "idx": 28}, {"begin": 71341, "end": 71492, "id": "b29", "idx": 29}, {"begin": 71496, "end": 71681, "id": "b30", "idx": 30}, {"begin": 71685, "end": 71901, "id": "b31", "idx": 31}, {"begin": 71905, "end": 72069, "id": "b32", "idx": 32}, {"begin": 72073, "end": 72252, "id": "b33", "idx": 33}, {"begin": 72256, "end": 72428, "id": "b34", "idx": 34}, {"begin": 72432, "end": 72583, "id": "b35", "idx": 35}, {"begin": 72587, "end": 72796, "id": "b36", "idx": 36}, {"begin": 72800, "end": 72941, "id": "b37", "idx": 37}, {"begin": 72945, "end": 73094, "id": "b38", "idx": 38}, {"begin": 73098, "end": 73235, "id": "b39", "idx": 39}, {"begin": 73239, "end": 73399, "id": "b40", "idx": 40}, {"begin": 73403, "end": 73627, "id": "b41", "idx": 41}, {"begin": 73631, "end": 73848, "id": "b42", "idx": 42}, {"begin": 73852, "end": 74016, "id": "b43", "idx": 43}, {"begin": 74020, "end": 74321, "id": "b44", "idx": 44}, {"begin": 74325, "end": 74531, "id": "b45", "idx": 45}, {"begin": 74535, "end": 74799, "id": "b46", "idx": 46}, {"begin": 74803, "end": 75088, "id": "b47", "idx": 47}, {"begin": 75092, "end": 75311, "id": "b48", "idx": 48}, {"begin": 75315, "end": 75576, "id": "b49", "idx": 49}, {"begin": 75580, "end": 75776, "id": "b50", "idx": 50}], "Sentence": [{"begin": 166, "end": 238, "idx": 0}, {"begin": 239, "end": 396, "idx": 1}, {"begin": 397, "end": 613, "idx": 2}, {"begin": 614, "end": 1036, "idx": 3}, {"begin": 1037, "end": 1226, "idx": 4}, {"begin": 1227, "end": 1410, "idx": 5}, {"begin": 1411, "end": 1495, "idx": 6}, {"begin": 1496, "end": 1681, "idx": 7}, {"begin": 1707, "end": 1864, "idx": 8}, {"begin": 1865, "end": 1964, "idx": 9}, {"begin": 1965, "end": 2141, "idx": 10}, {"begin": 2142, "end": 2238, "idx": 11}, {"begin": 2239, "end": 2440, "idx": 12}, {"begin": 2441, "end": 2646, "idx": 13}, {"begin": 2647, "end": 2706, "idx": 14}, {"begin": 2707, "end": 2842, "idx": 15}, {"begin": 2843, "end": 2919, "idx": 16}, {"begin": 2920, "end": 3066, "idx": 17}, {"begin": 3067, "end": 3246, "idx": 18}, {"begin": 3247, "end": 3438, "idx": 19}, {"begin": 3439, "end": 3685, "idx": 20}, {"begin": 3686, "end": 3861, "idx": 21}, {"begin": 3862, "end": 3979, "idx": 22}, {"begin": 3980, "end": 4138, "idx": 23}, {"begin": 4139, "end": 4257, "idx": 24}, {"begin": 4258, "end": 4423, "idx": 25}, {"begin": 4424, "end": 4642, "idx": 26}, {"begin": 4678, "end": 4806, "idx": 27}, {"begin": 4817, "end": 4937, "idx": 28}, {"begin": 4938, "end": 5077, "idx": 29}, {"begin": 5078, "end": 5232, "idx": 30}, {"begin": 5233, "end": 5353, "idx": 31}, {"begin": 5354, "end": 5530, "idx": 32}, {"begin": 5531, "end": 5648, "idx": 33}, {"begin": 5649, "end": 5801, "idx": 34}, {"begin": 5820, "end": 5894, "idx": 35}, {"begin": 5895, "end": 6022, "idx": 36}, {"begin": 6023, "end": 6258, "idx": 37}, {"begin": 6259, "end": 6326, "idx": 38}, {"begin": 6327, "end": 6405, "idx": 39}, {"begin": 6406, "end": 6489, "idx": 40}, {"begin": 6490, "end": 6630, "idx": 41}, {"begin": 6631, "end": 6768, "idx": 42}, {"begin": 6769, "end": 6889, "idx": 43}, {"begin": 6890, "end": 7012, "idx": 44}, {"begin": 7013, "end": 7103, "idx": 45}, {"begin": 7104, "end": 7196, "idx": 46}, {"begin": 7197, "end": 7374, "idx": 47}, {"begin": 7375, "end": 7601, "idx": 48}, {"begin": 7602, "end": 7776, "idx": 49}, {"begin": 7777, "end": 7890, "idx": 50}, {"begin": 7891, "end": 7987, "idx": 51}, {"begin": 7988, "end": 8143, "idx": 52}, {"begin": 8144, "end": 8348, "idx": 53}, {"begin": 8349, "end": 8405, "idx": 54}, {"begin": 8406, "end": 8602, "idx": 55}, {"begin": 8603, "end": 8738, "idx": 56}, {"begin": 8739, "end": 8814, "idx": 57}, {"begin": 8815, "end": 8974, "idx": 58}, {"begin": 8975, "end": 9113, "idx": 59}, {"begin": 9114, "end": 9200, "idx": 60}, {"begin": 9201, "end": 9404, "idx": 61}, {"begin": 9405, "end": 9542, "idx": 62}, {"begin": 9543, "end": 9689, "idx": 63}, {"begin": 9690, "end": 9930, "idx": 64}, {"begin": 9950, "end": 10136, "idx": 65}, {"begin": 10137, "end": 10297, "idx": 66}, {"begin": 10298, "end": 10441, "idx": 67}, {"begin": 10442, "end": 10589, "idx": 68}, {"begin": 10590, "end": 10770, "idx": 69}, {"begin": 10771, "end": 10935, "idx": 70}, {"begin": 10936, "end": 11106, "idx": 71}, {"begin": 11107, "end": 11315, "idx": 72}, {"begin": 11433, "end": 11440, "idx": 73}, {"begin": 11441, "end": 11490, "idx": 74}, {"begin": 11491, "end": 11561, "idx": 75}, {"begin": 11562, "end": 11621, "idx": 76}, {"begin": 11622, "end": 11766, "idx": 77}, {"begin": 11767, "end": 11894, "idx": 78}, {"begin": 11895, "end": 12067, "idx": 79}, {"begin": 12068, "end": 12235, "idx": 80}, {"begin": 12236, "end": 12308, "idx": 81}, {"begin": 12309, "end": 12451, "idx": 82}, {"begin": 12463, "end": 12607, "idx": 83}, {"begin": 12608, "end": 12774, "idx": 84}, {"begin": 12775, "end": 12924, "idx": 85}, {"begin": 12939, "end": 13083, "idx": 86}, {"begin": 13084, "end": 13300, "idx": 87}, {"begin": 13301, "end": 13412, "idx": 88}, {"begin": 13413, "end": 13824, "idx": 89}, {"begin": 13855, "end": 14070, "idx": 90}, {"begin": 14071, "end": 14160, "idx": 91}, {"begin": 14161, "end": 14200, "idx": 92}, {"begin": 14260, "end": 14360, "idx": 93}, {"begin": 14361, "end": 14548, "idx": 94}, {"begin": 14549, "end": 14621, "idx": 95}, {"begin": 14702, "end": 14818, "idx": 96}, {"begin": 14819, "end": 14902, "idx": 97}, {"begin": 14903, "end": 14970, "idx": 98}, {"begin": 15002, "end": 15078, "idx": 99}, {"begin": 15079, "end": 15154, "idx": 100}, {"begin": 15186, "end": 15323, "idx": 101}, {"begin": 15324, "end": 15532, "idx": 102}, {"begin": 15533, "end": 15610, "idx": 103}, {"begin": 15611, "end": 15667, "idx": 104}, {"begin": 15668, "end": 15814, "idx": 105}, {"begin": 15815, "end": 16025, "idx": 106}, {"begin": 16026, "end": 16272, "idx": 107}, {"begin": 16323, "end": 16458, "idx": 108}, {"begin": 16459, "end": 16507, "idx": 109}, {"begin": 16508, "end": 16642, "idx": 110}, {"begin": 16689, "end": 16830, "idx": 111}, {"begin": 16831, "end": 16966, "idx": 112}, {"begin": 16967, "end": 17003, "idx": 113}, {"begin": 17004, "end": 17014, "idx": 114}, {"begin": 17040, "end": 17128, "idx": 115}, {"begin": 17129, "end": 17275, "idx": 116}, {"begin": 17276, "end": 17369, "idx": 117}, {"begin": 17370, "end": 17493, "idx": 118}, {"begin": 17554, "end": 17732, "idx": 119}, {"begin": 17733, "end": 17782, "idx": 120}, {"begin": 17815, "end": 17852, "idx": 121}, {"begin": 17853, "end": 18053, "idx": 122}, {"begin": 18101, "end": 18164, "idx": 123}, {"begin": 18165, "end": 18235, "idx": 124}, {"begin": 18319, "end": 18342, "idx": 125}, {"begin": 18343, "end": 18453, "idx": 126}, {"begin": 18454, "end": 18623, "idx": 127}, {"begin": 18624, "end": 18633, "idx": 128}, {"begin": 18674, "end": 18786, "idx": 129}, {"begin": 18787, "end": 18873, "idx": 130}, {"begin": 18874, "end": 18983, "idx": 131}, {"begin": 19018, "end": 19119, "idx": 132}, {"begin": 19120, "end": 19280, "idx": 133}, {"begin": 19334, "end": 19395, "idx": 134}, {"begin": 19396, "end": 19398, "idx": 135}, {"begin": 19399, "end": 19430, "idx": 136}, {"begin": 19474, "end": 19649, "idx": 137}, {"begin": 19650, "end": 19747, "idx": 138}, {"begin": 19748, "end": 19942, "idx": 139}, {"begin": 19943, "end": 20004, "idx": 140}, {"begin": 20005, "end": 20215, "idx": 141}, {"begin": 20251, "end": 20462, "idx": 142}, {"begin": 20463, "end": 20561, "idx": 143}, {"begin": 20562, "end": 20783, "idx": 144}, {"begin": 20784, "end": 20873, "idx": 145}, {"begin": 20874, "end": 21004, "idx": 146}, {"begin": 21005, "end": 21163, "idx": 147}, {"begin": 21164, "end": 21258, "idx": 148}, {"begin": 21344, "end": 21533, "idx": 149}, {"begin": 21534, "end": 21639, "idx": 150}, {"begin": 21676, "end": 21892, "idx": 151}, {"begin": 21893, "end": 22070, "idx": 152}, {"begin": 22071, "end": 22214, "idx": 153}, {"begin": 22215, "end": 22529, "idx": 154}, {"begin": 22530, "end": 22661, "idx": 155}, {"begin": 22662, "end": 22746, "idx": 156}, {"begin": 22747, "end": 22875, "idx": 157}, {"begin": 22876, "end": 22906, "idx": 158}, {"begin": 22907, "end": 22910, "idx": 159}, {"begin": 22911, "end": 23135, "idx": 160}, {"begin": 23136, "end": 23224, "idx": 161}, {"begin": 23225, "end": 23350, "idx": 162}, {"begin": 23401, "end": 23585, "idx": 163}, {"begin": 23586, "end": 23659, "idx": 164}, {"begin": 23660, "end": 23863, "idx": 165}, {"begin": 23864, "end": 24211, "idx": 166}, {"begin": 24212, "end": 24317, "idx": 167}, {"begin": 24318, "end": 24509, "idx": 168}, {"begin": 24510, "end": 24607, "idx": 169}, {"begin": 24608, "end": 24749, "idx": 170}, {"begin": 24750, "end": 24826, "idx": 171}, {"begin": 24827, "end": 24857, "idx": 172}, {"begin": 24858, "end": 24861, "idx": 173}, {"begin": 24862, "end": 24987, "idx": 174}, {"begin": 24988, "end": 25149, "idx": 175}, {"begin": 25150, "end": 25342, "idx": 176}, {"begin": 25343, "end": 25450, "idx": 177}, {"begin": 25512, "end": 25713, "idx": 178}, {"begin": 25714, "end": 25792, "idx": 179}, {"begin": 25793, "end": 25935, "idx": 180}, {"begin": 25936, "end": 26081, "idx": 181}, {"begin": 26082, "end": 26304, "idx": 182}, {"begin": 26305, "end": 26398, "idx": 183}, {"begin": 26399, "end": 26483, "idx": 184}, {"begin": 26484, "end": 26568, "idx": 185}, {"begin": 26613, "end": 26716, "idx": 186}, {"begin": 26717, "end": 26840, "idx": 187}, {"begin": 26841, "end": 27052, "idx": 188}, {"begin": 27053, "end": 27257, "idx": 189}, {"begin": 27258, "end": 27321, "idx": 190}, {"begin": 27322, "end": 27485, "idx": 191}, {"begin": 27486, "end": 27664, "idx": 192}, {"begin": 27665, "end": 27915, "idx": 193}, {"begin": 27916, "end": 28029, "idx": 194}, {"begin": 28030, "end": 28093, "idx": 195}, {"begin": 28094, "end": 28282, "idx": 196}, {"begin": 28283, "end": 28472, "idx": 197}, {"begin": 28473, "end": 28583, "idx": 198}, {"begin": 28625, "end": 28721, "idx": 199}, {"begin": 28722, "end": 28845, "idx": 200}, {"begin": 28846, "end": 29018, "idx": 201}, {"begin": 29019, "end": 29064, "idx": 202}, {"begin": 29065, "end": 29180, "idx": 203}, {"begin": 29181, "end": 29261, "idx": 204}, {"begin": 29262, "end": 29337, "idx": 205}, {"begin": 29338, "end": 29596, "idx": 206}, {"begin": 29597, "end": 29700, "idx": 207}, {"begin": 29701, "end": 29807, "idx": 208}, {"begin": 29808, "end": 29875, "idx": 209}, {"begin": 29876, "end": 29956, "idx": 210}, {"begin": 29957, "end": 30042, "idx": 211}, {"begin": 30043, "end": 30197, "idx": 212}, {"begin": 30198, "end": 30301, "idx": 213}, {"begin": 30302, "end": 30400, "idx": 214}, {"begin": 30401, "end": 30507, "idx": 215}, {"begin": 30559, "end": 30657, "idx": 216}, {"begin": 30658, "end": 30875, "idx": 217}, {"begin": 30876, "end": 30904, "idx": 218}, {"begin": 30905, "end": 31057, "idx": 219}, {"begin": 31058, "end": 31270, "idx": 220}, {"begin": 31271, "end": 31504, "idx": 221}, {"begin": 31505, "end": 31662, "idx": 222}, {"begin": 31663, "end": 31753, "idx": 223}, {"begin": 31754, "end": 31807, "idx": 224}, {"begin": 31808, "end": 32071, "idx": 225}, {"begin": 32072, "end": 32181, "idx": 226}, {"begin": 32182, "end": 32461, "idx": 227}, {"begin": 32462, "end": 32622, "idx": 228}, {"begin": 32654, "end": 32779, "idx": 229}, {"begin": 32780, "end": 32913, "idx": 230}, {"begin": 32914, "end": 33068, "idx": 231}, {"begin": 33069, "end": 33222, "idx": 232}, {"begin": 33223, "end": 33366, "idx": 233}, {"begin": 33367, "end": 33475, "idx": 234}, {"begin": 33476, "end": 33688, "idx": 235}, {"begin": 33689, "end": 33794, "idx": 236}, {"begin": 33795, "end": 33929, "idx": 237}, {"begin": 33930, "end": 34121, "idx": 238}, {"begin": 34122, "end": 34228, "idx": 239}, {"begin": 34229, "end": 34383, "idx": 240}, {"begin": 34384, "end": 34437, "idx": 241}, {"begin": 34438, "end": 34507, "idx": 242}, {"begin": 34508, "end": 34825, "idx": 243}, {"begin": 34826, "end": 34925, "idx": 244}, {"begin": 34926, "end": 34989, "idx": 245}, {"begin": 35008, "end": 35189, "idx": 246}, {"begin": 35190, "end": 35378, "idx": 247}, {"begin": 35379, "end": 35500, "idx": 248}, {"begin": 35501, "end": 35606, "idx": 249}, {"begin": 35607, "end": 35751, "idx": 250}, {"begin": 35752, "end": 35901, "idx": 251}, {"begin": 35902, "end": 36015, "idx": 252}, {"begin": 36016, "end": 36148, "idx": 253}, {"begin": 36149, "end": 36299, "idx": 254}, {"begin": 36300, "end": 36537, "idx": 255}, {"begin": 36585, "end": 36700, "idx": 256}, {"begin": 36701, "end": 36825, "idx": 257}, {"begin": 36826, "end": 36973, "idx": 258}, {"begin": 36974, "end": 37036, "idx": 259}, {"begin": 37037, "end": 37333, "idx": 260}, {"begin": 37334, "end": 37461, "idx": 261}, {"begin": 37462, "end": 37594, "idx": 262}, {"begin": 37595, "end": 37670, "idx": 263}, {"begin": 37671, "end": 37839, "idx": 264}, {"begin": 37840, "end": 38012, "idx": 265}, {"begin": 38013, "end": 38307, "idx": 266}, {"begin": 38308, "end": 38439, "idx": 267}, {"begin": 38440, "end": 38649, "idx": 268}, {"begin": 38650, "end": 38780, "idx": 269}, {"begin": 38781, "end": 38970, "idx": 270}, {"begin": 38971, "end": 39062, "idx": 271}, {"begin": 39063, "end": 39185, "idx": 272}, {"begin": 39186, "end": 39240, "idx": 273}, {"begin": 39241, "end": 39320, "idx": 274}, {"begin": 39321, "end": 39338, "idx": 275}, {"begin": 39380, "end": 39474, "idx": 276}, {"begin": 39475, "end": 39619, "idx": 277}, {"begin": 39620, "end": 39741, "idx": 278}, {"begin": 39742, "end": 39994, "idx": 279}, {"begin": 39995, "end": 40179, "idx": 280}, {"begin": 40180, "end": 40277, "idx": 281}, {"begin": 40278, "end": 40505, "idx": 282}, {"begin": 40506, "end": 40620, "idx": 283}, {"begin": 40621, "end": 40735, "idx": 284}, {"begin": 40736, "end": 40861, "idx": 285}, {"begin": 40862, "end": 40932, "idx": 286}, {"begin": 40933, "end": 41054, "idx": 287}, {"begin": 41055, "end": 41187, "idx": 288}, {"begin": 41188, "end": 41406, "idx": 289}, {"begin": 41407, "end": 41528, "idx": 290}, {"begin": 41529, "end": 41676, "idx": 291}, {"begin": 41677, "end": 41739, "idx": 292}, {"begin": 41740, "end": 42030, "idx": 293}, {"begin": 42031, "end": 42259, "idx": 294}, {"begin": 42260, "end": 42406, "idx": 295}, {"begin": 42407, "end": 42602, "idx": 296}, {"begin": 42603, "end": 42756, "idx": 297}, {"begin": 42757, "end": 42892, "idx": 298}, {"begin": 42893, "end": 43060, "idx": 299}, {"begin": 43061, "end": 43205, "idx": 300}, {"begin": 43236, "end": 43414, "idx": 301}, {"begin": 43415, "end": 43613, "idx": 302}, {"begin": 43614, "end": 43758, "idx": 303}, {"begin": 43759, "end": 43920, "idx": 304}, {"begin": 43921, "end": 44075, "idx": 305}, {"begin": 44076, "end": 44170, "idx": 306}, {"begin": 44171, "end": 44246, "idx": 307}, {"begin": 44247, "end": 44446, "idx": 308}, {"begin": 44447, "end": 44627, "idx": 309}, {"begin": 44628, "end": 44733, "idx": 310}, {"begin": 44734, "end": 44847, "idx": 311}, {"begin": 44848, "end": 45011, "idx": 312}, {"begin": 45012, "end": 45151, "idx": 313}, {"begin": 45152, "end": 45422, "idx": 314}, {"begin": 45423, "end": 45556, "idx": 315}, {"begin": 45557, "end": 45733, "idx": 316}, {"begin": 45734, "end": 45846, "idx": 317}, {"begin": 45847, "end": 46015, "idx": 318}, {"begin": 46088, "end": 46192, "idx": 319}, {"begin": 46193, "end": 46438, "idx": 320}, {"begin": 46439, "end": 46520, "idx": 321}, {"begin": 46521, "end": 46708, "idx": 322}, {"begin": 46709, "end": 46839, "idx": 323}, {"begin": 46840, "end": 47094, "idx": 324}, {"begin": 47095, "end": 47173, "idx": 325}, {"begin": 47174, "end": 47283, "idx": 326}, {"begin": 47284, "end": 47444, "idx": 327}, {"begin": 47465, "end": 47708, "idx": 328}, {"begin": 47709, "end": 47844, "idx": 329}, {"begin": 47845, "end": 47999, "idx": 330}, {"begin": 48000, "end": 48133, "idx": 331}, {"begin": 48134, "end": 48281, "idx": 332}, {"begin": 48282, "end": 48401, "idx": 333}, {"begin": 48402, "end": 48505, "idx": 334}, {"begin": 48506, "end": 48650, "idx": 335}, {"begin": 48651, "end": 48745, "idx": 336}, {"begin": 48795, "end": 48886, "idx": 337}, {"begin": 48887, "end": 49054, "idx": 338}, {"begin": 49055, "end": 49207, "idx": 339}, {"begin": 49208, "end": 49481, "idx": 340}, {"begin": 49499, "end": 49730, "idx": 341}, {"begin": 49731, "end": 49827, "idx": 342}, {"begin": 49828, "end": 50076, "idx": 343}, {"begin": 50077, "end": 50152, "idx": 344}, {"begin": 50153, "end": 50281, "idx": 345}, {"begin": 50282, "end": 50519, "idx": 346}, {"begin": 50534, "end": 50725, "idx": 347}, {"begin": 50726, "end": 50979, "idx": 348}, {"begin": 50980, "end": 51125, "idx": 349}, {"begin": 51126, "end": 51262, "idx": 350}, {"begin": 51263, "end": 51380, "idx": 351}, {"begin": 51381, "end": 51535, "idx": 352}, {"begin": 51536, "end": 51724, "idx": 353}, {"begin": 51725, "end": 51777, "idx": 354}, {"begin": 51778, "end": 51957, "idx": 355}, {"begin": 52009, "end": 52068, "idx": 356}, {"begin": 52069, "end": 52193, "idx": 357}, {"begin": 52194, "end": 52274, "idx": 358}, {"begin": 52275, "end": 52450, "idx": 359}, {"begin": 52451, "end": 52574, "idx": 360}, {"begin": 52575, "end": 52766, "idx": 361}, {"begin": 52767, "end": 52875, "idx": 362}, {"begin": 52876, "end": 52929, "idx": 363}, {"begin": 53104, "end": 53157, "idx": 364}, {"begin": 53158, "end": 53412, "idx": 365}, {"begin": 53413, "end": 53638, "idx": 366}, {"begin": 53639, "end": 53729, "idx": 367}, {"begin": 53730, "end": 53893, "idx": 368}, {"begin": 53894, "end": 54020, "idx": 369}, {"begin": 54021, "end": 54123, "idx": 370}, {"begin": 54124, "end": 54236, "idx": 371}, {"begin": 54237, "end": 54298, "idx": 372}, {"begin": 54299, "end": 54477, "idx": 373}, {"begin": 54478, "end": 54526, "idx": 374}, {"begin": 54527, "end": 54629, "idx": 375}, {"begin": 54630, "end": 54820, "idx": 376}, {"begin": 54821, "end": 54943, "idx": 377}, {"begin": 54944, "end": 55033, "idx": 378}, {"begin": 55034, "end": 55361, "idx": 379}, {"begin": 55472, "end": 55696, "idx": 380}, {"begin": 55773, "end": 55967, "idx": 381}, {"begin": 56065, "end": 56148, "idx": 382}, {"begin": 56149, "end": 56199, "idx": 383}, {"begin": 56280, "end": 56324, "idx": 384}, {"begin": 56391, "end": 56533, "idx": 385}, {"begin": 56534, "end": 56677, "idx": 386}, {"begin": 56720, "end": 56935, "idx": 387}, {"begin": 56936, "end": 56990, "idx": 388}, {"begin": 56991, "end": 57086, "idx": 389}, {"begin": 57087, "end": 57156, "idx": 390}, {"begin": 57157, "end": 57232, "idx": 391}, {"begin": 57233, "end": 57350, "idx": 392}, {"begin": 57351, "end": 57484, "idx": 393}, {"begin": 57485, "end": 57626, "idx": 394}, {"begin": 57627, "end": 57840, "idx": 395}, {"begin": 57841, "end": 57937, "idx": 396}, {"begin": 57938, "end": 58159, "idx": 397}, {"begin": 58160, "end": 58233, "idx": 398}, {"begin": 58270, "end": 58449, "idx": 399}, {"begin": 58450, "end": 58492, "idx": 400}, {"begin": 58493, "end": 58638, "idx": 401}, {"begin": 58639, "end": 58784, "idx": 402}, {"begin": 58785, "end": 58895, "idx": 403}, {"begin": 58896, "end": 59098, "idx": 404}, {"begin": 59099, "end": 59240, "idx": 405}, {"begin": 59241, "end": 59414, "idx": 406}, {"begin": 59415, "end": 59538, "idx": 407}, {"begin": 59539, "end": 59641, "idx": 408}, {"begin": 59642, "end": 59749, "idx": 409}, {"begin": 59750, "end": 60033, "idx": 410}, {"begin": 60034, "end": 60244, "idx": 411}, {"begin": 60245, "end": 60326, "idx": 412}, {"begin": 60327, "end": 60398, "idx": 413}, {"begin": 60399, "end": 60563, "idx": 414}, {"begin": 60564, "end": 60737, "idx": 415}, {"begin": 60738, "end": 60841, "idx": 416}, {"begin": 60842, "end": 61054, "idx": 417}, {"begin": 61055, "end": 61230, "idx": 418}, {"begin": 61231, "end": 61361, "idx": 419}, {"begin": 61362, "end": 61589, "idx": 420}, {"begin": 61590, "end": 61818, "idx": 421}, {"begin": 61819, "end": 61980, "idx": 422}, {"begin": 61981, "end": 62068, "idx": 423}, {"begin": 62069, "end": 62206, "idx": 424}, {"begin": 62207, "end": 62263, "idx": 425}, {"begin": 62264, "end": 62456, "idx": 426}, {"begin": 62457, "end": 62641, "idx": 427}, {"begin": 62642, "end": 62738, "idx": 428}, {"begin": 62739, "end": 62920, "idx": 429}, {"begin": 62960, "end": 63090, "idx": 430}, {"begin": 63091, "end": 63182, "idx": 431}, {"begin": 63183, "end": 63312, "idx": 432}, {"begin": 63313, "end": 63403, "idx": 433}, {"begin": 63404, "end": 63441, "idx": 434}, {"begin": 63442, "end": 63504, "idx": 435}, {"begin": 63526, "end": 63656, "idx": 436}, {"begin": 63657, "end": 63782, "idx": 437}, {"begin": 63783, "end": 64028, "idx": 438}, {"begin": 64029, "end": 64208, "idx": 439}, {"begin": 64209, "end": 64312, "idx": 440}, {"begin": 64313, "end": 64471, "idx": 441}, {"begin": 64472, "end": 64544, "idx": 442}, {"begin": 64545, "end": 64676, "idx": 443}, {"begin": 64677, "end": 64698, "idx": 444}, {"begin": 64699, "end": 64724, "idx": 445}, {"begin": 64814, "end": 64970, "idx": 446}, {"begin": 64971, "end": 65085, "idx": 447}, {"begin": 65086, "end": 65399, "idx": 448}, {"begin": 65441, "end": 65591, "idx": 449}, {"begin": 65592, "end": 65663, "idx": 450}, {"begin": 65664, "end": 65773, "idx": 451}, {"begin": 65774, "end": 65897, "idx": 452}, {"begin": 65898, "end": 65960, "idx": 453}], "ReferenceToFigure": [{"begin": 11438, "end": 11439, "target": "#fig_7", "idx": 0}, {"begin": 12593, "end": 12594, "target": "#fig_7", "idx": 1}, {"begin": 13901, "end": 13902, "target": "#fig_7", "idx": 2}, {"begin": 13947, "end": 13948, "target": "#fig_7", "idx": 3}, {"begin": 15307, "end": 15308, "target": "#fig_7", "idx": 4}, {"begin": 18629, "end": 18631, "target": "#fig_4", "idx": 5}, {"begin": 21599, "end": 21601, "target": "#fig_8", "idx": 6}, {"begin": 22210, "end": 22212, "target": "#fig_1", "idx": 7}, {"begin": 25757, "end": 25758, "target": "#fig_3", "idx": 8}, {"begin": 27911, "end": 27913, "target": "#fig_4", "idx": 9}, {"begin": 31054, "end": 31055, "target": "#fig_1", "idx": 10}, {"begin": 34118, "end": 34119, "target": "#fig_3", "idx": 11}, {"begin": 34504, "end": 34506, "target": "#fig_11", "idx": 12}, {"begin": 35743, "end": 35744, "target": "#fig_5", "idx": 13}, {"begin": 36449, "end": 36451, "idx": 14}, {"begin": 38967, "end": 38968, "target": "#fig_6", "idx": 15}, {"begin": 46189, "end": 46190, "target": "#fig_3", "idx": 16}, {"begin": 47688, "end": 47689, "target": "#fig_5", "idx": 17}, {"begin": 48277, "end": 48279, "idx": 18}, {"begin": 52016, "end": 52018, "target": "#fig_3", "idx": 19}, {"begin": 53111, "end": 53113, "target": "#fig_4", "idx": 20}, {"begin": 53889, "end": 53891, "target": "#fig_10", "idx": 21}, {"begin": 54462, "end": 54464, "target": "#fig_8", "idx": 22}, {"begin": 54805, "end": 54807, "target": "#fig_8", "idx": 23}, {"begin": 54980, "end": 54982, "target": "#fig_8", "idx": 24}, {"begin": 56920, "end": 56922, "target": "#fig_3", "idx": 25}, {"begin": 57335, "end": 57337, "target": "#fig_3", "idx": 26}, {"begin": 57669, "end": 57671, "target": "#fig_3", "idx": 27}, {"begin": 58331, "end": 58333, "target": "#fig_4", "idx": 28}, {"begin": 59399, "end": 59401, "target": "#fig_4", "idx": 29}, {"begin": 60016, "end": 60018, "target": "#fig_4", "idx": 30}, {"begin": 60185, "end": 60187, "target": "#fig_10", "idx": 31}, {"begin": 63076, "end": 63078, "target": "#fig_1", "idx": 32}, {"begin": 63777, "end": 63780, "target": "#fig_1", "idx": 33}, {"begin": 64023, "end": 64026, "target": "#fig_1", "idx": 34}, {"begin": 64671, "end": 64674, "target": "#fig_1", "idx": 35}], "Abstract": [{"begin": 156, "end": 1681, "idx": 0}], "SectionFootnote": [{"begin": 65962, "end": 66046, "idx": 0}], "Footnote": [{"begin": 65973, "end": 66046, "id": "foot_0", "idx": 0}]}}