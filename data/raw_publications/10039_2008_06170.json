{"text": "Privacy Preserving Vertical Federated Learning for Tree-based Models\n\nAbstract:\nFederated learning (FL) is an emerging paradigm that enables multiple organizations to jointly train a model without revealing their private data to each other. This paper studies vertical federated learning, which tackles the scenarios where (i) collaborating organizations own data of the same set of users but with disjoint features, and (ii) only one organization holds the labels. We propose Pivot, a novel solution for privacy preserving vertical decision tree training and prediction, ensuring that no intermediate information is disclosed other than those the clients have agreed to release (i.e., the final tree model and the prediction output). Pivot does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m \u2212 1 out of m clients. We further identify two privacy leakages when the trained decision tree model is released in plaintext and propose an enhanced protocol to mitigate them. The proposed solution can also be extended to tree ensemble models, e.g., random forest (RF) and gradient boosting decision tree (GBDT) by treating single decision trees as building blocks. Theoretical and experimental analysis suggest that Pivot is efficient for the privacy achieved.\n\nMain:\n\n\n\n1. INTRODUCTION\nThere has been a growing interest in exploiting data from distributed databases of multiple organizations, for providing better customer service and acquisition. Federated learning (FL) [52, 53] (or collaborative learning [43]) is an emerging paradigm for machine learning that enables multiple data owners (i.e., clients) to jointly train a model without revealing their private data to each other. The basic idea of FL is to iteratively let each client (i) perform some local computations on her data to derive certain intermediate results, and then (ii) exchange these results with other clients in a secure manner to advance the training process, until a final model is obtained. The advantage of FL is that it helps each client protect her data assets, so as to abide by privacy regulations (e.g., GDPR [3] and CCPA [1]) or to maintain a competitive advantage from proprietary data. Existing work on FL has mainly focused on the horizontal setting [9, 52, 53, 81, 66, 60, 8, 59, 55], which assumes that each client's data have the same schema, but no tuple is shared by multiple clients. In practice, however, there is often a need for vertical federated learning, where all clients hold the same set of records, while each client only has a disjoint subset of features. For example, Figure 1 illustrates a digital banking scenario, where a bank and a Fintech company aim to jointly build a machine learning model that evaluates credit card applications. The bank has some partial information about the users (e.g., account balances), while the Fintech company has some other information (e.g., the users' online transactions). In this scenario, vertical FL could enable the bank to derive a more accurate model, while the Fintech company could benefit from a pay-per-use model [73] for its contribution to the training and prediction.\nTo our knowledge, there exist only a few solutions [71, 44, 67, 68, 69, 21, 50, 60] for privacy preserving vertical FL. These solutions, however, are insufficient in terms of either efficiency or data privacy. In particular, [71, 44] assume that the labels in the training data could be shared with all participating clients in plaintext, whereas in practice, the labels often exist in one client's data only and could not be revealed to other clients without violating privacy. For instance, in the scenario illustrated in Figure 1, the training data could be a set of historical credit card applications, and each label would be a ground truth that indicates whether the application should have been approved. In this case, the labels are only available to the bank and could not be directly shared with the Fintech company. As a consequence, the solutions in [71, 44] are inapplicable. Meanwhile, [67, 68, 69, 21, 50] assume that some intermediate results during the execution could be revealed in plaintext; nevertheless, such intermediate results could be exploited by an adversarial client to infer the sensitive information in other clients' data. The solution in [60], on the other hand, relies on secure hardware [51] for privacy protection, but such secure hardware may not be trusted by all parties [81] and could be vulnerable to side channel attacks [76]. The method in [57] utilizes secure multiparty computation (MPC) [78], but assumes that each client's data could be outsourced to a number of noncolluding servers. This assumption is rather strong, as it is often challenging in practice to ensure that those servers do not collude and to convince all clients about it.\nTo address the above issues, we propose Pivot, a novel and efficient solution for vertical FL that does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m \u2212 1 out of m clients. Pivot is a part of our Falcon 1 (federated learning with privacy protection) system, and it ensures that no intermediate information is disclosed during the training or prediction process. Specifically, Pivot is designed for training decision tree (DT) models, which are well adopted for financial risk management [21, 50], healthcare analytics [6], and fraud detection [16] due to their good interpretability. The core of Pivot is a hybrid framework that utilizes both threshold partially homomorphic encryption (TPHE) and MPC, which are two cryptographic techniques that complement each other especially in the vertical FL setting: TPHE is relatively efficient in terms of communication cost but can only support a restrictive set of computations, whereas MPC could support an arbitrary computation but incurs expensive communication overheads. Pivot employs TPHE as much as possible to facilitate clients' local computation, and only invokes MPC in places where TPHE is inadequate in terms of functionality. This leads to a solution that is not only secure but also highly efficient for vertical tree models, as demonstrated in Section 8. Specifically, we make the following contributions:\n\u2022 We propose a basic protocol of Pivot that supports the training of both classification trees and regression trees, as well as distributed prediction using the tree models obtained. This basic protocol guarantees that each client only learns the final tree model but nothing else. To our knowledge, Pivot is the first vertical FL solution that achieves such a guarantee.\n\u2022 We enhance the basic protocol of Pivot to handle a more stringent case where parts of the final tree model need to be concealed for better privacy protection. In addition, we propose extensions of Pivot for training several ensemble tree-based models, including random forest (RF) and gradient boosting decision trees (GBDT).\n\u2022 We implement DT, RF, and GBDT models based on Pivot and conduct extensive evaluations on both real and synthetic datasets. The results demonstrate that Pivot offers accuracy comparable to non-private algorithms and provides high efficiency. The basic and enhanced protocols of Pivot achieve up to 37.5x and 4.5x speedup (w.r.t. training time) over an MPC baseline.\n\n2. PRELIMINARIES 2.1 Partially Homomorphic Encryption\nA partially homomorphic encryption (PHE) scheme is a probabilistic asymmetric encryption scheme for restricted computation over the ciphertexts. In this paper, we utilize the Paillier cryptosystem [61], which consists of three algorithms (Gen, Enc, Dec):\n\u2022 The key generation algorithm (sk, pk) = Gen(keysize) which returns secret key sk and public key pk, given a security parameter keysize.\n\u2022 The encryption algorithm c = Enc(x, pk), which maps a plaintext x to a ciphertext c using pk.\n\u2022 The decryption algorithm x = Dec(c, sk), which reverses the encryption by sk and outputs the plaintext x. Interested readers are referred to [27] for the exact construction of Enc and Dec. For simplicity, we omit the public key pk in the Enc algorithm and write Enc(x) as [x] in the rest of the paper. Let x1, x2 denote two plaintexts. We utilize the following properties of PHE: Homomorphic addition: given two ciphertexts [x1], [x2], the ciphertext of the sum x1 + x2 can be obtained by multiplying the ciphertexts, i.e.,[x1] \u2295 [x2] \u2236 [x1] \u22c5 [x2] = [x1 + x2]\nHomomorphic multiplication: given a plaintext x1 and a ciphertext [x2], the ciphertext of the product x1x2 can be obtained by raising [x2] to the power x1:x1 \u2297 [x2] \u2236 [x2] x 1 = [x1x2]\nHomomorphic dot product: given a ciphertext vector [v] = ([v1], \u22ef, [vm]) T and a plaintext vector x = (x1, \u22ef, xm), the ciphertext of the dot product v \u22c5 x can be obtained by:x \u2299 [v] \u2236 (x1 \u2297 [v1]) \u2295 \u22ef \u2295 (xm \u2297 [vm]) = [x1v1 + \u22ef + xmvm]= [x \u22c5 v]\nWe utilize a threshold variant of the PHE scheme (i.e., TPHE) with the following additional properties. First, the public key pk is known to everyone, while each client only holds a partial secret key. Second, the decryption of a ciphertext requires inputs from a certain number of clients. In this paper, we use a full threshold structure, which requires all clients to participate in order to decrypt a ciphertext.\n\n2.2 Secure Multiparty Computation\nSecure multiparty computation (MPC) allows participants to compute a function over their inputs while keeping the inputs private. In this paper, we utilize the additive secret sharing scheme SPDZ [28] for MPC. We refer to a value a \u2208 Zq that is additively shared among clients as a secretly shared value, and denote it as \u27e8a\u27e9 = (\u27e8a\u27e9 1 , \u22ef, \u27e8a\u27e9 m ), where \u27e8a\u27e9 i is a random share of \u27e8a\u27e9 hold by client i. To reconstruct a secretly shared value \u27e8a\u27e9, i.e., Rec(\u27e8a\u27e9), every client can send its own share to a specific client who computes a = (\u2211 m i=1 \u27e8a\u27e9 i ) mod q. Given secretly shared values, we have the following secure computation primitives. For ease of exposition, we omit the modular operation in the following formulations. Secure addition: given two secretly shared values \u27e8a\u27e9 and \u27e8b\u27e9, the secretly shared sum c = a + b can be obtained by having client i non-interactively compute \u27e8c\u27e9 i = \u27e8a\u27e9 i + \u27e8b\u27e9 i . Then \u27e8c\u27e9 i is a share of \u27e8c\u27e9 owned by client i. Secure multiplication: given two secretly shared values \u27e8a\u27e9 and \u27e8b\u27e9, the secretly shared multiplication c = a \u22c5 b can be obtained using Beaver's pre-computed multiplication triplet technique [7]. Assuming that the clients have already shared \u27e8u\u27e9, \u27e8v\u27e9, \u27e8z\u27e9 where u, v are random values in Zq and z = u \u22c5 v mod q, then client i locally computes \u27e8e\u27e9 i = \u27e8a\u27e9 i \u2212 \u27e8u\u27e9 i and \u27e8f \u27e9 i = \u27e8b\u27e9 i \u2212\u27e8v\u27e9 i , and the clients run Rec(\u27e8e\u27e9) and Rec(\u27e8f \u27e9). Finally, every client i computes \u27e8c\u27e9 i = \u2212i \u22c5 e \u22c5 f + f \u22c5 \u27e8a\u27e9 i + e \u22c5 \u27e8b\u27e9 i + \u27e8z\u27e9 i . Then \u27e8c\u27e9 i is a share of \u27e8c\u27e9 owned by client i. Secure comparison: given two shared values \u27e8a\u27e9 and \u27e8b\u27e9, the secure comparison operation (e.g., \u27e8a\u27e9 > \u27e8b\u27e9) returns a secretly shared \u27e80\u27e9 or \u27e81\u27e9. The basic idea is to first truncate the two values by 2 k , then execute \u27e8a\u27e9 \u2212 \u27e8b\u27e9, and finally output the secretly shared sign bit after dividing by 2 k\u22121 . We refer the interested readers to [17, 18] for details.\nBased on the above primitives, other primitives including secure division and secure exponential can be approximated, which are also supported in SPDZ [18, 28, 5]. In this paper, we use these secure computations in SPDZ as building blocks by default for the calculation concerning secretly shared values, which means that the outputs are also secretly shared values unless they are reconstructed. The secret sharing based MPC has two phases: an offline phase that is independent of the function and generates pre-computed Beaver's triplets, and an online phase that computes the designated function using these triplets.\n\n2.3 Tree-based Models\nIn this paper, we consider the classification and regression trees (CART) algorithm [13] with binary structure, while we note that other variants (e.g., ID3 [64], C4.5 [65]) can be easily generalized. We assume there is a training dataset D with n data points {x1, \u22ef, xn} each containing d features and the corresponding output label set Y = {y1, \u22ef, yn}.\nAlgorithm 1 describes the CART algorithm, which builds a tree recursively. For each tree node, it first decides whether some pruning conditions are satisfied, e.g., feature set is empty, tree reaches the maximum depth, the number of samples is less than a threshold. If any condition is satisfied, then it returns a leaf node with the class of majority samples for classification or the mean label value for regression. Otherwise, it determines the best split to construct two sub-trees that are built recursively. In order to find the best split feature and split threshold, CART uses Gini impurity [13] as a metric in classification. Let c be the number of classes and K = {1, \u22ef, c} be the class set. Let D be sample set on a given node, the Gini impurity is:I G (D) = 1 \u2212 k\u2208K (p k ) 2\nwhere p k is the fraction of samples in D labeled with class k. Let F be the set of available features, given any split feature j \u2208 F and split value \u03c4 \u2208 Domain(j), the sample set D can be split into two partitions D l and Dr. Then, the impurity gain of this split is as follows:gain = I G (D) \u2212 (w l \u22c5 I G (D l ) + wr \u22c5 I G (Dr)) = w l k\u2208K (p l,k ) 2 + wr k\u2208K (p r,k ) 2 \u2212 k\u2208K (p k ) 2\nwhere w l = D l D and wr = Dr D , and p l,k (resp. p r,k ) is the fraction of samples in D l (resp. Dr) that are labeled with class k \u2208 K. The split with the maximum impurity gain is considered the best split of the node. For regression, CART uses the label variance as a metric. Let Y be the set of labels of D, then the label variance is:I V (D) = E(Y 2 ) \u2212 (E(Y )) 2 = 1 n n i=1 y 2 i \u2212 ( 1 n n i=1 yi) 2\nSimilar to Eqn (5), the best split is determined by maximizing the variance gain. With CART, ensemble models can be trained to obtain better predictive performance, such as random forest (RF) [12], gradient boosting decision tree (GBDT) [35, 36], XGBoost [20], etc.\n\n3. SOLUTION OVERVIEW\n\n\n3.1 System Model\nWe consider a set of m distributed clients (or data owners) {u1, \u22ef, um} who want to train a decision tree model by consolidating their respective dataset {D1, \u22ef, Dm}. Each row in the datasets corresponds to a data sample, and each column corresponds to a feature. Let n be the number of samples and di be the number of features in Di, where i \u2208 {1, \u22ef, m}. We denote Di = {xit} n t=1 where xit represents the t-th sample of Di. Let Y = {yt} n t=1 be the set of sample labels. Table 1 summarizes the frequently used notations.\nPivot focuses on the vertical federated learning scenario [77], where the datasets {D1, \u22ef, Dm} share the same sample ids while with different features. In particular, we assume that the clients have determined and aligned their common samples using private set intersection techniques [54, 62, 19, 63] without revealing any information about samples not in the intersection. In addition, we assume that the label set Y is held by only one client (i.e., super client) and cannot be directly shared with other clients.\n\n3.2 Threat Model\nWe consider the semi-honest model [57, 75, 74, 23, 22, 38] where every client follows the protocol exactly as specified, but may try to infer other clients' private information based on the messages received. Like any other client, no additional trust is assumed of the super client. We assume that an adversary A can corrupt up to m \u2212 1 clients and the adversary's corruption strategy is static, such that the set of corrupted clients is fixed before the protocol execution and remains unchanged during the execution.\n\n3.3 Problem Formulation\nTo protect the private data of honest clients, we require that an adversary learns nothing more than the data of the clients he has corrupted and the final output. Similar to previous work [57, 81, 23], we formalize our problem under the ideal/real paradigm. Let F be an ideal functionality such that the clients send their data to a trusted third party for computation and receive the final output from that party. Let \u03c0 be a real world protocol executed by the clients. We say a real protocol \u03c0 behaviors indistinguishably as the ideal functionality F if the following formal definition is satisfied. Definition 1. ( [15, 25, 57]). A protocol \u03c0 securely realizes an ideal functionality F if for every adversary A attacking the real interaction, there exists a simulator S attacking encrypted mask vector for a tree node the ideal interaction, such that for all environments Z, the following quantity is negligible (in \u03bb):Pr[real(Z, A, \u03c0, \u03bb) = 1] \u2212 Pr[ideal(Z, S, F, \u03bb) = 1] .\u25fb\nIn this paper, we identify two ideal functionalities F DTT and F DTP for the model training and model prediction, respectively. In F DTT , the input is every client's dataset while the output is the trained model that all clients have agreed to release. In F DTP , the input is the released model and a sample while the output is the predicted label of that sample. The output of F DTT is part of the input of F DTP . Specifically, in our basic protocol (Section 4), we assume that the output of F DTT is the plaintext tree model, including the split feature and the split threshold on each internal node, and the label for prediction on each leaf node. While in our enhanced protocol (Section 5), the released plaintext information is assumed to include only the split feature on each internal node, whereas the split threshold and the leaf label are concealed for better privacy protection.\n\n3.4 Protocol Overview\nWe now provide the protocol overview of Pivot. The protocols are composed of three stages: initialization, model training, and model prediction. Initialization stage. In this stage, the m clients agree to run a designated algorithm (i.e., the decision tree model) over their joint data and release the pre-defined information (e.g., the trained model) among themselves. The clients collaboratively determine and align the joint samples. The clients also build consensus on some hyper-parameters, such as security parameters (e.g., key size), pruning thresholds, and so on. The m clients jointly generate the keys of threshold homomorphic encryption and every client ui receives the public key pk and a partial secret key ski.\nModel training stage. The m clients build the designated tree model iteratively. In each iteration, the super client first broadcasts some encrypted information to facilitate the other clients to compute encrypted necessary statistics at local. After that, the clients jointly convert those statistics into MPC-compatible inputs, i.e., secretly shared values, to determine the best split of the current tree node using secure computations. Finally, the secretly shared best split is revealed (in the Pivot basic protocol) or is converted back into an encrypted form (in the Pivot enhanced protocol), for clients to update the model. Throughout the whole process, no intermediate information is disclosed to any client. Model prediction stage. After model training, the clients obtain a tree model. In the basic protocol of Pivot (Section 4), the whole tree is released in plaintext. In the Pivot enhanced protocol (Section 5), the split threshold on each internal node and the prediction label on each leaf node are concealed from all clients, in secretly shared form. Given an input sample with distributed feature values, the clients can jointly produce a prediction. Pivot guarantees that no information except for the predicted label is revealed during the prediction process.\n\n4. BASIC PROTOCOL\nIn this section, we present our basic protocol of Pivot. The output of the model training stage is assumed to be the whole plaintext tree model. Note that prior work [71, 44, 67, 68, 69, 21, 50] is not applicable to our problem since they simplify the problem by revealing either the training labels or intermediate results in plaintext, which discloses too much information regarding the client's private data.\nTo satisfy Definition 1 for vertical tree training, a straightforward solution is to directly use the MPC framework. For example, the clients can apply the additive secret sharing scheme (see Section 2.2) to convert private datasets and labels into secretly shared data, and train the model by secure computations. However, this solution incurs high communication complexity because it involves O(nd) secretly shared values and most secure computations are communication intensive. On the other hand, while TPHE could enable each client to compute encrypted split statistics at local by providing the super client's encrypted label information, it does not support some operations (e.g., comparison), which are needed in best split determination. Based on these observations and inspired by [81], we design our basic protocol using a hybrid framework of TPHE and MPC for vertical tree training. The basic idea is that each client executes as many local computations (e.g., computing split statistics) as possible with the help of TPHE and uses MPC only when TPHE is insufficient (e.g., deciding the best split). As a consequence, most computations are executed at local and the secretly shared values involved in MPC are reduced to O(db), where b denotes the maximum number of split values for any feature and db is the number of total splits. Section 4.1 and Section 4.2 present our training protocol for classification tree and regression tree, respectively. Section 4.3 proposes our tree model prediction method. The security analysis is provided in Section 4.4.\n\n4.1 Classification Tree Training\nIn our training protocol, the clients use an mask vector of size n to indicate which samples are available on a tree node, but keep the vector in an encrypted form to avoid disclosing the sample set. Specifically, let \u03b1 = (\u03b11, \u22ef, \u03b1n) be an indicator vector for a tree node. Then, for any i \u2208 {1, \u22ef, n}, \u03b1i = 1 indicates that the i-th sample is available on the node, and \u03b1i = 0 otherwise. We use Before the training starts, each client initializes a decision tree with only a root node, and associates the root node with an encrypted indicator vector [\u03b1] where all elements are [1] (since all samples are available on the root node). Then, the clients work together to recursively split the root node. In what follows, we will use an example to illustrate how our protocol decides the best split for a given tree node based on Gini impurity.\nConsider the example in Figure 2, where we have three clients u1, u2, and u3. Among them, u1 is the super client, (super client)[6 % ] = ([0], [1], [0], [0], [1])\nConsider a split value 15000 on deposit:* ) = (1, 1, 0, 1, 0) [3 ),& ] = * ) \u2299 6 & = [1] [3 ),% ] = * ) \u2299 6 % = [1]\nCompute by MPC:\n. ' (/ ) ) = 0.5 ). This vector indicates that Samples 1 and 3 are on the node to be split, and they belong to Class 1. Similarly, u1 also generates an encrypted indicator vector [\u03b3 2 ] for Class 2. After that, u1 broadcasts [\u03b3 1 ] and [\u03b3 2 ] to all clients.\nAfter receiving [\u03b3 1 ] and [\u03b3 2 ], each client combines them with her local training data to compute several statistics that are required to choose the best split of the current node. In particular, to evaluate the quality of a split based on Gini impurity (see Section 2.3), each client needs to examine the two child nodes that would result from the split, and then compute the following statistics for each child node: (i) the total number of samples that belong to the child node, and (ii) the number of samples among them that are associated with label class k, for each k \u2208 K.\nFor example, suppose that u3 considers a split that divides the current node based on whether the deposit values are larger than 15000. Then, u3 first examines her local samples, and divide them into two partitions. The first partition (referred to as the left partition) consists of Samples 1, 2, and 4, i.e., the local samples whose deposit values are no more than 15000. Meanwhile, the second partition (referred to as the right partition) contains Samples 3 and 5. Accordingly, for the left (resp. right) partition, u3 constructs an indicator vector v l = (1, 1, 0, 1, 0) (resp. vr = (0, 0, 1, 0, 1)) to specify the samples that it contains. After that, u3 performs a homomorphic dot product between v l and [\u03b3 1 ] to obtain an encrypted number [g l,1 ]. Observe that g l,1 equals the exact number of Class 1 samples that belong to the left child node of the split. Similarly, u3 uses v l and [\u03b3 2 ] to generate [g l,2 ], an encrypted version of the number of Class 2 samples that belong to the left child node. Using the same approach, u3 also computes the encrypted numbers of Classes 1 and 2 samples associated with the right child node. Further, u3 derives an encrypted total number of samples in the left (resp. right) child node, using a homomorphic dot product between v l and [\u03b1] (resp. vr and [\u03b1]).for i \u2208 [1, m] do 2 [r i ] \u2190 u i randomly chooses r i \u2208 Zq and encrypts it 3 u i sends [r i ] to u 1 4 u 1 computes [e] = [x] \u2295 [r 1 ] \u2295 \u22ef \u2295 [rm] 5 e \u2190 clients jointly decrypt [e] 6 u 1 sets \u27e8x\u27e9 1 = e \u2212 r 1 mod q 7 for i \u2208 [2, m] do 8 u i sets \u27e8x\u27e9 i = \u2212r i mod q\nSuppose that each client computes the encrypted numbers associated with each possible split of the current node, using the approach illustrated for u3 above. Then, they can convert them into MPC-compatible inputs, and then invoke an MPC protocol to securely identify the best split of the current node. We will elaborate the details shortly.\nIn general, the clients split each node in three steps: local computation, MPC computation, and model update. In the following, we discuss the details of each step. Local computation. Suppose that the clients are to split a node that is associated with an encrypted mask vector [\u03b1] = ([\u03b11], \u22ef, [\u03b1n]), indicating the available samples on the node. First, the super client constructs, for each class label k \u2208 K, an auxiliary indicator vector , each client ui uses it along with her local data to derive several statistics for identifying the tree node's best split, as previously illustrated in our example. In particular, let Fi be the set of features that ui has, and Sij be the set of split values for a feature j \u2208 Fi. Then, for any split value \u03c4 \u2208 Sij, ui first constructs two size-n indicator vectors v l and vr, such that (i) the t-th element in v l equals 1 if Sample t's feature j is no more than \u03c4 , and 0 otherwise, and (ii) vr complements v l . Consider the two possible child nodes induced by the split value \u03c4 . For each class k \u2208 K, let g l,k (resp. g r,k ) be the number of samples labeled with class k that belong to the left (resp. right) child node. ui computes the encrypted versions of g l,k and g r,k using homomorphic dot products (see Section 2.1) as follows:\u03b2 k = (\u03b2 k,1 , \u22ef, \u03b2 k,n ), such that \u03b2 k,t = 1 if Sample t's label is k,[g l,k ] = v l \u2299 [\u03b3 k ], [g r,k ] = vr \u2299 [\u03b3 k ].\nLet n l (resp. nr) be the number of samples in the left (resp. right) child node. ui computes[n l ] = v l \u2299 [\u03b1] and [nr] = vr \u2299[\u03b1].\nIn total, for each split value \u03c4 , ui generates 2\u22c5 K +2 encrypted numbers, where K = c is the number of classes. MPC computation. After the clients generate the encrypted statistics mentioned above (i.e.,[g l,k ], [g r,k ], [n l ],\n[nr]), they execute an MPC protocol to identify the best split of the current node. Towards this end, the clients first invoke Algorithm 2 to convert each encrypted number [x] into a set of secret shares {\u27e8x\u27e9i} m i=1 , where \u27e8x\u27e9i is given to ui. The general idea of Algorithm 2 is from [24, 28, 81]. We use \u27e8x\u27e9 to denote that the x is secretly shared among the clients.\nAfter the above conversion, the clients obtain secretly return a tree with j * -th feature and s * -th split value that has two edges, build tree recursively shared statistics \u27e8n l \u27e9, \u27e8nr\u27e9, \u27e8g l,k \u27e9, and \u27e8g r,k \u27e9 (for each class k \u2208 K) for each possible split \u03c4 of the current node. Using these statistics, the clients identify the best split of the current node as follows.\nConsider a split \u03c4 and the two child nodes that it induces. To evaluate the Gini impurity of the left (resp. right) child node, the clients need to derive, for each class k \u2208 K, the fraction p l,k (resp. p r,k ) of samples on the node that are labeled with k. Observe thatp l,k = g l,k \u2211 k \u2032 \u2208K g l,k \u2032 , p r,k = g r,k \u2211 k \u2032 \u2208K g r,k \u2032 .\nIn addition, recall that the clients have obtained, for each class k \u2208 K, the secretly shared values \u27e8g l,k \u27e9 and \u27e8g r,k \u27e9. Therefore, the clients can jointly compute \u27e8p l,k \u27e9 and \u27e8p r,k \u27e9 using the secure addition and secure division operators in SPDZ (see Section 2.2), without disclosing p l,k and p r,k to any client. With the same approach, the clients use \u27e8n l \u27e9 and \u27e8nr\u27e9 to securely compute \u27e8w l \u27e9 and \u27e8wr\u27e9, where w l = n l n l +nr and wr = nr n l +nr . Given \u27e8p l,k \u27e9, \u27e8p r,k \u27e9, \u27e8w l \u27e9, and \u27e8wr\u27e9, the clients can then compute the impurity gain of each split \u03c4 (see Eqn. ( 5)) in secretly shared form, using the secure addition and secure multiplication operators in SPDZ.\nFinally, the clients jointly determine the best split using a secure maximum computation as follows. First, each client ui assigns an identifier (i, j, s) to the s-th split on the jth feature that she holds. Next, the clients initialize four secretly shared values \u27e8gain max \u27e9, \u27e8i * \u27e9, \u27e8j * \u27e9, \u27e8s * \u27e9, all with \u27e8\u22121\u27e9. After that, they will compare the secretly shared impurity gains of all splits, and securely record the identifier and impurity gain of the best split in \u27e8i * \u27e9, \u27e8j * \u27e9, \u27e8s * \u27e9, and \u27e8gain max \u27e9, respectively. Specifically, for each split \u03c4 , the clients compare its impurity gain \u27e8gain \u03c4 \u27e9 with \u27e8gain max \u27e9 using secure comparison (see Section 2.2). Let \u27e8sign\u27e9 be the result of the secure comparison, i.e., sign = 1 if gain \u03c4 > gain max , and sign = 0 otherwise. Then, the clients securely update \u27e8gain max \u27e9 using the secretly shared values, such that gain max = gain max \u22c5 (1 \u2212 sign) + gain \u03c4 \u22c5 sign. The best split identifier is updated in the same manner. After examining all splits, the clients obtain the secretly shared best split identifier (\u27e8i * \u27e9, \u27e8j * \u27e9, \u27e8s * \u27e9). Model update. Recall that in the basic protocol, the tree model can be released in plaintext. Therefore, the clients  reconstruct the secretly shared identifier. Then, the i * -th client can retrieve the two indicator vectors v l and vr for the s * -th split of the j * -th feature. After that, she executes element-wise homomorphic multiplication on the two vectors by [\u03b1], obtaining [\u03b1 l ] and [\u03b1r] for the two branches, and broadcasts them to the other clients. Note that [\u03b1 l ] and [\u03b1r] exactly specify the available samples on the two child nodes, respectively. For example, in Figure 2, if the current split of the 'deposit' feature is selected,u3 can com- pute [\u03b1 l ] = ([1], [1], [0], [0], [0]) using v l and [\u03b1],\nindicating that Samples 1 and 2 are available on the left child node.\n\n4.2 Regression Tree Training\nFor the regression tree, since the label is continuous, the central part is to compute the label variance. The MPC computation step and model update step are similar to that of classification, thus, we only present the difference in the local computation step.\nAccording to the label variance formula Eqn. ( 6), the super client can construct two auxiliary vectors \u03b2 1 = (y1, \u22ef, yn) and \u03b2 2 = (y 2 1 , \u22ef, y 2 n ), where the elements in \u03b2 1 are the original training labels while the elements in \u03b2 2 are the square of the original training labels. Next, she computes element-wise homomorphic multiplication on \u03b2 1 (resp.\u03b2 2 ) by [\u03b1], obtaining [\u03b3 1 ] (resp. [\u03b3 2 ]). Then, she broadcasts [ L ] = {[\u03b3 1 ], [\u03b3 2 ]\n} to all clients. Similarly, each client computes the following encrypted statistics for any local split:[n l ] = v l \u2299 [\u03b1], [g l,1 ] = v l \u2299 [\u03b3 1 ], [g l,2 ] = v l \u2299 [\u03b3 2 ] (9)\nwhere[n l ], [g l,1 ], [g l,2\n] are the encrypted number of samples, and the encrypted sum of [\u03b3 1 ] and [\u03b3 2 ] of the available samples, for the left branch. Similarly, these encrypted statistics can be converted into secretly shared values and the best split identifier can be decided based on Eqn.  (6).\nAlgorithm 3 describes the privacy preserving decision tree training protocol. Lines 1-3 check the pruning conditions and compute the leaf label if any condition is satisfied. Note that with the encrypted statistics, these executions can be easily achieved by secure computations. Lines 5-13 find the best split and build the tree recursively, where lines 5-9 are the local computation step for computing encrypted split statistics; line 10-11 are the MPC computation step that converts the encrypted statistics into secretly shared values and decides the best split identifier using MPC; and line 12 is the model update step, which computes the encrypted indicator vectors for the child nodes given the best split.\n\n4.3 Tree Model Prediction\nAfter releasing the plaintext tree model, the clients can jointly make a prediction given a sample. In vertical FL, the features of a sample are distributed among the clients. Figure 3a shows an example of a released model, where each internal node represents a feature with a split threshold owned by a client, and each leaf node represents a predicted label on that path. To predict a sample, a naive method is to let the super client coordinate the prediction process [21] : starting from the root node, the client who has the node feature compares its value with the split threshold, and notifies super client the next branch; then the prediction is forwarded to the next node until a leaf node is reached. However, this method discloses the prediction path, from which a client can infer the other client's feature value along that path.\nTo ensure that no additional information other than the predicted output is leaked, we propose a distributed prediction method, as shown in Algorithm 4. Let z = (z1, \u22ef, zt+1) be the leaf label vector of the leaf nodes in the tree model, where t is the number of internal nodes. Note that all clients know z since the tree model is public in this protocol. Given a sample, clients collaborate to update an encrypted prediction vector [\u03b7] = ([1], \u22ef, [1]) with size t + 1 in a round-robin manner. Each element in [\u03b7] indicates if a prediction path is possible with encrypted form.\nWithout loss of generality, we assume that the prediction starts with um and ends with u1. If a prediction path is possible from the perspective of a client, then the client multiplies the designated element in [\u03b7] by 1 using homomorphic multiplication, otherwise by 0. Figure 3b illustrates an example of this method. Starting from u3, given the feature value 'deposit = 6000', u3 initializes [\u03b7] and updates it to([0], [1], [1], [0], [1]\n), since she can eliminate the first and fourth prediction paths after comparing her value with the split threshold 'deposit = 5000'. Then, u3 sends [\u03b7] to the next client for updates. After all clients' updates, there is only one [1] in [\u03b7], which indicates the true prediction path. Finally, u1 computes z \u2299 [\u03b7] to get the encrypted prediction output, and decrypts it jointly with all clients.\n\n4.4 Security Guarantees\nTheorem 1. The basic protocol of Pivot securely realizes the ideal functionalities FDTT and FDTP against a semihonest adversary who can statically corrupt up to m \u2212 1 out of m clients.\nProof Sketch. We need to show that, in the view of an adversary A, any information learned by the protocol can be learned directly from the input it has corrupted and the output it receives.\nFor model training, the proof can be reduced to the computations on one tree node because each node can be computed separately given that its output is public [47, 48]. There are two cases. First, when a given node is an internal node: (i) if the super client is corrupted, nothing is revealed in the local computation step regarding the honest client's data; while the MPC conversion [24] and additive secret sharing scheme [28] are secure, thus, the MPC computation step is secure; finally, in the model update step, if i * is an honest client, the transmitted message [\u03b1] is secure for the threshold Paillier scheme [61] is secure. (ii) if the super client is not corrupted, the only difference is the transmitted encrypted label information [ L ], which is also secure. Second, Algorithm 4: Pivot DT prediction (basic protocol)\nInput: T : decision tree model, {x i } m i=1 : input sample pk: the public key, {sk i } m i=1 : partial secret keys Output: k: predicted label For model prediction, the adversary A views an encrypted prediction vector [\u03b7] updated by the honest client(s) and the encrypted prediction output [ k], thus, no more information is learned (the decrypted prediction output is public) for the threshold Paillier scheme is secure. \u25fb1 for i \u2208 [m, 1] do 2 if i == m then 3 u i initializes [\u03b7] = ([1], \u22ef, [1]) with size t + 1 4 if i > 1 then 5 u i updates [\u03b7] using (T , x i ) 6 u i sends [\u03b7] to u i\u22121 7 else 8 u i updates [\u03b7] using (T , x i ) 9 u i initializes label vector z = (z 1 , \u22ef, z t+1 ) 10 u i computes [ k] = z \u2299 [\u03b7]\n\n5. ENHANCED PROTOCOL\nThe basic protocol guarantees that no intermediate information is disclosed. However, after obtaining the public model, colluding clients may extract private information of a target client's training dataset, with the help of their own datasets. We first present two possible privacy leakages in Section 5.1 and then propose an enhanced protocol that mitigates this problem by concealing some model information in Section 5.2. The security analysis is given in Section 5.3.\n\n5.1 Privacy Leakages\nWe identify two possible privacy leakages: the training label leakage and the feature value leakage, regarding a target client's training dataset. The intuition behind the leakages is that the colluding clients are able to split the sample set based on the split information in the model and their own datasets. We illustrate them by the following two examples given the tree model in Figure 3.\nExample 1. (Training label leakage). Assume that u2 and u3 collude, let us see the right branch of the root node. u2 knows exactly the sample set in this branch, say Dage > 30, as all samples are available on the root node and he can just split his local samples based on 'age = 30'. Then, u3 can classify this set into two subsets given the 'deposit=5000' split, say D age > 30 \u22c0 deposit \u2264 5000 and D age > 30 \u22c0 deposit > 5000 , respectively. Consequently, according to the plaintext class labels on the two leaf nodes, colluding clients may infer that the samples in D age > 30 \u22c0 deposit \u2264 5000 are with class 2 and vise versa, with high probability.\nExample 2. (Feature value leakage). Assume that u1 and u2 collude, let us see the path of u2 \u2192 u1 \u2192 u3 (with red arrows). Similar to Example 1, u1 and u2 can exactly know the training sample set on the 'u3' node before splitting, say D \u2032 . In addition, recall that u1 is the super client who has all sample labels, thus, he can easily classify D \u2032 into two sets by class, say D \u2032 1 and D \u2032 2 , respectively. Consequently, the colluding clients may infer that the samples in D \u2032 2 have 'deposit \u2264 5000' and vise versa, with high probability.\nNote that these two leakages happen when the clients (except the target client) along a path collude. Essentially, given the model, the colluding clients (without super client) may infer labels of some samples in the training dataset if there is no feature belongs to the super client along a tree path; similarly, if the super client involves in collusion, the feature values of some samples in the training dataset of a target client may be inferred.\n\n5.2 Hiding Label and Split Threshold\nOur observation is that these privacy leakages can be mitigated if the split thresholds on the internal nodes and the leaf labels on the leaf nodes in the model are concealed from all clients. Without such information, the colluding clients can neither determine how to split the sample set nor what leaf label a path owns. We now discuss how to hide these information in the model.\nFor the leaf label on each leaf node, the clients can convert it to an encrypted value, instead of reconstructing its plaintext. Specifically, after obtaining the secretly shared leaf label (e.g., \u27e8k\u27e9) using secure computations (Lines 1-3 in Algorithm 3), each client encrypts her own share of \u27e8k\u27e9 and broadcasts to all clients. Then, the encrypted leaf label can be computed by summing up these encrypted shares using homomorphic addition. As such, the leaf label is concealed.\nFor the split threshold on each internal node, the clients hide it by two additional computations in the model update step. Recall that in the basic protocol, the best split identifier (\u27e8i * \u27e9, \u27e8j * \u27e9, \u27e8s * \u27e9) is revealed to all clients after the MPC computation in each iteration. In the enhanced protocol, we assume that \u27e8s * \u27e9 is not revealed, thus the split threshold can be concealed. To support the tree model update without disclosing s * to the i * -th client, we first use the private information retrieval (PIR) [75, 74] technique to privately select the split indicator vectors of s * . Private split selection. Let n \u2032 = Sij denote the number of splits of the j * -th feature of the i * -th client. We assume n \u2032 is public for simplicity. Note that the clients can further protect n \u2032 by padding placeholders to a pre-defined threshold number. Instead of revealing \u27e8s * \u27e9 to the i * -th client, the clients jointly convert \u27e8s * \u27e9 into an encrypted indicator vector [\u03bb] = ([\u03bb1], \u22ef, [\u03bb n \u2032 ]) T , such that \u03bbt = 1 when t = s * and \u03bbt = 0 otherwise, where t \u2208 {1, \u22ef, n \u2032 }. This vector is sent to the i * -th client for private split selection at local. Let V n\u00d7n \u2032 = (v1, \u22ef, v n \u2032 ) be the split indicator matrix, where vt is the split indicator vector of the t-th split of the j * -th feature (see Section 4.1). The following theorem [74] suggests that the i * -th client can compute the encrypted indicator vector for the s * -th split without disclosing s * .\n\nTheorem 2. Given an encrypted indicator vector\n[\u03bb] = ([\u03bb1], \u22ef, [\u03bb n \u2032 ]) T such that [\u03bb s * ] = [1] and [\u03bbt] = [0]for all t \u2260 s * , and the indicator matrixV n\u00d7n \u2032 = (v1, \u22ef, v n \u2032 ), then [v s * ] = V \u2297[\u03bb]. \u25fb\nThe notion \u2297 represents the homomorphic matrix multiplication, which executes homomorphic dot product operations between each row in V and [\u03bb]. We refer the interested readers to [74] for the details.\nFor simplicity, we denote the selected [v s * ] as [v]. The encrypted split threshold can also be obtained by homomorphic dot product between the encrypted indicator vector [\u03bb] and the plaintext split value vector of the j * -th feature. Encrypted mask vector updating. After finding the encrypted split vector [v], we need to update the encrypted mask vector [\u03b1] for protecting the sample set recursively. This requires element-wise multiplication between [\u03b1] and [v]. Thanks to the MPC conversion algorithm, we can compute [\u03b1] \u22c5 [v] as follows [24]. For each element pair [\u03b1j] and [vj] where j \u2208 [1, n], we first convert [\u03b1j] into \u27e8\u03b1j\u27e9 = (\u27e8\u03b1j\u27e9 1 , \u22ef, \u27e8\u03b1j\u27e9 m ) using Algorithm 2, where \u27e8\u03b1j\u27e9 i (i \u2208 {1, \u22ef, m}) is the share hold by ui; then each client ui executes homomorphic multiplication \u27e8\u03b1j\u27e9 i \u2297 [vj] = [\u27e8\u03b1j\u27e9 i \u22c5 vj] and sends the result to the i * -th client; finally, the i * -th client can sum up the results using homomorphic addition:[\u03b1 \u2032 j ] = [\u27e8\u03b1j\u27e9 1 \u22c5 vj] \u2295 \u22ef \u2295 [\u27e8\u03b1j\u27e9 m \u22c5 vj] = [\u03b1j \u22c5 vj]\nAfter updating [\u03b1], the tree can also be built recursively, similar to the basic protocol.\nSecret sharing based model prediction. The prediction method in the basic protocol is not applicable here as the clients cannot directly compare their feature values with the encrypted split thresholds. Hence, the clients first convert the encrypted split thresholds and encrypted leaf labels into secretly shared form and make predictions on the secretly shared model using MPC. Let \u27e8z\u27e9 with size (t + 1) denote the secretly shared leaf label vector, where t is the number of internal nodes.\nTo make the prediction given a sample, the clients also provide the distributed feature values in secretly shared form. Similar to the prediction in the basic protocol, the clients initialize a secretly shared prediction vector \u27e8\u03b7\u27e9 with size (t + 1), indicating if a prediction path is possible. Then, they compute this vector as follows.\nThe clients initialize a secretly shared marker \u27e81\u27e9 for the root node. Starting from root node, the clients recursively compute the markers of its child nodes until all leaf nodes are reached. Then, the marker of each leaf node is assigned to the corresponding position in \u27e8\u03b7\u27e9, and there is only one \u27e81\u27e9 element in \u27e8\u03b7\u27e9, specifying the real prediction path in a secret manner. Specifically, each marker is computed by secure multiplication between its parent node's marker and a secure comparison result (between the secretly shared feature value and split threshold on this node). For example, in Figure 3a, the split threshold on the root node will be \u27e830\u27e9 while the feature value will be \u27e825\u27e9, then \u27e81\u27e9 is assigned to its left child and \u27e80\u27e9 to its right child. The clients know nothing about the assigned markers due to the computations are secure. Finally, the secretly shared prediction output can be computed easily by a dot product between \u27e8z\u27e9 and \u27e8\u03b7\u27e9, using secure computations. Discussion. A noteworthy aspect is that the clients can also choose to hide the feature \u27e8j * \u27e9 by defining n \u2032 as the total number of splits on the i * -th client, or even the client \u27e8i * \u27e9 that has the best feature by defining n \u2032 as the total number of splits among all clients. By doing so, the leakages could be further alleviated. However, the efficiency and interpretability would be degraded greatly. In fact, there is a trade-off between privacy and efficiency (interpretability) for the released model. The less information the model reveals, the higher privacy while the lower efficiency and less interpretability the clients obtain, and vise versa.\n\n5.3 Security Guarantees\nTheorem 3. The enhanced protocol of Pivot securely realizes the ideal functionalities FDTT and FDTP against a semi-honest adversary who can statically corrupt up to m\u22121 out of m clients. Proof Sketch. For model training, the only difference from the basic protocol is the two additional computations (private split selection and encrypted mask vector updating) in the model update step, which are computed using threshold Paillier scheme and MPC conversion. Thus, the security follows. For model prediction, since the additive secret sharing scheme is secure and the clients compute a secretly shared marker for every possible path, the adversary learns nothing except the final prediction output. \u25fb\n\n6. THEORETICAL ANALYSIS\nWe theoretically analyze the Pivot basic protocol and Pivot enhanced protocol in terms of computational cost for model training and model prediction, as summarized in Table 2. Let Ce and Cs roughly denote the costs for computations on a homomorphic encrypted value and on a secretly shared value, respectively. Due to that the threshold decryption (involving decryption of each client and combination via network communication) and secure comparison (involving multi-round network communications among the clients) are more time-consuming than the other computations, we consider these two operations separately for better analysis, and denote the costs of them by C d and Cc, respectively. Let d = max({di} m i=1 ) be the maximum number of features any client holds, b be the maximum number of splits any feature has, and c be the number of classes. Model prediction. With the basic protocol, the computational cost of prediction is updating an encrypted prediction vector with size (t + 1) in a Robin round, i.e., O(mt)Ce, the homomorphic dot product between the encrypted prediction vector and the plaintext label vector, i.e., O(t)Ce, and the threshold decryption of the final prediction output, i.e., O(1)C d . Thus, the total cost is O(mt)Ce + O(1)C d . With the enhanced protocol, the computational cost includes the secure comparison of t internal nodes and the secure dot product between the prediction vector and the label vector, i.e., O(t)(Cs + Cc).\nIn summary, regarding model training, the computational cost of the enhanced protocol is always larger than that of the basic protocol because the two additional computations are extra costs. Regarding model prediction, whether the basic protocol is better depends on the number of clients m and the relationship between ciphertext computation cost and secure computation cost. We will experimentally evaluate the two protocols in Section 8.\n\n7. EXTENSIONS TO OTHER ML MODELS\nSo far, Pivot supports a single tree model. Now we briefly present how to extend the basic protocol to ensemble tree models, including random forest (RF) [12] and gradient boosting decision tree (GBDT) [35, 36] in Section 7.1 and Section 7.2, respectively. Same as the basic protocol, we assume that all the trees can be released in plaintext. The extension to other machine learning models is discussed in Section 7.3.\n\n7.1 Random Forest\nRF constructs a set of independent decision trees in the training stage and outputs the class that is the mode of the classes (for classification) or mean prediction (for regression) of those trees in the prediction stage.\nFor model training, the extension from a single decision tree is natural since each tree can be built (using Algorithm 3) and released separately. For model prediction, after obtaining the encrypted predicted label of each tree, the clients can easily convert these encrypted labels into secret shares for majority voting using secure maximum computation (for classification) or compute the encrypted mean prediction by homomorphic computations (for regression).\n\n7.2 Gradient Boosting Decision Trees\nGBDT uses decision trees as weak learners and improves model quality with a boosting strategy [34]. The trees are built sequentially where the training labels for the next tree are the prediction losses between the ground truth labels and the prediction outputs of previous trees. Model training. The extension to GBDT is non-trivial, since we need to prevent the super client from knowing the training labels of each tree except the first tree (i.e., intermediate information) while facilitating the training process.\nWe first consider GBDT regression. Let W be the number of rounds and a regression tree is built in each round. Let Y w be the training label vector of the w-th tree. We aim to protect Y w by keeping it in an encrypted form. After building the w-th tree where w \u2208 {1, \u22ef, W \u2212 1}, the clients jointly make predictions for all training samples to get an encrypted estimation vector [ \u0232 w ]; then the clients can compute the encrypted training labels [Y w+1  ] of the (w + 1)-th tree given [Y w  ] and [ \u0232 w ]. Besides, note that in Section 4.2, an encrypted label square vector [\u03b3 w+1 2 ] is needed, which is computed by element-wise homomorphic multiplication between \u03b2 w+1 ] and [\u03b1] once at the beginning of each round, which reduces the cost.\nFor GBDT classification, we use the one-vs-the-rest technique by combining a set of binary classifiers. Essentially, the clients need to build a GBDT regression forest for each class, resulting in W * c regression trees in total (c is the number of classes). After each round in the training stage, the clients obtain c trees; and for each training sample, the clients make a prediction on each tree, resulting in c encrypted prediction outputs. Then, the clients jointly convert them into secretly shared values for computing secure softmax (which can be constructed using secure exponential, secure addition, and secure division, as mentioned in Section 2.2), and convert them back into an encrypted form as encrypted estimations. The rest of the computation is the same as regression. Model prediction. For GBDT regression, the prediction output can be decrypted after homomorphic aggregating the encrypted predictions of all trees. For GBDT classification, the encrypted prediction for each class is the same as that for regression; then the clients jointly convert these encrypted results into secretly shared values for deciding the final prediction output by secure softmax function.\n\n7.3 Other Machine Learning Models\nThough we consider tree-based models in this paper, the proposed solution can be easily adopted in other vertical FL models, such as logistic regression (LR), neural networks, and so on. The rationale is that these models can often be partitioned into the three steps described in Section 4.1. As a result, the TPHE primitives, conversion algorithm, and secure computation operators can be re-used.\nFor example, the clients can train a vertical LR model as follows. To protect the intermediate weights of the LR model during the training, the clients initialize an encrypted weight vector, [\u03b8] = ([\u03b81], \u22ef, [\u03b8m]), where [\u03b8i] corresponds to the encrypted weights of features held by client i. In each iteration, for a Sample t, each client i first locally aggregates an encrypted partial sum, say [\u03beit], by homomorphic dot product between [\u03b8i] and Sample t's local features xit. Then the clients jointly convert {[\u03beit]} m i=1 into secretly shared values using Algorithm 2, and securely aggregate them before computing the secure logistic function. Meanwhile, the su-per client also provides Sample t's label as a secretly shared value, such that the clients can jointly compute the secretly shared loss of Sample t. After that, the clients convert the loss back into the encrypted form (see Section 5.2), and each client can update her encrypted weights [\u03b8i] using homomorphic properties, without knowing the loss. Besides, the model prediction is a half component of one iteration in training, which can be easily computed.\n\n8. EXPERIMENTS\nWe evaluate the performance of Pivot basic protocol (Section 4) and Pivot enhanced protocol (Section 5) on the decision tree model, as well as the ensemble extensions (Section 7). We present the accuracy evaluation in Section 8.2 and the efficiency evaluation in Section 8.3.\nWe implement Pivot in C++ and employ the GMP 2 library for big integer computation and the libhcs 3 library for operations of the threshold Paillier scheme. We utilize the SPDZ 4 library for semi-honest additive secret sharing computations. Besides, we apply the libscapi 5 library to provide network communications among clients. Since the cryptographic primitives only support big integer computations, we convert the floating point datasets into fixed-point integer representation.\n\n8.1 Experimental Setup\nWe conduct experiments on a cluster of machines in a local area network (LAN). Each machine is equipped with Intel (R) Xeon (R) CPU E5-1650 v3 @ 3.50GHz\u00d712 and 32GB of RAM, running Ubuntu 16.04 LTS. Unless noted otherwise, the keysize of threshold Paillier scheme is 1024 bits and the security parameter of SPDZ configuration is 128 bits. Datasets. We evaluate the model accuracy using three realworld datasets: credit card data (30000 samples with 25 features) [79], bank marketing data (4521 samples with 17 features) [58], and appliances energy prediction data (19735 samples with 29 features) [14]. The former two datasets are for classification while the third dataset is for regression.\nWe evaluate the efficiency using synthetic datasets, which are generated with sklearn 6 library. Specifically, we vary the number of samples (n) and the number of total features (d) to generate datasets and then equally split these datasets w.r.t. features into m partitions, which are held by m clients, respectively. We denote d = d m as the number of features each client holds. For classification tasks, the number of classes is set to 4, and only one client holds the labels. Baselines. For accuracy evaluation, we adopt the nonprivate decision tree (NP-DT), non-private random forest (NP-RF), and non-private gradient-boosting decision tree (NP-GBDT) algorithms from sklearn for comparison. For a fair comparison, we adopt the same hyper-parameters for both our protocols and the baselines, e.g., the maximum tree depth, the pruning conditions, the number of trees, etc.\nFor efficiency evaluation, to our knowledge, there is no existing work providing the same privacy guarantee as Pivot. Therefore, we implement a secret sharing based decision tree algorithm using the SPDZ library (namely, SPDZ-DT) as a baseline. The security parameter of SPDZ-DT is also 128 bits. Besides, we also implement a non-private distributed decision tree (NPD-DT) algorithm as another baseline to illustrate the overhead of protecting the data privacy. In NPD-DT, the super client broadcasts plaintext labels to all clients, each client computes split statistics and exchanges them in plaintext with others to decide the best split.\nMetrics. For model accuracy, we measure the number of samples that are correctly classified over the total testing samples for classification; and the mean square error (MSE) between the predicted labels and the ground truth labels for regression. For efficiency, we measure the total running time of the model training stage and the prediction running time per sample of the model prediction stage. In all experiments, we report the running time of the online phase because SPDZ did not support the offline time benchmark for the semi-honest additive secret sharing protocol.\n\n8.2 Evaluation of Accuracy\nIn terms of accuracy, we compare the performance of the proposed decision tree (Pivot-DT), random forest (Pivot-RF) and gradient boosting decision tree (Pivot-GBDT) algorithms with their non-private baselines on three real world datasets. In these experiments, the keysize of threshold Paillier scheme is set to 512 bits. We conduct 10 independent trials of each experiment and report the average result.\nTable 3 summarizes the comparison results. We can notice that the Pivot algorithms achieve accuracy comparable to the non-private baselines. There are two reasons for the slight loss of accuracy. First, we use the fixed-point integer to represent float values, whose precision is thus truncated. Second, Pivot has only implemented the basic algorithms, which are not optimized as the adopted baselines.\n\n8.3 Evaluation for Efficiency\nIn terms of efficiency, we evaluate the training and prediction time of Pivot with the two protocols (namely Pivot-Basic and Pivot-Enhanced) in Section 8.3.1 and Section 8.3.2, by varying the number of clients (m), the number of samples (n), the number of features of each client ( d), the maximum number of splits (b), the maximum tree depth (h), and the number of trees (W ) for ensemble methods.\nWe employ parallelism for threshold decryption of multiple ciphertexts with 6 cores, which is observed to be the most time-consuming part in Pivot. The secure computations using SPDZ are not parallelized because the current SPDZ cannot express parallelism effectively and flexibly. These partially parallelized versions are denoted as Pivot-Basic-PP and Pivot-Enhanced-PP, respectively. The comparison with the baselines is reported in Section 8.3.3. Table 4 describes the ranges and default settings of the evaluated parameters.\n\n8.3.1 Evaluation on Training Efficiency\nVarying m. In addition, the gap between Pivot-Basic and Pivot-Enhanced is stable as d or b increases. This is because that d does not affect the additional costs in Pivot-Enhanced, and b only has small impact via private split selection (i.e., O(nb) ciphertext computations) which is negligible comparing to the encrypted mask vector updating computation.\nVarying h. Figure 4e shows the performance for varying h. Since the generated synthetic datasets are sampled uniformly, the trained models tend to construct a full binary tree, where the number of internal nodes is 2 h \u2212 1 given the maximum tree depth h. Therefore, the training time of all algorithms approximately double when h increases by one. Varying W . Figure 4f shows the performance for varying W in ensemble methods. RF classification is slightly slower than RF regression as the default c is 4 in classification comparing to 2 in regression. GBDT regression is slightly slower than RF regression, since additional computations are required by GBDT to protect intermediate training labels. Besides, the training time of GBDT classification is much longer than GBDT regression for two overheads: one is the one-vs-the-rest strategy, which means W * c trees are trained; the other is the secure softmax computation on c encrypted\n\n8.3.2 Evaluation on Prediction Efficiency\nVarying m. Figure 4g compares the prediction time per sample for varying m. Results show that the prediction time of Pivot-Enhanced is higher than Pivot-Basic, because the cost of secure comparisons is higher than the homomorphic computations. Besides, the prediction time of Pivot-Basic increases faster than that of Pivot-Enhanced as m increases. The reason is that the communication round for distributed prediction in Pivot-Basic scales linearly with m; while in Pivot-Enhanced, the number of secure comparisons remains the same, the increasing of m only incurs slight overhead. Varying h. Figure 4h compares the prediction time per sample for varying h. When h = 2, Pivot-Enhanced takes less prediction time because the number of internal nodes (i.e. secure comparisons) is very small. Pivot-Basic outperforms Pivot-Enhanced when h \u2265 3 and this advantage increases as h increases for two reasons. Firstly, the number of internal nodes is proportional to 2 h \u2212 1. Secondly, as described in Figure 4g, the number of clients dominates the prediction time of Pivot-Basic; although the size of the prediction vector also scales to h, its effect is insignificant since the size is still very small, leading to stable performance.\n\n8.3.3 Comparison with Baseline Solution\nWe compare the Pivot protocols with the baselines SPDZ-DT and NPD-DT. For NPD-DT, we report the training time for varying m and n in Figure 5a -5b, and the prediction time per sample for varying m and h in Figure 4g-4h. In all the evaluated NPD-DT experiments, the training time is less than 1 minute, and the prediction time is less than 1 ms. Nevertheless, the efficiency of NPD-DT is at the cost of data privacy. For SPDZ-DT, since it is not parallelized, we adopt the non-parallelized versions We omit the comparison of prediction time, because the model prediction in SPDZ-DT is similar to that in Pivot-Enhanced. We compare with SPDZ-DT for varying m and n. Varying m. Figure 5a shows the comparison for varying m. When m = 2, Pivot-Enhanced and SPDZ-DT achieve simi-lar performance. However, the training time of SPDZ-DT increases much faster as m increases because almost every secure computation in SPDZ-DT requires communication among all clients while most computations in Pivot protocols can be executed locally. We can notice that Pivot-Basic and Pivot-Enhanced can achieve up to about 19.8x and 4.5x speedup over SPDZ-DT, respectively. Varying n. Figure 5b shows the comparison for varying n. Both Pivot-Enhanced and SPDZ-DT scale linearly to n and SPDZ-DT increases more quickly than Pivot-Enhanced. When n is small (e.g., n = 5K), the three algorithms achieve almost the same performance. While when n = 200K, Pivot-Basic and Pivot-Enhanced are able to achieve about 37.5x and 1.8x speedup over SPDZ-DT.\n\n9. FURTHER PROTECTIONS\nThis section extends Pivot to account for malicious adversaries (in Section 9.1), and to incorporate differential privacy for enhanced protection (in Section 9.2).\n\n9.1 Extension to Malicious Model\nWe demonstrate how to extend Pivot to account for malicious adversaries. Recall that we assumed a semi-honest adversary in Pivot, which means the clients do the executions correctly. In the malicious model, an adversary may deviate from the specified protocol to infer the private data. For example, in Algorithm 2, if u1 only adds its own encrypted share [r1] to compute [e] (line 4), then it can infer the private data x after the threshold decryption. To prevent such malicious behaviors, we let each client prove that it executes the specified protocol on the correct data (i.e., the data a client promises to use) step by step. Once a client deviates from the protocol or uses incorrect data in any step, the other clients will detect it and abort the execution.\nFor this purpose, we extend Pivot using zero-knowledge proofs (ZKP) [24, 26] and authenticated shares in SPDZ [28, 46], inspired by [81]. We first present some building blocks in Section 9.1.1, then we introduce the extension to the basic protocol and the enhanced protocol in Section 9.1.2 and Section 9.1.3, respectively. Zero-knowledge proofs (ZKP). We use ZKP [24, 11, 26, 81] to ensure that each client performs the local computation correctly, even if up to m \u2212 1 clients collude maliciously. Generally, ZKP enables a prover to prove to a verifier that a certain statement is true, without conveying any secret information for the statement. We mainly use the following existing building blocks of \u03a3-protocol for ZKP.\n\u2022 Proof of plaintext knowledge (POPK): it takes a ciphertext [a] as input and proves that the prover knows the plaintext a * such that a * = Dec([a]) [24].\n\u2022 ) [81]. Note that the interactive \u03a3-protocol with honest verifier can be transformed into efficient non-interactive zero-knowledge (with random oracle assumption) and full zero-knowledge using existing techniques [31, 37, 81]. SPDZ authenticated shares. SPDZ can ensure malicious security even up to m \u2212 1 clients may deviate arbitrarily from the protocol using the information-theoretic message authentication code (MAC) [28, 46]. The secure computation building blocks described in Section 2.2 are supported accordingly. In SPDZ, given a value a \u2208 Zq, its authenticated secretly shared value is represented by \u27e8a\u27e9 = (\u27e8a\u27e9 1 , \u22ef, \u27e8a\u27e9 m , \u27e8\u03b4\u27e9 1 , \u22ef, \u27e8\u03b4\u27e9 m , \u27e8\u2206\u27e9 1 , \u22ef, \u27e8\u2206\u27e9 m ), such that client i holds the random share \u27e8a\u27e9 i , the random MAC share \u27e8\u03b4\u27e9 i and the fixed MAC key share \u27e8\u2206\u27e9 i , and the MAC relation \u03b4 = a\u22c5\u2206 holds. The MAC-related shares ensure that no client can modify \u27e8a\u27e9 i without being detected. When reconstructing a secretly shared value \u27e8a\u27e9, every client i \u2208 {1, \u22ef, m} first broadcast their shares \u27e8a\u27e9 i and compute a = \u2211 m i=1 \u27e8a\u27e9 i . To ensure that a is correct, every client then checks the MAC by computing and opening \u27e8\u03b4\u27e9 i \u2212a\u22c5\u27e8\u2206\u27e9 i , then checking these shares sum up to zero. If the MAC is incorrect, then the malicious behavior can be detected. Modified MPC conversion. Since Pivot applies a hybrid framework of TPHE and MPC, we need to modify Algorithm 1 as follows to make the MPC conversion process satisfy malicious security [24]. Specifically, we further let each client i: (i) broadcast [ri] together with POPK (line 3), ensuring that client i knows ri; (ii) compute [e] and call threshold decryption (line 4), ensuring that every client have computed the same e; and (iii) broadcast [xi] together with POPK (lines 6-8) for committing its own share. Then, the verifier can easily compute [e \u2212 r1] (if i = 1) or [\u2212ri] (if i \u2260 1) using homomorphic properties (since both [e] and [ri] are known to all), and check if it matches [xi] using a secure equality protocol under malicious model (e.g., [45]). . Therefore, each client can broadcast these encrypted statistics and prove that she performs these computations correctly using POHDP.\n\n9.1.2 Basic Protocol Extension\nMPC computation. After utilizing the modified MPC conversion algorithm, the clients obtain random shares for the encrypted statistics. To ensure that the random shares are not modified before combining with the MAC shares and computing with SPDZ, the clients also need to verify that these shares (along with the MAC shares) are valid and indeed match with the converted encrypted values [81]. The rest of SPDZ computations are malicious secure as the authenticated secret sharing scheme is malicious secure, and the best split identifier \u27e8i * \u27e9, \u27e8j * \u27e9, \u27e8s * \u27e9 can be found and revealed to all clients.\nModel update. In this step, client i * first selects the corresponding split indicator vectors v l and vr of the s * -th split of the j * -th feature, and computes [\u03b1 l ] and [\u03b1r] by element-wise homomorphic multiplication using [\u03b1], which can be proved by POPCM. Note that the other clients can select the corresponding [v l ] and [vr] for the verification given the best split identifier, as they have been committed beforehand.\nSimilarly, the pruning condition check and leaf label computation can be proved. For example, if any pruning condition is satisfied, the super client can first compute and broadcast the encrypted number of samples for each class k (say [g k ]) by summing up all elements in [\u03b3 k ], where k \u2208 K. Note that the verifier can execute the same computations (since [\u03b3 k ] is known to all) and use the secure equality pro-tocol (e.g., [45]) to verify the correctness. Then the clients convert them into shares, i.e., \u27e8g k \u27e9, and find the leaf label \u27e8k\u27e9 that has the maximum \u27e8g k \u27e9 using the secure maximum operation (see Section 4.1). The correctness can be ensured by the modified MPC conversion algorithm and the MACbased SPDZ scheme. Finally, the leaf label can be revealed.\nThe above constructions guarantee that each client correctly follows the specified training protocol and uses the same data (i.e., split indicator vectors, and label indicator vectors, as committed before training) during the whole process. If the verification of any computation is incorrect, the execution will be aborted. Model prediction. To ensure malicious security in the model prediction process, each client also needs to prove that she executes the specified computations using the correct data. Similar to the model training stage, each client can commit her data by encrypting and broadcasting the indicator of each sample's value comparing to the corresponding split threshold in the tree model. For example, the clients can commit their testing data for prediction along with the training data using the split indicator vectors; then a verifier can retrieve the other clients' committed indicators (for both left branch and right branch) given the sample index, client index, feature index and split index.\nAs described in Algorithm 4, the clients execute the prediction process in a round-robin manner. At first, client m initializes an encrypted prediction vector [\u03b7] = ([1], \u22ef, [1]) with size t + 1 and broadcasts it to the other clients, where t is the number of internal nodes. The clients can jointly decrypt [\u03b7] to check the correctness since the elements in this vector are known to all at the beginning. Then client m updates [\u03b7] using her local sample indicators. For example, in Figure 3, there is one feature in the tree model that belongs to u3, and the indicators are 0 (for the left branch) and 1 (for the right branch). For any tree node in the model, the client updates the corresponding leaf indexes in [\u03b7] using the indicators. For example, u3 updates the first and fourth elements by homomorphic multiplication using 0 while the second and fifth elements by homomorphic multiplication using 1. For any update, the client broadcasts the updated [\u03b7] together with POPCM such that the other clients can verify the correctness. After [\u03b7] is updated by the last client (i.e., u1 in Algorithm 4), each client can homomorphicly aggregate [\u03b7] and call threshold decryption to check if the sum is 1, ensuring that there is only one valid prediction path. Consequently, each client computes the homomorphic dot product operation between [\u03b7] and the leaf label vector z and calls threshold decryption to get the prediction output.\n\n9.1.3 Enhanced Protocol Extension\nModel training. The commitment and the local computation step are exactly the same as those of the basic protocol. In the MPC computation step, instead of revealing \u27e8s * \u27e9, the clients compute a secretly shared indicator vector \u27e8\u03bb\u27e9 = (\u27e8\u03bb1\u27e9, \u22ef, \u27e8\u03bb n \u2032 \u27e9) using SPDZ, where \u03bbt = 1 when t = s * and \u03bbt = 0 otherwise. This step is malicious secure since SPDZ is malicious secure. Then, for each value in \u27e8\u03bb\u27e9, the clients convert it into an encrypted value, by encrypting and broadcasting each share, and homomorphicly aggregating them together. The clients also need to verify that each encrypted value indeed match with the converted secretly shared value [81]. , where V is the split indicator matrix for all the splits of the j * feature and has been committed before training. The private split selection actually executes n homomorphic dot product operations, which can be proved using POHDP. After that, client i * executes an element-wise ciphertext multiplication between the selected encrypted split indicator vector [v] and the encrypted mask vector [\u03b1] (see Section 5.2). The correctness can be ensured by the modified MPC conversion algorithm, the homomorphic multiplication together with POPCM, and the conversion from secretly shared value to ciphertext (as discussed above).\nSimilarly, after obtaining the secretly shared leaf label, the clients can jointly convert it into ciphertext, instead of revealing it. Therefore, the model training satisfies malicious security. Model prediction. Recall that in the enhanced protocol, the clients first convert the tree model (with an encrypted split threshold on each internal node and encrypted leaf label on each leaf node) into secretly shared tree model, as well as convert their input feature values into secretly shared form. The conversion can be performed by the modified MPC conversion algorithm together with additional verification of the SPDZ authenticated shares (as discussed in Section 9.1.1). After that, the rest of the computations can be executed using malicious secure SPDZ, and the prediction output can be obtained.\n\n9.2 Incorporating Differential Privacy\nWe can incorporate differential privacy (DP) [30, 42, 66, 22, 72, 41] to provide further protection, ensuring that the released model (even in the plaintext form) leaks limited information about individual's private data in the training dataset. In a nutshell, a computation is differentially private if the probability of producing a given output does not depend very much on whether a particular sample is included in the input dataset [30, 66]. Formally, for any two datasets D and D \u2032 differing in a single sample and any output O of a function f ,Pr[f (D) \u2208 O] \u2264 e \u22c5 Pr[f (D \u2032 ) \u2208 O]\nThe parameter is the differential privacy budget that controls the tradeoff between the accuracy of f and how much information it discloses. In our case, f trains a CART tree model with multiple iterations where a tree node is built in each iteration. In the centralized DP (CDP) setting, a typical method for training a differentially private CART tree model is to make three queries satisfy DP in each iteration [33, 32] : (i) pruning condition query (check if the number of samples n on a tree node is less than a threshold); (ii) non-leaf query (determine the best split as a whole query); and (iii) leaf query (compute the leaf label). Moreover, recall that in Pivot, no intermediate information is disclosed other than the tree model (i.e., each tree node) to be released. As a result, the executions of Pivot essentially mimic the CDP setting in a way similar to [22]. Therefore, we can incorporate the above CDP method into Pivot to make the training differentially private. We briefly introduce the classification tree case as follows (the extension to the regression tree is similar). We assign a DP budget to each query. First, the clients can compute [n] by homomorphicly aggregating [\u03b1] and convert it to secretly shared \u27e8n\u27e9. Before checking the condition, the clients jointly add a secretly shared random noise \u27e8Lap(\u2206 )\u27e9 to \u27e8n\u27e9 according to the Laplace mechanism [30], where \u2206 is the sensitivity of the query, denoting the largest possible difference that one sample can have on the output of the query. Here \u2206 = 1 since the count query can affect the output by maximum 1. Note that the random noise can be easily generated in an MPC way since the required primitives are all supported in SPDZ, such that no client knows the plaintext noise. Algorithm 5 describes how to sample a secretly shared value from Laplace distribution using SPDZ. There are two steps: (i) uniformly samples a secretly shared value \u27e8U \u27e9 within (\u2212 1 2 , 1 2 ) (line 1), the primitive is also supported in SPDZ [4, 28]; and (ii) computes the secretly shared value \u27e8X\u27e9 = \u00b5 \u2212 b \u22c5 sgn(\u27e8U \u27e9) ln(1 \u2212 2 \u22c5 \u27e8U \u27e9 ) (line 2-9), where \u00b5 and b are the location parameter and scale parameter of the Laplace distribution. According to the inverse transform sampling [29, 70, 2], the result \u27e8X\u27e9 follows the Laplace distribution with parameters \u00b5 and b. In our case, \u00b5 = 0 and b = \u2206 . Consequently, the clients obtain the desired secretly shared random noise to be added on \u27e8n\u27e9, and no one learns the plaintext noise.\nSecond, if the condition is not satisfied, the clients compute the best split as described in Section 4.1, then the clients can jointly use the exponential mechanism [30] to choose the best split where the sensitivity of the Gini impurity gain is \u2206 = 2 [33]. Algorithm 6 describes the random selection using SPDZ based on the exponential mechanism, where the inputs are a number of R secretly shared scores, the differential privacy budget, and the sensitivity of the score function. The clients first compute the secretly shared probabilities according to the exponential mechanism (line 1-2). Next, the clients normalize these probabilities such that the sum is \u27e81\u27e9; meanwhile, the clients compute the secretly shared cumulative probability for each index (line 3-7), which will be used for randomly sampling from discrete distribution (as what does in the exponential mechanism). After that, the clients arrange R sub-intervals within (0, 1) according to the cumulative probabilities (line 7). Then they uniformly sample a secretly shared value \u27e8U \u27e9 within (0, 1) (line 8), and find the sub-interval that \u27e8U \u27e9 falls into (lines 9-14). The secretly shared index of the sub-interval follows the discrete distribution computed above [40, 29], which satisfies the exponential mechanism. Importantly, all the computations are executed in an MPC way using SPDZ, such that the clients learn nothing. In our case, in particular, after computing the impurity gain for all possible splits, the clients obtain a set of secretly shared impurity gains, which can be viewed as scores of the splits. The clients can use Algorithm 6 to decide the best split while satisfying DP.\nThird, if the condition is satisfied, the clients compute the encrypted number of samples for each class k \u2208 K, convert them into secretly shared values, and add Laplace noise \u27e8Lap(\u2206 )\u27e9 to each value using Algorithm 5 before computing the leaf label, where \u2206 = 1. Since each class contains disjoint samples, the noise adding can be composed in parallel [30]. Notice that the budgets of queries on different tree nodes on the same depth do not accumulate, as they are carried out on disjoint samples [33]. Besides, each tree node consumes 2 budget since the pruning condition query is indispensable. As a result, the training satisfies B -DP, where B = 2(h + 1) and h is the maximum tree depth [33]. Similar to [22], the DP guarantee is under computational differential privacy [56], as the adversary in the MPC setting is assumed to be computationally bounded.\nThe integration of DP with the enhanced protocol is the same as the basic protocol since the two additional computations are independent of the DP operations. Meanwhile, it has an additive protection effect when integrating DP with the enhanced protocol, because an adversary needs to reverse the concealed model first before obtaining the differentially private model. A noteworthy aspect is that, since both the Laplace noise generation (Algorithm 5) and the random selection using exponential mechanism (Algorithm 6) are computed using SPDZ, we can easily incorporate DP into the malicious model (see Section 9.1) by replacing the semihonest SPDZ scheme with the authenticated SPDZ scheme.\n\n10. RELATED WORK\nThe works most related to ours are [71, 67, 68, 69, 21, 50, 44] for privacy preserving vertical tree models. None of these solutions, however, achieve the same privacy guarantee as our solution.  [71, 44] assume that the super client's labels can be directly shared in plaintext with other clients, which obviously violates the privacy regulations.  [67, 68, 69, 21, 50] allow that some intermediate information during the training or prediction process can be revealed in plaintext, which compromises the client's data privacy. For example, all these solutions assume that the available sample ids on a tree node is public, from which any adversarial client can easily infer that those samples belong to the same class (given a leaf node) or have similar feature values (given any node except the root node) with regards to the split feature on its parent node, with high probability. Also, [21, 50] allow the split statistics for determining the best split to be revealed in plaintext to the super client, which discloses the client's data distribution.\nMeanwhile, although several general techniques may be applicable to our problem, they suffer from either privacy deficiency or inefficiency. Secure hardware (e.g., Intel SGX [51]) protects client's private data using secure enclaves [60, 80]. However, it relies on the assumption of a trusted third party (e.g., Intel Corporation) and is vulnerable to side channel attacks [76], which is not acceptable to many organizations. While secure multiparty computation (MPC) [78] could provide a strong privacy guarantee, training machine learning models using generic MPC frameworks is extremely inefficient [81]. Some other tailored MPC solutions (e.g., [57, 59]) that propose to outsource client's data to non-colluding servers are unrealistic in practice since it is difficult to find those qualified servers convincing the clients. Our solution falls into the tailored MPC technique and does not rely on any external party, achieving accuracy comparable to the non-private solutions.  [81] also uses a hybrid framework of TPHE and MPC, but it mainly focuses on linear models in horizontal FL, while our work addresses tree-based models in vertical FL and further considers the privacy leakages after releasing the model.\nFinally, there are a number of works on collaborative prediction [23, 10, 39, 49] that consists of two parties, one is the server holds the private model and the other is the client holds private data. After prediction, nothing more than the predicted output is revealed to both parties. However, these solutions cannot be directly adopted in our problem. Since every client knows (a share of) the tree model and holds a subset of feature values in our setting. Although [21, 50] consider the same vertical FL scenario as ours, their methods disclose the prediction path (see Section 4.3) and thus leak client's data privacy along that path. In contrast, our solution guarantees that no intermediate information other than the final prediction output is revealed.\n\n11. CONCLUSIONS\nWe have proposed Pivot, a privacy preserving solution with two protocols for vertical tree-based models. With the basic protocol, Pivot guarantees that no intermediate information is disclosed during the execution. With the enhanced protocol, Pivot further mitigates the possible privacy leakages occurring in the basic protocol. To our best knowledge, this is the first work that provides strong privacy guarantees for vertical tree-based models. The experimental results demonstrate Pivot achieves accuracy comparable to non-private algorithms and is highly efficient.\n\nFootnotes:\n1: https://www.comp.nus.edu.sg/ ~dbsystem/fintech/ project/falcon/\n2: http://gmplib.org\n3: https://github.com/tiehuis/libhcs\n4: https://github.com/data61/MP-SPDZ\n5: https://github.com/cryptobiu/libscapi\n6: https://scikit-learn.org/stable/\n\nReferences:\n\n- Laplace distribution. https://en.wikipedia.org/wiki/laplace distribution.- Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). oj, 2016-04-27.\n\n- data61/mp-spdz: Versatile framework for multiparty computation. https://github.com/data61/mp-spdz. Accessed: 2019-11-25.\n\n- T. Araki, A. Barak, J. Furukawa, M. Keller, Y. Lindell, K. Ohara, and H. Tsuchida. Generalizing the SPDZ compiler for other protocols. In CCS, pages 880-895, 2018.\n\n- A. T. Azar and S. M. El-Metwally. Decision tree classifiers for automated medical diagnosis. Neural Computing and Applications, 23(7-8):2387-2403, 2013.\n\n- D. Beaver. Efficient multiparty protocols using circuit randomization. In CRYPTO, 1991.\n\n- A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. B. Calo. Analyzing federated learning through an adversarial lens. In ICML, pages 634-643, 2019.\n\n- K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In CCS, pages 1175-1191, 2017.\n\n- R. Bost, R. A. Popa, S. Tu, and S. Goldwasser. Machine learning classification over encrypted data. In NDSS, 2015.\n\n- F. Boudot. Efficient proofs that a committed number lies in an interval. In EUROCRYPT, pages 431-444, 2000.\n\n- L. Breiman. Random forests. Machine Learning, 45(1):5-32, 2001.\n\n- L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and regression trees. 1984.\n\n- L. M. Candanedo, V. Feldheim, and D. Deramaix. Data driven prediction models of energy use of appliances in a low-energy house. Energy and Buildings, 140:81-97, 2017.\n\n- R. Canetti. Universally composable security: A new paradigm for cryptographic protocols. In FOCS, pages 136-145, 2001.\n\n- S. Cao, X. Yang, C. Chen, J. Zhou, X. Li, and Y. Qi. Titant: Online real-time transaction fraud detection in ant financial. PVLDB, 12(12):2082-2093, 2019.\n\n- O. Catrina and S. de Hoogh. Improved primitives for secure multiparty integer computation. In SCN, pages 182-199, 2010.\n\n- O. Catrina and A. Saxena. Secure computation with fixed-point numbers. In FC, pages 35-50, 2010.\n\n- H. Chen, K. Laine, and P. Rindal. Fast private set intersection from homomorphic encryption. In CCS, pages 1243-1255, 2017.\n\n- T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In SIGKDD, pages 785-794, 2016.\n\n- K. Cheng, T. Fan, Y. Jin, Y. Liu, T. Chen, and Q. Yang. Secureboost: A lossless federated learning framework. CoRR, abs/1901.08755, 2019.\n\n- A. R. Chowdhury, C. Wang, X. He, A. Machanavajjhala, and S. Jha. Crypt: Crypto-assisted differential privacy on untrusted servers. SIGMOD, 2020.\n\n- M. D. Cock, R. Dowsley, C. Horst, R. S. Katti, A. C. A. Nascimento, W. Poon, and S. Truex. Efficient and private scoring of decision trees, support vector machines and logistic regression models based on pre-computation. IEEE TDSC, 16(2):217-230, 2019.\n\n- R. Cramer, I. Damg\u00e5rd, and J. B. Nielsen. Multiparty computation from threshold homomorphic encryption. In EUROCRYPT, pages 280-299, 2001.\n\n- R. Cramer, I. B. Damgrd, and J. B. Nielsen. Secure multiparty computation and secret sharing. 2015.\n\n- I. Damg\u00e5rd. On \u03c3-protocol. In Lecture Notes, 2010.\n\n- I. Damg\u00e5rd and M. Jurik. A generalisation, a simplification and some applications of paillier's probabilistic public-key system. In Public Key Cryptography, pages 119-136, 2001.\n\n- I. Damg\u00e5rd, V. Pastro, N. P. Smart, and S. Zakarias. Multiparty computation from somewhat homomorphic encryption. In CRYPTO, pages 643-662, 2012.\n\n- L. Devroye. Non-Uniform Random Variate Generation. Springer, 1986.\n\n- C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.\n\n- S. Faust, M. Kohlweiss, G. A. Marson, and D. Venturi. On the non-malleability of the fiat-shamir transform. In INDOCRYPT, pages 60-79, 2012.\n\n- S. Fletcher and M. Z. Islam. Decision tree classification with differential privacy: A survey. ACM Comput. Surv., 52(4):83:1-83:33, 2019.\n\n- A. Friedman and A. Schuster. Data mining with differential privacy. In SIGKDD, pages 493-502, 2010.\n\n- J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28:2000, 1998.\n\n- J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189-1232, 2000.\n\n- F. Fu, J. Jiang, Y. Shao, and B. Cui. An experimental evaluation of large scale GBDT systems. PVLDB, 12(11):1357-1370, 2019.\n\n- J. A. Garay, P. D. MacKenzie, and K. Yang. Strengthening zero-knowledge protocols using signatures. In EUROCRYPT, pages 177-194, 2003.\n\n- C. Ge, I. F. Ilyas, and F. Kerschbaum. Secure multi-party functional dependency discovery. PVLDB, 13(2):184-196, 2019.\n\n- R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and J. Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In ICML, pages 201-210, 2016.\n\n- J. Goldstick. Introduction to statistical computing, statistics 406, notes -lab 5. Lecture Notes, Department of Statistics, 2009.\n\n- M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private histograms through consistency. PVLDB, 3(1):1021-1032, 2010.\n\n- X. He, A. Machanavajjhala, C. J. Flynn, and D. Srivastava. Composing differential privacy and secure computation: A case study on scaling private record linkage. In CCS, pages 1389-1406, 2017.\n\n- B. Hitaj, G. Ateniese, and F. P\u00e9rez-Cruz. Deep models under the GAN: information leakage from collaborative deep learning. In CCS, pages 603-618, 2017.\n\n- Y. Hu, D. Niu, J. Yang, and S. Zhou. FDML: A collaborative machine learning framework for distributed features. In SIGKDD, pages 2232-2240, 2019.\n\n- M. Kantarcioglu and O. Kardes. Privacy-preserving data mining in the malicious model. IJICS, 2(4):353-375, 2008.\n\n- M. Keller, E. Orsini, and P. Scholl. MASCOT: faster malicious arithmetic secure computation with oblivious transfer. In CCS, pages 830-842, 2016.\n\n- Y. Lindell and B. Pinkas. Privacy preserving data mining. In CRYPTO, pages 36-54, 2000.\n\n- Y. Lindell and B. Pinkas. Secure multiparty computation for privacy-preserving data mining. J. Priv. Confidentiality, 1(1), 2009.\n\n- J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network predictions via minionn transformations. In CCS, pages 619-631, 2017.\n\n- Y. Liu, Y. Liu, Z. Liu, J. Zhang, C. Meng, and Y. Zheng. Federated forest. CoRR, abs/1905.10053, 2019.\n\n- F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shafi, V. Shanbhogue, and U. R. Savagaonkar. Innovative instructions and software model for isolated execution. In HASP, page 10, 2013.\n\n- H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas. Federated learning of deep networks using model averaging. CoRR, abs/1602.05629, 2016.\n\n- H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. In ICLR, 2018.\n\n- C. A. Meadows. A more efficient cryptographic matchmaking protocol for use in the absence of a continuously available third party. In IEEE S&P, pages 134-137, 1986.\n\n- L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting unintended feature leakage in collaborative learning. In IEEE S&P, pages 691-706, 2019.\n\n- I. Mironov, O. Pandey, O. Reingold, and S. P. Vadhan. Computational differential privacy. In CRYPTO, pages 126-142, 2009.\n\n- P. Mohassel and Y. Zhang. Secureml: A system for scalable privacy-preserving machine learning. In IEEE S&P, pages 19-38, 2017.\n\n- S. Moro, P. Cortez, and P. Rita. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22-31, 2014.\n\n- V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-preserving ridge regression on hundreds of millions of records. In IEEE S&P, pages 334-348, 2013.\n\n- O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and M. Costa. Oblivious multi-party machine learning on trusted processors. In USENIX Security Symposium, pages 619-636, 2016.\n\n- P. Paillier. Public-key cryptosystems based on composite degree residuosity classes. In EUROCRYPT, pages 223-238, 1999.\n\n- B. Pinkas, T. Schneider, G. Segev, and M. Zohner. Phasing: Private set intersection using permutation-based hashing. In USENIX Security Symposium, pages 515-530, 2015.\n\n- B. Pinkas, T. Schneider, and M. Zohner. Scalable private set intersection based on OT extension. ACM Trans. Priv. Secur., 21(2):7:1-7:35, 2018.\n\n- J. R. Quinlan. Induction of decision trees. Mach. Learn., 1(1):81-106, Mar. 1986.\n\n- J. R. Quinlan. C4.5: Programs for machine learning. Morgan Kaufmann Publishers Inc., 1993.\n\n- R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In CCS, pages 1310-1321, 2015.\n\n- J. Vaidya and C. Clifton. Privacy-preserving decision trees over vertically partitioned data. In DBSec, pages 139-152, 2005.\n\n- J. Vaidya, C. Clifton, M. Kantarcioglu, and A. S. Patterson. Privacy-preserving decision trees over vertically partitioned data. TKDD, 2(3):14:1-14:27, 2008.\n\n- J. Vaidya, B. Shafiq, W. Fan, D. Mehmood, and D. Lorenzi. A random decision tree framework for privacy-preserving data mining. IEEE TDSC, 11(5):399-411, 2014.\n\n- C. R. Vogel. Computational Methods for Inverse Problems, volume 23 of Frontiers in Applied Mathematics. SIAM, 2002.\n\n- K. Wang, Y. Xu, R. She, and P. S. Yu. Classification spanning private databases. In AAAI, pages 293-298, 2006.\n\n- N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu. Collecting and analyzing multidimensional data with local differential privacy. In ICDE, pages 638-649, 2019.\n\n- D. J. Wu, J. Zimmerman, J. Planul, and J. C. Mitchell. Privacy-preserving shortest path computation. In NDSS, 2016.\n\n- Y. Wu, K. Wang, R. Guo, Z. Zhang, D. Zhao, H. Chen, and C. Li. Enhanced privacy preserving group nearest neighbor search. IEEE TKDE, 2019.\n\n- Y. Wu, K. Wang, Z. Zhang, W. Lin, H. Chen, and C. Li. Privacy preserving group nearest neighbor search. In EDBT, pages 277-288, 2018.\n\n- Y. Xu, W. Cui, and M. Peinado. Controlled-channel attacks: Deterministic side channels for untrusted operating systems. In IEEE S&P, pages 640-656, 2015.\n\n- Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications. ACM TIST, 10(2):12:1-12:19, 2019.\n\n- A. C. Yao. Protocols for secure computations (extended abstract). In FOCS, pages 160-164, 1982.\n\n- I. Yeh and C. Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Syst. Appl., 36(2):2473-2480, 2009.\n\n- W. Zheng, A. Dave, J. G. Beekman, R. A. Popa, J. E. Gonzalez, and I. Stoica. Opaque: An oblivious and encrypted distributed analytics platform. In NSDI, pages 283-298, 2017.\n\n- W. Zheng, R. A. Popa, J. E. Gonzalez, and I. Stoica. Helen: Maliciously secure coopetitive learning for linear models. In IEEE S&P, pages 915-929, 2019.\n\n", "annotations": {"ReferenceToTable": [{"begin": 14623, "end": 14624, "target": "#tab_0", "idx": 0}, {"begin": 46998, "end": 46999, "target": "#tab_2", "idx": 1}, {"begin": 57965, "end": 57966, "target": "#tab_3", "idx": 2}, {"begin": 59249, "end": 59250, "target": "#tab_4", "idx": 3}], "ReferenceToFootnote": [{"begin": 5146, "end": 5147, "target": "#foot_0", "idx": 0}, {"begin": 54273, "end": 54274, "target": "#foot_1", "idx": 1}, {"begin": 54326, "end": 54327, "target": "#foot_2", "idx": 2}, {"begin": 54405, "end": 54406, "target": "#foot_3", "idx": 3}, {"begin": 54500, "end": 54501, "target": "#foot_4", "idx": 4}, {"begin": 55516, "end": 55517, "target": "#foot_5", "idx": 5}], "SectionMain": [{"begin": 1329, "end": 83931, "idx": 0}], "ReferenceToFormula": [{"begin": 29004, "end": 29005, "target": "#formula_5", "idx": 0}, {"begin": 31326, "end": 31327, "target": "#formula_6", "idx": 1}], "SectionReference": [{"begin": 84184, "end": 95423, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1329, "idx": 0}], "Div": [{"begin": 80, "end": 1321, "idx": 0}, {"begin": 1332, "end": 7375, "idx": 1}, {"begin": 7377, "end": 9327, "idx": 2}, {"begin": 9329, "end": 11873, "idx": 3}, {"begin": 11875, "end": 14100, "idx": 4}, {"begin": 14102, "end": 14123, "idx": 5}, {"begin": 14125, "end": 15183, "idx": 6}, {"begin": 15185, "end": 15720, "idx": 7}, {"begin": 15722, "end": 17617, "idx": 8}, {"begin": 17619, "end": 19647, "idx": 9}, {"begin": 19649, "end": 21644, "idx": 10}, {"begin": 21646, "end": 30987, "idx": 11}, {"begin": 30989, "end": 32928, "idx": 12}, {"begin": 32930, "end": 35212, "idx": 13}, {"begin": 35214, "end": 37161, "idx": 14}, {"begin": 37163, "end": 37657, "idx": 15}, {"begin": 37659, "end": 39721, "idx": 16}, {"begin": 39723, "end": 42094, "idx": 17}, {"begin": 42096, "end": 46074, "idx": 18}, {"begin": 46076, "end": 46799, "idx": 19}, {"begin": 46801, "end": 48728, "idx": 20}, {"begin": 48730, "end": 49182, "idx": 21}, {"begin": 49184, "end": 49887, "idx": 22}, {"begin": 49889, "end": 52377, "idx": 23}, {"begin": 52379, "end": 53935, "idx": 24}, {"begin": 53937, "end": 54712, "idx": 25}, {"begin": 54714, "end": 57525, "idx": 26}, {"begin": 57527, "end": 58361, "idx": 27}, {"begin": 58363, "end": 59321, "idx": 28}, {"begin": 59323, "end": 60656, "idx": 29}, {"begin": 60658, "end": 61928, "idx": 30}, {"begin": 61930, "end": 63489, "idx": 31}, {"begin": 63491, "end": 63677, "idx": 32}, {"begin": 63679, "end": 67529, "idx": 33}, {"begin": 67531, "end": 71821, "idx": 34}, {"begin": 71823, "end": 73948, "idx": 35}, {"begin": 73950, "end": 80286, "idx": 36}, {"begin": 80288, "end": 83343, "idx": 37}, {"begin": 83345, "end": 83931, "idx": 38}], "Head": [{"begin": 1332, "end": 1347, "n": "1.", "idx": 0}, {"begin": 7377, "end": 7430, "n": "2.", "idx": 1}, {"begin": 9329, "end": 9362, "n": "2.2", "idx": 2}, {"begin": 11875, "end": 11896, "n": "2.3", "idx": 3}, {"begin": 14102, "end": 14122, "n": "3.", "idx": 4}, {"begin": 14125, "end": 14141, "n": "3.1", "idx": 5}, {"begin": 15185, "end": 15201, "n": "3.2", "idx": 6}, {"begin": 15722, "end": 15745, "n": "3.3", "idx": 7}, {"begin": 17619, "end": 17640, "n": "3.4", "idx": 8}, {"begin": 19649, "end": 19666, "n": "4.", "idx": 9}, {"begin": 21646, "end": 21678, "n": "4.1", "idx": 10}, {"begin": 30989, "end": 31017, "n": "4.2", "idx": 11}, {"begin": 32930, "end": 32955, "n": "4.3", "idx": 12}, {"begin": 35214, "end": 35237, "n": "4.4", "idx": 13}, {"begin": 37163, "end": 37183, "n": "5.", "idx": 14}, {"begin": 37659, "end": 37679, "n": "5.1", "idx": 15}, {"begin": 39723, "end": 39759, "n": "5.2", "idx": 16}, {"begin": 42096, "end": 42142, "idx": 17}, {"begin": 46076, "end": 46099, "n": "5.3", "idx": 18}, {"begin": 46801, "end": 46824, "n": "6.", "idx": 19}, {"begin": 48730, "end": 48762, "n": "7.", "idx": 20}, {"begin": 49184, "end": 49201, "n": "7.1", "idx": 21}, {"begin": 49889, "end": 49925, "n": "7.2", "idx": 22}, {"begin": 52379, "end": 52412, "n": "7.3", "idx": 23}, {"begin": 53937, "end": 53951, "n": "8.", "idx": 24}, {"begin": 54714, "end": 54736, "n": "8.1", "idx": 25}, {"begin": 57527, "end": 57553, "n": "8.2", "idx": 26}, {"begin": 58363, "end": 58392, "n": "8.3", "idx": 27}, {"begin": 59323, "end": 59362, "n": "8.3.1", "idx": 28}, {"begin": 60658, "end": 60699, "n": "8.3.2", "idx": 29}, {"begin": 61930, "end": 61969, "n": "8.3.3", "idx": 30}, {"begin": 63491, "end": 63513, "n": "9.", "idx": 31}, {"begin": 63679, "end": 63711, "n": "9.1", "idx": 32}, {"begin": 67531, "end": 67561, "n": "9.1.2", "idx": 33}, {"begin": 71823, "end": 71856, "n": "9.1.3", "idx": 34}, {"begin": 73950, "end": 73988, "n": "9.2", "idx": 35}, {"begin": 80288, "end": 80304, "n": "10.", "idx": 36}, {"begin": 83345, "end": 83360, "n": "11.", "idx": 37}], "Paragraph": [{"begin": 80, "end": 1321, "idx": 0}, {"begin": 1348, "end": 3188, "idx": 1}, {"begin": 3189, "end": 4875, "idx": 2}, {"begin": 4876, "end": 6308, "idx": 3}, {"begin": 6309, "end": 6680, "idx": 4}, {"begin": 6681, "end": 7008, "idx": 5}, {"begin": 7009, "end": 7375, "idx": 6}, {"begin": 7431, "end": 7685, "idx": 7}, {"begin": 7686, "end": 7823, "idx": 8}, {"begin": 7824, "end": 7919, "idx": 9}, {"begin": 7920, "end": 8445, "idx": 10}, {"begin": 8483, "end": 8638, "idx": 11}, {"begin": 8668, "end": 8842, "idx": 12}, {"begin": 8911, "end": 9327, "idx": 13}, {"begin": 9363, "end": 11252, "idx": 14}, {"begin": 11253, "end": 11873, "idx": 15}, {"begin": 11897, "end": 12251, "idx": 16}, {"begin": 12252, "end": 13013, "idx": 17}, {"begin": 13040, "end": 13319, "idx": 18}, {"begin": 13427, "end": 13767, "idx": 19}, {"begin": 13835, "end": 14100, "idx": 20}, {"begin": 14142, "end": 14666, "idx": 21}, {"begin": 14667, "end": 15183, "idx": 22}, {"begin": 15202, "end": 15720, "idx": 23}, {"begin": 15746, "end": 16669, "idx": 24}, {"begin": 16725, "end": 17617, "idx": 25}, {"begin": 17641, "end": 18366, "idx": 26}, {"begin": 18367, "end": 19647, "idx": 27}, {"begin": 19667, "end": 20078, "idx": 28}, {"begin": 20079, "end": 21644, "idx": 29}, {"begin": 21679, "end": 22520, "idx": 30}, {"begin": 22521, "end": 22649, "idx": 31}, {"begin": 22684, "end": 22724, "idx": 32}, {"begin": 22800, "end": 22815, "idx": 33}, {"begin": 22816, "end": 23074, "idx": 34}, {"begin": 23075, "end": 23657, "idx": 35}, {"begin": 23658, "end": 24969, "idx": 36}, {"begin": 25232, "end": 25573, "idx": 37}, {"begin": 25574, "end": 26856, "idx": 38}, {"begin": 26977, "end": 27070, "idx": 39}, {"begin": 27109, "end": 27313, "idx": 40}, {"begin": 27341, "end": 27710, "idx": 41}, {"begin": 27711, "end": 28085, "idx": 42}, {"begin": 28086, "end": 28358, "idx": 43}, {"begin": 28424, "end": 29103, "idx": 44}, {"begin": 29104, "end": 30847, "idx": 45}, {"begin": 30918, "end": 30987, "idx": 46}, {"begin": 31018, "end": 31278, "idx": 47}, {"begin": 31279, "end": 31637, "idx": 48}, {"begin": 31729, "end": 31834, "idx": 49}, {"begin": 31907, "end": 31912, "idx": 50}, {"begin": 31937, "end": 32213, "idx": 51}, {"begin": 32214, "end": 32928, "idx": 52}, {"begin": 32956, "end": 33798, "idx": 53}, {"begin": 33799, "end": 34376, "idx": 54}, {"begin": 34377, "end": 34792, "idx": 55}, {"begin": 34817, "end": 35212, "idx": 56}, {"begin": 35238, "end": 35422, "idx": 57}, {"begin": 35423, "end": 35613, "idx": 58}, {"begin": 35614, "end": 36445, "idx": 59}, {"begin": 36446, "end": 36869, "idx": 60}, {"begin": 37184, "end": 37657, "idx": 61}, {"begin": 37680, "end": 38074, "idx": 62}, {"begin": 38075, "end": 38727, "idx": 63}, {"begin": 38728, "end": 39268, "idx": 64}, {"begin": 39269, "end": 39721, "idx": 65}, {"begin": 39760, "end": 40142, "idx": 66}, {"begin": 40143, "end": 40621, "idx": 67}, {"begin": 40622, "end": 42094, "idx": 68}, {"begin": 42210, "end": 42252, "idx": 69}, {"begin": 42305, "end": 42505, "idx": 70}, {"begin": 42506, "end": 43449, "idx": 71}, {"begin": 43506, "end": 43596, "idx": 72}, {"begin": 43597, "end": 44089, "idx": 73}, {"begin": 44090, "end": 44428, "idx": 74}, {"begin": 44429, "end": 46074, "idx": 75}, {"begin": 46100, "end": 46799, "idx": 76}, {"begin": 46825, "end": 48286, "idx": 77}, {"begin": 48287, "end": 48728, "idx": 78}, {"begin": 48763, "end": 49182, "idx": 79}, {"begin": 49202, "end": 49424, "idx": 80}, {"begin": 49425, "end": 49887, "idx": 81}, {"begin": 49926, "end": 50444, "idx": 82}, {"begin": 50445, "end": 51186, "idx": 83}, {"begin": 51187, "end": 52377, "idx": 84}, {"begin": 52413, "end": 52811, "idx": 85}, {"begin": 52812, "end": 53935, "idx": 86}, {"begin": 53952, "end": 54227, "idx": 87}, {"begin": 54228, "end": 54712, "idx": 88}, {"begin": 54737, "end": 55429, "idx": 89}, {"begin": 55430, "end": 56306, "idx": 90}, {"begin": 56307, "end": 56948, "idx": 91}, {"begin": 56949, "end": 57525, "idx": 92}, {"begin": 57554, "end": 57958, "idx": 93}, {"begin": 57959, "end": 58361, "idx": 94}, {"begin": 58393, "end": 58791, "idx": 95}, {"begin": 58792, "end": 59321, "idx": 96}, {"begin": 59363, "end": 59718, "idx": 97}, {"begin": 59719, "end": 60656, "idx": 98}, {"begin": 60700, "end": 61928, "idx": 99}, {"begin": 61970, "end": 63489, "idx": 100}, {"begin": 63514, "end": 63677, "idx": 101}, {"begin": 63712, "end": 64479, "idx": 102}, {"begin": 64480, "end": 65203, "idx": 103}, {"begin": 65204, "end": 65359, "idx": 104}, {"begin": 65360, "end": 67529, "idx": 105}, {"begin": 67562, "end": 68165, "idx": 106}, {"begin": 68166, "end": 68596, "idx": 107}, {"begin": 68597, "end": 69367, "idx": 108}, {"begin": 69368, "end": 70388, "idx": 109}, {"begin": 70389, "end": 71821, "idx": 110}, {"begin": 71857, "end": 73142, "idx": 111}, {"begin": 73143, "end": 73948, "idx": 112}, {"begin": 73989, "end": 74541, "idx": 113}, {"begin": 74578, "end": 77066, "idx": 114}, {"begin": 77067, "end": 78732, "idx": 115}, {"begin": 78733, "end": 79593, "idx": 116}, {"begin": 79594, "end": 80286, "idx": 117}, {"begin": 80305, "end": 81360, "idx": 118}, {"begin": 81361, "end": 82579, "idx": 119}, {"begin": 82580, "end": 83343, "idx": 120}, {"begin": 83361, "end": 83931, "idx": 121}], "ReferenceToBib": [{"begin": 1534, "end": 1538, "target": "#b50", "idx": 0}, {"begin": 1539, "end": 1542, "target": "#b51", "idx": 1}, {"begin": 1570, "end": 1574, "target": "#b41", "idx": 2}, {"begin": 2156, "end": 2159, "target": "#b1", "idx": 3}, {"begin": 2301, "end": 2304, "target": "#b7", "idx": 4}, {"begin": 2305, "end": 2308, "target": "#b50", "idx": 5}, {"begin": 2309, "end": 2312, "target": "#b51", "idx": 6}, {"begin": 2313, "end": 2316, "target": "#b79", "idx": 7}, {"begin": 2317, "end": 2320, "target": "#b64", "idx": 8}, {"begin": 2321, "end": 2324, "target": "#b58", "idx": 9}, {"begin": 2325, "end": 2327, "target": "#b6", "idx": 10}, {"begin": 2328, "end": 2331, "target": "#b57", "idx": 11}, {"begin": 2332, "end": 2335, "target": "#b53", "idx": 12}, {"begin": 3131, "end": 3135, "target": "#b71", "idx": 13}, {"begin": 3240, "end": 3244, "target": "#b69", "idx": 14}, {"begin": 3245, "end": 3248, "target": "#b42", "idx": 15}, {"begin": 3249, "end": 3252, "target": "#b65", "idx": 16}, {"begin": 3253, "end": 3256, "target": "#b66", "idx": 17}, {"begin": 3257, "end": 3260, "target": "#b67", "idx": 18}, {"begin": 3261, "end": 3264, "target": "#b19", "idx": 19}, {"begin": 3265, "end": 3268, "target": "#b48", "idx": 20}, {"begin": 3269, "end": 3272, "target": "#b58", "idx": 21}, {"begin": 3414, "end": 3418, "target": "#b69", "idx": 22}, {"begin": 3419, "end": 3422, "target": "#b42", "idx": 23}, {"begin": 4051, "end": 4055, "target": "#b69", "idx": 24}, {"begin": 4056, "end": 4059, "target": "#b42", "idx": 25}, {"begin": 4089, "end": 4093, "target": "#b65", "idx": 26}, {"begin": 4094, "end": 4097, "target": "#b66", "idx": 27}, {"begin": 4098, "end": 4101, "target": "#b67", "idx": 28}, {"begin": 4102, "end": 4105, "target": "#b19", "idx": 29}, {"begin": 4106, "end": 4109, "target": "#b48", "idx": 30}, {"begin": 4360, "end": 4364, "target": "#b58", "idx": 31}, {"begin": 4411, "end": 4415, "target": "#b49", "idx": 32}, {"begin": 4499, "end": 4503, "target": "#b79", "idx": 33}, {"begin": 4552, "end": 4556, "target": "#b74", "idx": 34}, {"begin": 4572, "end": 4576, "target": "#b55", "idx": 35}, {"begin": 4622, "end": 4626, "target": "#b76", "idx": 36}, {"begin": 5430, "end": 5434, "target": "#b19", "idx": 37}, {"begin": 5435, "end": 5438, "target": "#b48", "idx": 38}, {"begin": 5461, "end": 5464, "target": "#b4", "idx": 39}, {"begin": 5486, "end": 5490, "target": "#b14", "idx": 40}, {"begin": 7628, "end": 7632, "target": "#b59", "idx": 41}, {"begin": 8063, "end": 8067, "target": "#b25", "idx": 42}, {"begin": 9559, "end": 9563, "target": "#b26", "idx": 43}, {"begin": 10514, "end": 10517, "target": "#b5", "idx": 44}, {"begin": 11231, "end": 11235, "target": "#b15", "idx": 45}, {"begin": 11236, "end": 11239, "target": "#b16", "idx": 46}, {"begin": 11404, "end": 11408, "target": "#b16", "idx": 47}, {"begin": 11409, "end": 11412, "target": "#b26", "idx": 48}, {"begin": 11413, "end": 11415, "target": "#b3", "idx": 49}, {"begin": 11981, "end": 11985, "target": "#b11", "idx": 50}, {"begin": 12054, "end": 12058, "target": "#b62", "idx": 51}, {"begin": 12065, "end": 12069, "target": "#b63", "idx": 52}, {"begin": 12852, "end": 12856, "target": "#b11", "idx": 53}, {"begin": 14027, "end": 14031, "target": "#b10", "idx": 54}, {"begin": 14072, "end": 14076, "target": "#b33", "idx": 55}, {"begin": 14077, "end": 14080, "target": "#b34", "idx": 56}, {"begin": 14090, "end": 14094, "target": "#b18", "idx": 57}, {"begin": 14725, "end": 14729, "target": "#b75", "idx": 58}, {"begin": 14952, "end": 14956, "target": "#b52", "idx": 59}, {"begin": 14957, "end": 14960, "target": "#b60", "idx": 60}, {"begin": 14961, "end": 14964, "target": "#b17", "idx": 61}, {"begin": 14965, "end": 14968, "target": "#b61", "idx": 62}, {"begin": 15236, "end": 15240, "target": "#b55", "idx": 63}, {"begin": 15241, "end": 15244, "target": "#b73", "idx": 64}, {"begin": 15245, "end": 15248, "target": "#b72", "idx": 65}, {"begin": 15249, "end": 15252, "target": "#b21", "idx": 66}, {"begin": 15253, "end": 15256, "target": "#b20", "idx": 67}, {"begin": 15257, "end": 15260, "target": "#b36", "idx": 68}, {"begin": 15935, "end": 15939, "target": "#b55", "idx": 69}, {"begin": 15940, "end": 15943, "target": "#b79", "idx": 70}, {"begin": 15944, "end": 15947, "target": "#b21", "idx": 71}, {"begin": 16365, "end": 16369, "target": "#b13", "idx": 72}, {"begin": 16370, "end": 16373, "target": "#b23", "idx": 73}, {"begin": 16374, "end": 16377, "target": "#b55", "idx": 74}, {"begin": 19833, "end": 19837, "target": "#b69", "idx": 75}, {"begin": 19838, "end": 19841, "target": "#b42", "idx": 76}, {"begin": 19842, "end": 19845, "target": "#b65", "idx": 77}, {"begin": 19846, "end": 19849, "target": "#b66", "idx": 78}, {"begin": 19850, "end": 19853, "target": "#b67", "idx": 79}, {"begin": 19854, "end": 19857, "target": "#b19", "idx": 80}, {"begin": 19858, "end": 19861, "target": "#b48", "idx": 81}, {"begin": 20870, "end": 20874, "target": "#b79", "idx": 82}, {"begin": 27627, "end": 27631, "target": "#b22", "idx": 83}, {"begin": 27632, "end": 27635, "target": "#b26", "idx": 84}, {"begin": 27636, "end": 27639, "target": "#b79", "idx": 85}, {"begin": 32209, "end": 32212, "target": "#b4", "idx": 86}, {"begin": 33427, "end": 33431, "target": "#b19", "idx": 87}, {"begin": 35773, "end": 35777, "target": "#b45", "idx": 88}, {"begin": 35778, "end": 35781, "target": "#b46", "idx": 89}, {"begin": 35999, "end": 36003, "target": "#b22", "idx": 90}, {"begin": 36039, "end": 36043, "target": "#b26", "idx": 91}, {"begin": 36233, "end": 36237, "target": "#b59", "idx": 92}, {"begin": 41144, "end": 41148, "target": "#b73", "idx": 93}, {"begin": 41149, "end": 41152, "target": "#b72", "idx": 94}, {"begin": 41967, "end": 41971, "target": "#b72", "idx": 95}, {"begin": 42484, "end": 42488, "target": "#b72", "idx": 96}, {"begin": 42557, "end": 42560, "idx": 97}, {"begin": 42971, "end": 42974, "idx": 98}, {"begin": 43052, "end": 43056, "target": "#b22", "idx": 99}, {"begin": 48917, "end": 48921, "target": "#b10", "idx": 100}, {"begin": 48965, "end": 48969, "target": "#b33", "idx": 101}, {"begin": 48970, "end": 48973, "target": "#b34", "idx": 102}, {"begin": 50020, "end": 50024, "target": "#b32", "idx": 103}, {"begin": 55199, "end": 55203, "target": "#b77", "idx": 104}, {"begin": 55257, "end": 55261, "target": "#b56", "idx": 105}, {"begin": 55334, "end": 55338, "target": "#b12", "idx": 106}, {"begin": 64548, "end": 64552, "target": "#b22", "idx": 107}, {"begin": 64553, "end": 64556, "target": "#b24", "idx": 108}, {"begin": 64590, "end": 64594, "target": "#b26", "idx": 109}, {"begin": 64595, "end": 64598, "target": "#b44", "idx": 110}, {"begin": 64612, "end": 64616, "target": "#b79", "idx": 111}, {"begin": 64844, "end": 64848, "target": "#b22", "idx": 112}, {"begin": 64849, "end": 64852, "target": "#b9", "idx": 113}, {"begin": 64853, "end": 64856, "target": "#b24", "idx": 114}, {"begin": 64857, "end": 64860, "target": "#b79", "idx": 115}, {"begin": 65354, "end": 65358, "target": "#b22", "idx": 116}, {"begin": 65364, "end": 65368, "target": "#b79", "idx": 117}, {"begin": 65575, "end": 65579, "target": "#b29", "idx": 118}, {"begin": 65580, "end": 65583, "target": "#b35", "idx": 119}, {"begin": 65584, "end": 65587, "target": "#b79", "idx": 120}, {"begin": 65784, "end": 65788, "target": "#b26", "idx": 121}, {"begin": 65789, "end": 65792, "target": "#b44", "idx": 122}, {"begin": 66818, "end": 66822, "target": "#b22", "idx": 123}, {"begin": 67387, "end": 67391, "target": "#b43", "idx": 124}, {"begin": 67950, "end": 67954, "target": "#b79", "idx": 125}, {"begin": 69025, "end": 69029, "target": "#b43", "idx": 126}, {"begin": 72510, "end": 72514, "target": "#b79", "idx": 127}, {"begin": 74034, "end": 74038, "target": "#b28", "idx": 128}, {"begin": 74039, "end": 74042, "target": "#b40", "idx": 129}, {"begin": 74043, "end": 74046, "target": "#b64", "idx": 130}, {"begin": 74047, "end": 74050, "target": "#b20", "idx": 131}, {"begin": 74051, "end": 74054, "target": "#b70", "idx": 132}, {"begin": 74055, "end": 74058, "target": "#b39", "idx": 133}, {"begin": 74427, "end": 74431, "target": "#b28", "idx": 134}, {"begin": 74432, "end": 74435, "target": "#b64", "idx": 135}, {"begin": 74992, "end": 74996, "target": "#b31", "idx": 136}, {"begin": 74997, "end": 75000, "target": "#b30", "idx": 137}, {"begin": 75448, "end": 75452, "target": "#b20", "idx": 138}, {"begin": 75955, "end": 75959, "target": "#b28", "idx": 139}, {"begin": 76576, "end": 76579, "target": "#b2", "idx": 140}, {"begin": 76580, "end": 76583, "target": "#b26", "idx": 141}, {"begin": 76817, "end": 76821, "target": "#b27", "idx": 142}, {"begin": 76822, "end": 76825, "target": "#b68", "idx": 143}, {"begin": 76826, "end": 76828, "target": "#b0", "idx": 144}, {"begin": 77233, "end": 77237, "target": "#b28", "idx": 145}, {"begin": 77320, "end": 77324, "target": "#b31", "idx": 146}, {"begin": 78300, "end": 78304, "target": "#b38", "idx": 147}, {"begin": 78305, "end": 78308, "target": "#b27", "idx": 148}, {"begin": 79086, "end": 79090, "target": "#b28", "idx": 149}, {"begin": 79232, "end": 79236, "target": "#b31", "idx": 150}, {"begin": 79426, "end": 79430, "target": "#b31", "idx": 151}, {"begin": 79443, "end": 79447, "target": "#b20", "idx": 152}, {"begin": 79510, "end": 79514, "target": "#b54", "idx": 153}, {"begin": 80340, "end": 80344, "target": "#b69", "idx": 154}, {"begin": 80345, "end": 80348, "target": "#b65", "idx": 155}, {"begin": 80349, "end": 80352, "target": "#b66", "idx": 156}, {"begin": 80353, "end": 80356, "target": "#b67", "idx": 157}, {"begin": 80357, "end": 80360, "target": "#b19", "idx": 158}, {"begin": 80361, "end": 80364, "target": "#b48", "idx": 159}, {"begin": 80365, "end": 80368, "target": "#b42", "idx": 160}, {"begin": 80501, "end": 80505, "target": "#b69", "idx": 161}, {"begin": 80506, "end": 80509, "target": "#b42", "idx": 162}, {"begin": 80655, "end": 80659, "target": "#b65", "idx": 163}, {"begin": 80660, "end": 80663, "target": "#b66", "idx": 164}, {"begin": 80664, "end": 80667, "target": "#b67", "idx": 165}, {"begin": 80668, "end": 80671, "target": "#b19", "idx": 166}, {"begin": 80672, "end": 80675, "target": "#b48", "idx": 167}, {"begin": 81197, "end": 81201, "target": "#b19", "idx": 168}, {"begin": 81202, "end": 81205, "target": "#b48", "idx": 169}, {"begin": 81535, "end": 81539, "target": "#b49", "idx": 170}, {"begin": 81594, "end": 81598, "target": "#b58", "idx": 171}, {"begin": 81599, "end": 81602, "target": "#b78", "idx": 172}, {"begin": 81734, "end": 81738, "target": "#b74", "idx": 173}, {"begin": 81829, "end": 81833, "target": "#b76", "idx": 174}, {"begin": 81963, "end": 81967, "target": "#b79", "idx": 175}, {"begin": 82010, "end": 82014, "target": "#b55", "idx": 176}, {"begin": 82015, "end": 82018, "target": "#b57", "idx": 177}, {"begin": 82344, "end": 82348, "target": "#b79", "idx": 178}, {"begin": 82645, "end": 82649, "target": "#b21", "idx": 179}, {"begin": 82650, "end": 82653, "target": "#b8", "idx": 180}, {"begin": 82654, "end": 82657, "target": "#b37", "idx": 181}, {"begin": 82658, "end": 82661, "target": "#b47", "idx": 182}, {"begin": 83051, "end": 83055, "target": "#b19", "idx": 183}, {"begin": 83056, "end": 83059, "target": "#b48", "idx": 184}], "ReferenceString": [{"begin": 84199, "end": 84272, "id": "b0", "idx": 0}, {"begin": 84274, "end": 84571, "id": "b1", "idx": 1}, {"begin": 84575, "end": 84695, "id": "b2", "idx": 2}, {"begin": 84699, "end": 84862, "id": "b3", "idx": 3}, {"begin": 84866, "end": 85018, "id": "b4", "idx": 4}, {"begin": 85022, "end": 85109, "id": "b5", "idx": 5}, {"begin": 85113, "end": 85258, "id": "b6", "idx": 6}, {"begin": 85262, "end": 85471, "id": "b7", "idx": 7}, {"begin": 85475, "end": 85589, "id": "b8", "idx": 8}, {"begin": 85593, "end": 85700, "id": "b9", "idx": 9}, {"begin": 85704, "end": 85767, "id": "b10", "idx": 10}, {"begin": 85771, "end": 85872, "id": "b11", "idx": 11}, {"begin": 85876, "end": 86042, "id": "b12", "idx": 12}, {"begin": 86046, "end": 86164, "id": "b13", "idx": 13}, {"begin": 86168, "end": 86322, "id": "b14", "idx": 14}, {"begin": 86326, "end": 86445, "id": "b15", "idx": 15}, {"begin": 86449, "end": 86545, "id": "b16", "idx": 16}, {"begin": 86549, "end": 86672, "id": "b17", "idx": 17}, {"begin": 86676, "end": 86774, "id": "b18", "idx": 18}, {"begin": 86778, "end": 86915, "id": "b19", "idx": 19}, {"begin": 86919, "end": 87063, "id": "b20", "idx": 20}, {"begin": 87067, "end": 87319, "id": "b21", "idx": 21}, {"begin": 87323, "end": 87461, "id": "b22", "idx": 22}, {"begin": 87465, "end": 87564, "id": "b23", "idx": 23}, {"begin": 87568, "end": 87618, "id": "b24", "idx": 24}, {"begin": 87622, "end": 87799, "id": "b25", "idx": 25}, {"begin": 87803, "end": 87948, "id": "b26", "idx": 26}, {"begin": 87952, "end": 88018, "id": "b27", "idx": 27}, {"begin": 88022, "end": 88174, "id": "b28", "idx": 28}, {"begin": 88178, "end": 88318, "id": "b29", "idx": 29}, {"begin": 88322, "end": 88459, "id": "b30", "idx": 30}, {"begin": 88463, "end": 88562, "id": "b31", "idx": 31}, {"begin": 88566, "end": 88707, "id": "b32", "idx": 32}, {"begin": 88711, "end": 88828, "id": "b33", "idx": 33}, {"begin": 88832, "end": 88956, "id": "b34", "idx": 34}, {"begin": 88960, "end": 89094, "id": "b35", "idx": 35}, {"begin": 89098, "end": 89216, "id": "b36", "idx": 36}, {"begin": 89220, "end": 89422, "id": "b37", "idx": 37}, {"begin": 89426, "end": 89555, "id": "b38", "idx": 38}, {"begin": 89559, "end": 89712, "id": "b39", "idx": 39}, {"begin": 89716, "end": 89908, "id": "b40", "idx": 40}, {"begin": 89912, "end": 90063, "id": "b41", "idx": 41}, {"begin": 90067, "end": 90212, "id": "b42", "idx": 42}, {"begin": 90216, "end": 90328, "id": "b43", "idx": 43}, {"begin": 90332, "end": 90477, "id": "b44", "idx": 44}, {"begin": 90481, "end": 90568, "id": "b45", "idx": 45}, {"begin": 90572, "end": 90701, "id": "b46", "idx": 46}, {"begin": 90705, "end": 90839, "id": "b47", "idx": 47}, {"begin": 90843, "end": 90945, "id": "b48", "idx": 48}, {"begin": 90949, "end": 91142, "id": "b49", "idx": 49}, {"begin": 91146, "end": 91287, "id": "b50", "idx": 50}, {"begin": 91291, "end": 91415, "id": "b51", "idx": 51}, {"begin": 91419, "end": 91583, "id": "b52", "idx": 52}, {"begin": 91587, "end": 91740, "id": "b53", "idx": 53}, {"begin": 91744, "end": 91865, "id": "b54", "idx": 54}, {"begin": 91869, "end": 91995, "id": "b55", "idx": 55}, {"begin": 91999, "end": 92142, "id": "b56", "idx": 56}, {"begin": 92146, "end": 92326, "id": "b57", "idx": 57}, {"begin": 92330, "end": 92529, "id": "b58", "idx": 58}, {"begin": 92533, "end": 92652, "id": "b59", "idx": 59}, {"begin": 92656, "end": 92823, "id": "b60", "idx": 60}, {"begin": 92827, "end": 92970, "id": "b61", "idx": 61}, {"begin": 92974, "end": 93055, "id": "b62", "idx": 62}, {"begin": 93059, "end": 93149, "id": "b63", "idx": 63}, {"begin": 93153, "end": 93245, "id": "b64", "idx": 64}, {"begin": 93249, "end": 93373, "id": "b65", "idx": 65}, {"begin": 93377, "end": 93534, "id": "b66", "idx": 66}, {"begin": 93538, "end": 93696, "id": "b67", "idx": 67}, {"begin": 93700, "end": 93815, "id": "b68", "idx": 68}, {"begin": 93819, "end": 93929, "id": "b69", "idx": 69}, {"begin": 93933, "end": 94118, "id": "b70", "idx": 70}, {"begin": 94122, "end": 94237, "id": "b71", "idx": 71}, {"begin": 94241, "end": 94379, "id": "b72", "idx": 72}, {"begin": 94383, "end": 94516, "id": "b73", "idx": 73}, {"begin": 94520, "end": 94673, "id": "b74", "idx": 74}, {"begin": 94677, "end": 94803, "id": "b75", "idx": 75}, {"begin": 94807, "end": 94902, "id": "b76", "idx": 76}, {"begin": 94906, "end": 95088, "id": "b77", "idx": 77}, {"begin": 95092, "end": 95265, "id": "b78", "idx": 78}, {"begin": 95269, "end": 95421, "id": "b79", "idx": 79}], "Sentence": [{"begin": 80, "end": 240, "idx": 0}, {"begin": 241, "end": 465, "idx": 1}, {"begin": 466, "end": 734, "idx": 2}, {"begin": 735, "end": 881, "idx": 3}, {"begin": 882, "end": 1035, "idx": 4}, {"begin": 1036, "end": 1225, "idx": 5}, {"begin": 1226, "end": 1321, "idx": 6}, {"begin": 1348, "end": 1509, "idx": 7}, {"begin": 1510, "end": 1747, "idx": 8}, {"begin": 1748, "end": 2031, "idx": 9}, {"begin": 2032, "end": 2235, "idx": 10}, {"begin": 2236, "end": 2440, "idx": 11}, {"begin": 2441, "end": 2623, "idx": 12}, {"begin": 2624, "end": 2807, "idx": 13}, {"begin": 2808, "end": 2980, "idx": 14}, {"begin": 2981, "end": 3188, "idx": 15}, {"begin": 3189, "end": 3308, "idx": 16}, {"begin": 3309, "end": 3398, "idx": 17}, {"begin": 3399, "end": 3667, "idx": 18}, {"begin": 3668, "end": 3900, "idx": 19}, {"begin": 3901, "end": 4015, "idx": 20}, {"begin": 4016, "end": 4077, "idx": 21}, {"begin": 4078, "end": 4343, "idx": 22}, {"begin": 4344, "end": 4557, "idx": 23}, {"begin": 4558, "end": 4720, "idx": 24}, {"begin": 4721, "end": 4875, "idx": 25}, {"begin": 4876, "end": 5115, "idx": 26}, {"begin": 5116, "end": 5304, "idx": 27}, {"begin": 5305, "end": 5526, "idx": 28}, {"begin": 5527, "end": 5962, "idx": 29}, {"begin": 5963, "end": 6126, "idx": 30}, {"begin": 6127, "end": 6308, "idx": 31}, {"begin": 6309, "end": 6491, "idx": 32}, {"begin": 6492, "end": 6590, "idx": 33}, {"begin": 6591, "end": 6680, "idx": 34}, {"begin": 6681, "end": 6841, "idx": 35}, {"begin": 6842, "end": 7008, "idx": 36}, {"begin": 7009, "end": 7133, "idx": 37}, {"begin": 7134, "end": 7251, "idx": 38}, {"begin": 7252, "end": 7338, "idx": 39}, {"begin": 7339, "end": 7375, "idx": 40}, {"begin": 7431, "end": 7575, "idx": 41}, {"begin": 7576, "end": 7685, "idx": 42}, {"begin": 7686, "end": 7823, "idx": 43}, {"begin": 7824, "end": 7919, "idx": 44}, {"begin": 7920, "end": 8027, "idx": 45}, {"begin": 8028, "end": 8110, "idx": 46}, {"begin": 8111, "end": 8223, "idx": 47}, {"begin": 8224, "end": 8257, "idx": 48}, {"begin": 8258, "end": 8445, "idx": 49}, {"begin": 8483, "end": 8638, "idx": 50}, {"begin": 8668, "end": 8842, "idx": 51}, {"begin": 8911, "end": 9014, "idx": 52}, {"begin": 9015, "end": 9112, "idx": 53}, {"begin": 9113, "end": 9201, "idx": 54}, {"begin": 9202, "end": 9327, "idx": 55}, {"begin": 9363, "end": 9492, "idx": 56}, {"begin": 9493, "end": 9572, "idx": 57}, {"begin": 9573, "end": 9766, "idx": 58}, {"begin": 9767, "end": 9924, "idx": 59}, {"begin": 9925, "end": 10007, "idx": 60}, {"begin": 10008, "end": 10092, "idx": 61}, {"begin": 10093, "end": 10274, "idx": 62}, {"begin": 10275, "end": 10518, "idx": 63}, {"begin": 10519, "end": 10759, "idx": 64}, {"begin": 10760, "end": 10845, "idx": 65}, {"begin": 10846, "end": 11037, "idx": 66}, {"begin": 11038, "end": 11195, "idx": 67}, {"begin": 11196, "end": 11252, "idx": 68}, {"begin": 11253, "end": 11416, "idx": 69}, {"begin": 11417, "end": 11649, "idx": 70}, {"begin": 11650, "end": 11873, "idx": 71}, {"begin": 11897, "end": 12097, "idx": 72}, {"begin": 12098, "end": 12251, "idx": 73}, {"begin": 12252, "end": 12326, "idx": 74}, {"begin": 12327, "end": 12518, "idx": 75}, {"begin": 12519, "end": 12671, "idx": 76}, {"begin": 12672, "end": 12766, "idx": 77}, {"begin": 12767, "end": 12887, "idx": 78}, {"begin": 12888, "end": 12954, "idx": 79}, {"begin": 12955, "end": 13013, "idx": 80}, {"begin": 13040, "end": 13103, "idx": 81}, {"begin": 13104, "end": 13266, "idx": 82}, {"begin": 13267, "end": 13319, "idx": 83}, {"begin": 13427, "end": 13477, "idx": 84}, {"begin": 13478, "end": 13526, "idx": 85}, {"begin": 13527, "end": 13565, "idx": 86}, {"begin": 13566, "end": 13648, "idx": 87}, {"begin": 13649, "end": 13706, "idx": 88}, {"begin": 13707, "end": 13767, "idx": 89}, {"begin": 13835, "end": 13916, "idx": 90}, {"begin": 13917, "end": 14100, "idx": 91}, {"begin": 14142, "end": 14308, "idx": 92}, {"begin": 14309, "end": 14405, "idx": 93}, {"begin": 14406, "end": 14497, "idx": 94}, {"begin": 14498, "end": 14568, "idx": 95}, {"begin": 14569, "end": 14616, "idx": 96}, {"begin": 14617, "end": 14666, "idx": 97}, {"begin": 14667, "end": 14818, "idx": 98}, {"begin": 14819, "end": 15041, "idx": 99}, {"begin": 15042, "end": 15183, "idx": 100}, {"begin": 15202, "end": 15410, "idx": 101}, {"begin": 15411, "end": 15485, "idx": 102}, {"begin": 15486, "end": 15720, "idx": 103}, {"begin": 15746, "end": 15909, "idx": 104}, {"begin": 15910, "end": 16004, "idx": 105}, {"begin": 16005, "end": 16161, "idx": 106}, {"begin": 16162, "end": 16217, "idx": 107}, {"begin": 16218, "end": 16348, "idx": 108}, {"begin": 16349, "end": 16379, "idx": 109}, {"begin": 16380, "end": 16669, "idx": 110}, {"begin": 16725, "end": 16852, "idx": 111}, {"begin": 16853, "end": 16978, "idx": 112}, {"begin": 16979, "end": 17090, "idx": 113}, {"begin": 17091, "end": 17142, "idx": 114}, {"begin": 17143, "end": 17378, "idx": 115}, {"begin": 17379, "end": 17617, "idx": 116}, {"begin": 17641, "end": 17687, "idx": 117}, {"begin": 17688, "end": 17785, "idx": 118}, {"begin": 17786, "end": 17807, "idx": 119}, {"begin": 17808, "end": 18010, "idx": 120}, {"begin": 18011, "end": 18077, "idx": 121}, {"begin": 18078, "end": 18213, "idx": 122}, {"begin": 18214, "end": 18366, "idx": 123}, {"begin": 18367, "end": 18388, "idx": 124}, {"begin": 18389, "end": 18447, "idx": 125}, {"begin": 18448, "end": 18611, "idx": 126}, {"begin": 18612, "end": 18806, "idx": 127}, {"begin": 18807, "end": 18999, "idx": 128}, {"begin": 19000, "end": 19085, "idx": 129}, {"begin": 19086, "end": 19109, "idx": 130}, {"begin": 19110, "end": 19164, "idx": 131}, {"begin": 19165, "end": 19249, "idx": 132}, {"begin": 19250, "end": 19435, "idx": 133}, {"begin": 19436, "end": 19536, "idx": 134}, {"begin": 19537, "end": 19647, "idx": 135}, {"begin": 19667, "end": 19723, "idx": 136}, {"begin": 19724, "end": 19811, "idx": 137}, {"begin": 19812, "end": 20078, "idx": 138}, {"begin": 20079, "end": 20195, "idx": 139}, {"begin": 20196, "end": 20393, "idx": 140}, {"begin": 20394, "end": 20560, "idx": 141}, {"begin": 20561, "end": 20825, "idx": 142}, {"begin": 20826, "end": 20973, "idx": 143}, {"begin": 20974, "end": 21190, "idx": 144}, {"begin": 21191, "end": 21422, "idx": 145}, {"begin": 21423, "end": 21539, "idx": 146}, {"begin": 21540, "end": 21594, "idx": 147}, {"begin": 21595, "end": 21644, "idx": 148}, {"begin": 21679, "end": 21878, "idx": 149}, {"begin": 21879, "end": 21952, "idx": 150}, {"begin": 21953, "end": 22067, "idx": 151}, {"begin": 22068, "end": 22312, "idx": 152}, {"begin": 22313, "end": 22380, "idx": 153}, {"begin": 22381, "end": 22520, "idx": 154}, {"begin": 22521, "end": 22598, "idx": 155}, {"begin": 22599, "end": 22649, "idx": 156}, {"begin": 22684, "end": 22724, "idx": 157}, {"begin": 22800, "end": 22815, "idx": 158}, {"begin": 22816, "end": 22835, "idx": 159}, {"begin": 22836, "end": 22935, "idx": 160}, {"begin": 22936, "end": 23074, "idx": 161}, {"begin": 23075, "end": 23258, "idx": 162}, {"begin": 23259, "end": 23657, "idx": 163}, {"begin": 23658, "end": 23793, "idx": 164}, {"begin": 23794, "end": 23873, "idx": 165}, {"begin": 23874, "end": 24031, "idx": 166}, {"begin": 24032, "end": 24159, "idx": 167}, {"begin": 24160, "end": 24240, "idx": 168}, {"begin": 24241, "end": 24303, "idx": 169}, {"begin": 24304, "end": 24416, "idx": 170}, {"begin": 24417, "end": 24527, "idx": 171}, {"begin": 24528, "end": 24673, "idx": 172}, {"begin": 24674, "end": 24802, "idx": 173}, {"begin": 24803, "end": 24878, "idx": 174}, {"begin": 24879, "end": 24956, "idx": 175}, {"begin": 24957, "end": 24969, "idx": 176}, {"begin": 25232, "end": 25389, "idx": 177}, {"begin": 25390, "end": 25534, "idx": 178}, {"begin": 25535, "end": 25573, "idx": 179}, {"begin": 25574, "end": 25683, "idx": 180}, {"begin": 25684, "end": 25738, "idx": 181}, {"begin": 25739, "end": 25757, "idx": 182}, {"begin": 25758, "end": 25920, "idx": 183}, {"begin": 25921, "end": 26180, "idx": 184}, {"begin": 26181, "end": 26295, "idx": 185}, {"begin": 26296, "end": 26529, "idx": 186}, {"begin": 26530, "end": 26598, "idx": 187}, {"begin": 26599, "end": 26637, "idx": 188}, {"begin": 26638, "end": 26722, "idx": 189}, {"begin": 26723, "end": 26741, "idx": 190}, {"begin": 26742, "end": 26856, "idx": 191}, {"begin": 26977, "end": 26991, "idx": 192}, {"begin": 26992, "end": 27039, "idx": 193}, {"begin": 27040, "end": 27058, "idx": 194}, {"begin": 27059, "end": 27070, "idx": 195}, {"begin": 27109, "end": 27221, "idx": 196}, {"begin": 27222, "end": 27238, "idx": 197}, {"begin": 27239, "end": 27313, "idx": 198}, {"begin": 27341, "end": 27424, "idx": 199}, {"begin": 27425, "end": 27586, "idx": 200}, {"begin": 27587, "end": 27640, "idx": 201}, {"begin": 27641, "end": 27710, "idx": 202}, {"begin": 27711, "end": 27993, "idx": 203}, {"begin": 27994, "end": 28085, "idx": 204}, {"begin": 28086, "end": 28145, "idx": 205}, {"begin": 28146, "end": 28194, "idx": 206}, {"begin": 28195, "end": 28289, "idx": 207}, {"begin": 28290, "end": 28345, "idx": 208}, {"begin": 28346, "end": 28358, "idx": 209}, {"begin": 28424, "end": 28547, "idx": 210}, {"begin": 28548, "end": 28745, "idx": 211}, {"begin": 28746, "end": 28884, "idx": 212}, {"begin": 28885, "end": 29103, "idx": 213}, {"begin": 29104, "end": 29204, "idx": 214}, {"begin": 29205, "end": 29311, "idx": 215}, {"begin": 29312, "end": 29420, "idx": 216}, {"begin": 29421, "end": 29629, "idx": 217}, {"begin": 29630, "end": 29770, "idx": 218}, {"begin": 29771, "end": 29883, "idx": 219}, {"begin": 29884, "end": 30023, "idx": 220}, {"begin": 30024, "end": 30080, "idx": 221}, {"begin": 30081, "end": 30195, "idx": 222}, {"begin": 30196, "end": 30209, "idx": 223}, {"begin": 30210, "end": 30289, "idx": 224}, {"begin": 30290, "end": 30357, "idx": 225}, {"begin": 30358, "end": 30478, "idx": 226}, {"begin": 30479, "end": 30660, "idx": 227}, {"begin": 30661, "end": 30762, "idx": 228}, {"begin": 30763, "end": 30847, "idx": 229}, {"begin": 30918, "end": 30987, "idx": 230}, {"begin": 31018, "end": 31124, "idx": 231}, {"begin": 31125, "end": 31278, "idx": 232}, {"begin": 31279, "end": 31564, "idx": 233}, {"begin": 31565, "end": 31637, "idx": 234}, {"begin": 31729, "end": 31746, "idx": 235}, {"begin": 31747, "end": 31834, "idx": 236}, {"begin": 31907, "end": 31912, "idx": 237}, {"begin": 31937, "end": 32065, "idx": 238}, {"begin": 32066, "end": 32207, "idx": 239}, {"begin": 32208, "end": 32213, "idx": 240}, {"begin": 32214, "end": 32291, "idx": 241}, {"begin": 32292, "end": 32388, "idx": 242}, {"begin": 32389, "end": 32493, "idx": 243}, {"begin": 32494, "end": 32928, "idx": 244}, {"begin": 32956, "end": 33055, "idx": 245}, {"begin": 33056, "end": 33131, "idx": 246}, {"begin": 33132, "end": 33329, "idx": 247}, {"begin": 33330, "end": 33666, "idx": 248}, {"begin": 33667, "end": 33798, "idx": 249}, {"begin": 33799, "end": 34076, "idx": 250}, {"begin": 34077, "end": 34154, "idx": 251}, {"begin": 34155, "end": 34292, "idx": 252}, {"begin": 34293, "end": 34376, "idx": 253}, {"begin": 34377, "end": 34467, "idx": 254}, {"begin": 34468, "end": 34695, "idx": 255}, {"begin": 34696, "end": 34792, "idx": 256}, {"begin": 34817, "end": 34950, "idx": 257}, {"begin": 34951, "end": 35001, "idx": 258}, {"begin": 35002, "end": 35101, "idx": 259}, {"begin": 35102, "end": 35212, "idx": 260}, {"begin": 35238, "end": 35248, "idx": 261}, {"begin": 35249, "end": 35422, "idx": 262}, {"begin": 35423, "end": 35436, "idx": 263}, {"begin": 35437, "end": 35613, "idx": 264}, {"begin": 35614, "end": 35782, "idx": 265}, {"begin": 35783, "end": 35803, "idx": 266}, {"begin": 35804, "end": 36248, "idx": 267}, {"begin": 36249, "end": 36387, "idx": 268}, {"begin": 36388, "end": 36445, "idx": 269}, {"begin": 36446, "end": 36867, "idx": 270}, {"begin": 36868, "end": 36869, "idx": 271}, {"begin": 37184, "end": 37260, "idx": 272}, {"begin": 37261, "end": 37429, "idx": 273}, {"begin": 37430, "end": 37610, "idx": 274}, {"begin": 37611, "end": 37657, "idx": 275}, {"begin": 37680, "end": 37826, "idx": 276}, {"begin": 37827, "end": 37991, "idx": 277}, {"begin": 37992, "end": 38074, "idx": 278}, {"begin": 38075, "end": 38111, "idx": 279}, {"begin": 38112, "end": 38188, "idx": 280}, {"begin": 38189, "end": 38358, "idx": 281}, {"begin": 38359, "end": 38518, "idx": 282}, {"begin": 38519, "end": 38727, "idx": 283}, {"begin": 38728, "end": 38763, "idx": 284}, {"begin": 38764, "end": 38849, "idx": 285}, {"begin": 38850, "end": 38967, "idx": 286}, {"begin": 38968, "end": 39135, "idx": 287}, {"begin": 39136, "end": 39268, "idx": 288}, {"begin": 39269, "end": 39370, "idx": 289}, {"begin": 39371, "end": 39721, "idx": 290}, {"begin": 39760, "end": 39952, "idx": 291}, {"begin": 39953, "end": 40083, "idx": 292}, {"begin": 40084, "end": 40142, "idx": 293}, {"begin": 40143, "end": 40271, "idx": 294}, {"begin": 40272, "end": 40471, "idx": 295}, {"begin": 40472, "end": 40583, "idx": 296}, {"begin": 40584, "end": 40621, "idx": 297}, {"begin": 40622, "end": 40745, "idx": 298}, {"begin": 40746, "end": 40903, "idx": 299}, {"begin": 40904, "end": 41011, "idx": 300}, {"begin": 41012, "end": 41219, "idx": 301}, {"begin": 41220, "end": 41244, "idx": 302}, {"begin": 41245, "end": 41332, "idx": 303}, {"begin": 41333, "end": 41372, "idx": 304}, {"begin": 41373, "end": 41477, "idx": 305}, {"begin": 41478, "end": 41704, "idx": 306}, {"begin": 41705, "end": 41784, "idx": 307}, {"begin": 41785, "end": 41944, "idx": 308}, {"begin": 41945, "end": 42094, "idx": 309}, {"begin": 42210, "end": 42252, "idx": 310}, {"begin": 42305, "end": 42448, "idx": 311}, {"begin": 42449, "end": 42505, "idx": 312}, {"begin": 42506, "end": 42561, "idx": 313}, {"begin": 42562, "end": 42743, "idx": 314}, {"begin": 42744, "end": 42775, "idx": 315}, {"begin": 42776, "end": 42912, "idx": 316}, {"begin": 42913, "end": 42975, "idx": 317}, {"begin": 42976, "end": 43057, "idx": 318}, {"begin": 43058, "end": 43449, "idx": 319}, {"begin": 43506, "end": 43596, "idx": 320}, {"begin": 43597, "end": 43635, "idx": 321}, {"begin": 43636, "end": 43799, "idx": 322}, {"begin": 43800, "end": 43976, "idx": 323}, {"begin": 43977, "end": 44089, "idx": 324}, {"begin": 44090, "end": 44209, "idx": 325}, {"begin": 44210, "end": 44385, "idx": 326}, {"begin": 44386, "end": 44428, "idx": 327}, {"begin": 44429, "end": 44499, "idx": 328}, {"begin": 44500, "end": 44621, "idx": 329}, {"begin": 44622, "end": 44804, "idx": 330}, {"begin": 44805, "end": 45009, "idx": 331}, {"begin": 45010, "end": 45191, "idx": 332}, {"begin": 45192, "end": 45279, "idx": 333}, {"begin": 45280, "end": 45414, "idx": 334}, {"begin": 45415, "end": 45426, "idx": 335}, {"begin": 45427, "end": 45695, "idx": 336}, {"begin": 45696, "end": 45750, "idx": 337}, {"begin": 45751, "end": 45822, "idx": 338}, {"begin": 45823, "end": 45926, "idx": 339}, {"begin": 45927, "end": 46074, "idx": 340}, {"begin": 46100, "end": 46286, "idx": 341}, {"begin": 46287, "end": 46300, "idx": 342}, {"begin": 46301, "end": 46557, "idx": 343}, {"begin": 46558, "end": 46585, "idx": 344}, {"begin": 46586, "end": 46797, "idx": 345}, {"begin": 46798, "end": 46799, "idx": 346}, {"begin": 46825, "end": 47135, "idx": 347}, {"begin": 47136, "end": 47515, "idx": 348}, {"begin": 47516, "end": 47675, "idx": 349}, {"begin": 47676, "end": 47693, "idx": 350}, {"begin": 47694, "end": 48040, "idx": 351}, {"begin": 48041, "end": 48084, "idx": 352}, {"begin": 48085, "end": 48286, "idx": 353}, {"begin": 48287, "end": 48478, "idx": 354}, {"begin": 48479, "end": 48664, "idx": 355}, {"begin": 48665, "end": 48728, "idx": 356}, {"begin": 48763, "end": 48806, "idx": 357}, {"begin": 48807, "end": 49019, "idx": 358}, {"begin": 49020, "end": 49106, "idx": 359}, {"begin": 49107, "end": 49182, "idx": 360}, {"begin": 49202, "end": 49424, "idx": 361}, {"begin": 49425, "end": 49571, "idx": 362}, {"begin": 49572, "end": 49887, "idx": 363}, {"begin": 49926, "end": 50025, "idx": 364}, {"begin": 50026, "end": 50206, "idx": 365}, {"begin": 50207, "end": 50222, "idx": 366}, {"begin": 50223, "end": 50444, "idx": 367}, {"begin": 50445, "end": 50479, "idx": 368}, {"begin": 50480, "end": 50555, "idx": 369}, {"begin": 50556, "end": 50610, "idx": 370}, {"begin": 50611, "end": 50668, "idx": 371}, {"begin": 50669, "end": 50950, "idx": 372}, {"begin": 50951, "end": 51186, "idx": 373}, {"begin": 51187, "end": 51290, "idx": 374}, {"begin": 51291, "end": 51445, "idx": 375}, {"begin": 51446, "end": 51632, "idx": 376}, {"begin": 51633, "end": 51919, "idx": 377}, {"begin": 51920, "end": 51974, "idx": 378}, {"begin": 51975, "end": 51992, "idx": 379}, {"begin": 51993, "end": 52122, "idx": 380}, {"begin": 52123, "end": 52377, "idx": 381}, {"begin": 52413, "end": 52599, "idx": 382}, {"begin": 52600, "end": 52706, "idx": 383}, {"begin": 52707, "end": 52811, "idx": 384}, {"begin": 52812, "end": 52878, "idx": 385}, {"begin": 52879, "end": 53103, "idx": 386}, {"begin": 53104, "end": 53289, "idx": 387}, {"begin": 53290, "end": 53458, "idx": 388}, {"begin": 53459, "end": 53626, "idx": 389}, {"begin": 53627, "end": 53825, "idx": 390}, {"begin": 53826, "end": 53935, "idx": 391}, {"begin": 53952, "end": 54131, "idx": 392}, {"begin": 54132, "end": 54227, "idx": 393}, {"begin": 54228, "end": 54384, "idx": 394}, {"begin": 54385, "end": 54468, "idx": 395}, {"begin": 54469, "end": 54558, "idx": 396}, {"begin": 54559, "end": 54712, "idx": 397}, {"begin": 54737, "end": 54815, "idx": 398}, {"begin": 54816, "end": 54935, "idx": 399}, {"begin": 54936, "end": 55075, "idx": 400}, {"begin": 55076, "end": 55085, "idx": 401}, {"begin": 55086, "end": 55339, "idx": 402}, {"begin": 55340, "end": 55429, "idx": 403}, {"begin": 55430, "end": 55526, "idx": 404}, {"begin": 55527, "end": 55677, "idx": 405}, {"begin": 55678, "end": 55748, "idx": 406}, {"begin": 55749, "end": 55811, "idx": 407}, {"begin": 55812, "end": 55910, "idx": 408}, {"begin": 55911, "end": 55921, "idx": 409}, {"begin": 55922, "end": 56126, "idx": 410}, {"begin": 56127, "end": 56306, "idx": 411}, {"begin": 56307, "end": 56424, "idx": 412}, {"begin": 56425, "end": 56551, "idx": 413}, {"begin": 56552, "end": 56603, "idx": 414}, {"begin": 56604, "end": 56768, "idx": 415}, {"begin": 56769, "end": 56948, "idx": 416}, {"begin": 56949, "end": 56957, "idx": 417}, {"begin": 56958, "end": 57196, "idx": 418}, {"begin": 57197, "end": 57348, "idx": 419}, {"begin": 57349, "end": 57525, "idx": 420}, {"begin": 57554, "end": 57792, "idx": 421}, {"begin": 57793, "end": 57875, "idx": 422}, {"begin": 57876, "end": 57958, "idx": 423}, {"begin": 57959, "end": 58001, "idx": 424}, {"begin": 58002, "end": 58099, "idx": 425}, {"begin": 58100, "end": 58154, "idx": 426}, {"begin": 58155, "end": 58254, "idx": 427}, {"begin": 58255, "end": 58361, "idx": 428}, {"begin": 58393, "end": 58791, "idx": 429}, {"begin": 58792, "end": 58939, "idx": 430}, {"begin": 58940, "end": 59073, "idx": 431}, {"begin": 59074, "end": 59178, "idx": 432}, {"begin": 59179, "end": 59242, "idx": 433}, {"begin": 59243, "end": 59321, "idx": 434}, {"begin": 59363, "end": 59373, "idx": 435}, {"begin": 59374, "end": 59464, "idx": 436}, {"begin": 59465, "end": 59718, "idx": 437}, {"begin": 59719, "end": 59776, "idx": 438}, {"begin": 59777, "end": 59973, "idx": 439}, {"begin": 59974, "end": 60066, "idx": 440}, {"begin": 60067, "end": 60145, "idx": 441}, {"begin": 60146, "end": 60271, "idx": 442}, {"begin": 60272, "end": 60418, "idx": 443}, {"begin": 60419, "end": 60656, "idx": 444}, {"begin": 60700, "end": 60943, "idx": 445}, {"begin": 60944, "end": 61048, "idx": 446}, {"begin": 61049, "end": 61282, "idx": 447}, {"begin": 61283, "end": 61358, "idx": 448}, {"begin": 61359, "end": 61455, "idx": 449}, {"begin": 61456, "end": 61490, "idx": 450}, {"begin": 61491, "end": 61601, "idx": 451}, {"begin": 61602, "end": 61667, "idx": 452}, {"begin": 61668, "end": 61928, "idx": 453}, {"begin": 61970, "end": 62039, "idx": 454}, {"begin": 62040, "end": 62189, "idx": 455}, {"begin": 62190, "end": 62314, "idx": 456}, {"begin": 62315, "end": 62385, "idx": 457}, {"begin": 62386, "end": 62588, "idx": 458}, {"begin": 62589, "end": 62633, "idx": 459}, {"begin": 62634, "end": 62690, "idx": 460}, {"begin": 62691, "end": 62759, "idx": 461}, {"begin": 62760, "end": 62994, "idx": 462}, {"begin": 62995, "end": 63119, "idx": 463}, {"begin": 63120, "end": 63176, "idx": 464}, {"begin": 63177, "end": 63284, "idx": 465}, {"begin": 63285, "end": 63374, "idx": 466}, {"begin": 63375, "end": 63489, "idx": 467}, {"begin": 63514, "end": 63677, "idx": 468}, {"begin": 63712, "end": 63784, "idx": 469}, {"begin": 63785, "end": 63894, "idx": 470}, {"begin": 63895, "end": 63998, "idx": 471}, {"begin": 63999, "end": 64166, "idx": 472}, {"begin": 64167, "end": 64344, "idx": 473}, {"begin": 64345, "end": 64479, "idx": 474}, {"begin": 64480, "end": 64617, "idx": 475}, {"begin": 64618, "end": 64673, "idx": 476}, {"begin": 64674, "end": 64770, "idx": 477}, {"begin": 64771, "end": 64789, "idx": 478}, {"begin": 64790, "end": 64803, "idx": 479}, {"begin": 64804, "end": 64832, "idx": 480}, {"begin": 64833, "end": 64978, "idx": 481}, {"begin": 64979, "end": 65127, "idx": 482}, {"begin": 65128, "end": 65203, "idx": 483}, {"begin": 65204, "end": 65359, "idx": 484}, {"begin": 65360, "end": 65369, "idx": 485}, {"begin": 65370, "end": 65588, "idx": 486}, {"begin": 65589, "end": 65615, "idx": 487}, {"begin": 65616, "end": 65793, "idx": 488}, {"begin": 65794, "end": 65884, "idx": 489}, {"begin": 65885, "end": 66187, "idx": 490}, {"begin": 66188, "end": 66273, "idx": 491}, {"begin": 66274, "end": 66416, "idx": 492}, {"begin": 66417, "end": 66563, "idx": 493}, {"begin": 66564, "end": 66633, "idx": 494}, {"begin": 66634, "end": 66658, "idx": 495}, {"begin": 66659, "end": 66823, "idx": 496}, {"begin": 66824, "end": 67144, "idx": 497}, {"begin": 67145, "end": 67395, "idx": 498}, {"begin": 67396, "end": 67529, "idx": 499}, {"begin": 67562, "end": 67578, "idx": 500}, {"begin": 67579, "end": 67696, "idx": 501}, {"begin": 67697, "end": 67955, "idx": 502}, {"begin": 67956, "end": 68165, "idx": 503}, {"begin": 68166, "end": 68179, "idx": 504}, {"begin": 68180, "end": 68429, "idx": 505}, {"begin": 68430, "end": 68596, "idx": 506}, {"begin": 68597, "end": 68677, "idx": 507}, {"begin": 68678, "end": 68891, "idx": 508}, {"begin": 68892, "end": 69057, "idx": 509}, {"begin": 69058, "end": 69224, "idx": 510}, {"begin": 69225, "end": 69326, "idx": 511}, {"begin": 69327, "end": 69367, "idx": 512}, {"begin": 69368, "end": 69608, "idx": 513}, {"begin": 69609, "end": 69692, "idx": 514}, {"begin": 69693, "end": 69710, "idx": 515}, {"begin": 69711, "end": 69873, "idx": 516}, {"begin": 69874, "end": 70076, "idx": 517}, {"begin": 70077, "end": 70388, "idx": 518}, {"begin": 70389, "end": 70485, "idx": 519}, {"begin": 70486, "end": 70664, "idx": 520}, {"begin": 70665, "end": 70794, "idx": 521}, {"begin": 70795, "end": 70855, "idx": 522}, {"begin": 70856, "end": 71017, "idx": 523}, {"begin": 71018, "end": 71128, "idx": 524}, {"begin": 71129, "end": 71295, "idx": 525}, {"begin": 71296, "end": 71425, "idx": 526}, {"begin": 71426, "end": 71647, "idx": 527}, {"begin": 71648, "end": 71821, "idx": 528}, {"begin": 71857, "end": 71872, "idx": 529}, {"begin": 71873, "end": 71971, "idx": 530}, {"begin": 71972, "end": 72170, "idx": 531}, {"begin": 72171, "end": 72232, "idx": 532}, {"begin": 72233, "end": 72397, "idx": 533}, {"begin": 72398, "end": 72515, "idx": 534}, {"begin": 72516, "end": 72633, "idx": 535}, {"begin": 72634, "end": 72750, "idx": 536}, {"begin": 72751, "end": 72935, "idx": 537}, {"begin": 72936, "end": 73142, "idx": 538}, {"begin": 73143, "end": 73278, "idx": 539}, {"begin": 73279, "end": 73338, "idx": 540}, {"begin": 73339, "end": 73356, "idx": 541}, {"begin": 73357, "end": 73642, "idx": 542}, {"begin": 73643, "end": 73819, "idx": 543}, {"begin": 73820, "end": 73948, "idx": 544}, {"begin": 73989, "end": 74234, "idx": 545}, {"begin": 74235, "end": 74436, "idx": 546}, {"begin": 74437, "end": 74541, "idx": 547}, {"begin": 74578, "end": 74718, "idx": 548}, {"begin": 74719, "end": 74829, "idx": 549}, {"begin": 74830, "end": 75218, "idx": 550}, {"begin": 75219, "end": 75356, "idx": 551}, {"begin": 75357, "end": 75453, "idx": 552}, {"begin": 75454, "end": 75560, "idx": 553}, {"begin": 75561, "end": 75672, "idx": 554}, {"begin": 75673, "end": 75709, "idx": 555}, {"begin": 75710, "end": 75816, "idx": 556}, {"begin": 75817, "end": 76095, "idx": 557}, {"begin": 76096, "end": 76164, "idx": 558}, {"begin": 76165, "end": 76333, "idx": 559}, {"begin": 76334, "end": 76431, "idx": 560}, {"begin": 76432, "end": 76772, "idx": 561}, {"begin": 76773, "end": 76902, "idx": 562}, {"begin": 76903, "end": 76933, "idx": 563}, {"begin": 76934, "end": 77066, "idx": 564}, {"begin": 77067, "end": 77325, "idx": 565}, {"begin": 77326, "end": 77550, "idx": 566}, {"begin": 77551, "end": 77661, "idx": 567}, {"begin": 77662, "end": 77949, "idx": 568}, {"begin": 77950, "end": 78063, "idx": 569}, {"begin": 78064, "end": 78204, "idx": 570}, {"begin": 78205, "end": 78352, "idx": 571}, {"begin": 78353, "end": 78462, "idx": 572}, {"begin": 78463, "end": 78654, "idx": 573}, {"begin": 78655, "end": 78732, "idx": 574}, {"begin": 78733, "end": 78996, "idx": 575}, {"begin": 78997, "end": 79091, "idx": 576}, {"begin": 79092, "end": 79237, "idx": 577}, {"begin": 79238, "end": 79331, "idx": 578}, {"begin": 79332, "end": 79431, "idx": 579}, {"begin": 79432, "end": 79593, "idx": 580}, {"begin": 79594, "end": 79752, "idx": 581}, {"begin": 79753, "end": 79963, "idx": 582}, {"begin": 79964, "end": 80286, "idx": 583}, {"begin": 80305, "end": 80413, "idx": 584}, {"begin": 80414, "end": 80499, "idx": 585}, {"begin": 80500, "end": 80653, "idx": 586}, {"begin": 80654, "end": 80833, "idx": 587}, {"begin": 80834, "end": 81190, "idx": 588}, {"begin": 81191, "end": 81360, "idx": 589}, {"begin": 81361, "end": 81501, "idx": 590}, {"begin": 81502, "end": 81603, "idx": 591}, {"begin": 81604, "end": 81786, "idx": 592}, {"begin": 81787, "end": 81968, "idx": 593}, {"begin": 81969, "end": 82190, "idx": 594}, {"begin": 82191, "end": 82342, "idx": 595}, {"begin": 82343, "end": 82579, "idx": 596}, {"begin": 82580, "end": 82781, "idx": 597}, {"begin": 82782, "end": 82867, "idx": 598}, {"begin": 82868, "end": 82935, "idx": 599}, {"begin": 82936, "end": 83041, "idx": 600}, {"begin": 83042, "end": 83221, "idx": 601}, {"begin": 83222, "end": 83343, "idx": 602}, {"begin": 83361, "end": 83465, "idx": 603}, {"begin": 83466, "end": 83575, "idx": 604}, {"begin": 83576, "end": 83690, "idx": 605}, {"begin": 83691, "end": 83808, "idx": 606}, {"begin": 83809, "end": 83931, "idx": 607}], "ReferenceToFigure": [{"begin": 2644, "end": 2645, "target": "#fig_0", "idx": 0}, {"begin": 3720, "end": 3721, "target": "#fig_0", "idx": 1}, {"begin": 22552, "end": 22553, "target": "#fig_4", "idx": 2}, {"begin": 30786, "end": 30787, "target": "#fig_4", "idx": 3}, {"begin": 33139, "end": 33141, "target": "#fig_8", "idx": 4}, {"begin": 34654, "end": 34656, "target": "#fig_8", "idx": 5}, {"begin": 38072, "end": 38073, "target": "#fig_8", "idx": 6}, {"begin": 45033, "end": 45035, "target": "#fig_8", "idx": 7}, {"begin": 59737, "end": 59739, "target": "#fig_13", "idx": 8}, {"begin": 60086, "end": 60088, "target": "#fig_13", "idx": 9}, {"begin": 60718, "end": 60720, "target": "#fig_13", "idx": 10}, {"begin": 61301, "end": 61303, "target": "#fig_13", "idx": 11}, {"begin": 61701, "end": 61703, "target": "#fig_13", "idx": 12}, {"begin": 62110, "end": 62112, "target": "#fig_18", "idx": 13}, {"begin": 62183, "end": 62188, "target": "#fig_13", "idx": 14}, {"begin": 62652, "end": 62654, "target": "#fig_18", "idx": 15}, {"begin": 63138, "end": 63140, "target": "#fig_18", "idx": 16}, {"begin": 70879, "end": 70880, "target": "#fig_8", "idx": 17}], "Abstract": [{"begin": 70, "end": 1321, "idx": 0}], "SectionFootnote": [{"begin": 83933, "end": 84182, "idx": 0}], "Footnote": [{"begin": 83944, "end": 84010, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 84011, "end": 84031, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 84032, "end": 84068, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 84069, "end": 84105, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 84106, "end": 84146, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 84147, "end": 84182, "id": "foot_5", "n": "6", "idx": 5}]}}