{"text": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations\n\nAbstract:\nBy design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.\n\n\n1 Introduction\nOne of the main limitations of mainstream word embeddings lies in their static nature, i.e., a word is associated with the same embedding, independently from the context in which it appears. Therefore, these embeddings are unable to reflect the dynamic nature of ambiguous words 1 , in that they can correspond to different (potentially unrelated) meanings depending on their usage in context (Camacho-Collados and Pilehvar, 2018). To get around this limitation dozens of proposals have been put forward, mainly in two categories: multi-prototype embeddings (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Pelevina et al., 2016), which usually leverage context clustering in order to learn distinct representations for individual meanings of words, and 1 Ambiguous words are important as they constitute the most frequent words in a natural language (Zipf, 1949). contextualized word embeddings (Melamud et al., 2016; Peters et al., 2018), which instead compute a single dynamic embedding for a given word which can adapt itself to arbitrary contexts for the word.\nDespite the popularity of research on these specialised embeddings, very few benchmarks exist for their evaluation. Most works in this domain either perform evaluations on word similarity datasets (in which words are presented in isolation; hence, they are not suitable for verifying the dynamic nature of word semantics) or carry out impact analysis in downstream NLP applications (usually, by taking word embeddings as baseline).\nDespite providing a suitable means of verifying the effectiveness of the embeddings, the downstream evaluation cannot replace generic evaluations as it is difficult to isolate the impact of embeddings from many other factors involved, including the algorithmic configuration and parameter setting of the system. To our knowledge, the Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) is the only existing benchmark that specifically focuses on the dynamic nature of word semantics. 2 In Section 4 we will explain the limitations of this dataset for the evaluation of recent work in the literature.\nIn this paper we propose WiC, a novel dataset that provides a high-quality benchmark for the evaluation of context-sensitive word embeddings. WiC provides multiple interesting characteristics:\n(1) it is suitable for evaluating a wide range of techniques, including contextualized word and sense representation and word sense disambiguation; (2) it is framed as a binary classification dataset, in which, unlike SCWS, identical words F There's a lot of trash on the bed of the river -I keep a glass of water next to my bed when I sleep F Justify the margins -The end justifies the means T Air pollution -Open a window and let in some air T The expanded window will give us time to catch the thieves -You have a two-hour window of clear weather to finish working on the lawn are paired with each other (in different contexts); hence, a context-insensitive word embedding model would perform similarly to a random baseline; and\n(3) it is constructed using high quality annotations curated by experts.\n2 WiC: the Word-in-Context dataset\nWe frame the task as binary classification. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts, c 1 and c 2 , are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in c 1 and c 2 correspond to the same meaning or not. Table 1 lists some examples from the dataset. In what follows in this section, we describe the construction procedure of the dataset.\n\n2.1 Construction\nContextual sentences in WiC were extracted from example usages provided for words in three lexical resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domainindependent verb-based resource; and (3) Wiktionary 3 , a large collaborative-constructed online dictionary. We used WordNet as our core resource, exploiting BabelNet's mappings (Navigli and Ponzetto, 2012) as a bridge between Wiktionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the construction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word.\n\n2.1.1 Compilation\nAs explained above, the dataset is composed of instances, each of which contain a target word and two examples containing the target word. An instance can be either positive or negative, depending on whether the corresponding c 1 and c 2 are 3 https://www.wiktionary.org/ listed for the same sense of w in the target resource. In order to compile the dataset, we first obtained all the possible positive and negative instances from all resources, with the only condition of the surface word form occurring in both c 1 and c 2 . 4 The total number of initial examples extracted from all resources at this stage were 23,949, 10,564 and 636 for WordNet, Wiktionary and VerbNet, respectively. We first compiled the test and development sets with two constraints: (1) not having more than three instances for the same target word, and ( 2) not having repeated contextual sentences across instances. These constraints were enforced to have a diverse and balanced set which covers as many unique words as possible. With all these constraints in mind, we set apart 1,600 and 800 instances for the test and development sets, respectively. We ensured that all the splits were balanced for their positive and negative examples. The remaining instances whose examples did not overlap with test and development formed our initial training dataset.\nSemi-automatic check. Even though very few in number, all resources (even exprt-based ones) contain errors such as incorrect part-of-speech tags or ill-formed examples. Moreover, the extraction of examples and the mappings across resources were not always accurate. In order to have as few resource-specific and mapping errors as possible, all training, development and test sets were semi-automatically post-processed, either with small fixes whenever possible or by removing problematic instances otherwise.\n\n2.1.2 Pruning\nWordNet is known to be a fine-grained resource (Navigli, 2006). Often, different senses of the same word are hardly distinguishable from one another even for humans. For example, more than 40 senses are listed for the verb run, with many of them corresponding to similar concepts, e.g., \"move fast\", \"travel rapidly\", and \"run with the ball\". In order to avoid this high-granularity, we performed an automatic pruning of the resource, removing instances with subtle sense distinctions.\nSense clustering is not a very well-defined problem (McCarthy et al., 2016) and there are different strategies to perform this sense distinction (Snow et al., 2007; Pilehvar et al., 2013; Mancini et al., 2017). We adopted a simple strategy and removed all pairs whose senses were first degree connections in the WordNet semantic graph, including sister senses, and those which belonged to the same supersense, i.e. sense clusters from the Wordnet lexicographer files 5. There are a total of 44 supersenses in WordNet, comprising semantic categories such as shape, substance or event. This coarsening of the WordNet sense inventory has been shown particularly useful in downstream applications (R\u00fcd et al., 2011; Severyn et al., 2013; Flekova and Gurevych, 2016; Pilehvar et al., 2017). In the next section we show that the pruning resulted in a significant boost in the clarity of the dataset.\n\n2.2 Quality check\nTo verify the quality and the difficulty of the dataset and to estimate the human-level performance upperbound, we randomly sampled four sets of 100 instances from the test set, with an overlap of 50 instances between two of the annotators. Each set was assigned to an annotator who was asked to label each instance based on whether they thought the two occurrences of the word referred to the same meaning or not. 6 The annotators were not provided with knowledge from any external lexical resource (such as WordNet). Specifically, the number of senses and the sense distinctions of the word (in the target sense inventory) were unknown to the annotators.\nWe found the average human accuracy on the dataset to be 80.0% (individual scores of 79%, 79%, 80% and 82%). We take this as an estimation of the human-level performance upperbound of the dataset. For the overlapping section, we computed the agreement between the two annotators to be 80%. Note that the annotators were not provided with sense distinctions to resemble the more difficult scenario for unsupervised models (which do not benefit from sense-based knowledge resources). Having access to sense definitions/distinctions would have substantially raised the performance bar.\n\nImpact of pruning.\nTo check the effectiveness of our pruning strategy, we also sampled a set of 100 instances from the batch of instances that were pruned from the dataset. Similarly, the annotators were asked to independently label instances in the set. We computed the average accuracy on this set to be 57% (56% and 58%), which is substantially lower than that for the final pruned set (i.e. 80%). This indicates the success of our pruning strategy in improving the semantic clarity of the dataset.\n\n2.3 Statistics\n\n\n3 Experiments\nWe experimented with recent multi-prototype and contextualized word embedding techniques. Evaluation of other embedding models as well as word sense disambiguation systems is left for future work.\nContextualized word embeddings. One of the pioneering contextualized word embedding models is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) language model. We used the 600-d UkWac pre-trained models 7. ELMo (Peters et al., 2018) is a character-based model which learns dynamic word embeddings that can change depending on the context. ELMo embeddings are essentially the internal states of a deep LSTM-based language model, pre-trained on a large text corpus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hidden state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The technique is built upon earlier contextual representations, including ELMo, but differs in the fact that, unlike those models which are mainly unidirectional, BERT is bidirectional, i.e., it considers contexts on both sides of the target word during representation.\nWe experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their target words not covered by these models. For such out-of-vocabulary cases, we used BERT's default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords' embeddings.\nMulti-prototype embeddings. We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings 10. JBT 11 (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embeddings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet. For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V 13 (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, producing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelevina et al. (2016) : for each example we retrieve the closest sense embedding to the context vector, which is computed by averaging its contained words' embeddings.\nSentence-level baselines. We also report results for two baseline models which view the task as context (sentence) similarity. system views the sentence as a bag of words and computes a simple embedding as average of its words. The system makes use of Word2vec (Mikolov et al., 2013b) 300-d embeddings pretrained on the Google News corpus. Sentence LSTM is another baseline, which differently from the other models, does not obtain explicit encoded representations of the target word or sentence.\nThe system has two LSTM layers with 50 units, one for each context side, which concatenates the outputs and passes that to a feedforward layer with 64 neurons, followed by a dropout layer at rate 0.5, and a final one-neuron output layer of sigmoid activation.\nWe used two simple binary classifiers in our experiments on top of all comparison systems (except for the LSTM baseline). MLP: a simple dense network with 100 hidden neurons (ReLU activation), and one output neuron (sigmoid activation), tuned on the development set (batch size: 32; optimizer: Adam; loss: binary crossentropy). Given the stochasticity of the network optimizer, we report average results for five runs (\u00b1 standard deviation). Threshold: a simple threshold-based classifier based on the cosine distance of the two input vectors, tuned with step size 0.02 on the development set.\n\n3.1 Results\nTable 3 shows the results on WiC. In general, the dataset proves to be very difficult for all the techniques, with the best model, i.e., BERT large , providing around 15.5% absolute improvement over a random baseline. Among the two classifiers, the simple threshold-based strategy, which computes the cosine distance between the two encodings, proves to be more efficient than the MLP network which might not be suitable for this setting with relatively small training data. The \u223c15% absolute accuracy difference between human-level upperbound and state-of-the-art performance suggests, however, a challenging dataset and encourages future research in context-sensitive word embeddings to leverage WiC in their evaluations.\nAmong the LSTM-based contextualized models, Context2vec, which does not include the embedding of the target word in its representation, proves more competitive than ELMo. However, surprisingly, neither ELMo nor Context2vec are able to significantly improve over the simple sentence BoW baseline, which in turn outperforms the sentence LSTM baseline. This raises a question about the ability of these models in capturing fine-grained semantics of words in various contexts. Finally, as far as multi-prototype techniques are concerned, DeConf is the best performer. We note that DeConf indirectly benefits from senselevel information from WordNet encoded in its embeddings. The same applies to SW2V, which leverages knowledge from a significantly larger lexical resource, i.e., BabelNet.\n\n4 Related work\nThe Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) comprises 2003 word pairs and is analogous to standard word similarity datasets, such as RG-65 (Rubenstein and Goodenough, 1965) and Sim-Lex (Hill et al., 2015), in which the task is to automatically estimate the semantic similarity of word pairs. Ideally, the estimated similarity scores should have high correlation with those given by human annotators. However, there is a fundamental difference between SCWS and other word similarity datasets: each word in SCWS is associated with a context which triggers a specific meaning of the word. The unique property of the dataset makes it a suitable benchmark for multiprototype and contextualized word embeddings.\nHowever, in the following, we highlight some of the limitations of the dataset which hinder its suitability for evaluating existing techniques.\nInter-rater agreement (IRA) is widely accepted as a metric to assess the annotation quality of a dataset. The metric reflects the homogeneity of ratings which is expected to be high for a welldefined task and a qualified set of annotators. For each word pair in SCWS ten scores were obtained through crowdsourcing. We computed the pairwise IRA to be 0.35 (in terms of Spearman \u03c1 correlation) which is a very low figure. The mean IRA (between each annotator and the average of others), which can be taken as a human-level performance upperbound, is 0.52. Moreover, most of the instances in SCWS have context pairs with different target words. 14 This makes it possible to test context-independent models, which only considers word pairs in isolation, on the dataset. Importantly, such a context-independent model can easily surpass the human-level performance upperbound. For instance, we computed the performance of the Google News Word2vec pretrained word embeddings (Mikolov et al., 2013b) on the dataset to be 0.65 (\u03c1), which is significantly higher than the optimistic IRA for the dataset. In fact, Dubossarsky et al. (2018) showed how the reported high performance of multi-prototype techniques in this dataset was not due to an accurate sense representation, but rather to a subsampling effect, which had not been controlled for in similarity datasets. In contrast, a context-insensitive word embedding model would perform no better than a random baseline on our dataset.\n\n5 Conclusions\nIn this paper we have presented a benchmark for evaluating context-sensitive word representations. The proposed dataset, WiC, is based on lexicographic examples, which constitute a reliable basis to validate different models in their ability to perceive and discern different meanings of words. We tested some of the recent state-of-the-art contextualized and multi-prototype embedding models on our dataset. The considerable gap between the performance of these models and the human-level upperbound suggests ample room for future work on modeling the semantics of words in context. 14 Only 8% (12% if ignoring PoS) of SCWS pairs are identical but their assigned scores (by average 6.8) are substantially higher than the dataset average of 3.6 on a [0,10] scale.\n\nFootnotes:\n2: With a similar goal in mind but focused on hypernymy, Vyas and Carpuat (2017) developed a benchmark to assess the capability of automatic systems to detect hypernymy relations in context.\n4: Given that WordNet provides examples for synsets (rather than word senses), a target word (sense) might not occur in all the examples of its corresponding synset.\n5: wordnet.princeton.edu/documentation/lexnames5wn\n6: Annotators were not lexicographers. To make the task more understandable, they were asked if in their opinion the two words would belong to the same dictionary entry or not.\n7: https://github.com/orenmel/context2vec\n8: https://www.tensorflow.org/hub/modules/google/elmo/1\n9: https://github.com/google-research/bert/blob/master/\n10: Multi-prototype embeddings are also referred to as sense embeddings in the literature.\n11: https://github.com/uhh-lt/sensegram\n12: https://pilehvar.github.io/deconf/\n13: http://lcl.uniroma1.it/sw2v\n\nReferences:\n\n- Jose Camacho-Collados and Taher Pilehvar. 2018. From word to sense embeddings: A survey on vec- tor representations of meaning. Journal of Artificial Intelligence Research, 63:743-788.- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of NAACL, Minneapolis, United States.\n\n- Haim Dubossarsky, Eitan Grossman, and Daphna Weinshall. 2018. Coming to your senses: on con- trols and evaluation sets in polysemy research. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1732-1740, Brussels, Belgium.\n\n- Christiane Fellbaum, editor. 1998. WordNet: An Elec- tronic Database. MIT Press, Cambridge, MA.\n\n- Lucie Flekova and Iryna Gurevych. 2016. Supersense embeddings: A unified model for supersense inter- pretation, prediction, and utilization. In Proceedings of ACL.\n\n- Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.\n\n- Eric H. Huang, Richard Socher, Christopher D. Man- ning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873-882, Jeju Island, Korea.\n\n- Karin Kipper-Schuler. 2005. VerbNet: A broad- coverage, comprehensive verb lexicon. Ph.D. the- sis.\n\n- Massimiliano Mancini, Jose Camacho-Collados, Igna- cio Iacobacci, and Roberto Navigli. 2017. Embed- ding words and senses together via joint knowledge- enhanced training. In Proceedings of CoNLL, pages 100-111, Vancouver, Canada.\n\n- Diana McCarthy, Marianna Apidianaki, and Katrin Erk. 2016. Word sense clustering and clusterability. Computational Linguistics.\n\n- Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional lstm. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 51-61, Berlin, Germany.\n\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word represen- tations in vector space. CoRR, abs/1301.3781.\n\n- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.\n\n- Roberto Navigli. 2006. Meaningful clustering of senses helps boost Word Sense Disambiguation per- formance. In Proceedings of COLING-ACL, pages 105-112, Sydney, Australia.\n\n- Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual se- mantic network. Artificial Intelligence, 193:217- 250.\n\n- Arvind Neelakantan, Jeevan Shankar, Alexandre Pas- sos, and Andrew McCallum. 2014. Efficient non- parametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP, pages 1059-1069, Doha, Qatar.\n\n- Maria Pelevina, Nikolay Arefyev, Chris Biemann, and Alexander Panchenko. 2016. Making sense of word embeddings. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 174-183.\n\n- M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. In Proceed- ings of NAACL, New Orleans, LA, USA.\n\n- Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, and Nigel Collier. 2017. Towards a Seamless Integration of Word Senses into Down- stream NLP Applications. In Proceedings of ACL, Vancouver, Canada.\n\n- Mohammad Taher Pilehvar and Nigel Collier. 2016. De-conflated semantic representations. In Proceed- ings of EMNLP, pages 1680-1690, Austin, TX.\n\n- Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: a Unified Approach for Measuring Seman- tic Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Lin- guistics, pages 1341-1351, Sofia, Bulgaria.\n\n- Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word mean- ing. In Proceedings of ACL, pages 109-117.\n\n- Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communica- tions of the ACM, 8(10):627-633.\n\n- Stefan R\u00fcd, Massimiliano Ciaramita, Jens M\u00fcller, and Hinrich Sch\u00fctze. 2011. Piggyback: Using search engines for robust cross-domain named entity recog- nition. In Proceedings of ACL-HLT, pages 965-975, Portland, Oregon, USA.\n\n- Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. Learning semantic textual similar- ity with structural representations. In Proceedings of ACL (2), pages 714-718, Sofia, Bulgaria.\n\n- Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y. Ng. 2007. Learning to merge word senses. In Proceedings of EMNLP, pages 1005- 1014, Prague, Czech Republic.\n\n- Yogarshi Vyas and Marine Carpuat. 2017. Detecting asymmetric semantic relations in context: A case- study on hypernymy detection. In Proceedings of the 6th Joint Conference on Lexical and Computa- tional Semantics (*SEM 2017), pages 33-43. Asso- ciation for Computational Linguistics.\n\n- George K. Zipf. 1949. Human Behaviour and the Prin- ciple of Least-Effort. Addison-Wesley, Cambridge, MA.\n\n", "annotations": {"Abstract": [{"begin": 91, "end": 1040, "idx": 0}], "Head": [{"begin": 1043, "end": 1057, "n": "1", "idx": 0}, {"begin": 4672, "end": 4688, "n": "2.1", "idx": 1}, {"begin": 5373, "end": 5390, "n": "2.1.1", "idx": 2}, {"begin": 7237, "end": 7250, "n": "2.1.2", "idx": 3}, {"begin": 8632, "end": 8649, "n": "2.2", "idx": 4}, {"begin": 9891, "end": 9909, "idx": 5}, {"begin": 10394, "end": 10408, "n": "2.3", "idx": 6}, {"begin": 10411, "end": 10424, "n": "3", "idx": 7}, {"begin": 14519, "end": 14530, "n": "3.1", "idx": 8}, {"begin": 16042, "end": 16056, "n": "4", "idx": 9}, {"begin": 18418, "end": 18431, "n": "5", "idx": 10}], "ReferenceToBib": [{"begin": 1451, "end": 1488, "target": "#b0", "idx": 0}, {"begin": 1616, "end": 1644, "target": "#b22", "idx": 1}, {"begin": 1645, "end": 1670, "target": "#b16", "idx": 2}, {"begin": 1671, "end": 1693, "target": "#b17", "idx": 3}, {"begin": 1915, "end": 1927, "target": "#b28", "idx": 4}, {"begin": 1960, "end": 1982, "target": "#b11", "idx": 5}, {"begin": 1983, "end": 2003, "target": "#b18", "idx": 6}, {"begin": 2947, "end": 2967, "target": "#b7", "idx": 7}, {"begin": 4811, "end": 4827, "target": "#b3", "idx": 8}, {"begin": 4887, "end": 4909, "target": "#b8", "idx": 9}, {"begin": 5107, "end": 5135, "target": "#b15", "idx": 10}, {"begin": 7298, "end": 7313, "target": "#b14", "idx": 11}, {"begin": 7789, "end": 7812, "target": "#b10", "idx": 12}, {"begin": 7882, "end": 7901, "target": "#b26", "idx": 13}, {"begin": 7902, "end": 7924, "target": "#b21", "idx": 14}, {"begin": 7925, "end": 7946, "target": "#b9", "idx": 15}, {"begin": 8430, "end": 8448, "target": "#b24", "idx": 16}, {"begin": 8449, "end": 8470, "target": "#b25", "idx": 17}, {"begin": 8471, "end": 8498, "target": "#b4", "idx": 18}, {"begin": 8499, "end": 8521, "target": "#b19", "idx": 19}, {"begin": 10728, "end": 10750, "target": "#b11", "idx": 20}, {"begin": 10880, "end": 10914, "target": "#b6", "idx": 21}, {"begin": 10982, "end": 11003, "target": "#b18", "idx": 22}, {"begin": 11435, "end": 11456, "target": "#b1", "idx": 23}, {"begin": 12324, "end": 12347, "target": "#b17", "idx": 24}, {"begin": 12735, "end": 12757, "target": "#b9", "idx": 25}, {"begin": 12786, "end": 12809, "target": "#b12", "idx": 26}, {"begin": 12998, "end": 13020, "target": "#b17", "idx": 27}, {"begin": 13428, "end": 13450, "target": "#b13", "idx": 28}, {"begin": 16112, "end": 16132, "target": "#b7", "idx": 29}, {"begin": 16228, "end": 16261, "target": "#b23", "idx": 30}, {"begin": 16274, "end": 16293, "target": "#b5", "idx": 31}, {"begin": 17907, "end": 17930, "target": "#b13", "idx": 32}, {"begin": 18042, "end": 18067, "target": "#b2", "idx": 33}, {"begin": 19265, "end": 19288, "target": "#b27", "idx": 34}], "ReferenceToFootnote": [{"begin": 3066, "end": 3067, "target": "#foot_0", "idx": 0}, {"begin": 5919, "end": 5920, "target": "#foot_1", "idx": 1}, {"begin": 8204, "end": 8205, "target": "#foot_2", "idx": 2}, {"begin": 9065, "end": 9066, "target": "#foot_3", "idx": 3}, {"begin": 10974, "end": 10975, "target": "#foot_4", "idx": 4}, {"begin": 11273, "end": 11274, "target": "#foot_5", "idx": 5}, {"begin": 11883, "end": 11884, "target": "#foot_6", "idx": 6}, {"begin": 12313, "end": 12315, "target": "#foot_7", "idx": 7}, {"begin": 12321, "end": 12323, "target": "#foot_8", "idx": 8}, {"begin": 12484, "end": 12486, "target": "#foot_9", "idx": 9}, {"begin": 12732, "end": 12734, "target": "#foot_10", "idx": 10}], "SectionFootnote": [{"begin": 19197, "end": 20148, "idx": 0}], "ReferenceString": [{"begin": 20165, "end": 20349, "id": "b0", "idx": 0}, {"begin": 20351, "end": 20559, "id": "b1", "idx": 1}, {"begin": 20563, "end": 20830, "id": "b2", "idx": 2}, {"begin": 20834, "end": 20929, "id": "b3", "idx": 3}, {"begin": 20933, "end": 21096, "id": "b4", "idx": 4}, {"begin": 21100, "end": 21258, "id": "b5", "idx": 5}, {"begin": 21262, "end": 21367, "id": "b6", "idx": 6}, {"begin": 21371, "end": 21589, "id": "b7", "idx": 7}, {"begin": 21593, "end": 21692, "id": "b8", "idx": 8}, {"begin": 21696, "end": 21925, "id": "b9", "idx": 9}, {"begin": 21929, "end": 22056, "id": "b10", "idx": 10}, {"begin": 22060, "end": 22306, "id": "b11", "idx": 11}, {"begin": 22310, "end": 22458, "id": "b12", "idx": 12}, {"begin": 22462, "end": 22694, "id": "b13", "idx": 13}, {"begin": 22698, "end": 22869, "id": "b14", "idx": 14}, {"begin": 22873, "end": 23077, "id": "b15", "idx": 15}, {"begin": 23081, "end": 23304, "id": "b16", "idx": 16}, {"begin": 23308, "end": 23505, "id": "b17", "idx": 17}, {"begin": 23509, "end": 23691, "id": "b18", "idx": 18}, {"begin": 23695, "end": 23908, "id": "b19", "idx": 19}, {"begin": 23912, "end": 24055, "id": "b20", "idx": 20}, {"begin": 24059, "end": 24339, "id": "b21", "idx": 21}, {"begin": 24343, "end": 24481, "id": "b22", "idx": 22}, {"begin": 24485, "end": 24612, "id": "b23", "idx": 23}, {"begin": 24616, "end": 24840, "id": "b24", "idx": 24}, {"begin": 24844, "end": 25041, "id": "b25", "idx": 25}, {"begin": 25045, "end": 25211, "id": "b26", "idx": 26}, {"begin": 25215, "end": 25499, "id": "b27", "idx": 27}, {"begin": 25503, "end": 25608, "id": "b28", "idx": 28}], "ReferenceToTable": [{"begin": 4543, "end": 4544, "target": "#tab_0", "idx": 0}, {"begin": 14537, "end": 14538, "target": "#tab_3", "idx": 1}], "Footnote": [{"begin": 19208, "end": 19398, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 19399, "end": 19564, "id": "foot_1", "n": "4", "idx": 1}, {"begin": 19565, "end": 19615, "id": "foot_2", "n": "5", "idx": 2}, {"begin": 19616, "end": 19792, "id": "foot_3", "n": "6", "idx": 3}, {"begin": 19793, "end": 19834, "id": "foot_4", "n": "7", "idx": 4}, {"begin": 19835, "end": 19890, "id": "foot_5", "n": "8", "idx": 5}, {"begin": 19891, "end": 19946, "id": "foot_6", "n": "9", "idx": 6}, {"begin": 19947, "end": 20037, "id": "foot_7", "n": "10", "idx": 7}, {"begin": 20038, "end": 20077, "id": "foot_8", "n": "11", "idx": 8}, {"begin": 20078, "end": 20116, "id": "foot_9", "n": "12", "idx": 9}, {"begin": 20117, "end": 20148, "id": "foot_10", "n": "13", "idx": 10}], "ReferenceToFormula": [{"begin": 6223, "end": 6224, "idx": 0}], "Paragraph": [{"begin": 101, "end": 1040, "idx": 0}, {"begin": 1058, "end": 2129, "idx": 1}, {"begin": 2130, "end": 2561, "idx": 2}, {"begin": 2562, "end": 3181, "idx": 3}, {"begin": 3182, "end": 3374, "idx": 4}, {"begin": 3375, "end": 4106, "idx": 5}, {"begin": 4107, "end": 4179, "idx": 6}, {"begin": 4180, "end": 4214, "idx": 7}, {"begin": 4215, "end": 4670, "idx": 8}, {"begin": 4689, "end": 5371, "idx": 9}, {"begin": 5391, "end": 6725, "idx": 10}, {"begin": 6726, "end": 7235, "idx": 11}, {"begin": 7251, "end": 7736, "idx": 12}, {"begin": 7737, "end": 8630, "idx": 13}, {"begin": 8650, "end": 9306, "idx": 14}, {"begin": 9307, "end": 9889, "idx": 15}, {"begin": 9910, "end": 10392, "idx": 16}, {"begin": 10425, "end": 10621, "idx": 17}, {"begin": 10622, "end": 11727, "idx": 18}, {"begin": 11728, "end": 12183, "idx": 19}, {"begin": 12184, "end": 13166, "idx": 20}, {"begin": 13167, "end": 13663, "idx": 21}, {"begin": 13664, "end": 13923, "idx": 22}, {"begin": 13924, "end": 14517, "idx": 23}, {"begin": 14531, "end": 15254, "idx": 24}, {"begin": 15255, "end": 16040, "idx": 25}, {"begin": 16057, "end": 16794, "idx": 26}, {"begin": 16795, "end": 16938, "idx": 27}, {"begin": 16939, "end": 18416, "idx": 28}, {"begin": 18432, "end": 19195, "idx": 29}], "SectionHeader": [{"begin": 0, "end": 1040, "idx": 0}], "SectionReference": [{"begin": 20150, "end": 25610, "idx": 0}], "Sentence": [{"begin": 101, "end": 264, "idx": 0}, {"begin": 265, "end": 411, "idx": 1}, {"begin": 412, "end": 566, "idx": 2}, {"begin": 567, "end": 775, "idx": 3}, {"begin": 776, "end": 988, "idx": 4}, {"begin": 989, "end": 1040, "idx": 5}, {"begin": 1058, "end": 1248, "idx": 6}, {"begin": 1249, "end": 1489, "idx": 7}, {"begin": 1490, "end": 1928, "idx": 8}, {"begin": 1929, "end": 2129, "idx": 9}, {"begin": 2130, "end": 2245, "idx": 10}, {"begin": 2246, "end": 2561, "idx": 11}, {"begin": 2562, "end": 2873, "idx": 12}, {"begin": 2874, "end": 3067, "idx": 13}, {"begin": 3068, "end": 3181, "idx": 14}, {"begin": 3182, "end": 3323, "idx": 15}, {"begin": 3324, "end": 3374, "idx": 16}, {"begin": 3375, "end": 4106, "idx": 17}, {"begin": 4107, "end": 4179, "idx": 18}, {"begin": 4180, "end": 4214, "idx": 19}, {"begin": 4215, "end": 4258, "idx": 20}, {"begin": 4259, "end": 4377, "idx": 21}, {"begin": 4378, "end": 4434, "idx": 22}, {"begin": 4435, "end": 4536, "idx": 23}, {"begin": 4537, "end": 4582, "idx": 24}, {"begin": 4583, "end": 4670, "idx": 25}, {"begin": 4689, "end": 5037, "idx": 26}, {"begin": 5038, "end": 5190, "idx": 27}, {"begin": 5191, "end": 5371, "idx": 28}, {"begin": 5391, "end": 5529, "idx": 29}, {"begin": 5530, "end": 5662, "idx": 30}, {"begin": 5663, "end": 5717, "idx": 31}, {"begin": 5718, "end": 5920, "idx": 32}, {"begin": 5921, "end": 6079, "idx": 33}, {"begin": 6080, "end": 6284, "idx": 34}, {"begin": 6285, "end": 6398, "idx": 35}, {"begin": 6399, "end": 6520, "idx": 36}, {"begin": 6521, "end": 6607, "idx": 37}, {"begin": 6608, "end": 6725, "idx": 38}, {"begin": 6726, "end": 6747, "idx": 39}, {"begin": 6748, "end": 6894, "idx": 40}, {"begin": 6895, "end": 6991, "idx": 41}, {"begin": 6992, "end": 7235, "idx": 42}, {"begin": 7251, "end": 7314, "idx": 43}, {"begin": 7315, "end": 7416, "idx": 44}, {"begin": 7417, "end": 7593, "idx": 45}, {"begin": 7594, "end": 7736, "idx": 46}, {"begin": 7737, "end": 7947, "idx": 47}, {"begin": 7948, "end": 8206, "idx": 48}, {"begin": 8207, "end": 8320, "idx": 49}, {"begin": 8321, "end": 8522, "idx": 50}, {"begin": 8523, "end": 8630, "idx": 51}, {"begin": 8650, "end": 8890, "idx": 52}, {"begin": 8891, "end": 9066, "idx": 53}, {"begin": 9067, "end": 9168, "idx": 54}, {"begin": 9169, "end": 9306, "idx": 55}, {"begin": 9307, "end": 9415, "idx": 56}, {"begin": 9416, "end": 9503, "idx": 57}, {"begin": 9504, "end": 9596, "idx": 58}, {"begin": 9597, "end": 9788, "idx": 59}, {"begin": 9789, "end": 9889, "idx": 60}, {"begin": 9910, "end": 10063, "idx": 61}, {"begin": 10064, "end": 10145, "idx": 62}, {"begin": 10146, "end": 10285, "idx": 63}, {"begin": 10286, "end": 10291, "idx": 64}, {"begin": 10292, "end": 10392, "idx": 65}, {"begin": 10425, "end": 10514, "idx": 66}, {"begin": 10515, "end": 10621, "idx": 67}, {"begin": 10622, "end": 10653, "idx": 68}, {"begin": 10654, "end": 10930, "idx": 69}, {"begin": 10931, "end": 10976, "idx": 70}, {"begin": 10977, "end": 11109, "idx": 71}, {"begin": 11110, "end": 11234, "idx": 72}, {"begin": 11235, "end": 11391, "idx": 73}, {"begin": 11392, "end": 11457, "idx": 74}, {"begin": 11458, "end": 11727, "idx": 75}, {"begin": 11728, "end": 11884, "idx": 76}, {"begin": 11885, "end": 11992, "idx": 77}, {"begin": 11993, "end": 12183, "idx": 78}, {"begin": 12184, "end": 12211, "idx": 79}, {"begin": 12212, "end": 12316, "idx": 80}, {"begin": 12317, "end": 12476, "idx": 81}, {"begin": 12477, "end": 12558, "idx": 82}, {"begin": 12559, "end": 12726, "idx": 83}, {"begin": 12727, "end": 12922, "idx": 84}, {"begin": 12923, "end": 13166, "idx": 85}, {"begin": 13167, "end": 13192, "idx": 86}, {"begin": 13193, "end": 13293, "idx": 87}, {"begin": 13294, "end": 13394, "idx": 88}, {"begin": 13395, "end": 13506, "idx": 89}, {"begin": 13507, "end": 13663, "idx": 90}, {"begin": 13664, "end": 13923, "idx": 91}, {"begin": 13924, "end": 14045, "idx": 92}, {"begin": 14046, "end": 14251, "idx": 93}, {"begin": 14252, "end": 14365, "idx": 94}, {"begin": 14366, "end": 14517, "idx": 95}, {"begin": 14531, "end": 14564, "idx": 96}, {"begin": 14565, "end": 14748, "idx": 97}, {"begin": 14749, "end": 15005, "idx": 98}, {"begin": 15006, "end": 15254, "idx": 99}, {"begin": 15255, "end": 15425, "idx": 100}, {"begin": 15426, "end": 15604, "idx": 101}, {"begin": 15605, "end": 15727, "idx": 102}, {"begin": 15728, "end": 15818, "idx": 103}, {"begin": 15819, "end": 15926, "idx": 104}, {"begin": 15927, "end": 16040, "idx": 105}, {"begin": 16057, "end": 16380, "idx": 106}, {"begin": 16381, "end": 16488, "idx": 107}, {"begin": 16489, "end": 16674, "idx": 108}, {"begin": 16675, "end": 16794, "idx": 109}, {"begin": 16795, "end": 16938, "idx": 110}, {"begin": 16939, "end": 17044, "idx": 111}, {"begin": 17045, "end": 17178, "idx": 112}, {"begin": 17179, "end": 17253, "idx": 113}, {"begin": 17254, "end": 17358, "idx": 114}, {"begin": 17359, "end": 17492, "idx": 115}, {"begin": 17493, "end": 17583, "idx": 116}, {"begin": 17584, "end": 17704, "idx": 117}, {"begin": 17705, "end": 17809, "idx": 118}, {"begin": 17810, "end": 18032, "idx": 119}, {"begin": 18033, "end": 18297, "idx": 120}, {"begin": 18298, "end": 18416, "idx": 121}, {"begin": 18432, "end": 18530, "idx": 122}, {"begin": 18531, "end": 18726, "idx": 123}, {"begin": 18727, "end": 18840, "idx": 124}, {"begin": 18841, "end": 19018, "idx": 125}, {"begin": 19019, "end": 19195, "idx": 126}], "Div": [{"begin": 101, "end": 1040, "idx": 0}, {"begin": 1043, "end": 4670, "idx": 1}, {"begin": 4672, "end": 5371, "idx": 2}, {"begin": 5373, "end": 7235, "idx": 3}, {"begin": 7237, "end": 8630, "idx": 4}, {"begin": 8632, "end": 9889, "idx": 5}, {"begin": 9891, "end": 10392, "idx": 6}, {"begin": 10394, "end": 10409, "idx": 7}, {"begin": 10411, "end": 14517, "idx": 8}, {"begin": 14519, "end": 16040, "idx": 9}, {"begin": 16042, "end": 18416, "idx": 10}, {"begin": 18418, "end": 19195, "idx": 11}], "SectionMain": [{"begin": 1040, "end": 19195, "idx": 0}]}}