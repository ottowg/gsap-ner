{"text": "On multivariate randomized classification trees: l 0 -based sparsity, VC dimension and decomposition methods\n\nAbstract:\nDecision trees are widely-used classification and regression models because of their interpretability and good accuracy. Classical methods such as CART are based on greedy approaches but a growing attention has recently been devoted to optimal decision trees. We investigate the nonlinear continuous optimization formulation proposed in Blanquero et al.\n\nMain:\n\n\n\n1. Introduction\nDecision trees are popular classification and regression models in the areas of Machine Learning (ML) and Data Mining. Because of their interpretability and their good accuracy, they are applied in a number of fields ranging from Medicine (see e.g.  [1, 2, 3]) to Business Analytics (see e.g.  [4, 5, 6]).\nSince building optimal binary decision trees is NP-hard [7] and large-scale datasets are often of interest, CART [8] pioneering work and later extensions, such as ID3 [9] and C4.5 [10], adopt a greedy and top-down approach aiming at minimizing an impurity measure. Then a pruning phase is used to simplify the tree topology in order to reduce overfitting and to obtain a more interpretable model.\nDue to the remarkable progress in the computational performance of Mixed-Integer Linear Optimization (MILO) and nonlinear optimization solvers, decision trees have been revisited during the last decade.\nMost previous work on optimal classification trees is concerned with deterministic trees where each input vector is univocally associated to a single class. In [11] an extreme point tabu search method is described to minimize the misclassification error of all decisions in a given tree concurrently.\nIn [12, 13], a MILO formulation and a local search approach are proposed for constructing optimal multivariate classification trees. In [14] an integer programming formulation is presented to design binary classification trees for categorical data. An efficient integer optimization encoding is proposed in [15] to construct classification and regression trees of depth D with univariate decisions at the branch nodes. In [16] a column generation heuristic is described to build univariate binary classification trees for larger datasets. A dynamic programming and search algorithm is presented in [17] for constructing optimal univariate decision trees. In [18] the authors propose a flow-based MILO formulation for optimal univariate classification trees where they exploit the problem structure and max-flow/min-cut duality to derive a Benders' decomposition method for handling large datasets.\nRecently, in [19, 20], a novel continuous nonlinear optimization approach has been proposed to build (sparse) optimal multivariate randomized classification trees. At each branch node a random variable is generated to determine to which branch (left or right) an input vector is forwarded to. An appealing feature of multivariate randomized classification trees with respect to deterministic ones is their probabilistic nature in terms of the posterior probability. Since such trees involve only continuous variables, they can be trained with a continuous constrained nonlinear programming solver. Although the formulation is nonconvex, some available solvers are guaranteed to converge to feasible solutions satisfying local optimality conditions. In [20], sparsity of multivariate randomized classification trees is achieved by adding l 1 and l \u221e regularization terms to the objective function. The interested reader is referred to [21] for a survey on optimal decision trees.\nIn this work, we investigate sparse multivariate randomized classification trees. In particular, we describe alternative sparsification strategies based on concave approximations of the l 0 \"norm\" 1 and we evaluate on 24 datasets their potential benefits compared with the above-mentioned regularizations.\nThen we discuss a theoretical aspect of such trees, namely their Vapnik-Chervonenkis (VC) dimension [22]. Finally, we propose a general proximal point decomposition scheme to reduce the training times for larger datasets. After commenting on the asymptotic convergence, we present an efficient specific version of the decomposition scheme and test it on 5 datasets in comparison with the original, not 1 l0 is not a proper norm since it does not satisfy the absolute homogeneity assumption.\n\ndecomposed version.\nThe remainder of the paper is organized as follows. In Section 2 we briefly summarize the formulation proposed in [19] for optimal randomized classification trees. In Section 3, after recalling how sparsity is pursued in [20], we present alternative l 0 -based strategies. The computational results are reported and discussed in Section 4. Section 5 is devoted to upper and lower bounds on the VC dimension of multivariate randomized classification trees. In Section 6, the general decomposition scheme is described and the results obtained with a specific version are reported. Finally, Section 7 contains some concluding remarks.\n\n2. Optimal randomized classification trees\nWe briefly recall the nonlinear continuous optimization formulation proposed in [19] to train Optimal Randomized Classification Trees (ORCTs).\nConsider a training set I = {(x i , y i )} 1\u2264i\u2264N consisting of N samples, where x i \u2208 R p is the pdimensional vector of predictor variables and y i \u2208 {1, . . . , K} the associated class label.\nRandomized Classification Trees are maximal binary trees of a given depth D, with D \u2265 1. Let \u03c4 L and \u03c4 B denote, respectively, the set of leaf nodes and of branch nodes. At branch nodes multivariate (hyperplane) splits are performed according to a probabilistic splitting rule. The probability of taking a branch is determined by a univariate cumulative density function (CDF) evaluated over a linear combination of the predictor variables. More precisely, for each input vector x i with i \u2208 {1, ...., N } and each branch node t \u2208 \u03c4 B the probability of taking the left branch is given byp it = F \u03b3 ( 1 p p j=1 a jt x ij \u2212 \u00b5 t ),\nwhere the coefficients a jt \u2208 [\u22121, 1] and \u00b5 t \u2208 [\u22121, 1] are the decision variables, and the logistic CDFF \u03b3 (v) = 1 1 + e (\u2212\u03b3v)\nwith parameter \u03b3 > 0 is considered. The right branch is taken with probability 1 \u2212 p it . See Figure 1 for an example of a Randomized Classification Tree of depth D = 2.\nSince the logistic CDF induces a soft splitting rule at each branch node, all input vectors in the training set fall into every leaf node with a certain probability. The probability that an input vector x i with i \u2208 {1, ...., N } falls into leaf node t \u2208 \u03c4 L is then given byP it = t l \u2208N L(t) p it l tr\u2208N R(t) (1 \u2212 p itr ),\nwhere N L (t) denotes the set of ancestor nodes of leaf node t whose left branch belongs to the path from the root to t, while N R (t) the set of ancestor nodes for the right branch. Then the problem of minimizing the expected misclassification error of the randomized classification tree over the training set can be formulated as the following mixed-integer nonlinear optimization problem: minN i=1 t\u2208\u03c4 L P it K k=1 w y i k c kt (2a) s.t. K k=1 c kt = 1 t \u2208 \u03c4 L ,L c kt \u2265 1 k \u2208 {1, ...., K},a jt \u2208 [\u22121, 1], \u00b5 t \u2208 [\u22121, 1] j \u2208 {1, . . . , p}, t \u2208 \u03c4 B ,c kt \u2208 {0, 1} k \u2208 {1, . . . , K}, t \u2208 \u03c4 L ,\nwhere constraints (2b) ensure that each leaf node is assigned to exactly one class label, and constraints (2c) that each class label k is associated with at least one leaf node. Remember that, for every pair i \u2208 {1, . . . , N } and t \u2208 \u03c4 L , the probability P it is a non linear function function of the variables a jt and\u00b5 t with j \u2208 {1, . . . , p} and t \u2208 N L (t) \u222a N R (t).\nAs shown in [19], the integrality of the binary variables c kt can be relaxed because the resulting nonlinear continuous formulation admits an optimal integer solution. Thus (2e) is substituted withc kt \u2208 [0, 1] k \u2208 {1, . . . , K}, t \u2208 \u03c4 L ,\nwhere the variable c kt can be viewed as the probability that a leaf node t is assigned to class label k.\nAfter the training phase, that is, after solving the above nonlinear optimization formulation (2a)-(2e), the class for a new unlabeled input vector x \u2208 R p is predicted by assigning x to the class for which the estimate t\u2208\u03c4 L c kt P xt of the probability that x belongs to class label k is maximum.\nAs shown in [19], the above formulation can be easily amended to account for other constraints such as minimum correct classification rates for different classes.\n\n3. Sparse optimal randomized classification trees\nIn many practical classification tasks the input vectors x i \u2208 R p include a large number p of predictor variables, referred to as features. The degree of sparsity of a model depends on both the number of features that are actually used and the number of nonzero parameters. Sparse models are important not only because they identify a subset of most relevant features (feature selection) but also because they are more interpretable. Interpretability is a crucial issue for ML methods since it broadens their range of applicability. Moreover, according to Occam's razor principle, simpler models also tend to avoid overfitting and to yield a smaller generalization error, i.e., a higher accuracy on input vectors not included in the training set.\nIn the Statistics and ML literature several approaches have been proposed to seek parsimonious models. A popular one consists in adding to the objective function ad hoc regularization terms inducing sparsity. For instance, in Lasso 2 regression (see e.g.  [23]) penalizing the l 1 norm of the parameters vector allows to perform both feature selection and regularization, which in turn enhance both prediction accuracy and interpretability.\nIn the context of classification trees, the degree of sparsity is related to number of features actually involved in the splitting rules implemented at the branch nodes. Two different types of sparsity naturally arise. Local sparsity corresponds to the total number of features occurring in the hyperplane splits at the branch nodes, while global sparsity corresponds to the number of features occurring across the whole tree.\nIn [20] the authors promote the sparsity of optimal randomized classification trees by adding to the expected misclassification error over the training set two regularization terms based on polyhedral norms of the parameters vector. Adopting the l 1 norm for local sparsity and the l \u221e norm for global sparsity, the overall objective function in sparse ORCT is:N i=1 t\u2208\u03c4 L P it K k=1 w y i k c kt + \u03bb L p j=1 a j\u2022 1 + \u03bb G p j=1 a j\u2022 \u221e\nwhere a j. denotes the |\u03c4 B |-dimensional vector of the coefficients of the j-th feature for all branch nodes t \u2208 \u03c4 B . An equivalent smooth formulation can be easily obtained by rewriting the two regularization terms using additional variables and constraints.\nFrom now on we will use the acronym MRCTs for Multivariate Randomized Classification Trees with possibly other objective functions.\n\n3.1. Sparse multivariate randomized classification trees via approximate l 0 regularization\nTo induce local and global sparsity in MRCTs, we consider penalizing the l 0 \"norm\" of the parameters vector rather than the l 1 and l \u221e norms. The l 0 \"norm\" of a vector v \u2208 R n is the number of nonzero components of v, namely which counts the total number of predictor variables (features) actually involved in the multivariate (hyperplane) splits implemented at the branch nodes. For global sparsity, we also add to the loss function (2a) the regularization term:v 0 = n l=1 1 R + (|v l |),\u03b2 0 = p j=1 1 R + (\u03b2 j )\nwhere the new variables\u03b2 j \u2208 [0, 1] are subject to \u2212\u03b2 j \u2264 a jt \u2264 \u03b2 j j \u2208 {1, . . . , p}, t \u2208 \u03c4 B .\nThis second term is equal to the number of features that are actually used across the whole tree.\nAlthough l 0 regularization is a natural way to induce local and global sparsity, the resulting nonlinear optimization problem is more challenging than the ones involving the l 1 and l \u221e norms because the step function 1 R + (u) is discontinuous. Since the l 0 penalty terms make the overall objective function non-smooth, we consider continuously differentiable concave approximations. This approach was introduced in [24] for linear classification models and further developed in [25] and in [26]. However, unlike in these and other previous works, the training of sparse MRCTs cannot be reduced to an overall concave optimization problem.\nSimilarly to [24], we replace the discontinuous step function 1 R + (u) with the smooth concave exponential approximation 1 \u2212 e \u2212\u03b1u on the non-negative real line u \u2265 0, with parameter \u03b1 > 0. This leads to the following approximate l 0 regularization term for local sparsityp j=1 t\u2208\u03c4 B (1 \u2212 e \u2212\u03b1z jt ),\nwhere the additional variables z jt \u2208 [0, 1] satisfy constraints\u2212z jt \u2264 a jt \u2264 z jt j \u2208 {1, . . . , p}, t \u2208 \u03c4 B ,\nand to the following approximate l 0 regularization term for global sparsity p j=1\n(1 \u2212 e \u2212\u03b1\u03b2 j ), (8) where the additional variables \u03b2 j \u2208 [0, 1] satisfy constraints (5).\nThus we obtain the alternative formulation for sparse multivariate randomized classification trees:min N i=1 t\u2208\u03c4 L P it K k=1 w y i k c kt + \u03bb L 0 p j=1 t\u2208\u03c4 B (1 \u2212 e \u2212\u03b1z jt ) + \u03bb G 0 p j=1 (1 \u2212 e \u2212\u03b1\u03b2 j ) K k=1 c kt = 1 t \u2208 \u03c4 L t\u2208\u03c4 L c kt \u2265 1 k \u2208 {1, ...., K} \u2212\u03b2 j \u2264 a jt \u2264 \u03b2 j , \u03b2 j \u2208 [0, 1] j \u2208 {1, . . . , p}, t \u2208 \u03c4 B \u2212z jt \u2264 a jt \u2264 z jt , z jt \u2208 [0, 1] j \u2208 {1, . . . , p}, t \u2208 \u03c4 B a jt \u2208 [\u22121, 1], \u00b5 t \u2208 [\u22121, 1] j \u2208 {1, . . . , p}, t \u2208 \u03c4 B c kt \u2208 [0, 1] k \u2208 {1, . . . , K}, t \u2208 \u03c4 L ,\nwhere \u03bb L 0 \u2265 0 and \u03bb G 0 \u2265 0 are, respectively, the local and global sparsity regularization parameters, and the additional variables \u03b2 j , z jt \u2208 [0, 1] satisfy the corresponding constraints ( 5) and (7).\nIn [20] the authors establish for the sparse ORCT formulation lower bounds on the regularization parameters \u03bb L and \u03bb G to ensure that the most sparse tree possible (i.e. a * = 0) is a stationary point.\nHere we derive in a different way a similar result for \u03bb L 0 and \u03bb G 0 of formulation (9).\nProposition 1. Assume that \u03bb L 0 and \u03bb G 0 are such that\u03bb L 0 + \u03bb G 0 \u2265 max j=1,...,p,t\u2208\u03c4 B |\u03be jt (0)| \u03b1 ,\nwhere \u03be jt (0) represents the partial derivative of the objective function (2a) with respect to a jt evaluated at zero. Then a stationary point (a * , \u00b5 * , c * ) for formulation (9) exists with a * = 0.\nProof. From Theorem 1 in [19] we know that formulation (9) admits an optimal solution (a * , \u00b5 * , c * ) such that c * kt \u2208 {0, 1} \u2200k = 1, ..., K, t \u2208 \u03c4 L . For the sake of proof simplicity, we rewrite the objective function in (9) without the z jt variables:N i=1 t\u2208\u03c4 L P it K k=1 w y i k c kt + \u03bb L 0 p j=1 t\u2208\u03c4 B (1 \u2212 e \u2212\u03b1|a jt | ) + \u03bb G 0 p j=1 (1 \u2212 e \u2212\u03b1\u03b2 j ) ()\nand we consider the formulation where (10) is minimized subject to all the constraints in formulation (9) except constraints (7) which involve the z jt variables. The two formulations are clearly equivalent since at optimality for each pair of inequality constraints (7) one is satisfied with equality.\nWe distinguish three cases: (i) \u03bb L 0 > 0 and \u03bb G 0 = 0, (ii) \u03bb L 0 = 0 and \u03bb G 0 > 0, and (iii) \u03bb L 0 > 0 and \u03bb G 0 > 0. First, we prove the result for case (i), then we show how it can be easily extended to cases (ii) and (iii).\nLet us consider the solution (0 * , \u00b5 * , c * ). By assuming \u039e = N i=1 t\u2208\u03c4 L P it K k=1 w y i k c kt (the first term of (10)), for a fixed predictor variable index j and branching node index t, the partial derivative of \u039e with respect to aj t (denoted as \u03bej t(aj t)) is:\u2202\u039e \u2202aj t = \u03bej t(aj t) = N i=1 t\u2208\u03c4 L ( t) l\u2208N L (t):l = t p il r\u2208N R (t):r = t(1 \u2212 p ir )(\u22121) b t t K k=1 w y i k c kt \u03b3e \u2212\u03b3( 1 p p j=1 a j tx ij \u2212\u00b5t) p(1 + e \u2212\u03b3( 1 p p j=1 a j tx ij \u2212\u00b5t) ) 2 x i j\nwhere \u03c4 L ( t) is the set of all the leaf nodes descendant from node t, formally\u03c4 L ( t) = {t : t \u2208 \u03c4 L , t \u2208 N L (t) \u2228 t \u2208 N R (t)},and b t t = \uf8f1 \uf8f2 \uf8f3 1 if t \u2208 N R (t), 0 if t \u2208 N L (t).\nAs to the sparsity regularization terms, since by assumption \u03bb G 0 = 0, we only need to consider the local sparsity termR loc = \u03bb L 0 p j=1 t\u2208\u03c4 B (1 \u2212 e \u2212\u03b1|a jt | ) whose single component R loc\njt with respect to aj t can be rewritten as:R loc jt = \u03bb L 0 (1 \u2212 e \u2212\u03b1|ajt| ) = \u03bb L 0 (\u2212 \u221e k=1 (\u2212\u03b1|aj t|) k k! ) = \u03bb L 0 (\u03b1|aj t| + o(|aj t|)).\nFor aj t = 0 the optimality condition is 0 \u2208 \u03bej t(0) + \u03bb L 0 \u2202R loc jt , where \u2202R loc jt is the subdifferential of R loc jt .\n\n\u2202R loc\njt is equal to [l(\u03b1, aj t), u(\u03b1, aj t)], where l(\u03b1, aj t) and u(\u03b1, aj t) are respectively the lower and upper bound of the subdifferential interval as functions of the parameter \u03b1 and the variables aj t. Whenaj t = 0, \u2202R loc jt coincides with the subdifferential of function \u03bb L 0 \u03b1|aj t|, therefore \u2202R loc jt = [\u2212\u03bb L 0 \u03b1, \u03bb L 0 \u03b1]. Thus, the optimality condition is 0 \u2208 \u03bej t(0) \u2212 \u03bb L 0 \u03b1, \u03bej t(0) + \u03bb L 0 \u03b1 , hence \u03bb L 0 \u2265 |\u03bej t(0)| \u03b1 .\nBy applying (12) to every possible pair j and t the result follows.\nConcerning case (ii) (\u03bb L 0 = 0), the only sparsity regularization term isR glob = \u03bb G 0 p j=1 (1 \u2212 e \u2212\u03b1\u03b2 j )\n, where\u03b2 j = max t\u2208\u03c4 B |a jt |, j \u2208 {1, . . . , p}.\nFor every predictor variable index j, we have \u03b2j = |aj t( j) |, where t( j) = argmax t\u2208\u03c4 B |aj t |. Thus the single component of the sparsification term corresponding to j amounts toR glob j = \u03bb G 0 (1 \u2212 e \u2212\u03b1|aj t( j) | ) = \u03bb G 0 (\u2212 \u221e k=1 (\u2212\u03b1|aj t( j) |) k k! ) = \u03bb G 0 (\u03b1|aj t( j) | + o(|aj t( j) |)).\nSince aj t = 0 for every t \u2208 \u03c4 B at (0 * , \u00b5 * , c * ), we can replace t( j) with any t and we obtain the result by applying the same reasoning of case (i) (replacing \u03bb L 0 with \u03bb G 0 ). In case (iii), the sparsity regularization term is R tot = \u03bb L 0 R loc + \u03bb G 0 R glob , with for every pair j and t a component equal toR tot jt = \u03bb L 0 (1 \u2212 e \u2212\u03b1|ajt| ) + \u03bb G 0 (1 \u2212 e \u2212\u03b1|ajt| ) = (\u03bb L 0 + \u03bb G 0 )(1 \u2212 e \u2212\u03b1|ajt| ) = (\u03bb L 0 + \u03bb G 0 )(\u2212 \u221e k=1 (\u2212\u03b1|ajt|) k k! ) = (\u03bb L 0 + \u03bb G 0 )(\u03b1|aj t| + o(|aj t|)).\nNotice that, in general, the global term \u03bb G 0 (1 \u2212 e \u2212\u03b1|ajt| ) of ( 14) is present only if t \u2261 t( j). However, as already pointed out, since aj t = 0 for every t \u2208 \u03c4 B , ( 14) applies to any pair j and t. Then, the result is obtained by applying the same reasoning of case (i), replacing\u03bb L 0 with \u03bb L 0 + \u03bb G 0 .\nFor comparison purposes, we also consider three variants of the above formulation based on three alternative concave approximations of l 0 . In particular, we test the two step function approximations\nproposed in [26], namely (u + \u03b5) q where 0 < q < 1 and \u03b5 > 0, \u2212 1 (u+\u03b5) q with q \u2265 1 and \u03b5 > 0, and the approximation ln(u + \u03b5) for u \u2265 0, where \u03b5 > 0 [25]. These approximations will be referred to as, respectively, appr 1 , appr 2 and log.\nFrom now on, the formulation proposed in [20] with the objective function ( 4) where \u03bb G = 0 and \u03bb L = 0 will be referred to as L 1 and, respectively, L \u221e . While formulation ( 9) with \u03bb G 0 = 0 and \u03bb L 0 = 0 will be referred to as L loc 0 and, respectively, L glob 0 .\n\n4. Computational results\nIn this section we evaluate the testing accuracy and sparsity of the MRCTs obtained via concave approximations of the l 0 \"norm\" on 24 datasets from the literature, and compare them with those of the ORCTs found via l 1 and l \u221e regularization as proposed in [20]. All the formulations are constructed using Pyomo optimization modeling language in Python 3.6. Since we deal with nonlinear nonconvex continuous constrained optimization problems, we adopt the IPOPT 3.11.1 [27] solver as in [20] and a multistart approach with 10 restarts from different random initial solutions. The experiments are carried out on a server with 24 processors Intel(R) Xeon(R) CPU e5645 @2.40GHz 16 GB of RAM.\nThe section is organized as follows. After mentioning dataset information and describing the experimental setup, we report and discuss the results obtained when inducing local and global sparsity separately. Then we summarize the results obtained when both types of sparsity are promoted simultaneously and we conclude with some overall observations.\n\n4.1. Datasets and experiments\nIn the experiments, we consider all the datasets from the UCI Machine Learning repository [28] used in [20] as well as 6 well-known datasets from the KEEL repository [29]. The purpose is to include also datasets with a larger number of features and classes. Table 1 reports the characteristics of the For each fold the model is trained from 10 different random starting solutions. The accuracy of each fold is the average of the accuracies of the 10 trained models. As in [20], the following two sparsity indices are considered. The local sparsity, denoted by \u03b4 L , is the percentage of predictor variables not used per branch node:\u03b4 L = 1 |\u03c4 B | t\u2208\u03c4 B |{a jt = 0, j = 1, . . . , p}| p \u00d7 100.\nThe global sparsity, denoted by \u03b4 G , is the percentage of predictor variables not used across the whole tree:\u03b4 G = |{a j = 0, j = 1, . . . , p}| p \u00d7 100.\nFor the misclassification costs w y i k and the CDF parameter \u03b3 we took the values as in [20] (if y i = k then w y i k = 0.5, else w y i k = 0, and \u03b3 = 512), while for the parameters of the concave approximations of the step function we set q = 1 and \u03b5 = 10e-6. Moreover, in all our experiments we use the minimum correct classification rate constraint, as defined in [19, 20] with the same parameter settings (for each class a minimum percentage of correctly classified data points equal to 10%).\nConcerning the equality constraints (1), two implementation strategies are possible: either (i) they are explicitly included in the formulation, or (ii) the P it terms in the objective function are replaced with the corresponding right-hand side of (1). Preliminary tests showed that, with the adopted optimization solver (the same as in [20]), strategy (i) tends to be substantially more robust with respect to the starting solutions, i.e., it sharply reduces the frequency with which the solver gets stuck in poor quality solutions nearby the starting ones. Nonetheless, it affects the computational times: with strategy (ii) the training times of all the models are comparable with the ones reported in [20], while strategy (i) leads to significantly higher training times. Since our experimental campaign focuses on the accuracy and sparsity levels of the trained models, strategy (i) has been chosen. Then, for instance, for the best performing models L 1 and L glob 0 and for trees of depth 1 the computational times range from 1.77 (L glob 0 ) and 1.63 (L 1 ) seconds for Monks-1 to 602.4 (L glob 0 ) and 713.3 (L 1 ) seconds for Spambase. For trees of depth 2 the computational times range from 13.8 (L glob 0 ) and 14.6 (L 1 ) seconds for Iris to 1102.5 (L glob 0 ) and 1566.3 (L 1 ) seconds for Thyroid. For trees of depth 3 the computational times are 3236.9 (L glob 0 ) and 3075.1 (L 1 ) seconds for Ecoli.\n\n4.2. Results for separate local or global sparsity\nThis section is devoted to the comparative results for the 24 datasets when local and global sparsity are considered separately. Tables 2 and 3 report the best results in terms of accuracy and sparsity obtained by each kind of model by varying the regularization parameters (\u03bb L or \u03bb G ) in the interval {2 r : \u22128 \u2264 r \u2264 5, r \u2208 Z} . As expected, for all the datasets and for both local or global sparsity, when the associated \u03bb grows the corresponding \u03b4 index increases. Moreover, for all the datasets, the sparsest tree (in local or global terms) is obtained when the corresponding \u03bb assumes the largest value, although it often has the worst accuracy. For all the regularization terms the best accuracy levels are often obtained for sparse models where some of the predictor variables are neglected.\nTable 2 summarizes the results for the 12 datasets with two classes and for the regularized modelsL 1 , L \u221e , L loc 0 , L glob 0 , appr 1 .\nThe results obtained with the appr 2 and log approximations are not reported because they are outperformed by those obtained with the other l 0 \"norm\" approximations.\nFor these datasets, even if L loc 0 and L glob 0 perform almost always better than appr 1 , the latter turns out to be comparable and in few cases also slightly preferable. Recall that for two-class datasets local and global sparsity coincide (L loc 0 and L glob 0 as well as L 1 and L \u221e ) since D = 1 and the trees contain a single branch node. As a consequence, the results are very similar and the slight differences are accounted for by the different initial solutions.\nAccording to the comparative results we can distinguish three cases. We describe two representative examples for each case.\nFor a first group of datasets, which includes Wisconsin, Monks-2, Monks-3 and Ozone, the accuracy obtained via L loc 0 and L glob 0 is slightly lower (never by more than 0.6%) compared to the regularizations in [20], but for all these dataset except Monks-3 great gain in sparsity is achieved, by often removing twice as many predictor variables. For Wisconsin the best l 0 -based model (L glob 0 ) reaches an accuracy of 96.1% by removing 48.89% of the features; while the best l 1 -l \u221e model (L 1 ) achieves an accuracy of 96.5% by removing 24.45% of the predictor variables. For Monks-3 the best l 0 -based model (appr 1 ) reaches an accuracy of 93.9% by removing 82.36% of the features, while the best l 1 -l \u221e model (both L 1 and L \u221e ) achieves an accuracy of 93.5% by removing 88.24% of the predictor variables.\nFor a second group of datasets, including Diabetes, Monks-1, Creditapproval, Germancredit and Spambase, the approximate l 0 regularizations are, in terms of accuracy, at least as good as the previous ones and a higher gain in sparsity is obtained. For Monks-1, the best l 0 -based model (L glob 0 ) reaches an accuracy of 87.8% by removing 48.83% of the features; while the L 1 yields an accuracy of 87.6% by removing 17.29% of the predictor variables and the L \u221e reaches an accuracy of 87.4% by removing 45.53% of the features. For Spambase the best l 0 -based model (L glob 0 ) and the best l 1 -l \u221e model (L \u221e ) reach both an accuracy of 89.9%, but the former removes 4.22% of the features, while the latter 1.97%.\nFor a third group of datasets, consisting of Sonar and Ionosphere, the l 1 -l \u221e regularizations compare favourably with the approximate l 0 regularizations in terms of accuracy and sparsity. For Sonar the best l 0 -based model (L glob 0 ) reaches an accuracy of 74.2% by removing 18.98% of the features; while the best l 1 -l \u221e model (L 1 ) achieves an accuracy of 76.2% by removing 27.14% of the predictor variables.\nIn the case of Ionosphere the best l 0 -based model (L loc 0 ) reaches an accuracy of 86.51% when removing 3.36% of the features; while the best l 1 -l \u221e model (L \u221e ) achieves an accuracy of 88% by removing 84.94% of the predictor variables. Finally, note that for the Banknote datatset all regularizations yield MRCTs with the same accuracy and lead to no sparsification.\nThe above results for two-class datasets indicate that the l 0 -based and the the l 1 -l \u221e -based models are comparable, with the l 0 -based often superior in terms of solution sparsity except for two datasets. Table 3 summarizes the results for 12 multi-class datasets with K \u2265 3 classes and for the L 1 , L \u221e , L loc 0 and L glob 0 models (appr 1 is not reported since it is outperformed by the other l 0 -based alternatives).\nFor such datasets, MRCTs with more than one branch node (D \u2265 2) are required, and global and local sparsity (associated to the measures \u03b4 G and \u03b4 L ) clearly differ. The plots in Figures 2 and 3 elaborate the results of Table 3 and allow the comparison, in terms of accuracy, \u03b4 G and \u03b4 L , of the regularizations for local and global sparsity.\nFigure 2 shows the comparison between the global models L glob 0 and L \u221e , while Figure 3 compares the local models L loc 0 and L 1 . Both figures report, from left to right, the accuracy, the local sparsity level \u03b4 L and the global sparsity \u03b4 L . The values for the l 0 -based models are plotted on the y-axis while those for the l 1 -l \u221e -based models on the x-axis, and each dataset is represented by a circle with an identification number. For datasets with K \u2265 3 classes, both figures show that the l 0 -based regularizations lead to higher sparsity while guaranteeing comparable accuracy. In particular, regarding the global sparsity regularizations, the area above the bisectors of the middle and left plots in Figure 2 show that even if the L glob 0 and L \u221e yield comparable accuracy (the circles associated to all datasets lie on around the bisector of the left plot), the former regularization is definitely preferable in terms of the sparsity indices \u03b4 L and \u03b4 G . Concerning the local sparsity regularizations, the left plot of Figure 3 shows that L loc 0 and L 1 lead to comparable accuracy and the middle and right plots of Figure 3 show that the L loc 0 turns out to be often better in terms of \u03b4 L , while it is almost always better in terms of \u03b4 G . shown in the Appendix. As in [20], for each dataset three heatmaps are reported. They respectively represent the average testing accuracy acc, the average local sparsity \u03b4 L and the average global sparsity \u03b4 G (over the 10 runs), as a function of the parameters \u03bb G and \u03bb L . The color range of each heatmap goes from dark red to white, where the white corresponds to, respectively, the maximum accuracy and the maximum local or global sparsity achieved. In general, we observe that the best level of accuracy is not always achieved when (\u03bb G , \u03bb L ) assume the minimum values. When comparing the two alternative regularizations, we note that as the values of \u03bb G and \u03bb L vary the behaviour in terms of accuracy is very similar. Comparing the heatmaps we notice that, as the values of the \u03bb L and \u03bb G regularizations increase, the accuracy decreases faster for the l 0 -based regularizations than for the l 1 and l \u221e . As also\nshown in [20], focusing on global sparsity, in general for a fixed \u03bb L , \u03b4 G has a growing trend and the same behaviour can be observed for \u03b4 L when \u03bb G is fixed. As expected, the gain in \u03b4 L is greater than the gain in \u03b4 G when the \u03bb G value changes.\n\n4.4. Overall observations\nThe above experimental results lead to the three following observations. First, for datasets with two classes, the l 0 -based regularization can improve both local and global sparsity, without compromising the classification accuracy. Indeed, most of the times it is comparable to l 1 regularization in terms of accuracy and often better in terms of sparsity. Second, for datasets with more than two classes, the l 0 -based models for global sparsity, that is L glob 0 , are almost always the best one in terms of both accuracy and sparsity. Third, when both local and global regularization terms are simultaneously considered the l 0 -based ones are comparable with the combined l 1 and l \u221e -based ones.\n\n5. VC dimension of multivariate randomized classification trees\nWe consider binary classification tasks with a generic input vector x \u2208 R p and class label y \u2208 {0, 1}.\nIn statistical learning theory, the Vapnik-Chervonenkis (VC) dimension of a binary classification model is a measure that plays a key role in the generalization bounds, that is, in the upper bounds on the test error (see e.g.  [30]). The VC dimension measures the expressive power and the complexity of the set H of all the functions that can be implemented by the considered classification model. Given a set of hypotheses H, the VC dimension of H is defined as the cardinality of the largest number of data points that can be shattered by H. A set of l data points {x 1 , . . . , x l } \u2208 R p is said to be shattered by H = {h(x, \u03b1)} indexed by a parameter vector \u03b1 if and only if, for every possible assignment of       class labels to those l points (every possible dichotomy) there exists at least one function in H that correctly classifies all the data points (is consistent with the dichotomy). In our case, H is the set of all the functions that can be implemented by a given maximal binary and multivariate randomized classification tree of depth D with p inputs, that is, where every branch node at depth smaller or equal to D \u2212 1 has exactly two child nodes.\u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb L 1 0.\u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb Acc. \u03b4 L =\u03b4 G \u03bb L 1 0.862 92.05\u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb L 1 0.\u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb Acc. \u03b4 L \u03b4 G \u03bb L 0.\nIt is worth recalling that most decision trees in the literature are deterministic. To the best of our knowledge, no explicit formula is known for the VC dimension of multivariate or randomized decision trees. However, some bounds and a few exact results are available for special cases of deterministic trees and for a related randomized ML model. In [31] the VC dimension of univariate deterministic decision trees with \u03bd nodes and p inputs is proved to be between \u2126(\u03bd) and O(\u03bd log p). In [32] it is shown that the VC dimension of the set of all the Boolean functions on p variables defined by decision trees of rank at most r is r k=1 p k . In [33] the author first shows structure-dependent lower bounds for the VC dimension of univariate deterministic decision trees with binary inputs and then extends them to decision trees with L children per node. In [34] the VC dimension of mixture-of-experts architectures with p inputs and m Bernoulli or logistic regression experts is proved to be bounded below by m and above by O(m 4 p 2 ).\nIn the remainder of this section, we determine lower and upper bounds on the VC dimension of maximal MRCTs of depth D with D \u2265 1 and two classes.\n\n5.1. Lower bounds\nWe start with a simple observation concerning MRCTs of depth D = 1, that is, with a single branch node. Let us recall that the well-known perceptron model (see e.g.  [30]) maps a generic pdimensional real input vector x to the binary output y(x) = 1 R + (w T x + b), where the parameters w j , with 1 \u2264 j \u2264 p, and b take real values. Observation 1. A MRCT of depth D = 1 with p real (binary) inputs is as powerful as a perceptron with p real (binary) inputs and hence its VC dimension is equal to p + 1.\nNote that a MRCT with p real (binary) inputs and a single branch node coincides with a binary logistic regression model whose response probability conditioned on the input variables is:P(y = 1 | x) = 1 + e (\u2212\u03b3(\u03b2 T x+\u03b2 0 )) \u22121\nwith the p + 1-dimensional parameter vector (\u03b2 T , \u03b2 0 ). Considering an appropriate threshold \u03c1 and defining y(x) = 1 R + (P(y = 1 | x) \u2212 \u03c1), there is an obvious equivalence with the perceptron model with n inputs. For any fixed value of \u03c1 (e.g. \u03c1 = 0.5), the equation of the separating hyperplane is1 1 + e (\u2212\u03b3(\u03b2 T x+\u03b2 0 ) \u2212 \u03c1 = 0 and hence \u03b2 T x + \u03b2 0 + 1 \u03b3 ln( 1 \u2212 \u03c1 \u03c1 ) = 0.\nSince the VC dimension of a perceptron with p real (binary) inputs is equal to p + 1 (see e.g.  [30]), a MRCT of depth D = 1 with p real (binary) inputs has the same VC dimension.\nFor MRCTs of depth D = 2, that is, with three branch nodes, we have:Proposition 2.\nThe VC dimension of a maximal MRCT of depth D = 2 with p real (binary) inputs is at least 2(p + 1).\nProof. To prove the result we need to exhibit a set of 2(p + 1) points in R p which is shattered by a maximal MRCT of depth D = 2 with p inputs. Since each branch node at depth 2 can be viewed as a perceptron with p inputs and its VC dimension is p + 1 even for binary inputs, we show that there exists a set of p + 1 vertices of the unit hypercube B p = {0, 1} p , denoted by V L , shattered by the left branch node and a set of p + 1 vertices of B p , denoted by V R , shattered by the right branch node such that V L \u2229 V R = \u2205 and their union V L \u222a V R can be shattered by the overall MRCT of depth D = 2. To do so it suffices to verify that there exist values for the parameters a and \u00b5 of the root node which guarantee the separation of points in V L from those in V R with a given probability. Indeed, this implies that the root node can forward all the points in V L to the left branch node and all those in V R to the right branch node.\nFor any given dimension p \u2265 2, we can consider the subset V R \u2286 B p containing the zero vector and the p vectors e i , with 1 \u2264 i \u2264 p, of the canonical base in dimension p, and the subset V L containing the all-ones vector 1 and the p vectors 1 \u2212 e i , with 1 \u2264 i \u2264 p. Obviously V L is the complement of V R and both sets V L and V R are full dimensional.\nWe exhibit values of the parameters a and \u00b5 associated to the root node and of \u03b3 in the CDF function such that the two sets V L and V R turn out to be separable with a given confidence margin.\nA possible choice for a and \u00b5 is as follows:a = p \u2212 1 p , p \u2212 1 p , ..., p \u2212 1 p T and \u00b5 = 1 p .\nThese values guarantee that, given any p \u2265 2 and threshold 0 < < 0.5, the probability for every point in V R to fall to the left of the root node is at most and the probability for every point in V L to fall to the right of the root node is at most . Indeed, for any point in V R , the maximum probability to fall to the left is:1 1 + e \u2212\u03b3( p\u22121 p 2 \u2212 1 p ) = 1 1 + e \u03b3( 1 p 2 )\n, which is at most when\u03b3 \u2265 p 2 ln( 1 \u2212 ).\nSimilarly, for any point in V L , the maximum probability to fall to the right is:1 \u2212 1 1 + e \u2212\u03b3( p 2 \u22123p+1 p 2 ) = 1 1 e \u2212\u03b3( p 2 \u22123p+1 p 2 ) + 1 , which is at most when \u03b3 \u2265 p 2 p 2 \u2212 3p + 1 ln( 1 \u2212 ).\nTherefore the MRCT of depth D = 2 whose root node has the above parameter values is guaranteed to shatter the 2(p + 1) points of the set V L \u222a V R with a high probability.\nClearly, since we have exhibited binary points the result is also valid for the special case of maximal MRCTs with binary inputs.\nFor maximal MRCTs of depth D \u2265 3, we have:Proposition 3.\nThe VC dimension of a maximal MRCT of depth D with p real (binary) inputs, whereD \u2265 3 and D \u2264 p + 2, is at least 2 D\u22121 (p \u2212 D + 3), assuming that p \u2212 D \u2264 2 p\u2212D+1 \u2212 3.\nProof. We use the lower bound in Proposition 2 for a MRCT of depth D = 2 as well as the following simple extensions of a result and a recursive procedure for univariate deterministic classification trees with binary inputs described in [33].\nThe extended result states that the VC dimension of a maximal MRCT of depth D \u2265 2 with p real (binary) inputs is at least the sum of the VC dimensions of its left and right subtrees restricted to p \u2212 1 inputs. Indeed, by setting for each data point the additional (p-th) variable to 0 or 1, we can use this variable at the root node to forward the data points to, respectively, the right subtree or the left subtree. The recursive procedure, denoted as LB-VC(T ,p), takes as input a MRCT T with p real Note that the limitation of the above lower bound lies in the fact that one dimension is lost at each depth exceeding 2 (for this reason we must have D \u2264 p + 2).\nwhereg 4 (x) = p 1 (x)p 2 (x), g 5 (x) = p 1 (x)(1 \u2212 p 2 (x)), g 6 (x) = (1 \u2212 p 1 (x))p 3 (x) and g 7 (x) = (1 \u2212 p 1 (x))(1 \u2212 p 3 (x)).\nFor each pair t \u2208 \u03c4 L and k \u2208 {0, 1}, the variable c kt can be viewed as the probability that the class label k is assigned to leaf node t. However, since optimal solutions are integer, we know that a single class label is assigned to each leaf. For MRCTs, the probability for a new input vector x to be assigned to the first class is:P(y = 1 | x) = t\u2208\u03c4 L c 1t P xt = t \u2208\u03c4 bottom B \u03c0 t (x)g t (x),\nwhere P xt is the probability for x to fall into leaf node t. Introducing a scalar threshold \u03c1 (e.g \u03c1 = 0.5), Equation ( 10) yields the following discriminant function:C \u03b8 (x) = 1 R + ( t\u2208\u03c4 L c 1t P xt \u2212 \u03c1),\nwhere the parameter vector \u03b8 includes all the parameters of the m logistic regressions, at level D,(\u03b2 T t , \u03b2 0t ), t \u2208 \u03c4 bottom B\nand the ones of theg t (x) functions (u T t , v t ), for t \u2208 \u03c4 bottom B , that is \u03b8 \u2208 R 2m(p+1) .\nNote that Equation (10) shows the connection between MRCTs and Mixtures of binary Experts (MbEs) [36, 37]. MbEs can be seen as a combination of binary experts which implement a probability model of the response conditioned on the input vector. Assuming that we have m \u2265 2 experts and each one of them has a probability function \u03c0 t (x), a MbE generates the following conditional probability of belonging to the class label y = 1:p(x) = t \u2208\u03c4 bottom B \u03c0 t (x)g t (x),\nwhere g t (x) are the local weights, called gating functions.\nIn [34] the author exploits the result in [38] concerning the VC dimension of neural networks with sigmoidal activation functions to establish an upper bound of O(m 4 p 2 ) on the VC dimension of MbEs with logistic regression models, where m is the number of experts and p the number of inputs. Since in our case m = 2 D\u22121 , we have the following upper bound:Proposition 4.\nThe VC dimension of a maximal MRCT of depth D with p inputs is at most O(2 4(D\u22121) p 2 ).\n\n6. Decomposition methods for sparse randomized classification trees\nMRCTs reveal to be promising ML models both in terms of accuracy and of interpretability.\nSparsity enhances interpretability also when the number of features grows. However, since the training of sparse MRCTs is formulated as a challenging nonconvex constrained nonlinear optimization problem, the long training times required for larger datasets affect their practical applicability.\nIn this section, we propose and investigate decomposition methods for training good quality MRCTs in significantly shorter computing times. First we present a general decomposition scheme, then we discuss possible versions of the general algorithm and we propose a specific one whose performance is tested on five datasets larger than most of those used in Section 4.2.\n\n6.1. A general decomposition scheme\nDecomposition techniques have been extensively considered in the literature for training various learning models such as Feedforward Neural Networks and Support Vector Machines (e.g., [39, 40, 41, 42, 43, 44, 45, 46]). Indeed, the increasing dimension of the available training sets often leads to very challenging large-scale optimization problems.\nHere The objective function of L glob 0 is the sum of the expected misclassification errors and the sparsity regularization term. According to the new notation, the error function is written asE(A + , A \u2212 , C) = i\u2208N t\u2208\u03c4 L \uf8ee \uf8f0 t l \u2208N L (t) F 1 p (A + \u2022t l \u2212 A \u2212 \u2022t l ) T x i (17) tr\u2208N R (t) 1 \u2212 F 1 p (A + \u2022tr \u2212 A \u2212 \u2022tr ) T x i k\u2208K w y i k c kt \uf8f9 \uf8fb ,\nwhile the sparsity regularization term as S(\u03b2) = p j=1 (1 \u2212 e \u2212\u03b1\u03b2 j ). Then the formulation amounts to minA + ,A \u2212 ,\u03b2,C O(A + , A \u2212 , \u03b2, C) = E(A + , A \u2212 , C) + \u03bb G 0 S(\u03b2)\ns.t.K k=1 c kt = 1 t \u2208 \u03c4 L t\u2208\u03c4 L c kt \u2265 1 k = 1, . . . , K \u03b2 j \u2265 a + jt + a \u2212 jt j = 1, . . . , p, t = 1, . . . , \u03c4 B A + , A \u2212 \u2208 [0, 1] p\u00d7|\u03c4 B | , \u03b2 \u2208 [0, 1] p , C \u2208 [0, 1] K\u00d7|\u03c4 L | .\nNow we are ready to present the proposed decomposition method which is a nodes based strategy.\nIndeed, at each decomposition step s a subset of nodes of the tree is selected and only the indices of the variables involved in such nodes are inserted in the working set W s . The latter is composed ofW s B \u2286\ni.e., the indices subsets of, respectively, the branch nodes and leaf nodes selected at step s, withW s B \u2261 {1, 2, . . . , |\u03c4 B |} \\ W s B and W s L \u2261 {1, 2, . . . , |\u03c4 L |} \\ W s L\nas complements. For simplicity, from now on the dependency of the working sets on s will be omitted. At decomposition step s, given the current feasible solution (A +,s , A \u2212,s , \u03b2 s , C s ) and working sets W B and W L , the proximal point modification of the decomposition subproblem is as follows: min\n\nLet us denote by\nA + W B (A \u2212 W B )A + W B ,A \u2212 W B ,\u03b2,C W L O(A + W B , A \u2212 W B , \u03b2, C W L )+ (21) \u03c8 2 \uf8ee \uf8f0 t\u2208W B A + \u2022t \u2212 A +,s \u2022t 2 + A \u2212 \u2022t \u2212 A \u2212,s \u2022t 2 + t\u2208W L C \u2022t \u2212 C s \u2022t 2 \uf8f9 \uf8fb s.t. C T \u2022t 1 = 1 t \u2208 W L C k\u2022 1 \u2265 1 k = 1, . . . , K \u03b2 j \u2265 a + jt + a \u2212 jt j = 1, . . . , p, t = 1, . . . , \u03c4 B A + , A \u2212 \u2208 [0, 1] p\u00d7|\u03c4 B | , \u03b2 \u2208 [0, 1] p , C \u2208 [0, 1] K\u00d7|\u03c4 L | .where \u03c8 \u2265 0, 1 is the vector of all ones andO(A + W B , A \u2212 W B , \u03b2, C W L )\ndenotes the decomposition version of function (18) In general, in the design of decomposition algorithms for constrained nonlinear programs a proximal point term is added to the objective function to ensure some asymptotic convergence properties (see [47]). However, in the considered ML context we are more interested in the classification accuracy of the trained model rather than in the asymptotic convergence toward local or global solutions of (21).\nA further reason not to focus on convergence issues is that in ML an excessive computational effort in solving the optimization training problem may lead to overfitting phenomena. Hence, the proposed decomposition scheme aims at obtaining a sufficiently accurate classification model in a limited CPUtime, in a sort of \"early-stopping\" setting. Nonetheless, as highlighted for instance in [48], adding a proximal point term in a decomposition subproblem may also have a beneficial effect from a numerical point of view, by \"convexifying\" the objective function. This is the rationale for including it into (21).\nAfter (approximately) solving (21) and obtaining a solution (A+ W B * , A \u2212 W B * , \u03b2 * , C W L *\n), the current solution of the original problem is updated as(A +,s+1 , A \u2212,s+1 , \u03b2 s+1 , C s+1 ) = ((A + W B * , A + W B s ), (A \u2212 W B * , A \u2212 W B s ), \u03b2 * , (C W L * , C W L s )).\nThe general decomposition scheme, referred to as NB-DEC (Node Based Decomposition), is shown in Algorithm 1.\nIn the NB-DEC initialization phase, a non-negative value is selected for the proximal point coefficient \u03c8 and a feasible starting solution (A +,0 , A \u2212,0 , \u03b2 0 , C 0 ) is provided. The main loop, which consists of three steps, is iterated until a certain stopping condition is met. In the first step, the working set selection is performed. In particular, the indices associated to the branch and leaf nodes to be added to, respectively, W B and W L are selected. In the second step, the subproblem ( 21) is (approximately) solved to obtain the partial solution (A+ W B * , A \u2212 W B * , \u03b2 * , C W L *\n). The latter is used in the third step to update the current solution. At the end of the main loop the current solution (A +,s , A \u2212,s , \u03b2 s , C s ) is Then, the feasible set would consist in the Cartesian product of closed convex sets with respect to the variables' blocks involved in each of the two types of working sets. Since the feasible set of every decomposition subproblem is compact and the objective function is continuous, by the Weierstrass Theorem each subproblem admits an optimal solution, so it is well defined as stated in [47]. Moreover, since the sequence {(A +,s , A \u2212,s , \u03b2 s , C s )} produced by C-NB-DEC is defined over a compact feasible set, it admits limit points. Hence, considering also the presence of the proximal point term, from Proposition 7 of [47] every limit point of the sequence produced by C-NB-DEC is a critical point for (21) (a feasible point is critical if no feasible descent directions exist at that point).\nNotice also that if the indices in W B at step (i) are divided into any partition and step (i) is splitted into a sequence of internal decomposition steps based on the considered partition, then the above-mentioned convergence property still holds (provided that the same partition is adopted at every step (i)).\n\n6.3. S-NB-DEC: an efficient practical version\nDespite its asymptotic convergence property, C-NB-DEC algorithm showed in preliminary experiments (not reported here for brevity) to be not that efficient as it is not suited to fully exploit the decomposition of the general scheme and the intrinsic structure of problem (21).\nFor this reason, here we present an efficient practical version of NB-DEC, referred to as S-NB-DEC (Single branch Node Based Decomposition), and compare it to the method without decomposition in order to test the benefits of the decomposition approach. Even though S-NB-DEC is a heuristic, it adopts an \"intense\" branch nodes decomposition that makes it more efficient than the not decomposed version and the aforementioned convergent C-NB-DEC.\nS-NB-DEC is obtained from NB-DEC by specifying the stopping criterion, the working set selection rule and the subproblem solver. In particular, at each decomposition step s, only a single index associated to a random branch node is inserted in W B , while all indices associated to the leaf nodes are inserted in W L . Each branch node is randomly selected only one time per macro-iteration, i.e., a sequence of decomposition steps in which all the branch nodes have been selected one time in W B .\nAs stopping criterion a maximum number of macro-iterations is adopted. An alternative criterion could be related to the satisfaction of the Karush-Khun-Tucker conditions for problem (18), but as previously mentioned the former is more suitable in case of limited training time.\nThe S-NB-DEC method, summarized in Algorithm 2, mainly consists of two nested loops. In the internal one, multiple decomposition steps are performed until all the branch nodes are randomly selected from the set List. The partial working set W B is constructed (instruction 6) on the basis of the random selection operated at instruction 5. It is worth mentioning that W L is always made up of indices of all leaf nodes for stability reasons, as changing the variables of a single branch node affects the optimality of the variables associated to all leaf nodes. However, this does not represent a significant limitation as the number of leaf nodes variables is not that large for most practical problems.\nS-NB-DEC can optionally be started with an initialization step in which all variables are inserted in the working set (no actual decomposition is performed) and a limited number of iterations (init iter)\nof an NLP solver is applied to formulation (21) (which in this case coincides with ( 18)). The solution obtained at the end of this phase (denoted as (A +,init iter , A \u2212,init iter , \u03b2 init iter , C init iter )), will be used as initial solution for the subsequent decomposition phase. In some cases, the Initialization may improve the stability of the method by providing the decomposition algorithm with more promising starting solutions, as the latter are obtained by using all variables' information. However, the Initialization may be out of reach for very large instances. Notice that, if no Initialization is applied, the starting solution (A +,0 , A \u2212,0 , \u03b2 0 , C 0 ) must be provided otherwise. In such cases, whenever the number of leaf nodes is larger or equal to the number of classes, an initial feasible solution can be easily obtained by setting to zero all variables a jt and \u03b2 j and setting c kt = 1/K with k = 1, . . . , K and t = 1, . . . , \u03c4 L .\nAlgorithm 2 S-NB-DEC apply NLP solver for init iter iterations to (21) 5:\nset (A +,0 , A \u2212,0 , \u03b2 0 , C 0 ) = (A +,init iter , A \u2212,init iter , \u03b2 init iter , C init iter ) 6: else 7:\nset (A +,0 , A \u2212,0 , \u03b2 0 , C 0 ) provided as input 8: while s < max iter do Decomposition set (A +,s+1 , A \u2212,s+1 , \u03b2 s+1 , C s+1 ) as in (22) 15:\nset s = s + 1 return (A +,s , A \u2212,s , \u03b2 s , C s ) the Initialization step, whenever computationally viable, helps in speeding up and stably driving the subsequent decomposition steps towards good quality solutions.\nLet us consider the three larger datasets, for which the Initialization has been disabled. As to the Splice dataset, S-NB-DEC without the proximal point term yields trees with a better accuracy than not-DEC within approximately the same CPU-time, while the proximal point version achieves the same accuracy as not-DEC at macro-iteration 11 (30% CPU-time saving) and then improves it from macro-iterations 12 to 15 (CPU-time savings are comprised between 30% and 15%).\nConcerning the Segment dataset, the one with the largest number of samples, S-NB-DEC without proximal point term provides trees with a testing accuracy lower than that of not-DEC by about 1%, but with a CPU-time saving of almost 70% (at macro-iteration 9), while the proximal point version obtains similar results but with a slightly lower accuracy than that without proximal term.\nFor the Dna dataset, the one with largest number of features, the S-NB-DEC yileds trees with an accuracy loss of about 2.5% with respect to not-DEC, without any significant CPU-time saving, while the proximal point version achieves the same accuracy of not-DEC with a CPU-time saving of almost 20% (macro-iteration 9) and it is also able to slightly improve the accuracy in correspondence of CPU-time savings between 10% and 5% (macro-iterations 10 and 11) or for slightly larger CPU-times.\nTo summarize, the above numerical results indicate that the S-NB-DEC decomposition approach allows to significantly reduce the computational time needed to solve the training problem (18), without compromising too much the classification trees accuracy. In certain cases, the decomposition even yields improved testing accuracy (see Splice and Dna). Whenever applicable, the Initialization step may facilitate faster progress towards a good quality solution, although for larger datasets the pure decomposition is able to achieve promising results both in terms of accuracy and CPU-time savings.\nAs expected, the presence of the proximal point term is often helpful in speeding up the training process.\n\n7. Concluding remarks\nWe have investigated the interesting nonlinear optimization formulation proposed in [19, 20] for training (sparse) MRCTs along three directions. First, we presented alternative methods to sparsify MRCTs based on concave approximations of the l 0 \"norm\" and we compared them with the original l 1 and l \u221e regularizations. Second, we derived lower and upper bounds on the VC dimension of MRCTs.\nThird, we proposed a general proximal point decomposition scheme to tackle larger datasets and we described an efficient version of the method.\nThe results reported for 24 datasets indicate that the alternative sparsification method based on approximate l 0 regularization compares favourably with the original approach and leads to more com-pact MRCTs. Moreover, the decomposition method yields promising results in terms of speed up and of testing accuracy on five larger datasets. Note that achieving a significant speed up in the training of MRCTs while maintaining comparable accuracy allows to widen the range of applicability of such ML models. This may also constitute a step toward the combination of such MRCTs, with other models or \u00e0 la ensemble, in an attempt to further improve accuracy.\nFuture work includes investigating different working set selection strategies for the decomposition and extending these decomposition methods to deal with additional side constraints such as costsensitivity and fairness as outlined in [21].\nwithout proximal term with proximal term without proximal term with proximal term\n\nFootnotes:\n2: Lasso stands for least absolute shrinkage and selection operator.\n\nReferences:\n\n- V. Podgorelec, P. Kokol, B. Stiglic, I. Rozman, Decision trees: an overview and their use in medicine, Journal of Medical Systems 26 (5) (2002) 445-463.- C. L. Tsien, I. S. Kohane, N. McIntosh, Multiple signal integration by decision tree induction to detect artifacts in the neonatal intensive care unit, Artificial Intelligence in Medicine 19 (3) (2000) 189-202.\n\n- C. Chelazzi, G. Villa, A. Manno, V. Ranfagni, E. Gemmi, S. Romagnoli, The new sumpot to predict postoperative complications using an artificial neural network, Scientific Reports 11 (1) (2021) 22692.\n\n- N. Ghatasheh, Business analytics using random forest trees for credit risk prediction: a comparison study, International Journal of Advanced Science and Technology 72 (2014) 19-30.\n\n- A. Ghodselahi, A. Amirmadhi, Application of artificial intelligence techniques for credit risk eval- uation, International Journal of Modeling and Optimization 1 (3) (2011) 243.\n\n- M. Ouahilal, M. El Mohajir, M. Chahhou, B. E. El Mohajir, A comparative study of predictive algorithms for business analytics and decision support systems: Finance as a case study, in: 2016 International Conference on Information Technology for Organizations Development (IT4OD), IEEE, 2016, pp. 1-6.\n\n- H. Laurent, R. L. Rivest, Constructing optimal binary decision trees is NP-complete, Information Processing Letters 5 (1) (1976) 15-17.\n\n- L. Breiman, J. H. Friedman, R. A. Olshen, C. J. Stone, Classification and regression trees, Rout- ledge, 2017.\n\n- J. R. Quinlan, Induction to decision trees, Machine Learning 1 (1) (1986) 81-106.\n\n- J. R. Quinlan, C4.5: Programs for Machine Learning, Elsevier, 2014.\n\n- K. P. Bennett, J. A. Blue, Optimal decision trees, Rensselaer Polytechnic Institute Math Report 214 (1996) 24.\n\n- D. Bertsimas, J. Dunn, Optimal classification trees, Machine Learning 106 (7) (2017) 1039-1082.\n\n- J. Dunn, Optimal trees for prediction and prescription, Ph.D. thesis, Massachusetts Institute of Technology (2018).\n\n- O. G\u00fcnl\u00fck, J. Kalagnanam, M. Menickelly, K. Scheinberg, Optimal decision trees for categorical data via integer programming, Journal of Global Optimization (2021) 1573-2916.\n\n- S. Verwer, Y. Zhang, Learning optimal classification trees using a binary linear program for- mulation, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, pp. 1625-1632.\n\n- M. Firat, G. Crognier, A. F. Gabor, C. Hurkens, Y. Zhang, Column generation based heuristic for learning classification trees, Computers & Operations Research 116 (2020).\n\n- E. Demirovi\u0107, A. Lukina, E. Hebrard, J. Chan, J. Bailey, C. Leckie, K. Ramamohanarao, P. J. Stuckey, Murtree: Optimal classification trees via dynamic programming and search, arXiv:2007.12652 (2020).\n\n- S. Aghaei, A. Gomez, P. Vayanos, Learning optimal classification trees: Strong max-flow formu- lations, arXiv preprint arXiv:2002.09142 (2020).\n\n- R. Blanquero, E. Carrizosa, C. Molero-R\u00edo, D. Romero Morales, Optimal randomized classification trees, Computers & Operations Research 132 (2021) 105281.\n\n- R. Blanquero, E. Carrizosa, C. Molero-R\u00edo, D. Romero Morales, Sparsity in optimal randomized classification trees, European Journal of Operational Research 284 (1) (2020) 255-272.\n\n- E. Carrizosa, C. Molero-R\u00edo, D. Romero Morales, Mathematical optimization in classification and regression trees, TOP 29 (2021) 5-33.\n\n- A. Blumer, A. Ehrenfeucht, D. Haussler, M. K. Warmuth, Learnability and the Vapnik- Chervonenkis dimension, Journal of the ACM (JACM) 36 (4) (1989) 929-965.\n\n- R. Tibshirani, Regression shrinkage and selection via the Lasso, Journal of the Royal Statistical Society: Series B (Methodological) 58 (1) (1996) 267-288.\n\n- P. S. Bradley, O. L. Mangasarian, Feature selection via concave minimization and support vector machines., in: ICML, Vol. 98, 1998, pp. 82-90.\n\n- J. Weston, A. Elisseeff, B. Sch\u00f6lkopf, M. Tipping, Use of the zero norm with linear models and kernel methods, The Journal of Machine Learning Research 3 (2003) 1439-1461.\n\n- F. Rinaldi, M. Sciandrone, Feature selection combining linear support vector machines and concave optimization, Optimization Methods & Software 25 (1) (2010) 117-128.\n\n- A. W\u00e4chter, L. T. Biegler, On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming, Mathematical Programming 106 (1) (2006) 25-57.\n\n- A. Asuncion, D. Newman, UCI Machine Learning Repository (2007).\n\n- J. Alcal\u00e1-Fdez, A. Fern\u00e1ndez, J. Luengo, J. Derrac, S. Garc\u00eda, L. S\u00e1nchez, F. Herrera, Keel data- mining software tool: data set repository, integration of algorithms and experimental analysis framework., Journal of Multiple-Valued Logic & Soft Computing 17 (2011).\n\n- M. Anthony, N. Biggs, Computational learning theory, Cambridge University Press, 1997.\n\n- Y. Mansour, Pessimistic decision tree pruning based on tree size, in: Proceedings of the Fourteenth International Conference on Machine Learning, Morgan Kaufmann, 1997, pp. 195-201.\n\n- H. U. Simon, The Vapnik-Chervonenkis dimension of decision trees with bounded rank, Informa- tion Processing Letters 39 (3) (1991) 137-141.\n\n- O. T. Y\u0131ld\u0131z, VC-dimension of univariate decision trees, IEEE Transactions on Neural Networks and Learning Systems 26 (2) (2015) 378-387.\n\n- W. Jiang, The VC-dimension for mixtures of binary classifiers, Neural Computation 12 (6) (2000) 1293-1301.\n\n- M. Anthony, P. L. Bartlett, Neural network learning: Theoretical foundations, Cambridge Uni- versity Press, 2009.\n\n- R. A. Jacobs, M. I. Jordan, S. J. Nowlan, G. E. Hinton, Adaptive mixtures of local experts, Neural Computation 3 (1) (1991) 79-87.\n\n- M. I. Jordan, R. A. Jacobs, Hierarchical mixtures of experts and the EM algorithm, Neural Computation 6 (2) (1994) 181-214.\n\n- M. Karpinski, A. Macintyre, Polynomial bounds for VC dimension of sigmoidal and general pfaffian neural networks, Journal of Computer and System Sciences 54 (1) (1997) 169-176.\n\n- L. Grippo, A. Manno, M. Sciandrone, Decomposition techniques for multilayer perceptron train- ing, IEEE Transactions on Neural Networks and Learning Systems 27 (11) (2015) 2146-2159.\n\n- G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: theory and applications, Neuro- computing 70 (1-3) (2006) 489-501.\n\n- D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv:1412.6980 (2014).\n\n- T. Joachims, Making large-scale svm learning practical, Technical Report 1998,28, Dortmund (1998).\n\n- S. Lucidi, L. Palagi, A. Risi, M. Sciandrone, A convergent decomposition algorithm for support vector machines, Computational Optimization and Applications 38 (2) (2007) 217-234.\n\n- C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3) (2011) 1-27.\n\n- A. Manno, L. Palagi, S. Sagratella, Parallel decomposition methods for linearly constrained prob- lems subject to simple bound with application to the SVMs training, Computational Optimization and Applications 71 (1) (2018) 115-145.\n\n- A. Manno, S. Sagratella, L. Livi, A convergent and fully distributable SVMs training algorithm, in: 2016 International Joint Conference on Neural Networks (IJCNN), IEEE, 2016, pp. 3076-3080.\n\n- L. Grippo, M. Sciandrone, On the convergence of the block nonlinear Gauss-Seidel method under convex constraints, Operations Research Letters 26 (3) (2000) 127-136.\n\n- L. Palagi, M. Sciandrone, On the convergence of a modified version of SVM light algorithm, Optimization Methods and Software 20 (2-3) (2005) 317-334.\n\n", "annotations": {"ReferenceToTable": [{"begin": 20170, "end": 20171, "target": "#tab_0", "idx": 0}, {"begin": 22859, "end": 22866, "target": "#tab_3", "idx": 1}, {"begin": 23530, "end": 23531, "target": "#tab_1", "idx": 2}, {"begin": 26973, "end": 26974, "target": "#tab_3", "idx": 3}, {"begin": 27411, "end": 27412, "target": "#tab_3", "idx": 4}], "ReferenceToFootnote": [{"begin": 9481, "end": 9482, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 481, "end": 54728, "idx": 0}], "ReferenceToFormula": [{"begin": 13665, "end": 13666, "target": "#formula_12", "idx": 0}, {"begin": 17850, "end": 17852, "target": "#formula_28", "idx": 1}, {"begin": 17954, "end": 17956, "target": "#formula_28", "idx": 2}, {"begin": 18614, "end": 18615, "target": "#formula_9", "idx": 3}, {"begin": 18715, "end": 18716, "target": "#formula_15", "idx": 4}, {"begin": 39399, "end": 39401, "target": "#formula_17", "idx": 5}, {"begin": 45815, "end": 45817, "idx": 6}, {"begin": 49722, "end": 49724, "target": "#formula_54", "idx": 7}], "SectionReference": [{"begin": 54811, "end": 62348, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 481, "idx": 0}], "Div": [{"begin": 120, "end": 473, "idx": 0}, {"begin": 484, "end": 4380, "idx": 1}, {"begin": 4382, "end": 5033, "idx": 2}, {"begin": 5035, "end": 8449, "idx": 3}, {"begin": 8451, "end": 10945, "idx": 4}, {"begin": 10947, "end": 16299, "idx": 5}, {"begin": 16301, "end": 18807, "idx": 6}, {"begin": 18809, "end": 19874, "idx": 7}, {"begin": 19876, "end": 22670, "idx": 8}, {"begin": 22672, "end": 29974, "idx": 9}, {"begin": 29976, "end": 30706, "idx": 10}, {"begin": 30708, "end": 33606, "idx": 11}, {"begin": 33608, "end": 40705, "idx": 12}, {"begin": 40707, "end": 41529, "idx": 13}, {"begin": 41531, "end": 43416, "idx": 14}, {"begin": 43418, "end": 47181, "idx": 15}, {"begin": 47183, "end": 53188, "idx": 16}, {"begin": 53190, "end": 54728, "idx": 17}], "Head": [{"begin": 484, "end": 499, "n": "1.", "idx": 0}, {"begin": 4382, "end": 4401, "idx": 1}, {"begin": 5035, "end": 5077, "n": "2.", "idx": 2}, {"begin": 8451, "end": 8500, "n": "3.", "idx": 3}, {"begin": 10947, "end": 11038, "n": "3.1.", "idx": 4}, {"begin": 16301, "end": 16307, "idx": 5}, {"begin": 18809, "end": 18833, "n": "4.", "idx": 6}, {"begin": 19876, "end": 19905, "n": "4.1.", "idx": 7}, {"begin": 22672, "end": 22722, "n": "4.2.", "idx": 8}, {"begin": 29976, "end": 30001, "n": "4.4.", "idx": 9}, {"begin": 30708, "end": 30771, "n": "5.", "idx": 10}, {"begin": 33608, "end": 33625, "n": "5.1.", "idx": 11}, {"begin": 40707, "end": 40774, "n": "6.", "idx": 12}, {"begin": 41531, "end": 41566, "n": "6.1.", "idx": 13}, {"begin": 43418, "end": 43434, "idx": 14}, {"begin": 47183, "end": 47228, "n": "6.3.", "idx": 15}, {"begin": 53190, "end": 53211, "n": "7.", "idx": 16}], "Paragraph": [{"begin": 120, "end": 473, "idx": 0}, {"begin": 500, "end": 805, "idx": 1}, {"begin": 806, "end": 1202, "idx": 2}, {"begin": 1203, "end": 1405, "idx": 3}, {"begin": 1406, "end": 1706, "idx": 4}, {"begin": 1707, "end": 2604, "idx": 5}, {"begin": 2605, "end": 3583, "idx": 6}, {"begin": 3584, "end": 3889, "idx": 7}, {"begin": 3890, "end": 4380, "idx": 8}, {"begin": 4402, "end": 5033, "idx": 9}, {"begin": 5078, "end": 5220, "idx": 10}, {"begin": 5221, "end": 5413, "idx": 11}, {"begin": 5414, "end": 6002, "idx": 12}, {"begin": 6044, "end": 6148, "idx": 13}, {"begin": 6172, "end": 6341, "idx": 14}, {"begin": 6342, "end": 6617, "idx": 15}, {"begin": 6667, "end": 7062, "idx": 16}, {"begin": 7263, "end": 7585, "idx": 17}, {"begin": 7640, "end": 7838, "idx": 18}, {"begin": 7882, "end": 7987, "idx": 19}, {"begin": 7988, "end": 8286, "idx": 20}, {"begin": 8287, "end": 8449, "idx": 21}, {"begin": 8501, "end": 9248, "idx": 22}, {"begin": 9249, "end": 9689, "idx": 23}, {"begin": 9690, "end": 10116, "idx": 24}, {"begin": 10117, "end": 10478, "idx": 25}, {"begin": 10552, "end": 10813, "idx": 26}, {"begin": 10814, "end": 10945, "idx": 27}, {"begin": 11039, "end": 11505, "idx": 28}, {"begin": 11557, "end": 11580, "idx": 29}, {"begin": 11656, "end": 11753, "idx": 30}, {"begin": 11754, "end": 12395, "idx": 31}, {"begin": 12396, "end": 12669, "idx": 32}, {"begin": 12698, "end": 12762, "idx": 33}, {"begin": 12812, "end": 12894, "idx": 34}, {"begin": 12895, "end": 12983, "idx": 35}, {"begin": 12984, "end": 13083, "idx": 36}, {"begin": 13470, "end": 13676, "idx": 37}, {"begin": 13677, "end": 13879, "idx": 38}, {"begin": 13880, "end": 13970, "idx": 39}, {"begin": 13971, "end": 14027, "idx": 40}, {"begin": 14078, "end": 14281, "idx": 41}, {"begin": 14282, "end": 14541, "idx": 42}, {"begin": 14648, "end": 14950, "idx": 43}, {"begin": 14951, "end": 15181, "idx": 44}, {"begin": 15182, "end": 15452, "idx": 45}, {"begin": 15649, "end": 15729, "idx": 46}, {"begin": 15836, "end": 15956, "idx": 47}, {"begin": 16030, "end": 16074, "idx": 48}, {"begin": 16174, "end": 16299, "idx": 49}, {"begin": 16308, "end": 16516, "idx": 50}, {"begin": 16746, "end": 16813, "idx": 51}, {"begin": 16814, "end": 16888, "idx": 52}, {"begin": 16924, "end": 16931, "idx": 53}, {"begin": 16976, "end": 17158, "idx": 54}, {"begin": 17279, "end": 17602, "idx": 55}, {"begin": 17781, "end": 18069, "idx": 56}, {"begin": 18096, "end": 18296, "idx": 57}, {"begin": 18297, "end": 18537, "idx": 58}, {"begin": 18538, "end": 18807, "idx": 59}, {"begin": 18834, "end": 19523, "idx": 60}, {"begin": 19524, "end": 19874, "idx": 61}, {"begin": 19906, "end": 20538, "idx": 62}, {"begin": 20599, "end": 20709, "idx": 63}, {"begin": 20754, "end": 21251, "idx": 64}, {"begin": 21252, "end": 22670, "idx": 65}, {"begin": 22723, "end": 23523, "idx": 66}, {"begin": 23524, "end": 23622, "idx": 67}, {"begin": 23664, "end": 23830, "idx": 68}, {"begin": 23831, "end": 24304, "idx": 69}, {"begin": 24305, "end": 24428, "idx": 70}, {"begin": 24429, "end": 25246, "idx": 71}, {"begin": 25247, "end": 25964, "idx": 72}, {"begin": 25965, "end": 26382, "idx": 73}, {"begin": 26383, "end": 26755, "idx": 74}, {"begin": 26756, "end": 27184, "idx": 75}, {"begin": 27185, "end": 27528, "idx": 76}, {"begin": 27529, "end": 29722, "idx": 77}, {"begin": 29723, "end": 29974, "idx": 78}, {"begin": 30002, "end": 30706, "idx": 79}, {"begin": 30772, "end": 30875, "idx": 80}, {"begin": 30876, "end": 32045, "idx": 81}, {"begin": 32421, "end": 33460, "idx": 82}, {"begin": 33461, "end": 33606, "idx": 83}, {"begin": 33626, "end": 34129, "idx": 84}, {"begin": 34130, "end": 34315, "idx": 85}, {"begin": 34356, "end": 34657, "idx": 86}, {"begin": 34736, "end": 34915, "idx": 87}, {"begin": 34916, "end": 34984, "idx": 88}, {"begin": 34999, "end": 35098, "idx": 89}, {"begin": 35099, "end": 36043, "idx": 90}, {"begin": 36044, "end": 36399, "idx": 91}, {"begin": 36400, "end": 36592, "idx": 92}, {"begin": 36593, "end": 36637, "idx": 93}, {"begin": 36690, "end": 37019, "idx": 94}, {"begin": 37068, "end": 37091, "idx": 95}, {"begin": 37110, "end": 37192, "idx": 96}, {"begin": 37312, "end": 37483, "idx": 97}, {"begin": 37484, "end": 37613, "idx": 98}, {"begin": 37614, "end": 37656, "idx": 99}, {"begin": 37671, "end": 37751, "idx": 100}, {"begin": 37838, "end": 38079, "idx": 101}, {"begin": 38080, "end": 38743, "idx": 102}, {"begin": 38744, "end": 38749, "idx": 103}, {"begin": 38880, "end": 39215, "idx": 104}, {"begin": 39278, "end": 39446, "idx": 105}, {"begin": 39486, "end": 39585, "idx": 106}, {"begin": 39617, "end": 39636, "idx": 107}, {"begin": 39715, "end": 40144, "idx": 108}, {"begin": 40181, "end": 40242, "idx": 109}, {"begin": 40243, "end": 40602, "idx": 110}, {"begin": 40617, "end": 40705, "idx": 111}, {"begin": 40775, "end": 40864, "idx": 112}, {"begin": 40865, "end": 41159, "idx": 113}, {"begin": 41160, "end": 41529, "idx": 114}, {"begin": 41567, "end": 41916, "idx": 115}, {"begin": 41917, "end": 42110, "idx": 116}, {"begin": 42267, "end": 42373, "idx": 117}, {"begin": 42439, "end": 42443, "idx": 118}, {"begin": 42624, "end": 42718, "idx": 119}, {"begin": 42719, "end": 42922, "idx": 120}, {"begin": 42930, "end": 43030, "idx": 121}, {"begin": 43112, "end": 43416, "idx": 122}, {"begin": 43781, "end": 43825, "idx": 123}, {"begin": 43858, "end": 44312, "idx": 124}, {"begin": 44313, "end": 44924, "idx": 125}, {"begin": 44925, "end": 44987, "idx": 126}, {"begin": 45023, "end": 45084, "idx": 127}, {"begin": 45205, "end": 45313, "idx": 128}, {"begin": 45314, "end": 45878, "idx": 129}, {"begin": 45914, "end": 46868, "idx": 130}, {"begin": 46869, "end": 47181, "idx": 131}, {"begin": 47229, "end": 47505, "idx": 132}, {"begin": 47506, "end": 47950, "idx": 133}, {"begin": 47951, "end": 48449, "idx": 134}, {"begin": 48450, "end": 48727, "idx": 135}, {"begin": 48728, "end": 49432, "idx": 136}, {"begin": 49433, "end": 49636, "idx": 137}, {"begin": 49637, "end": 50602, "idx": 138}, {"begin": 50603, "end": 50676, "idx": 139}, {"begin": 50677, "end": 50783, "idx": 140}, {"begin": 50784, "end": 50929, "idx": 141}, {"begin": 50930, "end": 51144, "idx": 142}, {"begin": 51145, "end": 51612, "idx": 143}, {"begin": 51613, "end": 51994, "idx": 144}, {"begin": 51995, "end": 52485, "idx": 145}, {"begin": 52486, "end": 53081, "idx": 146}, {"begin": 53082, "end": 53188, "idx": 147}, {"begin": 53212, "end": 53604, "idx": 148}, {"begin": 53605, "end": 53748, "idx": 149}, {"begin": 53749, "end": 54405, "idx": 150}, {"begin": 54406, "end": 54646, "idx": 151}, {"begin": 54647, "end": 54728, "idx": 152}], "ReferenceToBib": [{"begin": 750, "end": 753, "target": "#b0", "idx": 0}, {"begin": 754, "end": 756, "target": "#b1", "idx": 1}, {"begin": 757, "end": 759, "target": "#b2", "idx": 2}, {"begin": 794, "end": 797, "target": "#b3", "idx": 3}, {"begin": 798, "end": 800, "target": "#b4", "idx": 4}, {"begin": 801, "end": 803, "target": "#b5", "idx": 5}, {"begin": 862, "end": 865, "target": "#b6", "idx": 6}, {"begin": 919, "end": 922, "target": "#b7", "idx": 7}, {"begin": 973, "end": 976, "target": "#b8", "idx": 8}, {"begin": 986, "end": 990, "target": "#b9", "idx": 9}, {"begin": 1566, "end": 1570, "target": "#b10", "idx": 10}, {"begin": 1710, "end": 1714, "target": "#b11", "idx": 11}, {"begin": 1715, "end": 1718, "target": "#b12", "idx": 12}, {"begin": 1843, "end": 1847, "target": "#b13", "idx": 13}, {"begin": 2014, "end": 2018, "target": "#b14", "idx": 14}, {"begin": 2129, "end": 2133, "target": "#b15", "idx": 15}, {"begin": 2305, "end": 2309, "target": "#b16", "idx": 16}, {"begin": 2365, "end": 2369, "target": "#b17", "idx": 17}, {"begin": 2618, "end": 2622, "target": "#b18", "idx": 18}, {"begin": 2623, "end": 2626, "target": "#b19", "idx": 19}, {"begin": 3357, "end": 3361, "target": "#b19", "idx": 20}, {"begin": 3539, "end": 3543, "target": "#b20", "idx": 21}, {"begin": 3990, "end": 3994, "target": "#b21", "idx": 22}, {"begin": 4516, "end": 4520, "target": "#b18", "idx": 23}, {"begin": 4623, "end": 4627, "target": "#b19", "idx": 24}, {"begin": 5158, "end": 5162, "target": "#b18", "idx": 25}, {"begin": 7652, "end": 7656, "target": "#b18", "idx": 26}, {"begin": 8299, "end": 8303, "target": "#b18", "idx": 27}, {"begin": 9505, "end": 9509, "target": "#b22", "idx": 28}, {"begin": 10120, "end": 10124, "target": "#b19", "idx": 29}, {"begin": 12173, "end": 12177, "target": "#b23", "idx": 30}, {"begin": 12236, "end": 12240, "target": "#b24", "idx": 31}, {"begin": 12248, "end": 12252, "target": "#b25", "idx": 32}, {"begin": 12409, "end": 12413, "target": "#b23", "idx": 33}, {"begin": 12911, "end": 12914, "target": "#b7", "idx": 34}, {"begin": 12979, "end": 12982, "target": "#b4", "idx": 35}, {"begin": 13672, "end": 13675, "target": "#b6", "idx": 36}, {"begin": 13680, "end": 13684, "target": "#b19", "idx": 37}, {"begin": 13966, "end": 13969, "target": "#b8", "idx": 38}, {"begin": 14307, "end": 14311, "target": "#b18", "idx": 39}, {"begin": 14510, "end": 14513, "target": "#b8", "idx": 40}, {"begin": 14686, "end": 14690, "target": "#b9", "idx": 41}, {"begin": 14773, "end": 14776, "target": "#b6", "idx": 42}, {"begin": 14915, "end": 14918, "target": "#b6", "idx": 43}, {"begin": 16758, "end": 16762, "target": "#b11", "idx": 44}, {"begin": 18309, "end": 18313, "target": "#b25", "idx": 45}, {"begin": 18448, "end": 18452, "target": "#b24", "idx": 46}, {"begin": 18579, "end": 18583, "target": "#b19", "idx": 47}, {"begin": 19092, "end": 19096, "target": "#b19", "idx": 48}, {"begin": 19304, "end": 19308, "target": "#b26", "idx": 49}, {"begin": 19322, "end": 19326, "target": "#b19", "idx": 50}, {"begin": 19996, "end": 20000, "target": "#b27", "idx": 51}, {"begin": 20009, "end": 20013, "target": "#b19", "idx": 52}, {"begin": 20072, "end": 20076, "target": "#b28", "idx": 53}, {"begin": 20378, "end": 20382, "target": "#b19", "idx": 54}, {"begin": 20843, "end": 20847, "target": "#b19", "idx": 55}, {"begin": 21122, "end": 21126, "target": "#b18", "idx": 56}, {"begin": 21127, "end": 21130, "target": "#b19", "idx": 57}, {"begin": 21590, "end": 21594, "target": "#b19", "idx": 58}, {"begin": 21958, "end": 21962, "target": "#b19", "idx": 59}, {"begin": 24640, "end": 24644, "target": "#b19", "idx": 60}, {"begin": 28825, "end": 28829, "target": "#b19", "idx": 61}, {"begin": 29732, "end": 29736, "target": "#b19", "idx": 62}, {"begin": 31103, "end": 31107, "target": "#b29", "idx": 63}, {"begin": 32773, "end": 32777, "target": "#b30", "idx": 64}, {"begin": 32912, "end": 32916, "target": "#b31", "idx": 65}, {"begin": 33068, "end": 33072, "target": "#b32", "idx": 66}, {"begin": 33281, "end": 33285, "target": "#b33", "idx": 67}, {"begin": 33792, "end": 33796, "target": "#b29", "idx": 68}, {"begin": 34832, "end": 34836, "target": "#b29", "idx": 69}, {"begin": 38074, "end": 38078, "target": "#b32", "idx": 70}, {"begin": 39734, "end": 39738, "target": "#b9", "idx": 71}, {"begin": 39812, "end": 39816, "target": "#b35", "idx": 72}, {"begin": 39817, "end": 39820, "target": "#b36", "idx": 73}, {"begin": 40246, "end": 40250, "target": "#b33", "idx": 74}, {"begin": 40285, "end": 40289, "target": "#b37", "idx": 75}, {"begin": 41751, "end": 41755, "target": "#b38", "idx": 76}, {"begin": 41756, "end": 41759, "target": "#b39", "idx": 77}, {"begin": 41760, "end": 41763, "target": "#b40", "idx": 78}, {"begin": 41764, "end": 41767, "target": "#b41", "idx": 79}, {"begin": 41768, "end": 41771, "target": "#b42", "idx": 80}, {"begin": 41772, "end": 41775, "target": "#b43", "idx": 81}, {"begin": 41776, "end": 41779, "target": "#b44", "idx": 82}, {"begin": 41780, "end": 41783, "target": "#b45", "idx": 83}, {"begin": 43904, "end": 43908, "target": "#b17", "idx": 84}, {"begin": 44109, "end": 44113, "target": "#b46", "idx": 85}, {"begin": 44307, "end": 44311, "target": "#b20", "idx": 86}, {"begin": 44702, "end": 44706, "target": "#b47", "idx": 87}, {"begin": 44919, "end": 44923, "target": "#b20", "idx": 88}, {"begin": 44955, "end": 44959, "target": "#b20", "idx": 89}, {"begin": 46456, "end": 46460, "target": "#b46", "idx": 90}, {"begin": 46694, "end": 46698, "target": "#b46", "idx": 91}, {"begin": 47500, "end": 47504, "target": "#b20", "idx": 92}, {"begin": 48632, "end": 48636, "target": "#b17", "idx": 93}, {"begin": 49680, "end": 49684, "target": "#b20", "idx": 94}, {"begin": 50669, "end": 50673, "target": "#b20", "idx": 95}, {"begin": 50921, "end": 50925, "target": "#b21", "idx": 96}, {"begin": 52669, "end": 52673, "target": "#b17", "idx": 97}, {"begin": 53296, "end": 53300, "target": "#b18", "idx": 98}, {"begin": 53301, "end": 53304, "target": "#b19", "idx": 99}, {"begin": 54641, "end": 54645, "target": "#b20", "idx": 100}], "ReferenceString": [{"begin": 54826, "end": 54978, "id": "b0", "idx": 0}, {"begin": 54980, "end": 55190, "id": "b1", "idx": 1}, {"begin": 55194, "end": 55393, "id": "b2", "idx": 2}, {"begin": 55397, "end": 55577, "id": "b3", "idx": 3}, {"begin": 55581, "end": 55758, "id": "b4", "idx": 4}, {"begin": 55762, "end": 56062, "id": "b5", "idx": 5}, {"begin": 56066, "end": 56201, "id": "b6", "idx": 6}, {"begin": 56205, "end": 56315, "id": "b7", "idx": 7}, {"begin": 56319, "end": 56400, "id": "b8", "idx": 8}, {"begin": 56404, "end": 56471, "id": "b9", "idx": 9}, {"begin": 56475, "end": 56585, "id": "b10", "idx": 10}, {"begin": 56589, "end": 56684, "id": "b11", "idx": 11}, {"begin": 56688, "end": 56803, "id": "b12", "idx": 12}, {"begin": 56807, "end": 56980, "id": "b13", "idx": 13}, {"begin": 56984, "end": 57184, "id": "b14", "idx": 14}, {"begin": 57188, "end": 57358, "id": "b15", "idx": 15}, {"begin": 57362, "end": 57561, "id": "b16", "idx": 16}, {"begin": 57565, "end": 57708, "id": "b17", "idx": 17}, {"begin": 57712, "end": 57865, "id": "b18", "idx": 18}, {"begin": 57869, "end": 58048, "id": "b19", "idx": 19}, {"begin": 58052, "end": 58185, "id": "b20", "idx": 20}, {"begin": 58189, "end": 58345, "id": "b21", "idx": 21}, {"begin": 58349, "end": 58504, "id": "b22", "idx": 22}, {"begin": 58508, "end": 58650, "id": "b23", "idx": 23}, {"begin": 58654, "end": 58825, "id": "b24", "idx": 24}, {"begin": 58829, "end": 58995, "id": "b25", "idx": 25}, {"begin": 58999, "end": 59183, "id": "b26", "idx": 26}, {"begin": 59187, "end": 59250, "id": "b27", "idx": 27}, {"begin": 59254, "end": 59519, "id": "b28", "idx": 28}, {"begin": 59523, "end": 59609, "id": "b29", "idx": 29}, {"begin": 59613, "end": 59794, "id": "b30", "idx": 30}, {"begin": 59798, "end": 59937, "id": "b31", "idx": 31}, {"begin": 59941, "end": 60078, "id": "b32", "idx": 32}, {"begin": 60082, "end": 60188, "id": "b33", "idx": 33}, {"begin": 60192, "end": 60305, "id": "b34", "idx": 34}, {"begin": 60309, "end": 60439, "id": "b35", "idx": 35}, {"begin": 60443, "end": 60566, "id": "b36", "idx": 36}, {"begin": 60570, "end": 60746, "id": "b37", "idx": 37}, {"begin": 60750, "end": 60932, "id": "b38", "idx": 38}, {"begin": 60936, "end": 61064, "id": "b39", "idx": 39}, {"begin": 61068, "end": 61156, "id": "b40", "idx": 40}, {"begin": 61160, "end": 61258, "id": "b41", "idx": 41}, {"begin": 61262, "end": 61440, "id": "b42", "idx": 42}, {"begin": 61444, "end": 61595, "id": "b43", "idx": 43}, {"begin": 61599, "end": 61831, "id": "b44", "idx": 44}, {"begin": 61835, "end": 62025, "id": "b45", "idx": 45}, {"begin": 62029, "end": 62193, "id": "b46", "idx": 46}, {"begin": 62197, "end": 62346, "id": "b47", "idx": 47}], "Sentence": [{"begin": 120, "end": 240, "idx": 0}, {"begin": 241, "end": 379, "idx": 1}, {"begin": 380, "end": 473, "idx": 2}, {"begin": 500, "end": 618, "idx": 3}, {"begin": 619, "end": 748, "idx": 4}, {"begin": 749, "end": 792, "idx": 5}, {"begin": 793, "end": 805, "idx": 6}, {"begin": 806, "end": 1070, "idx": 7}, {"begin": 1071, "end": 1202, "idx": 8}, {"begin": 1203, "end": 1405, "idx": 9}, {"begin": 1406, "end": 1562, "idx": 10}, {"begin": 1563, "end": 1706, "idx": 11}, {"begin": 1707, "end": 1839, "idx": 12}, {"begin": 1840, "end": 1955, "idx": 13}, {"begin": 1956, "end": 2125, "idx": 14}, {"begin": 2126, "end": 2245, "idx": 15}, {"begin": 2246, "end": 2361, "idx": 16}, {"begin": 2362, "end": 2604, "idx": 17}, {"begin": 2605, "end": 2768, "idx": 18}, {"begin": 2769, "end": 2897, "idx": 19}, {"begin": 2898, "end": 3070, "idx": 20}, {"begin": 3071, "end": 3202, "idx": 21}, {"begin": 3203, "end": 3353, "idx": 22}, {"begin": 3354, "end": 3501, "idx": 23}, {"begin": 3502, "end": 3583, "idx": 24}, {"begin": 3584, "end": 3665, "idx": 25}, {"begin": 3666, "end": 3889, "idx": 26}, {"begin": 3890, "end": 3995, "idx": 27}, {"begin": 3996, "end": 4111, "idx": 28}, {"begin": 4112, "end": 4380, "idx": 29}, {"begin": 4402, "end": 4453, "idx": 30}, {"begin": 4454, "end": 4565, "idx": 31}, {"begin": 4566, "end": 4674, "idx": 32}, {"begin": 4675, "end": 4857, "idx": 33}, {"begin": 4858, "end": 4980, "idx": 34}, {"begin": 4981, "end": 5033, "idx": 35}, {"begin": 5078, "end": 5220, "idx": 36}, {"begin": 5221, "end": 5380, "idx": 37}, {"begin": 5381, "end": 5413, "idx": 38}, {"begin": 5414, "end": 5502, "idx": 39}, {"begin": 5503, "end": 5583, "idx": 40}, {"begin": 5584, "end": 5691, "idx": 41}, {"begin": 5692, "end": 5854, "idx": 42}, {"begin": 5855, "end": 6002, "idx": 43}, {"begin": 6044, "end": 6148, "idx": 44}, {"begin": 6172, "end": 6207, "idx": 45}, {"begin": 6208, "end": 6261, "idx": 46}, {"begin": 6262, "end": 6341, "idx": 47}, {"begin": 6342, "end": 6507, "idx": 48}, {"begin": 6508, "end": 6617, "idx": 49}, {"begin": 6667, "end": 6849, "idx": 50}, {"begin": 6850, "end": 7062, "idx": 51}, {"begin": 7263, "end": 7440, "idx": 52}, {"begin": 7441, "end": 7484, "idx": 53}, {"begin": 7485, "end": 7585, "idx": 54}, {"begin": 7640, "end": 7808, "idx": 55}, {"begin": 7809, "end": 7838, "idx": 56}, {"begin": 7882, "end": 7987, "idx": 57}, {"begin": 7988, "end": 8286, "idx": 58}, {"begin": 8287, "end": 8449, "idx": 59}, {"begin": 8501, "end": 8641, "idx": 60}, {"begin": 8642, "end": 8775, "idx": 61}, {"begin": 8776, "end": 8935, "idx": 62}, {"begin": 8936, "end": 9034, "idx": 63}, {"begin": 9035, "end": 9248, "idx": 64}, {"begin": 9249, "end": 9351, "idx": 65}, {"begin": 9352, "end": 9457, "idx": 66}, {"begin": 9458, "end": 9503, "idx": 67}, {"begin": 9504, "end": 9689, "idx": 68}, {"begin": 9690, "end": 9859, "idx": 69}, {"begin": 9860, "end": 9908, "idx": 70}, {"begin": 9909, "end": 10116, "idx": 71}, {"begin": 10117, "end": 10349, "idx": 72}, {"begin": 10350, "end": 10478, "idx": 73}, {"begin": 10552, "end": 10562, "idx": 74}, {"begin": 10563, "end": 10671, "idx": 75}, {"begin": 10672, "end": 10813, "idx": 76}, {"begin": 10814, "end": 10945, "idx": 77}, {"begin": 11039, "end": 11182, "idx": 78}, {"begin": 11183, "end": 11421, "idx": 79}, {"begin": 11422, "end": 11505, "idx": 80}, {"begin": 11557, "end": 11580, "idx": 81}, {"begin": 11656, "end": 11753, "idx": 82}, {"begin": 11754, "end": 12000, "idx": 83}, {"begin": 12001, "end": 12140, "idx": 84}, {"begin": 12141, "end": 12253, "idx": 85}, {"begin": 12254, "end": 12395, "idx": 86}, {"begin": 12396, "end": 12669, "idx": 87}, {"begin": 12698, "end": 12762, "idx": 88}, {"begin": 12812, "end": 12894, "idx": 89}, {"begin": 12895, "end": 12983, "idx": 90}, {"begin": 12984, "end": 13083, "idx": 91}, {"begin": 13470, "end": 13676, "idx": 92}, {"begin": 13677, "end": 13879, "idx": 93}, {"begin": 13880, "end": 13970, "idx": 94}, {"begin": 13971, "end": 14027, "idx": 95}, {"begin": 14078, "end": 14197, "idx": 96}, {"begin": 14198, "end": 14281, "idx": 97}, {"begin": 14282, "end": 14288, "idx": 98}, {"begin": 14289, "end": 14438, "idx": 99}, {"begin": 14439, "end": 14541, "idx": 100}, {"begin": 14648, "end": 14810, "idx": 101}, {"begin": 14811, "end": 14950, "idx": 102}, {"begin": 14951, "end": 15181, "idx": 103}, {"begin": 15182, "end": 15230, "idx": 104}, {"begin": 15231, "end": 15452, "idx": 105}, {"begin": 15649, "end": 15729, "idx": 106}, {"begin": 15836, "end": 15956, "idx": 107}, {"begin": 16030, "end": 16074, "idx": 108}, {"begin": 16174, "end": 16299, "idx": 109}, {"begin": 16308, "end": 16511, "idx": 110}, {"begin": 16512, "end": 16516, "idx": 111}, {"begin": 16746, "end": 16813, "idx": 112}, {"begin": 16814, "end": 16888, "idx": 113}, {"begin": 16924, "end": 16931, "idx": 114}, {"begin": 16976, "end": 17075, "idx": 115}, {"begin": 17076, "end": 17158, "idx": 116}, {"begin": 17279, "end": 17465, "idx": 117}, {"begin": 17466, "end": 17602, "idx": 118}, {"begin": 17781, "end": 17883, "idx": 119}, {"begin": 17884, "end": 17986, "idx": 120}, {"begin": 17987, "end": 18069, "idx": 121}, {"begin": 18096, "end": 18236, "idx": 122}, {"begin": 18237, "end": 18296, "idx": 123}, {"begin": 18297, "end": 18453, "idx": 124}, {"begin": 18454, "end": 18537, "idx": 125}, {"begin": 18538, "end": 18694, "idx": 126}, {"begin": 18695, "end": 18807, "idx": 127}, {"begin": 18834, "end": 19097, "idx": 128}, {"begin": 19098, "end": 19192, "idx": 129}, {"begin": 19193, "end": 19410, "idx": 130}, {"begin": 19411, "end": 19523, "idx": 131}, {"begin": 19524, "end": 19560, "idx": 132}, {"begin": 19561, "end": 19731, "idx": 133}, {"begin": 19732, "end": 19874, "idx": 134}, {"begin": 19906, "end": 20077, "idx": 135}, {"begin": 20078, "end": 20163, "idx": 136}, {"begin": 20164, "end": 20286, "idx": 137}, {"begin": 20287, "end": 20371, "idx": 138}, {"begin": 20372, "end": 20434, "idx": 139}, {"begin": 20435, "end": 20538, "idx": 140}, {"begin": 20599, "end": 20709, "idx": 141}, {"begin": 20754, "end": 21015, "idx": 142}, {"begin": 21016, "end": 21251, "idx": 143}, {"begin": 21252, "end": 21505, "idx": 144}, {"begin": 21506, "end": 21811, "idx": 145}, {"begin": 21812, "end": 22028, "idx": 146}, {"begin": 22029, "end": 22157, "idx": 147}, {"begin": 22158, "end": 22398, "idx": 148}, {"begin": 22399, "end": 22565, "idx": 149}, {"begin": 22566, "end": 22621, "idx": 150}, {"begin": 22622, "end": 22670, "idx": 151}, {"begin": 22723, "end": 22851, "idx": 152}, {"begin": 22852, "end": 23054, "idx": 153}, {"begin": 23055, "end": 23192, "idx": 154}, {"begin": 23193, "end": 23375, "idx": 155}, {"begin": 23376, "end": 23523, "idx": 156}, {"begin": 23524, "end": 23622, "idx": 157}, {"begin": 23664, "end": 23830, "idx": 158}, {"begin": 23831, "end": 24003, "idx": 159}, {"begin": 24004, "end": 24176, "idx": 160}, {"begin": 24177, "end": 24304, "idx": 161}, {"begin": 24305, "end": 24373, "idx": 162}, {"begin": 24374, "end": 24428, "idx": 163}, {"begin": 24429, "end": 24775, "idx": 164}, {"begin": 24776, "end": 25006, "idx": 165}, {"begin": 25007, "end": 25246, "idx": 166}, {"begin": 25247, "end": 25494, "idx": 167}, {"begin": 25495, "end": 25775, "idx": 168}, {"begin": 25776, "end": 25964, "idx": 169}, {"begin": 25965, "end": 26155, "idx": 170}, {"begin": 26156, "end": 26382, "idx": 171}, {"begin": 26383, "end": 26624, "idx": 172}, {"begin": 26625, "end": 26755, "idx": 173}, {"begin": 26756, "end": 26966, "idx": 174}, {"begin": 26967, "end": 27184, "idx": 175}, {"begin": 27185, "end": 27350, "idx": 176}, {"begin": 27351, "end": 27528, "idx": 177}, {"begin": 27529, "end": 27662, "idx": 178}, {"begin": 27663, "end": 27776, "idx": 179}, {"begin": 27777, "end": 27972, "idx": 180}, {"begin": 27973, "end": 28123, "idx": 181}, {"begin": 28124, "end": 28504, "idx": 182}, {"begin": 28505, "end": 28795, "idx": 183}, {"begin": 28796, "end": 28818, "idx": 184}, {"begin": 28819, "end": 28876, "idx": 185}, {"begin": 28877, "end": 29071, "idx": 186}, {"begin": 29072, "end": 29250, "idx": 187}, {"begin": 29251, "end": 29373, "idx": 188}, {"begin": 29374, "end": 29524, "idx": 189}, {"begin": 29525, "end": 29714, "idx": 190}, {"begin": 29715, "end": 29722, "idx": 191}, {"begin": 29723, "end": 29885, "idx": 192}, {"begin": 29886, "end": 29974, "idx": 193}, {"begin": 30002, "end": 30074, "idx": 194}, {"begin": 30075, "end": 30236, "idx": 195}, {"begin": 30237, "end": 30361, "idx": 196}, {"begin": 30362, "end": 30543, "idx": 197}, {"begin": 30544, "end": 30706, "idx": 198}, {"begin": 30772, "end": 30875, "idx": 199}, {"begin": 30876, "end": 31101, "idx": 200}, {"begin": 31102, "end": 31109, "idx": 201}, {"begin": 31110, "end": 31273, "idx": 202}, {"begin": 31274, "end": 31419, "idx": 203}, {"begin": 31420, "end": 31455, "idx": 204}, {"begin": 31456, "end": 31777, "idx": 205}, {"begin": 31778, "end": 32045, "idx": 206}, {"begin": 32421, "end": 32504, "idx": 207}, {"begin": 32505, "end": 32630, "idx": 208}, {"begin": 32631, "end": 32769, "idx": 209}, {"begin": 32770, "end": 32908, "idx": 210}, {"begin": 32909, "end": 33064, "idx": 211}, {"begin": 33065, "end": 33277, "idx": 212}, {"begin": 33278, "end": 33460, "idx": 213}, {"begin": 33461, "end": 33606, "idx": 214}, {"begin": 33626, "end": 33729, "idx": 215}, {"begin": 33730, "end": 33790, "idx": 216}, {"begin": 33791, "end": 33959, "idx": 217}, {"begin": 33960, "end": 33974, "idx": 218}, {"begin": 33975, "end": 34129, "idx": 219}, {"begin": 34130, "end": 34315, "idx": 220}, {"begin": 34356, "end": 34413, "idx": 221}, {"begin": 34414, "end": 34571, "idx": 222}, {"begin": 34572, "end": 34602, "idx": 223}, {"begin": 34603, "end": 34657, "idx": 224}, {"begin": 34736, "end": 34830, "idx": 225}, {"begin": 34831, "end": 34915, "idx": 226}, {"begin": 34916, "end": 34984, "idx": 227}, {"begin": 34999, "end": 35098, "idx": 228}, {"begin": 35099, "end": 35105, "idx": 229}, {"begin": 35106, "end": 35243, "idx": 230}, {"begin": 35244, "end": 35707, "idx": 231}, {"begin": 35708, "end": 35898, "idx": 232}, {"begin": 35899, "end": 36043, "idx": 233}, {"begin": 36044, "end": 36399, "idx": 234}, {"begin": 36400, "end": 36592, "idx": 235}, {"begin": 36593, "end": 36637, "idx": 236}, {"begin": 36690, "end": 36940, "idx": 237}, {"begin": 36941, "end": 37019, "idx": 238}, {"begin": 37068, "end": 37091, "idx": 239}, {"begin": 37110, "end": 37192, "idx": 240}, {"begin": 37312, "end": 37483, "idx": 241}, {"begin": 37484, "end": 37613, "idx": 242}, {"begin": 37614, "end": 37656, "idx": 243}, {"begin": 37671, "end": 37751, "idx": 244}, {"begin": 37838, "end": 37844, "idx": 245}, {"begin": 37845, "end": 38079, "idx": 246}, {"begin": 38080, "end": 38289, "idx": 247}, {"begin": 38290, "end": 38496, "idx": 248}, {"begin": 38497, "end": 38743, "idx": 249}, {"begin": 38744, "end": 38749, "idx": 250}, {"begin": 38880, "end": 39019, "idx": 251}, {"begin": 39020, "end": 39125, "idx": 252}, {"begin": 39126, "end": 39215, "idx": 253}, {"begin": 39278, "end": 39339, "idx": 254}, {"begin": 39340, "end": 39446, "idx": 255}, {"begin": 39486, "end": 39585, "idx": 256}, {"begin": 39617, "end": 39636, "idx": 257}, {"begin": 39715, "end": 39821, "idx": 258}, {"begin": 39822, "end": 39958, "idx": 259}, {"begin": 39959, "end": 40144, "idx": 260}, {"begin": 40181, "end": 40242, "idx": 261}, {"begin": 40243, "end": 40537, "idx": 262}, {"begin": 40538, "end": 40602, "idx": 263}, {"begin": 40617, "end": 40705, "idx": 264}, {"begin": 40775, "end": 40864, "idx": 265}, {"begin": 40865, "end": 40939, "idx": 266}, {"begin": 40940, "end": 41159, "idx": 267}, {"begin": 41160, "end": 41299, "idx": 268}, {"begin": 41300, "end": 41529, "idx": 269}, {"begin": 41567, "end": 41785, "idx": 270}, {"begin": 41786, "end": 41916, "idx": 271}, {"begin": 41917, "end": 42046, "idx": 272}, {"begin": 42047, "end": 42110, "idx": 273}, {"begin": 42267, "end": 42337, "idx": 274}, {"begin": 42338, "end": 42373, "idx": 275}, {"begin": 42439, "end": 42443, "idx": 276}, {"begin": 42624, "end": 42718, "idx": 277}, {"begin": 42719, "end": 42896, "idx": 278}, {"begin": 42897, "end": 42922, "idx": 279}, {"begin": 42930, "end": 43030, "idx": 280}, {"begin": 43112, "end": 43127, "idx": 281}, {"begin": 43128, "end": 43212, "idx": 282}, {"begin": 43213, "end": 43416, "idx": 283}, {"begin": 43781, "end": 43825, "idx": 284}, {"begin": 43858, "end": 44115, "idx": 285}, {"begin": 44116, "end": 44312, "idx": 286}, {"begin": 44313, "end": 44492, "idx": 287}, {"begin": 44493, "end": 44657, "idx": 288}, {"begin": 44658, "end": 44874, "idx": 289}, {"begin": 44875, "end": 44924, "idx": 290}, {"begin": 44925, "end": 44987, "idx": 291}, {"begin": 45023, "end": 45084, "idx": 292}, {"begin": 45205, "end": 45313, "idx": 293}, {"begin": 45314, "end": 45494, "idx": 294}, {"begin": 45495, "end": 45595, "idx": 295}, {"begin": 45596, "end": 45654, "idx": 296}, {"begin": 45655, "end": 45777, "idx": 297}, {"begin": 45778, "end": 45878, "idx": 298}, {"begin": 45914, "end": 45916, "idx": 299}, {"begin": 45917, "end": 45985, "idx": 300}, {"begin": 45986, "end": 46239, "idx": 301}, {"begin": 46240, "end": 46461, "idx": 302}, {"begin": 46462, "end": 46606, "idx": 303}, {"begin": 46607, "end": 46868, "idx": 304}, {"begin": 46869, "end": 47181, "idx": 305}, {"begin": 47229, "end": 47505, "idx": 306}, {"begin": 47506, "end": 47758, "idx": 307}, {"begin": 47759, "end": 47950, "idx": 308}, {"begin": 47951, "end": 48079, "idx": 309}, {"begin": 48080, "end": 48269, "idx": 310}, {"begin": 48270, "end": 48449, "idx": 311}, {"begin": 48450, "end": 48520, "idx": 312}, {"begin": 48521, "end": 48727, "idx": 313}, {"begin": 48728, "end": 48812, "idx": 314}, {"begin": 48813, "end": 48944, "idx": 315}, {"begin": 48945, "end": 49067, "idx": 316}, {"begin": 49068, "end": 49289, "idx": 317}, {"begin": 49290, "end": 49432, "idx": 318}, {"begin": 49433, "end": 49636, "idx": 319}, {"begin": 49637, "end": 49727, "idx": 320}, {"begin": 49728, "end": 49922, "idx": 321}, {"begin": 49923, "end": 50141, "idx": 322}, {"begin": 50142, "end": 50215, "idx": 323}, {"begin": 50216, "end": 50340, "idx": 324}, {"begin": 50341, "end": 50573, "idx": 325}, {"begin": 50574, "end": 50594, "idx": 326}, {"begin": 50595, "end": 50602, "idx": 327}, {"begin": 50603, "end": 50676, "idx": 328}, {"begin": 50677, "end": 50783, "idx": 329}, {"begin": 50784, "end": 50929, "idx": 330}, {"begin": 50930, "end": 51144, "idx": 331}, {"begin": 51145, "end": 51235, "idx": 332}, {"begin": 51236, "end": 51612, "idx": 333}, {"begin": 51613, "end": 51994, "idx": 334}, {"begin": 51995, "end": 52485, "idx": 335}, {"begin": 52486, "end": 52739, "idx": 336}, {"begin": 52740, "end": 52835, "idx": 337}, {"begin": 52836, "end": 53081, "idx": 338}, {"begin": 53082, "end": 53188, "idx": 339}, {"begin": 53212, "end": 53356, "idx": 340}, {"begin": 53357, "end": 53532, "idx": 341}, {"begin": 53533, "end": 53604, "idx": 342}, {"begin": 53605, "end": 53748, "idx": 343}, {"begin": 53749, "end": 53958, "idx": 344}, {"begin": 53959, "end": 54088, "idx": 345}, {"begin": 54089, "end": 54256, "idx": 346}, {"begin": 54257, "end": 54405, "idx": 347}, {"begin": 54406, "end": 54646, "idx": 348}, {"begin": 54647, "end": 54728, "idx": 349}], "ReferenceToFigure": [{"begin": 6273, "end": 6274, "target": "#fig_0", "idx": 0}, {"begin": 27372, "end": 27379, "target": "#fig_4", "idx": 1}, {"begin": 27536, "end": 27537, "target": "#fig_3", "idx": 2}, {"begin": 27617, "end": 27618, "target": "#fig_4", "idx": 3}, {"begin": 28254, "end": 28255, "target": "#fig_3", "idx": 4}, {"begin": 28576, "end": 28577, "target": "#fig_4", "idx": 5}, {"begin": 28674, "end": 28675, "target": "#fig_4", "idx": 6}], "Abstract": [{"begin": 110, "end": 473, "idx": 0}], "SectionFootnote": [{"begin": 54730, "end": 54809, "idx": 0}], "Footnote": [{"begin": 54741, "end": 54809, "id": "foot_0", "n": "2", "idx": 0}]}}