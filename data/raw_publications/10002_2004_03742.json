{"text": "Towards Evaluating the Robustness of Chinese BERT Classifiers\n\nAbstract:\nRecent advances in large-scale language representation models such as BERT have improved the state-of-the-art performances in many NLP tasks. Meanwhile, character-level Chinese NLP models, including BERT for Chinese, have also demonstrated that they can outperform the existing models. In this paper, we show that, however, such BERT-based models are vulnerable under character-level adversarial attacks. We propose a novel Chinese char-level attack method against BERT-based classifiers. Essentially, we generate \"small\" perturbation on the character level in the embedding space and guide the character substitution procedure. Extensive experiments show that the classification accuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating less than 2 characters on average based on the proposed attack. Human evaluations also confirm that our generated Chinese adversarial examples barely affect human performance on these NLP tasks.\nCCS CONCEPTS\n\u2022 Information systems \u2192 Evaluation of retrieval results.\n\nMain:\n\n\n\n1 INTRODUCTION\nRecently, the impressive performance of BERT [4] has inspired many pre-trained large-scale language models [11, 22, 25], which have obtained state-of-the-art results over many downstream NLP tasks. Besides its dominant performance in English datasets, Tenney et al. [19] point out that BERT is also effective in ambiguous languages such as Chinese, whose granularity of words is less well defined than English [5], because BERT models can disambiguate information from high-level representation. Moreover, Li et al. [13] find that in the Chinese environment, using character-based models (e.g., BERT) is more suitable than word-based models, as the latter often suffer from data sparsity and out-of-vocabulary problems.\nHowever, are Chinese char-based models such as BERT robust under adversarial settings? To the best of our knowledge, we are the first to study this problem in the Chinese domain. While a large number of studies focus on generating adversarial examples in the continuous data domain (e.g. image and audio), generating adversarial text examples in the discrete domain is much more challenging. Current adversarial text generation work [1, 8, 9, 12] is mainly heuristic and not scalable to Chinese in that Chinese characters are intrinsically polysemous. Some char-level adversarial attacks  in the English context [6] are shown ineffective for Chinese charlevel attacks, as the sizes of candidate characters increase by two orders of magnitude and the computational costs surge, especially for BERT-based classifiers.\nIn this paper, we propose an efficient Chinese char-level adversarial attack approach AdvChar towards evaluating the robustness of BERT-based models. Our algorithm adds perturbation in the character embedding space guided by the gradient and controls the magnitude of perturbation as small as possible so that only a very few characters are replaced. We find out that existing char-level Chinese BERT classifiers are vulnerable under our adversarial attack. Model accuracy can drop from 91.8% to 0% by simply flipping one or two characters, as Figure 1 shows. Though these adversarial examples are generated in the whitebox settings, they can transfer to blackbox classifiers and significantly impair their performances.\nIn summary, our contributions lie on: 1) To the best of our knowledge, this is the first study to generate Chinese adversarial examples against the state-of-the-art BERT classifiers.\n2) We propose a novel and efficient Chinese char-level adversarial targeted and untargeted attack algorithm that is able to handle thousands of Chinese characters and perturb a very few of tokens.\nBoth quantitative and human evaluations demonstrate the effectiveness and validity of our attacks over BERT-based models on several large-scale Chinese datasets.\n\n2 RELATED WORKS\nIn contrast to a large amount of adversarial attacks in the continuous data domain [3, 7, 23], there are a few studies focusing on the discrete text domain. Jia and Liang [8] use handcrafted rulebased heuristic methods along with crowd-sourcing to generate valid adversarial sentences and fool the question answering models. Similarly, Niven and Kao [14], Thorne and Vlachos [20] use rule-based heuristics to attack specific tasks such as fact verification and argument reasoning comprehension. However, these methods cannot be applied to other NLP tasks [15, 16] and need human-crafted rules to guide the search. To automatically find the valid adversarial examples, Ebrahimi et al. [6] propose a whitebox gradient-based attack method to find character-level substitution. However, this method is not efficient when it comes to the Chinese language, where there are thousands of common characters compared with 26 English characters. Additionally, the following studies [1, 9, 17, 24] consider different strategies to perform wordlevel replacement while maintaining grammatical correctness and sematic similarity.\nIn addition, while pretrained language models such as BERT [4] and XLNet [22] have achieved state-of-the-art results in various NLP tasks, the robustness of these language models are challenged. Niven and Kao [14] point out that BERT is only learning the statistical cues, which can be broken by simply putting negation ahead. Jin et al. [9] also finds BERT is vulnerable under adversarial attacks.\n\n3 METHODOLOGY 3.1 Problem Formulation\nGiven the input x = [x 0 , x 1 , ..., x n ], where x 0 is a special token [CLS] prepended to every input and x i is a Chinese character, the BERT-based classification model f maps the input to the final logits z = f (x) \u2208 R C , where C is the number of class, and outputs a label = arg max f (x). Formally, the BERT-based classifier f first encodes the input[h 0 , h 1 , ..., h n ] = BERT([x 0 , x 1 , ..., x n )],\nand outputs the logits z via a fully connected layer based on the hidden state h 0 of [CLS], which represents the sentence embedding for classification tasks [4].\nDuring the adversarial evaluation, we investigate our attack algorithm efficiency by calculating the targeted attack success rate (TSR):TSR = 1 |D adv | x \u2032 \u2208D adv \u00bd[arg max f (x \u2032 ) \u2261 * ] (2)\nand untargeted attack success rate (USR):USR = 1 |D adv | x \u2032 \u2208D adv \u00bd[arg max f (x \u2032 ) ]\nwhere D adv is the adversarial dataset, * is the targeted false class, is the ground truth label, and \u00bd(\u2022) is the indicator function. for i = 1, 2, ..., n do 7:x \u2032 i \u2190 arg min([e \u2032 i ; e \u2032 i ; ..., e \u2032 i ] \u2212 M e ) 8:\nend for 9:\n// Phase II: Optimize over the e *L(e * k ) \u2190 ||e * k || p + c \u2022 (x \u2032 )\n11:e * k +1 \u2190 e * k \u2212 \u03b1 \u2207L(e * k ) 12: end for 13: return x \u2032\n\n3.2 Algorithm\nThe whole pipeline is shown in Algorithm 1.\nCharacter Substitution Procedure. Due to the discrete nature of text, it is hard to directly utilize the gradient to guide character substitution in the character space. However, in BERT, each discrete characterx i \u2208 R |V | (one-hot\n. Therefore, we can search the perturbation in the embedding space and map the perturbed character embedding back to characters. Suppose we already have an optimal perturbation e * in the embedding space that can achieve the attack goal and is the minimal perturbation. We can choose the perturbed character x \u2032 i as the semantically closest character to the perturbed embedding e\u2032 i e \u2032 i = e i + e * i ,\nx\u2032 i = arg min([e \u2032 i ; e \u2032 i ; ..., e \u2032 i ] \u2212 M e ).\nIf we control the perturbation e * to be small enough, most characters will remain the same and a very few characters is perturbed to its semantic close neighbors. In this way, the adversarial examples look still valid to the human but can fool the machines.\nOptimization-based Search. We use the neural network to search for the optimal perturbation variable e * . We freeze all the parameters of the BERT-based classifier f and optimize the only variable e * . Following Carlini and Wagner [2], we define the loss function asL(e * ) = ||e * || p + c \u2022 (x \u2032 ),\nwhere the first term controls the magnitude of perturbation, while (\u2022) is the attack objective function depending on the attack scenario. c weighs the attack goal against the attack cost. In the targeted attack scenario, we define (\u2022) as(x \u2032 ) = max[max{ f (x \u2032 ) i : i t } \u2212 f (x \u2032 ) t , \u2212\u03ba],\nwhere t is the targeted false class and f (x \u2032 ) i is the i-th class logit.\nA larger \u03ba encourages the classifier output targeted false class with higher confidence.\nIn the untargeted attack scenario, (\u2022) becomes(x \u2032 ) = max[f (x \u2032 ) t \u2212 max{ f (x \u2032 ) i : i t }, \u2212\u03ba],\nwhere t is the ground truth class.\n\n4 EXPERIMENTAL RESULTS\nWe conduct experiments on two Chinese classification datasets. We first perform the adversarial evaluation in the whitebox settings and validate the effectiveness of our proposed attack. We also explore the transferability of these adversarial examples. The following human evaluation confirms that our generated adversarial examples barely affect human performances.\n\n4.1 Datasets\nTHUCTC [18] is a public Chinese news classification dataset. It consists of more than 740k news articles between 2005 and 2011 extracted from Sina News. These articles are classified into 14 categories, including education, technology, society and politics. To speed up the evaluation process, we use the news titles for the classification. We evenly sampled articles from all classes. We use 585,390 articles as the training set, 250,682 articles as the development set, and another 1,000 articles as the testing set to perform the adversarial evaluation. Wechat Finance Dataset. This dataset is a private dataset from the Wechat team, who collect 13,051 subscription accounts in the finance domain. Based on the account description, they use crowdsourcing to classify the account into 11 sub-classes, including insurance, banks, credit cards and funds. Each account description has 94.18 Chinese characters on average. We split the dataset into the training set (10,000 descriptions), the validation set (1,163 descriptions) and the rest as the testing set (1,888 descriptions).\n\n4.2 Adversarial Evaluation\nBaseline. As there are no existing efficient Chinese character-level adversarial approaches, we propose a simple attack strategy as our baseline. We first cluster the character embedding by K-means and generate 1,000 embedding clusters. During attack, we randomly choose two or three characters and replace each of them with a random character belong to another random cluster. Implementation Details. We set the max optimization steps m to 100 and use \u2113 2 norm in the loss function (equation 7) that is iteratively optimized via Adam [10]. We use the pretrained BERT-base model and fine-tune BERT on each dataset independently with a batch size of 64, learning rate of 2e-5 and early stopping. We experiment with different attack strategies, e.g., setting the targeted false class as a specific class or the numerically next class to the ground truth. In this paper, we choose the targeted attack class as \"entertainment news\" for THUCTC dataset and \"Fund Account\" for Wechat Finance Dataset (when the ground truth label is the targeted class, we switch the target to another random class), which respectively achieves the highest targeted attack success rate in a set of ablation studies . More detailed settings of BERT-based classifiers and adversarial analysis on adversarial attack success rates are discussed in Appendix A.1 and A.2.\nResults. We perform our char-level adversarial attack on BERTbased classifiers for two datasets in both targeted and untargeted attack scenarios. The attack results are shown in Table 2.\nWe can see the untargeted attack can always achieve 100% attack success rate on both datasets, making the model performance drop to 0% by manipulating merely less than two tokens on average on the Chinese News Dataset. In the targeted attack scenario, we can always make BERT output our expected false class on the Wechat dataset, and achieve around 95% targeted attack success rate on the THUCTC dataset.\nSurprised by the fragility of Chinese BERT, we conduct several case studies on the generated adversarial text. We conjecture that Chinese BERT classifiers tend to make predictions based on a certain set of characters (statistical cues) without understanding the sentences. Therefore, the adversarial attack can easily succeed by replacing such critical characters. For the topic prediction example in Table 1, \"Yu\" is a Chinese celebrity name and only appears in the Entertainment News in the training set. Therefore, the BERT classifier takes \"Yu\" as a strong signal to classify the news as the Entertainment News. Similarly, another wrong account prediction in Table 1 is because term \"qi\" is a frequent financial products (petroleum and gas) used in other financial management accounts.\nWe also find that increasing the constants c and \u03ba can improve the attack success rate at the cost of more perturbed characters. Additionally, because the Wechat dataset has longer text than the THUCTC dataset, it is expected that more characters are manipulated on the Wechat dataset in order to break the statistical cues.\nTransferability. The previous experiments are conducted in the whitebox settings, i.e., we have all the access to the model parameters and gradients, which is a strong assumption that does not often hold in the real world. Therefore, we want to further investigate whether our algorithm can still attack BERT classifiers in the blackbox settings, when we cannot access model parameters. In this setting, we can attack a blackbox BERT classifier of different parameters by using the adversarial text generated from a whitebox BERT trained by ourselves.\nThe transferability-based attack results are shown in Table 3. We find the adversarial text can still substantially affect the accuracy of blackbox models. In addition, the targeted attack success rate turns out to be stronger than the untargeted attack. Particularly, the targeted adversarial examples on the Wechat dataset can make the blackbox BERT classifier performance drop from 88.2% to 14.3%.\n\n4.3 Human Evaluation\nTo confirm that our generated adversarial examples are valid to human, we conduct the following human evaluation. We randomly sample 50 clean sentences and 50 adversarial sentences generated by AdvChar (untargeted c/\u03ba = 5/5) on the Wechat dataset. We give volunteers two labels: a ground truth label and a fake label. For the clean sentence, the fake label is a random label different from the groud truth. As for the adversarial data, the fake label is the model's wrong prediction. Both clean text and adversarial text are mixed together. Ten native Chinese student volunteers are asked to choose the correct label. These native Chinese students are not required to be equipped with professional finance knowledge, so their evaluation results could contain errors and may be not as accurate as the financial annotators employed by the Wechat team.\nThe evaluation results are shown in Table 4. We find that our adversarial text barely impacts the human perception, since the human performance on adversarial data is only 4% lower than the clean data, in contrast to the huge performance drop of BERT classifiers from 82% to 0%.\n\n5 CONCLUSION\nIn this paper, we propose a novel character-level adversarial attack method to probe the robustness of BERT-based Chinese classifiers. Our experiments show that existing character-level BERTbased models are not robust in both whitebox and blackbox settings. While we observe the impressive improvements using the pretrained language models, we expect our study can encourage further research into the robustness problems of current pretrained language understanding models. Attack Strategy. As we have achieved 100% attack success rate in the untargeted attack scenario, we now focus on the targeted attack scenario and see which factor contributes to the targeted attack success rate. It is straightfoward to think different targeted attack strategy will impact the targeted attack success rate, because maybe some classes look \"farther\" than semantic closer classes. So we tried two strategies on THUCTC dataset: 1) as used in the main paper, we set the targeted false class as \"entertainment news\". 2) we enumerate all the classes and set the targeted false class to be numerically the next class. The targeted attack success rate is shown in Table 7. We do find choosing different attack strategy will impact the attack success rate.\n\nFootnotes:\n\nReferences:\n\n- Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivas- tava, and Kai-Wei Chang. 2018. Generating Natural Language Adversarial Ex- amples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Bel- gium, 2890-2896. https://doi.org/10.18653/v1/D18-1316- Nicholas Carlini and David A. Wagner. 2016. Towards Evaluating the Robustness of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP) (2016), 39-57.\n\n- Nicholas Carlini and David A. Wagner. 2018. Audio Adversarial Examples: Tar- geted Attacks on Speech-to-Text. 2018 IEEE Security and Privacy Workshops (SPW) (2018), 1-7.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT.\n\n- Ruixue Ding, Pengjun Xie, Xiaoyan Zhang, Wei Lu, Linlin Li, and Luo Si. 2019. A Neural Multi-digraph Model for Chinese NER with Gazetteers. In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics. Association for Computational Linguistics, Florence, Italy, 1462-1467. https://doi.org/10.18653/v1/P19-1141\n\n- Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White- Box Adversarial Examples for Text Classification. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Melbourne, Australia, 31- 36. https://doi.org/10.18653/v1/P18-2006\n\n- Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Xiaodong Song. 2017. Robust Physical-World Attacks on Deep Learning Models.\n\n- Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Read- ing Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis- tics, Copenhagen, Denmark, 2021-2031. https://doi.org/10.18653/v1/D17-1215\n\n- Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment. ArXiv abs/1907.11932 (2019).\n\n- Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti- mization. CoRR abs/1412.6980 (2014).\n\n- Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. ArXiv abs/1909.11942 (2019).\n\n- Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. TextBugger: Gen- erating Adversarial Text Against Real-world Applications. ArXiv abs/1812.05271 (2018).\n\n- Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. 2019. Is Word Segmentation Necessary for Deep Learning of Chinese Rep- resentations?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 3242-3252. https://doi.org/10.18653/v1/P19-1314\n\n- Timothy Niven and Hung-Yu Kao. 2019. Probing Neural Network Comprehen- sion of Natural Language Arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 4658-4664. https://doi.org/10.18653/v1/P19-1459\n\n- Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, and Huan Sun. 2019. Reinforced Dy- namic Reasoning for Conversational Question Generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2114-2124.\n\n- Boyuan Pan, Yazheng Yang, Hao Li, Zhou Zhao, Yueting Zhuang, Deng Cai, and Xiaofei He. 2018. Macnet: Transferring knowledge from machine comprehen- sion to sequence-to-sequence models. In Advances in Neural Information Pro- cessing Systems. 6092-6102.\n\n- Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1085- 1097. https://doi.org/10.18653/v1/P19-1103\n\n- M Sun, J Li, Z Guo, Z Yu, Y Zheng, X Si, and Z Liu. 2016. THUCTC: an efficient Chinese text classifier. GitHub Repository, https://github. com/thunlp/THUCTC (2016, accessed 17 May 2017). Google Scholar (2016).\n\n- Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Clas- sical NLP Pipeline. In ACL.\n\n- James Thorne and Andreas Vlachos. 2019. Adversarial attacks against Fact Ex- traction and VERification. ArXiv abs/1903.05543 (2019).\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS.\n\n- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdi- nov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. ArXiv abs/1906.08237 (2019).\n\n- Zhuolin Yang, Bo Li, Pin-Yu Chen, and Dawn Xiaodong Song. 2018. Char- acterizing Audio Adversarial Examples Using Temporal Dependency. ArXiv abs/1809.10875 (2018).\n\n- Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. 2019. Gener- ating Fluent Adversarial Examples for Natural Languages. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguis- tics. Association for Computational Linguistics, Florence, Italy, 5564-5569. https://doi.org/10.18653/v1/P19-1559\n\n- Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced Language Representation with Informative Entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics. Association for Computational Linguistics, Florence, Italy, 1441-1451. https://doi.org/10.18653/v1/P19-1139\n\n", "annotations": {"ReferenceToTable": [{"begin": 11659, "end": 11660, "target": "#tab_1", "idx": 0}, {"begin": 12475, "end": 12476, "target": "#tab_0", "idx": 1}, {"begin": 12737, "end": 12738, "target": "#tab_0", "idx": 2}, {"begin": 13795, "end": 13796, "target": "#tab_3", "idx": 3}, {"begin": 15050, "end": 15051, "target": "#tab_4", "idx": 4}, {"begin": 16453, "end": 16454, "target": "#tab_6", "idx": 5}], "SectionMain": [{"begin": 1099, "end": 16538, "idx": 0}], "SectionReference": [{"begin": 16552, "end": 22541, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1099, "idx": 0}], "Div": [{"begin": 73, "end": 1021, "idx": 0}, {"begin": 1022, "end": 1091, "idx": 1}, {"begin": 1102, "end": 3915, "idx": 2}, {"begin": 3917, "end": 5446, "idx": 3}, {"begin": 5448, "end": 6708, "idx": 4}, {"begin": 6710, "end": 8618, "idx": 5}, {"begin": 8620, "end": 9010, "idx": 6}, {"begin": 9012, "end": 10105, "idx": 7}, {"begin": 10107, "end": 14135, "idx": 8}, {"begin": 14137, "end": 15286, "idx": 9}, {"begin": 15288, "end": 16538, "idx": 10}], "Head": [{"begin": 1022, "end": 1034, "idx": 0}, {"begin": 1102, "end": 1116, "n": "1", "idx": 1}, {"begin": 3917, "end": 3932, "n": "2", "idx": 2}, {"begin": 5448, "end": 5485, "n": "3", "idx": 3}, {"begin": 6710, "end": 6723, "n": "3.2", "idx": 4}, {"begin": 8620, "end": 8642, "n": "4", "idx": 5}, {"begin": 9012, "end": 9024, "n": "4.1", "idx": 6}, {"begin": 10107, "end": 10133, "n": "4.2", "idx": 7}, {"begin": 14137, "end": 14157, "n": "4.3", "idx": 8}, {"begin": 15288, "end": 15300, "n": "5", "idx": 9}], "Paragraph": [{"begin": 73, "end": 1021, "idx": 0}, {"begin": 1035, "end": 1091, "idx": 1}, {"begin": 1117, "end": 1836, "idx": 2}, {"begin": 1837, "end": 2652, "idx": 3}, {"begin": 2653, "end": 3373, "idx": 4}, {"begin": 3374, "end": 3556, "idx": 5}, {"begin": 3557, "end": 3753, "idx": 6}, {"begin": 3754, "end": 3915, "idx": 7}, {"begin": 3933, "end": 5047, "idx": 8}, {"begin": 5048, "end": 5446, "idx": 9}, {"begin": 5486, "end": 5844, "idx": 10}, {"begin": 5901, "end": 6063, "idx": 11}, {"begin": 6064, "end": 6200, "idx": 12}, {"begin": 6257, "end": 6298, "idx": 13}, {"begin": 6347, "end": 6507, "idx": 14}, {"begin": 6564, "end": 6574, "idx": 15}, {"begin": 6575, "end": 6609, "idx": 16}, {"begin": 6647, "end": 6650, "idx": 17}, {"begin": 6724, "end": 6767, "idx": 18}, {"begin": 6768, "end": 6979, "idx": 19}, {"begin": 7001, "end": 7381, "idx": 20}, {"begin": 7407, "end": 7408, "idx": 21}, {"begin": 7461, "end": 7719, "idx": 22}, {"begin": 7720, "end": 7988, "idx": 23}, {"begin": 8023, "end": 8260, "idx": 24}, {"begin": 8317, "end": 8392, "idx": 25}, {"begin": 8393, "end": 8481, "idx": 26}, {"begin": 8482, "end": 8528, "idx": 27}, {"begin": 8584, "end": 8618, "idx": 28}, {"begin": 8643, "end": 9010, "idx": 29}, {"begin": 9025, "end": 10105, "idx": 30}, {"begin": 10134, "end": 11474, "idx": 31}, {"begin": 11475, "end": 11661, "idx": 32}, {"begin": 11662, "end": 12067, "idx": 33}, {"begin": 12068, "end": 12857, "idx": 34}, {"begin": 12858, "end": 13182, "idx": 35}, {"begin": 13183, "end": 13734, "idx": 36}, {"begin": 13735, "end": 14135, "idx": 37}, {"begin": 14158, "end": 15007, "idx": 38}, {"begin": 15008, "end": 15286, "idx": 39}, {"begin": 15301, "end": 16538, "idx": 40}], "ReferenceToBib": [{"begin": 1162, "end": 1165, "target": "#b3", "idx": 0}, {"begin": 1224, "end": 1228, "target": "#b10", "idx": 1}, {"begin": 1229, "end": 1232, "target": "#b21", "idx": 2}, {"begin": 1233, "end": 1236, "target": "#b24", "idx": 3}, {"begin": 1383, "end": 1387, "target": "#b18", "idx": 4}, {"begin": 1527, "end": 1530, "target": "#b4", "idx": 5}, {"begin": 1633, "end": 1637, "target": "#b12", "idx": 6}, {"begin": 2270, "end": 2273, "target": "#b0", "idx": 7}, {"begin": 2274, "end": 2276, "target": "#b7", "idx": 8}, {"begin": 2277, "end": 2279, "target": "#b8", "idx": 9}, {"begin": 2280, "end": 2283, "target": "#b11", "idx": 10}, {"begin": 2449, "end": 2452, "target": "#b5", "idx": 11}, {"begin": 4016, "end": 4019, "target": "#b2", "idx": 12}, {"begin": 4020, "end": 4022, "target": "#b6", "idx": 13}, {"begin": 4023, "end": 4026, "target": "#b22", "idx": 14}, {"begin": 4104, "end": 4107, "target": "#b7", "idx": 15}, {"begin": 4283, "end": 4287, "target": "#b13", "idx": 16}, {"begin": 4308, "end": 4312, "target": "#b19", "idx": 17}, {"begin": 4488, "end": 4492, "target": "#b14", "idx": 18}, {"begin": 4493, "end": 4496, "target": "#b15", "idx": 19}, {"begin": 4617, "end": 4620, "target": "#b5", "idx": 20}, {"begin": 4904, "end": 4907, "target": "#b0", "idx": 21}, {"begin": 4908, "end": 4910, "target": "#b8", "idx": 22}, {"begin": 4911, "end": 4914, "target": "#b16", "idx": 23}, {"begin": 4915, "end": 4918, "target": "#b23", "idx": 24}, {"begin": 5107, "end": 5110, "target": "#b3", "idx": 25}, {"begin": 5121, "end": 5125, "target": "#b21", "idx": 26}, {"begin": 5257, "end": 5261, "target": "#b13", "idx": 27}, {"begin": 5386, "end": 5389, "target": "#b8", "idx": 28}, {"begin": 6059, "end": 6062, "target": "#b3", "idx": 29}, {"begin": 7953, "end": 7956, "target": "#b1", "idx": 30}, {"begin": 9032, "end": 9036, "target": "#b17", "idx": 31}, {"begin": 10669, "end": 10673, "target": "#b9", "idx": 32}], "Sentence": [{"begin": 73, "end": 214, "idx": 0}, {"begin": 215, "end": 358, "idx": 1}, {"begin": 359, "end": 477, "idx": 2}, {"begin": 478, "end": 561, "idx": 3}, {"begin": 562, "end": 701, "idx": 4}, {"begin": 702, "end": 890, "idx": 5}, {"begin": 891, "end": 1021, "idx": 6}, {"begin": 1035, "end": 1091, "idx": 7}, {"begin": 1117, "end": 1314, "idx": 8}, {"begin": 1315, "end": 1612, "idx": 9}, {"begin": 1613, "end": 1836, "idx": 10}, {"begin": 1837, "end": 1923, "idx": 11}, {"begin": 1924, "end": 2015, "idx": 12}, {"begin": 2016, "end": 2124, "idx": 13}, {"begin": 2125, "end": 2228, "idx": 14}, {"begin": 2229, "end": 2388, "idx": 15}, {"begin": 2389, "end": 2652, "idx": 16}, {"begin": 2653, "end": 2802, "idx": 17}, {"begin": 2803, "end": 3003, "idx": 18}, {"begin": 3004, "end": 3110, "idx": 19}, {"begin": 3111, "end": 3212, "idx": 20}, {"begin": 3213, "end": 3373, "idx": 21}, {"begin": 3374, "end": 3556, "idx": 22}, {"begin": 3557, "end": 3753, "idx": 23}, {"begin": 3754, "end": 3915, "idx": 24}, {"begin": 3933, "end": 4089, "idx": 25}, {"begin": 4090, "end": 4257, "idx": 26}, {"begin": 4258, "end": 4427, "idx": 27}, {"begin": 4428, "end": 4546, "idx": 28}, {"begin": 4547, "end": 4706, "idx": 29}, {"begin": 4707, "end": 4867, "idx": 30}, {"begin": 4868, "end": 5047, "idx": 31}, {"begin": 5048, "end": 5242, "idx": 32}, {"begin": 5243, "end": 5374, "idx": 33}, {"begin": 5375, "end": 5446, "idx": 34}, {"begin": 5486, "end": 5782, "idx": 35}, {"begin": 5783, "end": 5844, "idx": 36}, {"begin": 5901, "end": 6063, "idx": 37}, {"begin": 6064, "end": 6200, "idx": 38}, {"begin": 6257, "end": 6298, "idx": 39}, {"begin": 6347, "end": 6480, "idx": 40}, {"begin": 6481, "end": 6507, "idx": 41}, {"begin": 6564, "end": 6574, "idx": 42}, {"begin": 6575, "end": 6609, "idx": 43}, {"begin": 6647, "end": 6650, "idx": 44}, {"begin": 6724, "end": 6767, "idx": 45}, {"begin": 6768, "end": 6801, "idx": 46}, {"begin": 6802, "end": 6937, "idx": 47}, {"begin": 6938, "end": 6979, "idx": 48}, {"begin": 7001, "end": 7129, "idx": 49}, {"begin": 7130, "end": 7270, "idx": 50}, {"begin": 7271, "end": 7381, "idx": 51}, {"begin": 7407, "end": 7408, "idx": 52}, {"begin": 7461, "end": 7624, "idx": 53}, {"begin": 7625, "end": 7719, "idx": 54}, {"begin": 7720, "end": 7746, "idx": 55}, {"begin": 7747, "end": 7826, "idx": 56}, {"begin": 7827, "end": 7923, "idx": 57}, {"begin": 7924, "end": 7988, "idx": 58}, {"begin": 8023, "end": 8160, "idx": 59}, {"begin": 8161, "end": 8210, "idx": 60}, {"begin": 8211, "end": 8260, "idx": 61}, {"begin": 8317, "end": 8392, "idx": 62}, {"begin": 8393, "end": 8481, "idx": 63}, {"begin": 8482, "end": 8528, "idx": 64}, {"begin": 8584, "end": 8618, "idx": 65}, {"begin": 8643, "end": 8705, "idx": 66}, {"begin": 8706, "end": 8829, "idx": 67}, {"begin": 8830, "end": 8896, "idx": 68}, {"begin": 8897, "end": 9010, "idx": 69}, {"begin": 9025, "end": 9085, "idx": 70}, {"begin": 9086, "end": 9177, "idx": 71}, {"begin": 9178, "end": 9282, "idx": 72}, {"begin": 9283, "end": 9365, "idx": 73}, {"begin": 9366, "end": 9410, "idx": 74}, {"begin": 9411, "end": 9581, "idx": 75}, {"begin": 9582, "end": 9605, "idx": 76}, {"begin": 9606, "end": 9725, "idx": 77}, {"begin": 9726, "end": 9879, "idx": 78}, {"begin": 9880, "end": 9945, "idx": 79}, {"begin": 9946, "end": 10105, "idx": 80}, {"begin": 10134, "end": 10143, "idx": 81}, {"begin": 10144, "end": 10279, "idx": 82}, {"begin": 10280, "end": 10370, "idx": 83}, {"begin": 10371, "end": 10511, "idx": 84}, {"begin": 10512, "end": 10535, "idx": 85}, {"begin": 10536, "end": 10674, "idx": 86}, {"begin": 10675, "end": 10828, "idx": 87}, {"begin": 10829, "end": 10986, "idx": 88}, {"begin": 10987, "end": 11325, "idx": 89}, {"begin": 11326, "end": 11474, "idx": 90}, {"begin": 11475, "end": 11483, "idx": 91}, {"begin": 11484, "end": 11620, "idx": 92}, {"begin": 11621, "end": 11661, "idx": 93}, {"begin": 11662, "end": 11880, "idx": 94}, {"begin": 11881, "end": 12067, "idx": 95}, {"begin": 12068, "end": 12178, "idx": 96}, {"begin": 12179, "end": 12340, "idx": 97}, {"begin": 12341, "end": 12432, "idx": 98}, {"begin": 12433, "end": 12574, "idx": 99}, {"begin": 12575, "end": 12683, "idx": 100}, {"begin": 12684, "end": 12857, "idx": 101}, {"begin": 12858, "end": 12986, "idx": 102}, {"begin": 12987, "end": 13182, "idx": 103}, {"begin": 13183, "end": 13199, "idx": 104}, {"begin": 13200, "end": 13405, "idx": 105}, {"begin": 13406, "end": 13569, "idx": 106}, {"begin": 13570, "end": 13734, "idx": 107}, {"begin": 13735, "end": 13797, "idx": 108}, {"begin": 13798, "end": 13890, "idx": 109}, {"begin": 13891, "end": 13989, "idx": 110}, {"begin": 13990, "end": 14135, "idx": 111}, {"begin": 14158, "end": 14271, "idx": 112}, {"begin": 14272, "end": 14405, "idx": 113}, {"begin": 14406, "end": 14475, "idx": 114}, {"begin": 14476, "end": 14564, "idx": 115}, {"begin": 14565, "end": 14641, "idx": 116}, {"begin": 14642, "end": 14698, "idx": 117}, {"begin": 14699, "end": 14775, "idx": 118}, {"begin": 14776, "end": 15007, "idx": 119}, {"begin": 15008, "end": 15052, "idx": 120}, {"begin": 15053, "end": 15286, "idx": 121}, {"begin": 15301, "end": 15435, "idx": 122}, {"begin": 15436, "end": 15558, "idx": 123}, {"begin": 15559, "end": 15774, "idx": 124}, {"begin": 15775, "end": 15791, "idx": 125}, {"begin": 15792, "end": 15986, "idx": 126}, {"begin": 15987, "end": 16169, "idx": 127}, {"begin": 16170, "end": 16302, "idx": 128}, {"begin": 16303, "end": 16401, "idx": 129}, {"begin": 16402, "end": 16455, "idx": 130}, {"begin": 16456, "end": 16538, "idx": 131}], "ReferenceToFigure": [{"begin": 3204, "end": 3205, "idx": 0}], "Abstract": [{"begin": 63, "end": 1091, "idx": 0}], "SectionFootnote": [{"begin": 16540, "end": 16550, "idx": 0}], "ReferenceString": [{"begin": 16567, "end": 16926, "id": "b0", "idx": 0}, {"begin": 16928, "end": 17089, "id": "b1", "idx": 1}, {"begin": 17093, "end": 17262, "id": "b2", "idx": 2}, {"begin": 17266, "end": 17433, "id": "b3", "idx": 3}, {"begin": 17437, "end": 17780, "id": "b4", "idx": 4}, {"begin": 17784, "end": 18139, "id": "b5", "idx": 5}, {"begin": 18143, "end": 18342, "id": "b6", "idx": 6}, {"begin": 18346, "end": 18652, "id": "b7", "idx": 7}, {"begin": 18656, "end": 18836, "id": "b8", "idx": 8}, {"begin": 18840, "end": 18951, "id": "b9", "idx": 9}, {"begin": 18955, "end": 19162, "id": "b10", "idx": 10}, {"begin": 19166, "end": 19333, "id": "b11", "idx": 11}, {"begin": 19337, "end": 19701, "id": "b12", "idx": 12}, {"begin": 19705, "end": 20011, "id": "b13", "idx": 13}, {"begin": 20015, "end": 20248, "id": "b14", "idx": 14}, {"begin": 20252, "end": 20503, "id": "b15", "idx": 15}, {"begin": 20507, "end": 20856, "id": "b16", "idx": 16}, {"begin": 20860, "end": 21069, "id": "b17", "idx": 17}, {"begin": 21073, "end": 21178, "id": "b18", "idx": 18}, {"begin": 21182, "end": 21314, "id": "b19", "idx": 19}, {"begin": 21318, "end": 21485, "id": "b20", "idx": 20}, {"begin": 21489, "end": 21695, "id": "b21", "idx": 21}, {"begin": 21699, "end": 21862, "id": "b22", "idx": 22}, {"begin": 21866, "end": 22187, "id": "b23", "idx": 23}, {"begin": 22191, "end": 22539, "id": "b24", "idx": 24}]}}