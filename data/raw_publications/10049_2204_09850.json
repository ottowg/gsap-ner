{"text": "FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation\n\nAbstract:\nContrastive learning is widely used for recommendation model learning, where selecting representative and informative negative samples is critical. Existing methods usually focus on centralized data, where abundant and high-quality negative samples are easy to obtain. However, centralized user data storage and exploitation may lead to privacy risks and concerns, while decentralized user data on a single client can be too sparse and biased for accurate contrastive learning. In this paper, we propose a federated contrastive learning method named FedCL for privacy-preserving recommendation, which can exploit high-quality negative samples for effective model training with privacy well protected. We first infer user embeddings from local user data through the local model on each client, and then perturb them with local differential privacy (LDP) before sending them to a central server for hard negative sampling. Since individual user embedding contains heavy noise due to LDP, we propose to cluster user embeddings on the server to mitigate the influence of noise, and the cluster centroids are used to retrieve hard negative samples from the item pool. These hard negative samples are delivered to user clients and mixed with the observed negative samples from local data as well as in-batch negatives constructed from positive samples for federated model training. Extensive experiments on four benchmark datasets show FedCL can empower various recommendation methods in a privacy-preserving way.\n\nMain:\n\n\n\n1 Introduction\nContrastive learning is a widely-adopted technique in personalized recommendation [Zhou et al., 2020]. Choosing informative negative samples is usually a necessity in contrastive recommendation model learning [Robinson et al., 2020], and various strategies have been explored in existing works [Wang et al., 2021]. For example, Rendle et al. [2009] proposed to compare each positive sample with a randomly selected negative sample from the entire item set.  Wu et al. [2019] randomly selected several displayed but not clicked news within a news impression as negative samples to contrast them with the positive sample. Some recommendation methods further introduce hard negatives to improve the informativeness of training samples. For instance, Ding et al. [2019] proposed to use reinforcement learning to select hard negative samples from unclicked data. These methods are usually learned on centralized user data, where abundant and representative negative samples can be drawn from the global dataset. However, user data is usually highly privacysensitive, and centralized storage and mining of it may bring considerable privacy risks and user concerns. Some strict data protection regulations like GDPR 1 also restrict the collection and centralized storage of user data. Thus, these methods are not feasible in privacy-preserving recommendation without centralized data to construct training samples.\nRecent works study to use federated learning (FL) [McMahan et al., 2017] to learn recommendation models in a decentralized way, where the raw user data is not necessarily uploaded to a server [Chai et al., 2020]. For example, Ammad et al. [2019] proposed a federated collaborative filtering method that only uploads the item embedding gradients, where negative samples are random items chosen from the item pool. These methods usually assume that there are sufficient negative items on each local device for negative sampling and model training, which may not be satisfied in real-world scenarios. In fact, the local data on user devices can be rather sparse and biased, and even there is no negative sample stored on user devices because many online services do not locally record non-interacted items [Ning et al., 2021], which poses great challenges to contrastive recommendation model learning under privacy protection.\nIn this paper, we propose a federated contrastive recommendation model learning method named FedCL, which can exploit high-quality negative samples in a privacy-preserving way to empower model training. In our approach, the local model maintained on each client first infers a user embedding based on local user profiles. To protect user privacy, we perturb user embeddings via local differential privacy (LDP) before sending them to a central server for hard negative sampling. Since the perturbed user embeddings are quite noisy, we propose to cluster them on the server via the Ward clustering algorithm to reduce the influence of noise. The cluster centroids can represent users with different types of interests, and we use the cluster centroids instead of user embeddings to retrieve informative negatives from the item pool via a semihard negative sampling method. These negatives are delivered to user clients for federated model training by mixing them with the real negative samples stored by local clients (if available) and the local in-batch negatives inferred from positive samples. Extensive experiments on four benchmark recommendation datasets show that FedCL can effectively improve the performance of various recommendation methods and meanwhile protect user privacy.\nThe main contributions of this paper include:\n\u2022 We propose a federated contrastive learning method for privacy-preserving recommendation that can exploit informative negatives for model training. \u2022 We propose a federated hard negative sampling method that can accurately find hard negative samples for contrastive model training in a privacy-preserving way. \u2022 We design a negative mix-up method to incorporate various kinds of negative samples within a unified contrastive training framework. \u2022 We conduct extensive experiments to verify the effectiveness of our approach in empowering various recommendation methods in federated learning scenarios.\n2 Related Work\n\n2.1 Contrastive Learning in Information Retrieval\nContrastive learning is a core technique in many information retrieval (IR) tasks such as web search [Xiong et al., 2020] and personalized recommendation [Wei et al., 2021]. A core problem of contrastive IR model training is selecting proper negative samples to contrast with the positive ones [Robinson et al., 2020]. One popular way is to randomly sample negatives from the candidate set. For example, the Bayesian Personalized Ranking (BPR) [Rendle et al., 2009] method uses a randomly selected negative sample to compare with a positive sample. Many other works explore learning IR models by optimizing the gaps between the positive sample and a certain number of random negative samples [Shen et al., 2014; Wu et al., 2019]. Another widely used way is in-batch negative sampling. For example, Yi et al. [2019] proposed to use the positive items within the same batch as shared negatives for all queries in this batch.\n\n2.2 Federated Recommendation\nIn recent years, the application of federated learning in privacy-preserving recommendation has been extensively studied [Chai et al., 2020; Qi et al., 2020; Liang et al., 2021]. For example, Ammad et al. [2019] proposed to locally learn user and item embeddings on local user data, and upload item embedding gradients to a server for global model updating and delivery.  Lin et al. [2020] proposed to randomly sample some unrated items and assign them virtual ratings to avoid communicating the full item embedding table.  Qi et al. [2021] proposed a unified method for privacy-preserving news recall and ranking by modeling user interests with linear combinations of basis interest vectors. However, these methods usually assume that there are sufficient negative samples available on local user devices. Unfortunately, in many scenarios such as product recommendation and web search, noninteracted items are typically not locally recorded due to efficiency concerns. It may also be suboptimal to simply use random negative items provided by the server without considering clients' characteristics. One recent work [Ning et al., 2021] studies learning recommendation models under limited locally available negative samples. They proposed to use a batch-insensitive loss that enables using local positive items only to construct training samples. However, useful negative signals cannot be considered by this method. By contrast, our proposed method can effectively exploit various kinds of negative samples under privacy protection to improve contrastive recommendation model learning.\n\n3 FedCL\nNext, we introduce our federated contrastive learning (FedCL) approach for recommendation in detail. Its overall framework is shown in Fig. 1. The server not only coordinates a large number of clients for collaborative model updating, but is also responsible for finding hard negative samples to empower model training. There are two major steps in FedCL, i.e., federated negative sampling and negative mix-up for local contrastive learning, which are described as follows.\n\n3.1 Federated Negative Sampling\nIn federated learning, there is no global dataset for hard negative sampling. In addition, since local user data is highly sparse and biased, it is also impractical to conduct hard negative sampling locally. To solve this problem, we propose a federated negative sampling method to choose hard negatives in a privacy-preserving manner. Assume that there are N clients in total. Each client first uses its maintained local user model to infer a user embedding from the local user profile. The user model can be a simple user ID embedding, or a more complicated one that learns user embeddings from the sequence of historical interacted items. We denote the embedding of user u i as u i . Since this user embedding still contains much private user information, we use local differential privacy (LDP) to perturb this embedding. Motivated by [Qi et al., 2020], we first clip each user embedding with a threshold \u03b4 according to its L 1 norm, and add Laplace noise n \u223c Laplace(0, 2\u03b4 ) to protect this embedding ( is the privacy budget, and a lower means better privacy protection). Then the protected user embeddings satisfy -DP. We refer to the protected user embedding of u i as \u00fbi .\nThe protected user embeddings on user clients are uploaded to the server for negative sampling. A simple way is directly measuring the similarity between candidate items and these embeddings to find negative samples. However, these embeddings contain heavy noise if the privacy budget is relatively small, and the selected negative samples are usually inaccurate, as shown in the middle plot in Fig. 2. To mitigate the influence of noise on negative sampling, we propose to cluster the user embeddings and use their cluster centroids to represent users with different types of interests. More specifically, we use the Ward clustering algorithm to aggregate the N user embeddings into C clusters. The centroid of each cluster is the element-wise average of user embeddings in this cluster. We use the cluster centroid to replace the user embeddings in this cluster to conduct hard negative sampling (right plot in Fig. 2). Since the noise added to the user embeddings is zero-mean, the errors brought by the noise can be mitigated to some extent by using cluster centroids. We denote the cluster centroid corresponding to the user embedding u i as c i .\nWe evaluate the relevance between c i and each item embedding in the candidate pool on the server to choose negative samples. A simple way is to pick the globally hardest negative samples [Xiong et al., 2020]. However, there may be many false negatives because the candidate items in the entire pool have not necessarily been displayed to the users. Thus, in our approach we propose to choose semi-hard negative samples to balance their difficulty and accuracy. For each centroid vector c i , we first retrieve R% of candidate samples with the highest difficulties to form a hard candidate subset (a smaller R means harder negatives). We then randomly select T negative samples from this subset as semihard negative samples. Note that for clients that are associated with the same cluster centroid, the sampling of negative items from the hard candidate subset is independent, which aims to ensure the comprehensiveness of negative samples. The semi-hard negative samples drawn from c i are denoted as[h i 1 , h i 2 , ..., h i T ]\n, which are further distributed to the i-th client for local model training. In this way, the user clients can exploit the hard negative samples obtained from the global candidate pool in a privacy-preserving way.\n\n3.2 Negative Mix-up for Contrastive Training\nNext, we introduce the proposed negative mix-up method for local contrastive model learning. Motivated by the in-batch negative sampling in prior work [Yi et al., 2019], we propose to incorporate in-batch negatives by regarding the local positive items as a mini-batch, where the samples within this batch can be used as negatives for each other. We denote the local positive samples on the client i as [p i 1 , p i 2 , ..., p i P ]. For the positive item p i j , its corresponding in-batch negatives are [p i 1 , ..., p i j\u22121 , p j+1 ..., p i P ]. Note that this kind of in-batch negative samples cannot be used for training ID-based user modeling methods because they cannot learn different user embeddings based on the item interaction histories. In addition, in real-world scenarios there can be a small number of nega-Perturb Cluster Figure 2: A schematic example of negative sampling based on original user embeddings, perturbed embeddings and cluster centroids. Red circles and blue triangles represent users with two different types of interests. The squares represent the selected hard negative samples. The perturbed user embeddings cannot select accurate hard negative samples, while using the cluster centroids can reduce the noise impact. for each user u i \u2208 U do 12:\nRetrieve semi-hard negatives from the item pool using the cluster centroid c i 13:\nSend negative items to the client i 14:\nend for Securely upload g i to the server 21:\nend for\n\n22:\nServer aggregates local model updates 23:\nUpdate global model and distribute it to clients 24: until model convergence tive items stored on user devices or randomly distributed by the server. To help our approach be compatible with this setting, we also incorporate the local negative items into model training, which are denoted as[n i 1 , n i 2 , ..., n i K ]\n. Given each positive item p i j , we mix up the in-batch negatives, local negatives, and semi-hard negatives into a unified list of negative samples, and we denote the list of their embeddings as N i j . The embedding of the positive item p i j is denoted as p i j The local contrastive training loss function L i of the i-th client is formulated as follows:L i = \u2212 P j=1 log( exp(u i j \u2022 p i j ) exp(u i j \u2022 p i j ) + n\u2208N i j exp(u i j \u2022 n) ),\nwhere u i j represents the user embedding for matching with the positive item p i j (inferred by the user model on client i).\n\n3.3 Federated Model Update\n\n\n4 Experiments\n\n\n4.1 Datasets and Experimental Settings\nWe conduct experiments on four widely used benchmark datasets for recommendation. Two of them are taken from the Amazon dataset dump [McAuley et al., 2015]. We use the \"Beauty\" and \"Sports and Outdoors\" (shorten as \"Sports\") domains for experiments. The third dataset is collected on Yelp. Following the settings in [Zhou et al., 2020], we use the transaction records after January 1st, 2019. The last dataset is MovieLens 1M (denoted as ML-1M) [Harper and Konstan, 2015], which is also a canonical recommendation dataset. The statistics of the four datasets are listed in Table 1. For each user, the last interacted item is used for test, and the item before the last one is used for validation. We use the Adam [Kingma and Ba, 2015] optimizer for model training. The learning rate is 1e-3, and the number of selected users per update is 16. The hidden dimension is 64. The user embedding norm clip threshold is 1. The privacy budget is 4. The number of clusters is 25. To simulate local negative items, we randomly sample 100 items on each\n\nMethods\n\n\nBeauty\nSports HR@5 HR@10 nDCG@5 nDCG@10 HR@5 HR@10 nDCG@5 nDCG@10 NCF 0.\n\nMethods\nYelp ML-1M HR@5 HR@10 nDCG@5 nDCG@10 HR@5 HR@10 nDCG@5 nDCG@10 NCF 0. client, and for each positive we randomly choose 10 local negatives to participate in contrastive learning. The difficulty R% of negative samples is set to 25%. The number of semihard negatives is 20. These hyperparameters are tuned on the validation sets. Motivated by [Krichene and Rendle, 2020], we evaluate the recommendation performance on the whole item set rather than the sampled subsets used in [Sun et al., 2019]. We use the Hit Ratio (HR) and normalized Discounted Cumulative Gain (nDCG) metrics over the top 5 or 10 ranked items. We repeat each experiment 5 times and report the average scores. We will soon release our code for reproducibility.\n\n4.2 Performance Evaluation\nTo 2 and 3. We find that compared with centralized learning, the performance of models learned in the standard federated framework usually decreases. This is mainly due to the non-IID property of user data decentralized on different clients and the scarcity of informative negative samples. In addition, the BIL method outperforms the federated baseline. This is because BIL learns uniform item representations via a spreadout regularization instead of learning from the limited local negative samples. Moreover, our proposed FedCL method outperforms other methods (t-test results show the improvement is significant, p < 0.001), and it can consistently boost the performance of different base models. This is because our approach can incorporate various kinds of negative samples into contrastive model training to exploit richer negative signals.\n\n4.3 Influence of Negative Samples\nNext, we study the impacts of different types of negative samples in our approach. We compare the model performance with one kind of negative samples removed, and the performance of replacing semi-hard negatives with globally hardest negatives [Xiong et al., 2020]. The results on the Yelp and ML-1M datasets are shown in Fig. 3. We find that the hard negatives obtained by the federated negative sampling method have a major contribution to model performance. However, the performance has a huge decline if they are replaced by globally hardest samples. This is mainly because  globally hardest samples may contain many false negatives, which are harmful to model training. In addition, both inbatch and local negatives have contributions, but the performance gain brought by the local negatives is not so large. Thus, the performance of FedCL can still be promising if there is no local negative sample available on user devices.\n\n4.4 Influence of Clustering\nWe further study the influence of clustering on the model performance. The results without clustering, with Kmeans clustering, or with Ward clustering are compared in Fig. 4. We observe that the performance is suboptimal when the user embeddings are directly used for hard negative sampling. This is intuitive because the perturbed user embeddings are quite noisy. In addition, both Kmeans and Ward clustering algorithms can improve the model performance. This may be because using the centroids of clusters to replace original user embeddings can mitigate the influence of noise. In addition, the Ward clustering method is better than Kmeans. This may be because the hierarchical clustering method is stronger in modeling the relations between similar user interests.\n\n4.5 Influence of Cluster Number\nWe then analyze the impacts of the number of clusters on model performance, as shown in Fig. 5. The results reveal that when the number of clusters is too small, the performance is suboptimal. This may be because user interests cannot be effectively distinguished when there are too few clusters. However, the performance starts to decrease when the number of clusters is larger than 25. This may be because the influence of noise encoded in user embeddings becomes large. Thus, we set the number of clusters C to 25.\n\n4.6 Influence of Privacy Budget\nFinally, we analyze the influence of the privacy budget of user embeddings for hard negative sampling on the model perfor- mance. We change the intensity of Laplace noise according to different privacy budgets, and the corresponding results are shown in Fig. 6. We find that the performance usually decreases when a smaller privacy budget is used. This is intuitive because the noise is stronger when is smaller, and the obtained negative samples are more inaccurate. In our approach, we choose = 4 that can yield a good performance and satisfactory user privacy protection ability.\n\n4.7 Experiments in Appendix\nIn the appendix, we analyze the impact of the negative sample difficulty (R%) as well as the number of local and semi-hard negative samples. The results show that selecting negatives from the top 25% relevant candidates and using a medium number of negative samples is suitable.\n\n5 Conclusion\nIn this paper, we propose a federated contrastive learning framework for privacy-preserving recommendation, which can effectively exploit various kinds of informative negative samples to empower federated recommendation model learning. In our approach, we propose a federated negative sampling method to choose semi-hard negatives from the candidate item pool in a privacy-preserving way based on the locally inferred and perturbed user embeddings. In addition, we explore using a negative mix-up method to jointly incorporate local, in-batch and semi-hard negative samples into a unified contrastive learning framework. Extensive experiments on four widely used benchmark recommendation datasets validate that our approach can effectively improve different recommendation methods in a privacy-preserving manner.\n\nAppendix Implementation Details\nSeveral implementation details are introduced as follows. We conducted experiments using a machine with Ubuntu 16.04 operating system and Python 3.6. The machine has a memory of 256GB and a Tesla V100 GPU with 32GB memory. We used Keras 2.2.4 and tensorflow 1.15 to implement deep learning models. The Faiss library is used to retrieve negative samples.\n\nInfluence of Negative Sample Difficulty\nFig. 7 shows the influence of using top R% relevant candidate items for semi-hard negative sampling. The results show that the performance is suboptimal when the ratio R is either too small or too large. This is because when R is too large the sampled items are not so informative, while a small R may lead to inaccurate negatives because they are too hard. Thus, we choose R = 25 in our experiments.\n\nInfluence of Sample Number\nWe also study the influence of semi-hard and local negative samples on the model performance. We first set the number of local negative samples to 10 and change the number of semi-hard negative samples, as shown in Fig. 8. We find the performance is optimal when about 20 semi-hard negative samples are used. This is because the negative signals cannot be effectively exploited when semi-hard negatives are insufficient, while the negative sets are not uniform enough when there are too many hard negatives. We further change the number of local negative samples and compare the results (Fig. 9). We find that the number of local negative samples used for model training also needs to be moderate. Thus, we choose to use 10 negative samples in model learning.\n\nFootnotes:\n\nReferences:\n\n- Ammad-Ud-Din et al., 2019] Muhammad Ammad-Ud- Din, Elena Ivannikova, Suleiman A Khan, Were Oyomno, Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. Fed- erated collaborative filtering for privacy-preserving personalized recommendation system. arXiv preprint arXiv:1901.09888, 2019.- Chai et al., 2020] Di Chai, Leye Wang, Kai Chen, and Qiang Yang. Secure federated matrix factorization. IEEE Intelligent Systems, 2020.\n\n- Ding et al., 2019] Jingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, and Depeng Jin. Reinforced negative sampling for recommendation with exposure data. In IJCAI, pages 2230-2236, 2019.\n\n- Harper and Konstan, 2015] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. TIIS, 5(4):1-19, 2015. [He et al., 2017] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collabo- rative filtering. In WWW, pages 173-182, 2017. [Hidasi et al., 2016] Bal\u00e1zs Hidasi, Alexandros Karat- zoglou, Linas Baltrunas, and Domonkos Tikk. Session- based recommendations with recurrent neural networks. In ICLR, 2016. [Kalantidis et al., 2020] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. arXiv preprint arXiv:2010.01028, 2020.\n\n- Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [Krichene and Rendle, 2020] Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In KDD, pages 1748-1757, 2020.\n\n- Liang et al., 2021] Feng Liang, Weike Pan, and Zhong Ming. Fedrec++: Lossless federated recommendation with explicit feedback. In AAAI, pages 4224-4231, 2021. [Lin et al., 2020] Guanyu Lin, Feng Liang, Weike Pan, and Zhong Ming. Fedrec: Federated recommendation with ex- plicit feedback. IEEE Intelligent Systems, 2020. [McAuley et al., 2015] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In SIGIR, pages 43-52, 2015. [McMahan et al., 2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar- cas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, pages 1273-1282. PMLR, 2017.\n\n- Ning et al., 2021] Lin Ning, Karan Singhal, Ellie X Zhou, and Sushant Prakash. Learning federated representa- tions and recommendations with limited negatives. arXiv preprint arXiv:2108.07931, 2021. [Qi et al., 2020] Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. Privacy-preserving news recommendation model learning. In EMNLP: Findings, pages 1423-1432, 2020.\n\n- Qi et al., 2021] Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. Uni-fedrec: A unified privacy-preserving news recommendation framework for model training and online serving. In EMNLP: Findings, pages 1438-1448, 2021. [Rendle et al., 2009] Steffen Rendle, Christoph Freuden- thaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, pages 452-461, 2009. [Robinson et al., 2020] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592, 2020. [Shen et al., 2014] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr\u00e9goire Mesnil. Learning seman- tic representations using convolutional neural networks for web search. In WWW, pages 373-374, 2014. [Sun et al., 2019] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequen- tial recommendation with bidirectional encoder represen- tations from transformer. In CIKM, pages 1441-1450, 2019. [Wang et al., 2021] Jinpeng Wang, Jieming Zhu, and Xi- uqiang He. Cross-batch negative sampling for training two-tower recommenders. In SIGIR, pages 1632-1636, 2021. [Wei et al., 2021] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. Contrastive learning for cold-start recommendation. In MM, pages 5382-5390, 2021.\n\n- Wu et al., 2019] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. Npa: Neural news recommendation with personalized attention. In KDD, pages 2576-2584, 2019.\n\n- Xiong et al., 2020] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor neg- ative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020.\n\n- Yi et al., 2019] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. Sampling-bias-corrected neural modeling for large corpus item recommendations. In Recsys, pages 269-277, 2019.\n\n- Zhou et al., 2020] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. S3-rec: Self-supervised learn- ing for sequential recommendation with mutual informa- tion maximization. In CIKM, pages 1893-1902, 2020.\n\n", "annotations": {"ReferenceToTable": [{"begin": 15698, "end": 15699, "target": "#tab_2", "idx": 0}, {"begin": 17013, "end": 17020, "target": "#tab_4", "idx": 1}], "SectionMain": [{"begin": 1602, "end": 23542, "idx": 0}], "SectionReference": [{"begin": 23556, "end": 28575, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1602, "idx": 0}], "Div": [{"begin": 87, "end": 1594, "idx": 0}, {"begin": 1605, "end": 5903, "idx": 1}, {"begin": 5905, "end": 6877, "idx": 2}, {"begin": 6879, "end": 8495, "idx": 3}, {"begin": 8497, "end": 8978, "idx": 4}, {"begin": 8980, "end": 12590, "idx": 5}, {"begin": 12592, "end": 14094, "idx": 6}, {"begin": 14096, "end": 15033, "idx": 7}, {"begin": 15035, "end": 15062, "idx": 8}, {"begin": 15064, "end": 15078, "idx": 9}, {"begin": 15080, "end": 16160, "idx": 10}, {"begin": 16162, "end": 16170, "idx": 11}, {"begin": 16172, "end": 16244, "idx": 12}, {"begin": 16246, "end": 16981, "idx": 13}, {"begin": 16983, "end": 17858, "idx": 14}, {"begin": 17860, "end": 18825, "idx": 15}, {"begin": 18827, "end": 19623, "idx": 16}, {"begin": 19625, "end": 20174, "idx": 17}, {"begin": 20176, "end": 20790, "idx": 18}, {"begin": 20792, "end": 21098, "idx": 19}, {"begin": 21100, "end": 21925, "idx": 20}, {"begin": 21927, "end": 22312, "idx": 21}, {"begin": 22314, "end": 22754, "idx": 22}, {"begin": 22756, "end": 23542, "idx": 23}], "Head": [{"begin": 1605, "end": 1619, "n": "1", "idx": 0}, {"begin": 5905, "end": 5954, "n": "2.1", "idx": 1}, {"begin": 6879, "end": 6907, "n": "2.2", "idx": 2}, {"begin": 8497, "end": 8504, "n": "3", "idx": 3}, {"begin": 8980, "end": 9011, "n": "3.1", "idx": 4}, {"begin": 12592, "end": 12636, "n": "3.2", "idx": 5}, {"begin": 14096, "end": 14099, "idx": 6}, {"begin": 15035, "end": 15061, "n": "3.3", "idx": 7}, {"begin": 15064, "end": 15077, "n": "4", "idx": 8}, {"begin": 15080, "end": 15118, "n": "4.1", "idx": 9}, {"begin": 16162, "end": 16169, "idx": 10}, {"begin": 16172, "end": 16178, "idx": 11}, {"begin": 16246, "end": 16253, "idx": 12}, {"begin": 16983, "end": 17009, "n": "4.2", "idx": 13}, {"begin": 17860, "end": 17893, "n": "4.3", "idx": 14}, {"begin": 18827, "end": 18854, "n": "4.4", "idx": 15}, {"begin": 19625, "end": 19656, "n": "4.5", "idx": 16}, {"begin": 20176, "end": 20207, "n": "4.6", "idx": 17}, {"begin": 20792, "end": 20819, "n": "4.7", "idx": 18}, {"begin": 21100, "end": 21112, "n": "5", "idx": 19}, {"begin": 21927, "end": 21958, "idx": 20}, {"begin": 22314, "end": 22353, "idx": 21}, {"begin": 22756, "end": 22782, "idx": 22}], "Paragraph": [{"begin": 87, "end": 1594, "idx": 0}, {"begin": 1620, "end": 3027, "idx": 1}, {"begin": 3028, "end": 3951, "idx": 2}, {"begin": 3952, "end": 5238, "idx": 3}, {"begin": 5239, "end": 5284, "idx": 4}, {"begin": 5285, "end": 5888, "idx": 5}, {"begin": 5889, "end": 5903, "idx": 6}, {"begin": 5955, "end": 6877, "idx": 7}, {"begin": 6908, "end": 8495, "idx": 8}, {"begin": 8505, "end": 8978, "idx": 9}, {"begin": 9012, "end": 10192, "idx": 10}, {"begin": 10193, "end": 11345, "idx": 11}, {"begin": 11346, "end": 12347, "idx": 12}, {"begin": 12377, "end": 12590, "idx": 13}, {"begin": 12637, "end": 13917, "idx": 14}, {"begin": 13918, "end": 14000, "idx": 15}, {"begin": 14001, "end": 14040, "idx": 16}, {"begin": 14041, "end": 14086, "idx": 17}, {"begin": 14087, "end": 14094, "idx": 18}, {"begin": 14100, "end": 14141, "idx": 19}, {"begin": 14142, "end": 14432, "idx": 20}, {"begin": 14462, "end": 14821, "idx": 21}, {"begin": 14908, "end": 15033, "idx": 22}, {"begin": 15119, "end": 16160, "idx": 23}, {"begin": 16179, "end": 16244, "idx": 24}, {"begin": 16254, "end": 16981, "idx": 25}, {"begin": 17010, "end": 17858, "idx": 26}, {"begin": 17894, "end": 18825, "idx": 27}, {"begin": 18855, "end": 19623, "idx": 28}, {"begin": 19657, "end": 20174, "idx": 29}, {"begin": 20208, "end": 20790, "idx": 30}, {"begin": 20820, "end": 21098, "idx": 31}, {"begin": 21113, "end": 21925, "idx": 32}, {"begin": 21959, "end": 22312, "idx": 33}, {"begin": 22354, "end": 22754, "idx": 34}, {"begin": 22783, "end": 23542, "idx": 35}], "ReferenceToBib": [{"begin": 1702, "end": 1721, "target": "#b11", "idx": 0}, {"begin": 1829, "end": 1852, "target": "#b7", "idx": 1}, {"begin": 1914, "end": 1933, "target": "#b7", "idx": 2}, {"begin": 1948, "end": 1968, "target": "#b7", "idx": 3}, {"begin": 2078, "end": 2094, "target": "#b8", "idx": 4}, {"begin": 2367, "end": 2385, "target": "#b2", "idx": 5}, {"begin": 3078, "end": 3100, "target": "#b5", "idx": 6}, {"begin": 3220, "end": 3239, "target": "#b1", "idx": 7}, {"begin": 3267, "end": 3273, "idx": 8}, {"begin": 3831, "end": 3850, "target": "#b6", "idx": 9}, {"begin": 6056, "end": 6076, "target": "#b9", "idx": 10}, {"begin": 6109, "end": 6127, "target": "#b7", "idx": 11}, {"begin": 6249, "end": 6272, "target": "#b7", "idx": 12}, {"begin": 6399, "end": 6420, "target": "#b7", "idx": 13}, {"begin": 6647, "end": 6666, "target": "#b7", "idx": 14}, {"begin": 6667, "end": 6683, "target": "#b8", "idx": 15}, {"begin": 6753, "end": 6769, "target": "#b10", "idx": 16}, {"begin": 7029, "end": 7048, "target": "#b1", "idx": 17}, {"begin": 7049, "end": 7065, "target": "#b7", "idx": 18}, {"begin": 7066, "end": 7085, "target": "#b5", "idx": 19}, {"begin": 7113, "end": 7119, "idx": 20}, {"begin": 7280, "end": 7297, "idx": 21}, {"begin": 7432, "end": 7448, "idx": 22}, {"begin": 8025, "end": 8044, "target": "#b6", "idx": 23}, {"begin": 9851, "end": 9868, "target": "#b7", "idx": 24}, {"begin": 11534, "end": 11554, "target": "#b9", "idx": 25}, {"begin": 12788, "end": 12805, "target": "#b10", "idx": 26}, {"begin": 15252, "end": 15274, "target": "#b5", "idx": 27}, {"begin": 15435, "end": 15454, "target": "#b11", "idx": 28}, {"begin": 15564, "end": 15590, "target": "#b3", "idx": 29}, {"begin": 15832, "end": 15853, "target": "#b4", "idx": 30}, {"begin": 16728, "end": 16746, "target": "#b7", "idx": 31}, {"begin": 18138, "end": 18158, "target": "#b9", "idx": 32}], "Sentence": [{"begin": 87, "end": 234, "idx": 0}, {"begin": 235, "end": 355, "idx": 1}, {"begin": 356, "end": 564, "idx": 2}, {"begin": 565, "end": 787, "idx": 3}, {"begin": 788, "end": 1007, "idx": 4}, {"begin": 1008, "end": 1249, "idx": 5}, {"begin": 1250, "end": 1462, "idx": 6}, {"begin": 1463, "end": 1594, "idx": 7}, {"begin": 1620, "end": 1722, "idx": 8}, {"begin": 1723, "end": 1934, "idx": 9}, {"begin": 1935, "end": 2076, "idx": 10}, {"begin": 2077, "end": 2239, "idx": 11}, {"begin": 2240, "end": 2352, "idx": 12}, {"begin": 2353, "end": 2477, "idx": 13}, {"begin": 2478, "end": 2626, "idx": 14}, {"begin": 2627, "end": 2778, "idx": 15}, {"begin": 2779, "end": 2897, "idx": 16}, {"begin": 2898, "end": 3027, "idx": 17}, {"begin": 3028, "end": 3240, "idx": 18}, {"begin": 3241, "end": 3440, "idx": 19}, {"begin": 3441, "end": 3625, "idx": 20}, {"begin": 3626, "end": 3951, "idx": 21}, {"begin": 3952, "end": 4154, "idx": 22}, {"begin": 4155, "end": 4273, "idx": 23}, {"begin": 4274, "end": 4430, "idx": 24}, {"begin": 4431, "end": 4592, "idx": 25}, {"begin": 4593, "end": 4823, "idx": 26}, {"begin": 4824, "end": 5048, "idx": 27}, {"begin": 5049, "end": 5238, "idx": 28}, {"begin": 5239, "end": 5284, "idx": 29}, {"begin": 5285, "end": 5434, "idx": 30}, {"begin": 5435, "end": 5596, "idx": 31}, {"begin": 5597, "end": 5731, "idx": 32}, {"begin": 5732, "end": 5888, "idx": 33}, {"begin": 5889, "end": 5903, "idx": 34}, {"begin": 5955, "end": 6128, "idx": 35}, {"begin": 6129, "end": 6273, "idx": 36}, {"begin": 6274, "end": 6345, "idx": 37}, {"begin": 6346, "end": 6503, "idx": 38}, {"begin": 6504, "end": 6684, "idx": 39}, {"begin": 6685, "end": 6739, "idx": 40}, {"begin": 6740, "end": 6877, "idx": 41}, {"begin": 6908, "end": 7086, "idx": 42}, {"begin": 7087, "end": 7278, "idx": 43}, {"begin": 7279, "end": 7430, "idx": 44}, {"begin": 7431, "end": 7600, "idx": 45}, {"begin": 7601, "end": 7714, "idx": 46}, {"begin": 7715, "end": 7877, "idx": 47}, {"begin": 7878, "end": 8008, "idx": 48}, {"begin": 8009, "end": 8133, "idx": 49}, {"begin": 8134, "end": 8255, "idx": 50}, {"begin": 8256, "end": 8325, "idx": 51}, {"begin": 8326, "end": 8495, "idx": 52}, {"begin": 8505, "end": 8605, "idx": 53}, {"begin": 8606, "end": 8647, "idx": 54}, {"begin": 8648, "end": 8824, "idx": 55}, {"begin": 8825, "end": 8978, "idx": 56}, {"begin": 9012, "end": 9089, "idx": 57}, {"begin": 9090, "end": 9219, "idx": 58}, {"begin": 9220, "end": 9347, "idx": 59}, {"begin": 9348, "end": 9389, "idx": 60}, {"begin": 9390, "end": 9499, "idx": 61}, {"begin": 9500, "end": 9653, "idx": 62}, {"begin": 9654, "end": 9698, "idx": 63}, {"begin": 9699, "end": 9837, "idx": 64}, {"begin": 9838, "end": 10088, "idx": 65}, {"begin": 10089, "end": 10136, "idx": 66}, {"begin": 10137, "end": 10192, "idx": 67}, {"begin": 10193, "end": 10288, "idx": 68}, {"begin": 10289, "end": 10409, "idx": 69}, {"begin": 10410, "end": 10780, "idx": 70}, {"begin": 10781, "end": 10888, "idx": 71}, {"begin": 10889, "end": 10981, "idx": 72}, {"begin": 10982, "end": 11114, "idx": 73}, {"begin": 11115, "end": 11265, "idx": 74}, {"begin": 11266, "end": 11345, "idx": 75}, {"begin": 11346, "end": 11471, "idx": 76}, {"begin": 11472, "end": 11555, "idx": 77}, {"begin": 11556, "end": 11695, "idx": 78}, {"begin": 11696, "end": 11807, "idx": 79}, {"begin": 11808, "end": 11980, "idx": 80}, {"begin": 11981, "end": 12070, "idx": 81}, {"begin": 12071, "end": 12286, "idx": 82}, {"begin": 12287, "end": 12347, "idx": 83}, {"begin": 12377, "end": 12453, "idx": 84}, {"begin": 12454, "end": 12590, "idx": 85}, {"begin": 12637, "end": 12729, "idx": 86}, {"begin": 12730, "end": 12983, "idx": 87}, {"begin": 12984, "end": 13070, "idx": 88}, {"begin": 13071, "end": 13185, "idx": 89}, {"begin": 13186, "end": 13386, "idx": 90}, {"begin": 13387, "end": 13605, "idx": 91}, {"begin": 13606, "end": 13691, "idx": 92}, {"begin": 13692, "end": 13749, "idx": 93}, {"begin": 13750, "end": 13888, "idx": 94}, {"begin": 13889, "end": 13917, "idx": 95}, {"begin": 13918, "end": 14000, "idx": 96}, {"begin": 14001, "end": 14040, "idx": 97}, {"begin": 14041, "end": 14086, "idx": 98}, {"begin": 14087, "end": 14094, "idx": 99}, {"begin": 14100, "end": 14141, "idx": 100}, {"begin": 14142, "end": 14291, "idx": 101}, {"begin": 14292, "end": 14432, "idx": 102}, {"begin": 14462, "end": 14666, "idx": 103}, {"begin": 14667, "end": 14821, "idx": 104}, {"begin": 14908, "end": 15033, "idx": 105}, {"begin": 15119, "end": 15200, "idx": 106}, {"begin": 15201, "end": 15275, "idx": 107}, {"begin": 15276, "end": 15368, "idx": 108}, {"begin": 15369, "end": 15408, "idx": 109}, {"begin": 15409, "end": 15511, "idx": 110}, {"begin": 15512, "end": 15641, "idx": 111}, {"begin": 15642, "end": 15700, "idx": 112}, {"begin": 15701, "end": 15815, "idx": 113}, {"begin": 15816, "end": 15883, "idx": 114}, {"begin": 15884, "end": 15961, "idx": 115}, {"begin": 15962, "end": 15989, "idx": 116}, {"begin": 15990, "end": 16034, "idx": 117}, {"begin": 16035, "end": 16089, "idx": 118}, {"begin": 16090, "end": 16160, "idx": 119}, {"begin": 16179, "end": 16244, "idx": 120}, {"begin": 16254, "end": 16431, "idx": 121}, {"begin": 16432, "end": 16484, "idx": 122}, {"begin": 16485, "end": 16524, "idx": 123}, {"begin": 16525, "end": 16580, "idx": 124}, {"begin": 16581, "end": 16747, "idx": 125}, {"begin": 16748, "end": 16865, "idx": 126}, {"begin": 16866, "end": 16930, "idx": 127}, {"begin": 16931, "end": 16981, "idx": 128}, {"begin": 17010, "end": 17159, "idx": 129}, {"begin": 17160, "end": 17300, "idx": 130}, {"begin": 17301, "end": 17364, "idx": 131}, {"begin": 17365, "end": 17512, "idx": 132}, {"begin": 17513, "end": 17711, "idx": 133}, {"begin": 17712, "end": 17858, "idx": 134}, {"begin": 17894, "end": 17976, "idx": 135}, {"begin": 17977, "end": 18159, "idx": 136}, {"begin": 18160, "end": 18223, "idx": 137}, {"begin": 18224, "end": 18354, "idx": 138}, {"begin": 18355, "end": 18448, "idx": 139}, {"begin": 18449, "end": 18568, "idx": 140}, {"begin": 18569, "end": 18707, "idx": 141}, {"begin": 18708, "end": 18825, "idx": 142}, {"begin": 18855, "end": 18925, "idx": 143}, {"begin": 18926, "end": 19146, "idx": 144}, {"begin": 19147, "end": 19219, "idx": 145}, {"begin": 19220, "end": 19310, "idx": 146}, {"begin": 19311, "end": 19435, "idx": 147}, {"begin": 19436, "end": 19498, "idx": 148}, {"begin": 19499, "end": 19623, "idx": 149}, {"begin": 19657, "end": 19752, "idx": 150}, {"begin": 19753, "end": 19849, "idx": 151}, {"begin": 19850, "end": 19953, "idx": 152}, {"begin": 19954, "end": 20044, "idx": 153}, {"begin": 20045, "end": 20129, "idx": 154}, {"begin": 20130, "end": 20174, "idx": 155}, {"begin": 20208, "end": 20337, "idx": 156}, {"begin": 20338, "end": 20469, "idx": 157}, {"begin": 20470, "end": 20555, "idx": 158}, {"begin": 20556, "end": 20675, "idx": 159}, {"begin": 20676, "end": 20790, "idx": 160}, {"begin": 20820, "end": 20960, "idx": 161}, {"begin": 20961, "end": 21098, "idx": 162}, {"begin": 21113, "end": 21348, "idx": 163}, {"begin": 21349, "end": 21561, "idx": 164}, {"begin": 21562, "end": 21733, "idx": 165}, {"begin": 21734, "end": 21925, "idx": 166}, {"begin": 21959, "end": 22016, "idx": 167}, {"begin": 22017, "end": 22108, "idx": 168}, {"begin": 22109, "end": 22181, "idx": 169}, {"begin": 22182, "end": 22256, "idx": 170}, {"begin": 22257, "end": 22312, "idx": 171}, {"begin": 22354, "end": 22454, "idx": 172}, {"begin": 22455, "end": 22557, "idx": 173}, {"begin": 22558, "end": 22711, "idx": 174}, {"begin": 22712, "end": 22754, "idx": 175}, {"begin": 22783, "end": 22876, "idx": 176}, {"begin": 22877, "end": 23005, "idx": 177}, {"begin": 23006, "end": 23091, "idx": 178}, {"begin": 23092, "end": 23290, "idx": 179}, {"begin": 23291, "end": 23379, "idx": 180}, {"begin": 23380, "end": 23480, "idx": 181}, {"begin": 23481, "end": 23542, "idx": 182}], "ReferenceToFigure": [{"begin": 8645, "end": 8646, "target": "#fig_0", "idx": 0}, {"begin": 10593, "end": 10594, "idx": 1}, {"begin": 11111, "end": 11112, "idx": 2}, {"begin": 18221, "end": 18222, "idx": 3}, {"begin": 19027, "end": 19028, "idx": 4}, {"begin": 19750, "end": 19751, "idx": 5}, {"begin": 20467, "end": 20468, "idx": 6}, {"begin": 22359, "end": 22360, "idx": 7}, {"begin": 23003, "end": 23004, "idx": 8}, {"begin": 23376, "end": 23377, "idx": 9}], "Abstract": [{"begin": 77, "end": 1594, "idx": 0}], "SectionFootnote": [{"begin": 23544, "end": 23554, "idx": 0}], "ReferenceString": [{"begin": 23571, "end": 23849, "id": "b0", "idx": 0}, {"begin": 23851, "end": 23986, "id": "b1", "idx": 1}, {"begin": 23990, "end": 24173, "id": "b2", "idx": 2}, {"begin": 24177, "end": 24851, "id": "b3", "idx": 3}, {"begin": 24855, "end": 25105, "id": "b4", "idx": 4}, {"begin": 25109, "end": 25838, "id": "b5", "idx": 5}, {"begin": 25842, "end": 26218, "id": "b6", "idx": 6}, {"begin": 26222, "end": 27613, "id": "b7", "idx": 7}, {"begin": 27617, "end": 27809, "id": "b8", "idx": 8}, {"begin": 27813, "end": 28059, "id": "b9", "idx": 9}, {"begin": 28063, "end": 28307, "id": "b10", "idx": 10}, {"begin": 28311, "end": 28573, "id": "b11", "idx": 11}]}}