{"text": "Network Compression via Central Filter\n\nAbstract:\nNeural network pruning has remarkable performance for reducing the complexity of deep network models. Recent network pruning methods usually focused on removing unimportant or redundant filters in the network. In this paper, by exploring the similarities between feature maps, we propose a novel filter pruning method-Central Filter (CF), which suggests that a filter is approximately equal to a set of other filters after appropriate adjustments. Our method is based on the discovery that the average similarity between feature maps changes very little, regardless of the number of input images. Based on this finding, we establish similarity graphs on feature maps and calculate the closeness centrality of each node to select the Central Filter. Moreover, we design a method to directly adjust weights in the next layer corresponding to the Central Filter, effectively minimizing the error caused by pruning. Through experiments on various benchmark networks and datasets, CF yields state-of-the-art performance. For example, with ResNet-56, CF reduces approximately 39.7% of FLOPs by removing 47.1% of the parameters, with even 0.33% accuracy improvement on CIFAR-10. With GoogLeNet, CF reduces approximately 63.2% of FLOPs by removing 55.6% of the parameters, with only a small loss of 0.35% in top-1 accuracy on CIFAR-10. With ResNet-50, CF reduces approximately 47.9% of FLOPs by removing 36.9% of the parameters, with only a small loss of 1.07% in top-1 accuracy on ImageNet. The codes can be available at https: //github.com/8ubpshLR23/Central-Filter.\n\nMain:\n\n\n\n1. Introduction\nRecent developments on Convolutional neural networks (CNNs) have achieved great performance in computer vision tasks such as image classification [9, 14], object detection [6, 29], semantic segmentation [2, 25], video analysis [5, 22], etc. However, state-of-the-art CNNs have higher requirements for computational resources and memory footprint, which greatly limits their application in edge devices such as IoT devices. To address this, one idea is to compress the existing network, including tensor decomposition [28], network pruning [19], parameter quantification [24], etc. Another idea is to build a new small network, including knowledge distillation [12] and compact network design [13].\nAmong these network compression strategies, network pruning has significant performance and is suitable for various applications. In terms of the granularity of network pruning, it can be divided into unstructured pruning [1, 8] and structured pruning [19, 26]. Some early methods were based on unstructured pruning, and the produced kernels were sparse. That is, there are many matrices with zero elements in the middle. Unless the underlying hardware and computing libraries support them better, it is difficult to achieve substantial performance gains in the pruned models. Besides, the sparse matrix cannot use the existing mature BLAS library to obtain additional performance benefits. Therefore, a lot of research in recent years has focused on structured pruning, especially filter pruning.\nThe basic idea of filter pruning is to prune the unimportant filters while minimizing the loss of accuracy. So, the challenge is how to measure the importance of filters. For example, Li et al. [18] assumed that a filter should be pruned first if it has a smaller l 1 norm. Liu et al. [23] established a link between the importance of the filter and the scaling factor of the BN (Batch Normalization) layer. Filters with a smaller scaling factor value are unimportant and will be arXiv:2112.05493v2 [cs.LG] 13 Dec 2021 pruned first. In [19], the High Rank (HRank) of the feature map is used to measure the importance of the filters in each layer. These methods are based on different experiences or assumptions and succeed in filter pruning.\nAnother idea is to focus on the redundancy of filters or feature maps while pruning. Deep neural networks have a lot of redundancy [4]. So empirically, some redundant filters can be removed for network compression. A popular solution is to consider the similarity of filters or feature maps. Singh et al. [32] searched for strongly correlated filter pairs from each layer and made them highly correlated to reduce accuracy loss, and finally discard one. Wang et al. [35], proposed Quantified Similarity of Feature Maps (QSFM) to find the redundant information in the threedimensional tensors. Most similarity-based designs achieve compression and acceleration of neural networks, but with a limitation in the assumption that redundancy is useless.\nOur work is based on the similarity of the feature maps, which belong to the filter pruning. Many of the existing methods are based more or less on assumptions or experiences that lack theoretical proof. In this paper, we propose a novel approach to prune redundant filters. Firstly, we explored the stability of the similarity between feature maps in each layer. As shown in Fig. 1, we calculated the average similarity of output feature maps of different models on CIFAR-10 with different numbers of input images. After multiple sets of experiments, we found that the similarity changes between different feature maps are few and can be ignored. The average rank of feature maps has been explored in Hrank [19], showing that the rank of each feature map is always the same in different models. This proves the stability of the properties of the feature map from another perspective. Then we propose a novel theory, named Central Filter(CF), to selectively prune filters that generate redundant feature maps. In a set of similar feature maps, we select one feature to cover the functions of other features by directly adjusting the corresponding filter weights. To support this, mathematical proof is provided. As we all know, there is a lot of redundancy in many models [4], so removing the redundancy is a common solution for network compression. However, simply removing redundancy may have inadequacies. For example, there may be information supplementation between similar feature maps [32], which may result in loss of accuracy when discarding some of them. The key point of our approach is to create the central filter by analyzing the relationships between feature maps and by directly adjusting the filter weights, which reduces the potential effect of pruning redundant filters, as will be discussed in Sec. 3. Moreover, a critical issue is how to determine which filters are central filters. As shown in Fig. 2, we present an approach based on the closeness centrality of graph theory to solve it, and will be discussed in Sec. 3.4. Finally, we iteratively prune similar filters layer by layer and directly adjust the weight of filters in the next layer, and then through fine-tuning to recover the accuracy of the pruned model.\nTo summarize, our main contributions are as follows:\n\u2022 Through a large number of experiments in different models, We empirically demonstrate that the average similarity between the output feature maps is stable, regardless of the number of input features. Inspired by this, we propose a method to use this similar information, named Central Filter.\n\u2022 We mathematically prove the rationality of the proposed method. More, we present an approach based on the closeness centrality of graph theory to determine which filters should be pruned and which weights should be adjusted.\n\u2022 Extensive experiments on CIFAR-10 [16] and Ima-geNet [17], using VGGNet [31], GoogLeNet [33], DenseNet [14], and ResNet [9], demonstrate the effectiveness of the proposed Central Filter method.\n\n2. Related Work\nFilter Pruning Based on Importance. Filter pruning approaches lie in the evaluation of the importance of filters. Many classic methods measure importance by studying and analyzing the intrinsic properties of filters and feature maps. For example, Li et al. [18] established a connection between the importance of the filter and the l 1 norm, which assumes that a filter is unimportant if it has a smaller l 1 norm. He et al. [10] points out the limitations of the l 1 norm-based methods, and the geometric median is used to measure the importance of filters, which the filters closest to it are pruned first. In [19], the rank of the output feature map is used to measure the importance of filters in each layer, which demonstrates that the feature maps with lower rank are unimportant. Beyond ranking the filters in each layer, Chin et al. [3] proposed a method to calculate the global ranking of filters across different layers. Based on the widespread use of the BN (Batch Normalization) layer, Liu et al. [23] added a channel-wise scaling factor to the BN layer and added an l 1 regularizer to make it sparse, then pruning the filters with a small scaling factor. In addition, another direction considers the subsequent impact after pruning, such as the loss of accuracy, or the changes of output feature maps, etc. In [11, 26], pruning filters by computing the statistical information of the next layer, and aims to minimize the feature reconstruction error where filters with a smaller error are discarded first.\nFilter Pruning Based on Redundancy. Different from the previous methods based on importance, which pruning filters by designing a filter evaluation function. Redundancy-based methods concern which filters are redundant and how to remove them.  [32] proposed an ap-proach to identify the pairs of strongly correlated filters and discard one from each such pair. The pruned filter is considered redundant because the performance capabilities of the two filters in the pair are the same. Wang et al. [35] proposed a novel theory, namely Quantified Similarity of Feature Maps (QSFM), to get redundant information in the high-dimensional tensors. In [30], cosine similarity is used to measure the similarity of feature maps or filters, and filters with high similarity are pruned. Wang et al. [34] discovered that the layers with more structural redundancy are more suitable to be pruned, and graph theory is used to measure the redundancy for each layer. Though redundancybased methods succeed in the compression and acceleration of different models, it is still inefficient at high compression rates because of missing theoretical proof and guidance.\nOur method also belongs to filter pruning and is based on redundancy. Compared with the previous redundancybased methods, we amply use the properties of similar output feature maps and directly adjust the relevant weights, achieving higher accuracy at the same compression rate.\n\n3. The Proposed Method\n\n\n3.1. Notations\nFor a CNN model, which has N layers. We let L i represent the i-th layer, andF L i = {f i 1 , f i 2 , ..., f i n i } \u2208 R n i\u22121 \u00d7n i \u00d7h i \u00d7w i\ndenotes the set of all filters of the i-th layer, where n i denote the numbers of filters, and h i , w i represent the size of the kernel of the i-th layer. We define the j-th filterf i j = {k i,j 1 , k i,j 2 , ..., k i,j n i\u22121 } \u2208 R n i\u22121 \u00d7h i \u00d7w i , where the t-th kernel k i,j t \u2208 R h i \u00d7w i . Let O L i = {o i 1 , o i 2 , ..., o i n i } \u2208 R c\u00d7n i \u00d7h i \u00d7w i\nbe the set of output features in the i-th layer, where o i j is the j-th feature involved the j-th filter. c is the size of input feature maps. We define S(o i\nx , o i y ), (0 \u2264 x, y \u2264 n i ) as the measure of similarity between output features. (f) ResNet-56 \u03bb = 0.5. For each subfigure, the x-axis represents the i-th layer and the y-axis is the accuracy, and \u03bb means compression rate. Furthermore, columns with green denote the accuracy of before adjustment, and red is the after. As can be seen, the accuracy has been improved after adjustment on each layer in different architectures.\n\n3.2. Feature Similarity Measure\nIn this paper, we adopt Pearson correlation coefficient to measure the similarity between output feature maps. The Pearson correlation coefficient is used to measure the degree of correlation between two variables X and Y , and is defined as the covariance of two variables divided by the product of their standard deviations.\u03c1 X,Y = cov(X, Y ) \u03c3 X \u03c3 Y = E[(X \u2212 \u00b5 X )(Y \u2212 \u00b5 Y )] \u03c3 X \u03c3 Y\nHere \u00b5 X , \u00b5 Y represent expected values, and \u03c3 X , \u03c3 Y are standard deviations respectively. The Pearson correlation coefficient varies from \u22121 to 1. The value of the coefficient is 1 means that X and Y can be well described by the straight-line equation. All the data points fall well on a straight line, and Y increases with the increase of X. The value of the coefficient \u22121 means that all data points fall on a straight line, and Y decreases with X increases. The value of the coefficient is 0 means that there is no linear relationship between the two variables. So, the similarity S(o i x , o i y ) can be formulated as:S(o i x , o i y ) = \u03c1 One(o i x ),One(o i y )\nwhere One(\u2022) reshape o i j into one dimension. For features similarity measure, the value of the coefficient is more near 1 means two features are more similar. When the value is near 0, the two features are not similar.\nThrough a large number of experiments with different models on CIFAR-10, we have observed that the similarity between the output feature maps is stable. As demonstrated in Fig. 1, we can effectively estimate the expectation of the similarity between individual feature maps in different models with a small number of input images. Then, using this similarity information, we design an approach called Central Filter to determine which filters to discard\n\n3.3. Central Filter\nFor the i-th layer of the CNN model, the output features can be formulated as:O L i = F L i \u2022 O L i\u22121 = {f i 1 , f i 2 , ..., f i n i } \u2022 O L i\u22121\nFor simplicity, functions such as Relu, Batch normalization are omitted here. For the j-th output feature o i j :o i j = f i j \u2022 O L i\u22121 = {k i,j 1 , k i,j 2 , ..., k i,j n i\u22121 } \u2022 {o i\u22121 1 , o i\u22121 2 , ..., o i\u22121 n i\u22121 } = n i\u22121 r=1 k i,j r * o i\u22121 r = k i,j 1 * o i\u22121 1 + k i,j 2 * o i\u22121 2 + ... + k i,j n i\u22121 * o i\u22121 n i\u22121 ()\nwhere \u2022 is the dot product. We assume a subset of output featuresI i\u22121 j = {o i\u22121 j , o i\u22121 x1 , o i\u22121 x2 , ..., o i\u22121\nxm } are similar to each other :o i\u22121 x0 \u2248 o i\u22121 x1 \u2248 o i\u22121 x2 \u2248 ... \u2248 o i\u22121 xm , s.t. 0 \u2264 m \u2264 n i\u22121\nThe set of filters corresponding toI i\u22121 j is C i\u22121 j = {f i\u22121 j , f i\u22121 x1 , f i\u22121 x2 , ..., f i\u22121\nxm } So,we can reformulate Eq. ( 4) as:o i j = f i j \u2022 (O L i\u22121 \u2212 I i\u22121 j ) + f i j \u2022 I i\u22121 j\nand,f i j \u2022 I i\u22121 j = (k i,j j + k i,j x1 + k i,j x2 + ... + k i,j xm ) \u2022{o i\u22121 j , o i\u22121 x1 , o i\u22121 x2 , ..., o i\u22121 xm }\nCombining Eq. ( 5) and Eq. ( 7) we see that,f i j \u2022 I i\u22121 j \u2248 (k i,j j + k i,j x1 + k i,j x2 + ... + k i,j xm ) * o i\u22121 j\nHence, we letk i,j j = k i,j j + k i,j x1 + k i,j x2 + ... + k i,j xm\nwe get:f i j \u2022 I i\u22121 j \u2248 k i,j j * o i\u22121 j ()\nSo we only need to keep o i\u22121 j and prune the others. In filter pruning of the (i \u2212 1)-th layer, the filter f i\u22121 j , corresponding to o i\u22121 j , will be reserved, and the others will be pruned. According to Eq. ( 9), we directly adjust the weight of filter f i j . Eventually, we obtain a smaller computational model, as described in Eq. (10). Overall, if the elements in I i\u22121 j are the same, then f i\u22121 j , which generates o i\u22121 j , can be regarded as a central filter that covers the function of other filters in C i\u22121 j , and without obvious loss of accuracy. Some of the previous works used redundancy to pruning, but they rely on an assumption: Redundancy is useless, which is a lack of rigorous mathematical proof. We use the similarity of the output feature maps and directly adjust the corresponding filters. The additional loss is mainly derived from the similar evaluation error of the output feature maps.\n\n3.4. Filter Selection\nOne critical problem is how to determine central filters, which respectively replace a subset of filters while minimizing the loss as much as possible. To address it, we explore the relationship between features. For any feature map o i j in the i-th layer, we set a threshold \u03b8 i and calculate the similarity between o i j and other feature maps. If it is less than the threshold \u03b8 i , then the undirected edges are connected between them so that a similarity graph D i j is obtained. By corresponding the similarity graph D i j on the feature map o i j to the filter f i j , we get the relationship graph G i j between the filters and the set of points is defined as C i j . Furthermore, the size of set C i j ranges from 1 to n i . So our method can be formulated as an optimization problem:min( N i=1 n i j=1 \u03b4 i j J (f i j , C))\nwhere \u03b4 i j is an indicator which is 1 if f i j is a central filter, otherwise is 0. J measures the loss caused by using f i j to represent C. Besides, for a defined compression rate, there are a certain number of central filters corresponding to it. A central filter may only replace itself when its output feature map is not similar to the others. In contrast, it can also replace more. Thus Eq. ( 11) is equal to finding an optimal set of central filters to minimize the loss. We adopt centrality evaluation to solve this problem. In a connected graph, the closeness centrality of a node is a measure of centrality in a network, calculated as the reciprocal of the sum of the length of the shortest paths between the node and all other nodes in the graph. Thus, the more central a node is, the closer it is to all other nodes and the value is greater. Similarly, in the similarity graph of features, the greater a feature's closeness centrality is, the more similar it is to all other features. We define the closeness centrality of feature o i j as:c i j = n o\u2208I i j ,o =o i j S(o i j , o)\nwhere n + 1 is the size of set I i j \u2212 o i j . For filter selection strategy, we iteratively select the filter corresponding to the feature map with the highest closeness centrality as the central filter, and then prune the other filters that are similar to its feature map. The number of central filters is determined by a defined compression rate. As discussed in Sec. 3.3, the error of our method mainly comes from the similarity measurement error. Hence, we preferentially select the filters with the highest closeness centrality as the central filter based on the greed principle. Besides, for filters that will be pruned, selecting a central filter with the most similar feature map is more in line with our proposed method. Overall, we rank the filters according to their closeness centrality from highest to lowest and then prune them in order, so that we can obtain the approximate optimal solution of Eq. (11). As shown in Fig. 2, filters A, B, and C are similar to each other. Assuming that the closeness centrality of A is greater than B and C, which means A is more similar to others averagely. Then, we adjust A = A + B + C and prune B and C. To illustrate, we plot the comparison of the accuracy before and after the adjustment in Fig. 3. It can be observed that the accuracy of the adjusted model has improved to varying degrees, which is affected by whether the feature maps are sufficiently similar. The more similar the feature maps, the greater the degree of improvement.\n\n3.5. Pruning Procedure\nWe summarize our pruning procedure as follows: First, based on the stability of the similarity between features, we estimate the average similarity between each pair of output feature maps for each layer. Then, we get the similarity matrix S i . Second, according to the defined compression rate, we calculate the threshold \u03b8 i of each layer. Then, we can get the relationship graph G i through \u03b8 i and S i . Third, we calculate the closeness centrality C i of all nodes(filters) of G i and determine which filters should be pruned. For each filter f i j that has not been pruned, we calculate its C i j according to the relationship in graph G i . Fourth, we adjust the corresponding weight according to f i j and C i j , as shown in Fig. 2 and Eq. ( 9). Lastly, we iteratively prune similar filters and adjust the corresponding weight of filters of the next layer, and then through fine-tuning to recover the accuracy.\n\n4. Experiments\nTo demonstrate the effectiveness of our proposed method in model compression, we conduct extensive experiments on CIFAR-10 [16] and ImageNet [17], using VGGNet [31], GoogLeNet [33], DenseNet [14], and ResNet [9], for image classification. We randomly sample 128 images (output feature maps) to estimate the average similarity of each layer of the different architectures on CIFAR-10. The difference is that we sample 16 images on ImageNet to save time. Furthermore, different architectures have different characteristics, such as ResNet with a residual block, GoogLeNet with an inception module, etc. So the pruning strategies are also different. For all architectures, we sample output feature maps in ReLU [7] layer before the next convolutional layer, prune filters in this layer, and adjust the corresponding weights in the next layer. In particular, for ResNet, due to the shortcut connection, it is necessary to keep the two feature maps in the same dimension and the correspondence between the channels remains unchanged after pruning, as shown in Fig. 4.\n\n4.1. Experimental Settings\nConfigurations. In this paper, we conduct all pruning experiments on Pytorch 1.8 [27] under Intel i7-8700K CPU @3.70GHz and NVIDIA GTX 1080Ti GPU, and use Stochastic Gradient Descent algorithm (SGD) for the optimization problem. For VGGNet-16, DenseNet-40, and ResNet-56, we set initial learning rate, batch size, momentum, and weight decay to 0.001, 128, 0.9 and 0.005, respectively. We fine-tune the model for 30 epochs after pruning of each layer and set the learning rate to 0.0001 at epoch 5. To improve the efficiency of pruning and save training time, we fine-tune GoogLeNet / ResNet-50 for 1-2 epochs after pruning of each inception/block to get the final network model. Then, the final model is fine-tuned for 30-40 epochs, in which the initial learning rate, batch size, momentum, and weight decay are set to 0.001, 64, 0.9, and 0.0005, respectively. We divide the learning rate by 10 at epochs 5,20. For a fair comparison, we fix FLOPs and parameters as benchmarks and measure the Top-1 and Top-5 accuracy. Then, we compare them with the previous methods [11, 15, 18-21, 23, 26, 36].\n\n4.2. Results and Analysis\n\n\n4.2.1 Results on CIFAR-10\nWe conduct experiments with several mainstream CNN models on CIFAR-10, including VGG-16, GoogLeNet, ResNet56, and DenseNet-40. The architecture of VGG-16 that we used is the same as [18]. Moreover, We adjust the output of the original GoogLeNet matching the number of categories of CIFAR-10 [21].\nVGG-16. The experimental results of VGG-16 are summarized in Tab. 1. Compared with L1, CF significantly reduces the complexity of the model (60.4% vs. 34.3% for FLOPs and 83.6% vs. 64.0% for parameters) with just a 0.29% loss in accuracy, whereas L1 suffers a 0.56% loss in accuracy. CF outperforms HRank and Zhao et al. in terms of acceleration rate (60.4% vs. 53.5% by HRank and 39.1% by Zhao et al.), while maintaining higher accuracy (93.67% vs. 93.43% by HRank and 93.18% by Zhao et al.). Compared with GAL-0.05 and SSS, our method achieves better accuracy (93.06% vs. 92.03% by GAL-0.05 and 93.02% by SSS.) under higher parameters (85.4% vs. 77.6% by GAL-0.05 and 73.8% by SSS.) and FLOPs (70.8% vs. 39.6% by GAL-0.05 and 41.7% by SSS.) reduction. Besides, compared with HRank, CF yields a better accuracy (92.49% vs. 91.23%) and a greater FLOPs reduction (79.0% vs. 76.5%) under the similar parameter reduction (89.9% vs. 88.1%). This demonstrates that our method is more likely to yield better results when highly efficient models are required.\nGoogLeNet. The results on GoogLeNet are displayed in Tab. 2. Our proposed method achieves a 63.2% FLOPs reduction with only a small loss of 0.35% in the top-1 ac- [18] 93.40 206.00M(34.3%) 5.40M(64.0%) Zhao et al. [36] 93.18 190.00M(39.1%) 3.92M(73.3%) GAL-0.05  [21] 92.03 189.49M(39.6%) 3.36M(77.6%) SSS [15] 93.02 183.13M(41.6%) 3.93M(73.8%) HRank [19] 93\n\n4.2.2 Results on ImageNet\nThe experimental results of ResNet-50 are summarized in Tab. 5. Compared with GAL-0.5 and HRank, CF achieves higher FLOPs (47.9% vs. 43.0% by GAL-0.5 and 43.8% by HRank) and parameters (36.9% vs. 16.9% by GAL-0.5 and 36.7% by HRank) reduction, while maintaining better top-1 accuracy (75.08% vs. 71.95% by GAL-0.5 and 74.98% by HRank). In terms of FLOPs and parameter pruning, CF outperforms other proposed methods such as SSS-26, SSS-32, and He et al. in all aspects, while it still yields better top-1 and top-5 accuracy. With similar FLOPs reductions, CF achieves better performance than GAL-1 and HRank, while reducing more parameters. Moreover, CF still performs well at high compression rates. For example, when compared with ThiNet-50, CF gains better top-1 (70.26% vs. 68.42%) and top-5 (89.82% vs. 88.30%) accuracies, though it reduces more FLOPs (76.2% vs. 73.1%). Similar results can be found when compared with HRank and GAL-1-joint. The results show that CF has excellent performance on the large-scale ImageNet dataset.\n\n4.3. Ablation Study\nWe performed a systematic ablation study on the effectiveness of the filter selection strategy and the adjusting step. Besides, we show the changes in the similarity between feature maps after pruning.\n\n4.3.1 Pruning without Adjusting\nWe show that directly adjusting filters during fine-tuning yields a better performance on various benchmark mod-    els. For brevity, we compare the results of the ResNet-56(93.59% in Tab. 3), VGGNet(93.67% in Tab. 1) and GoogLeNet(94.70% in Tab. 2) respectively. For each model in Fig. 5, we observe that CF outperforms CF-N, which suggests that the adjusting step contributes to accuracy recovery.\n\n4.3.2 Variants of Filter Selection Strategy\nFig. 6 shows that preferentially preserving filters with higher closeness centrality of feature maps is superior. We propose two variants: (1)Random: Filters are randomly pruned. (2)Reverse: Filters with higher closeness centrality are pruned first. The pruning settings for VGGNet(93.67%) / GoogLeNet(94.70%) are the same as the before experiments in Tab. 1,2, respectively. Among the variants, we observe that CF shows the best performance, while the Reverse CF has the worst performance, demonstrating that filters with higher closeness centrality are more suitable as central filter and can significantly reduce the additional errors caused by pruning.\n\n4.3.3 Feature Similarity after Pruning\nWe show the comparison in the similarity between feature maps in the first layer of VGGNet at different compression rates, as shown in Fig. 7. We can observe that the number of feature map pairs with similarity greater than 0.7 gradually decreases as the compression rate increases. When the compression rate is 70%, the similarities between feature maps are all less than 0.6. This indicates that CF can effectively reduce the redundancy of the model.\n\n5. Conclusions\nIn this paper, we propose a novel filter pruning method called Central Filter (CF), which prunes filters by using the similarity information between feature maps. Unlike the previous methods based on filter importance or redundancy, our approach mathematically proves that there are such filters that can replace other sets of filters after proper adjustment. To support the proposed approach, we analyzed the average similarity of the output feature maps of the different models in CIFAR-10, which confirmed the feasibility of CF. The advanced performance of CF in compressing networks is demonstrated by various experiments on benchmark network models. In addition, we apply closeness centrality in graph theory to filter selection, proving its effectiveness through extensive ablation studies. In future work, we will further study these existing or potential stable properties of feature maps.\n\n6. Acknowledge\n\n\nFootnotes:\n\nReferences:\n\n- Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n and Yerlan Idelbayev. \"learning-compression\" algorithms for neural net pruning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8532-8541, 2018. 1- Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin P. Murphy, and Alan Loddon Yuille. Deeplab: Seman- tic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:834-848, 2018. 1\n\n- Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Mar- culescu. Towards efficient model compression via learned global ranking. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1515-1525, 2020. 2\n\n- Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. 2013. 2\n\n- Rohit Girdhar, Du Tran, Lorenzo Torresani, and Deva Ra- manan. Distinit: Learning video representations without a single labeled video. 2019 IEEE/CVF International Confer- ence on Computer Vision (ICCV), pages 852-861, 2019. 1\n\n- Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. 2014 IEEE Conference on Com- puter Vision and Pattern Recognition, pages 580-587, 2014. 1\n\n- Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. 2011. 6\n\n- Song Han, Jeff Pool, John Tran, and William J. Dally. Learn- ing both weights and connections for efficient neural net- work. ArXiv, abs/1506.02626, 2015. 1\n\n- Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. 1, 2, 6\n\n- Yang He, Ping Liu, Ziwei Wang, and Yi Yang. Pruning fil- ter via geometric median for deep convolutional neural net- works acceleration. ArXiv, abs/1811.00250, 2018. 2\n\n- Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. 2017 IEEE In- ternational Conference on Computer Vision (ICCV), pages 1398-1406, 2017. 2, 6, 7, 8\n\n- Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015. 1\n\n- Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient convolu- tional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017. 1\n\n- Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261-2269, 2017. 1, 2, 6\n\n- Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. ArXiv, abs/1707.01213, 2018. 6, 7, 8\n\n- Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. 2, 6\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. Communications of the ACM, 60:84 -90, 2012. 2, 6\n\n- Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. ArXiv, abs/1608.08710, 2017. 1, 2, 6, 7\n\n- Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao. Hrank: Filter pruning using high-rank feature map. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1526-1535, 2020. 1, 2, 6, 7, 8\n\n- Shaohui Lin, R. Ji, Yuchao Li, Yongjian Wu, Feiyue Huang, and Baochang Zhang. Accelerating convolutional networks via global & dynamic filter pruning. 2018. 6, 8\n\n- Shaohui Lin, R. Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, and David S. Doermann. Towards optimal structured cnn pruning via generative ad- versarial learning. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2785-2794, 2019. 6, 7, 8\n\n- Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action pro- posal generation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3888-3897, 2019. 1\n\n- Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2755-2763, 2017. 1, 2, 6, 7\n\n- Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang- Ting Cheng. Reactnet: Towards precise binary neu- ral network with generalized activation functions. ArXiv, abs/2003.03488, 2020. 1\n\n- Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. 2015 IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 3431-3440, 2015. 1\n\n- Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A fil- ter level pruning method for deep neural network compres- sion. 2017 IEEE International Conference on Computer Vi- sion (ICCV), pages 5068-5076, 2017. 1, 2, 6, 8\n\n- Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zach DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif- ferentiation in pytorch. 2017. 6\n\n- A. Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov, Julia Gusak, Petr Tichavsk\u00fd, Valeriy Glukhov, I. Oseledets, and Andrzej Cichocki. Stable low-rank tensor decomposition for compression of convolutional neural net- work. In ECCV, pages 522-539, 2020. 1\n\n- Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137-1149, 2015. 1\n\n- Ming-Wen Shao, Junhui Dai, Jiandong Kuang, and Deyu Meng. A dynamic cnn pruning method based on matrix sim- ilarity. Signal, Image and Video Processing, 15:381-389, 2021. 3\n\n- Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2015. 2, 6\n\n- Pravendra Singh, Vinay Kumar Verma, Piyush Rai, and Vinay P. Namboodiri. Leveraging filter correlations for deep model compression. 2020 IEEE Winter Conference on Appli- cations of Computer Vision (WACV), pages 824-833, 2020. 2, 3\n\n- Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1-9, 2015. 2, 6\n\n- Zi Wang, Chengcheng Li, and Xiangyang Wang. Convo- lutional neural network pruning with structural redundancy reduction. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14908-14917, 2021. 3\n\n- Zidu Wang, Xuexin Liu, Long Huang, Yunqing Chen, Yufei Zhang, Zhikang Lin, and Rui Wang. Model prun- ing based on quantified similarity of feature maps. ArXiv, abs/2105.06052, 2021. 2, 3\n\n- Chenglong Zhao, Bingbing Ni, Jia yu Zhang, Qiwei Zhao, Wenjun Zhang, and Qi Tian. Variational convolutional neural network pruning. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2775-2784, 2019. 6, 7\n\n", "annotations": {"SectionMain": [{"begin": 1618, "end": 27883, "idx": 0}], "ReferenceToFormula": [{"begin": 14348, "end": 14349, "target": "#formula_5", "idx": 0}, {"begin": 14547, "end": 14548, "target": "#formula_8", "idx": 1}, {"begin": 14560, "end": 14561, "target": "#formula_11", "idx": 2}, {"begin": 14982, "end": 14983, "target": "#formula_13", "idx": 3}, {"begin": 16944, "end": 16946, "target": "#formula_16", "idx": 4}, {"begin": 19906, "end": 19907, "target": "#formula_13", "idx": 5}], "SectionReference": [{"begin": 27897, "end": 35217, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1618, "idx": 0}], "Div": [{"begin": 50, "end": 1610, "idx": 0}, {"begin": 1621, "end": 7635, "idx": 1}, {"begin": 7637, "end": 10598, "idx": 2}, {"begin": 10600, "end": 10623, "idx": 3}, {"begin": 10625, "end": 11731, "idx": 4}, {"begin": 11733, "end": 13499, "idx": 5}, {"begin": 13501, "end": 15686, "idx": 6}, {"begin": 15688, "end": 19129, "idx": 7}, {"begin": 19131, "end": 20074, "idx": 8}, {"begin": 20076, "end": 21153, "idx": 9}, {"begin": 21155, "end": 22276, "idx": 10}, {"begin": 22278, "end": 22304, "idx": 11}, {"begin": 22306, "end": 24040, "idx": 12}, {"begin": 24042, "end": 25101, "idx": 13}, {"begin": 25103, "end": 25324, "idx": 14}, {"begin": 25326, "end": 25757, "idx": 15}, {"begin": 25759, "end": 26459, "idx": 16}, {"begin": 26461, "end": 26952, "idx": 17}, {"begin": 26954, "end": 27866, "idx": 18}, {"begin": 27868, "end": 27883, "idx": 19}], "Head": [{"begin": 1621, "end": 1636, "n": "1.", "idx": 0}, {"begin": 7637, "end": 7652, "n": "2.", "idx": 1}, {"begin": 10600, "end": 10622, "n": "3.", "idx": 2}, {"begin": 10625, "end": 10639, "n": "3.1.", "idx": 3}, {"begin": 11733, "end": 11764, "n": "3.2.", "idx": 4}, {"begin": 13501, "end": 13520, "n": "3.3.", "idx": 5}, {"begin": 15688, "end": 15709, "n": "3.4.", "idx": 6}, {"begin": 19131, "end": 19153, "n": "3.5.", "idx": 7}, {"begin": 20076, "end": 20090, "n": "4.", "idx": 8}, {"begin": 21155, "end": 21181, "n": "4.1.", "idx": 9}, {"begin": 22278, "end": 22303, "n": "4.2.", "idx": 10}, {"begin": 22306, "end": 22331, "n": "4.2.1", "idx": 11}, {"begin": 24042, "end": 24067, "n": "4.2.2", "idx": 12}, {"begin": 25103, "end": 25122, "n": "4.3.", "idx": 13}, {"begin": 25326, "end": 25357, "n": "4.3.1", "idx": 14}, {"begin": 25759, "end": 25802, "n": "4.3.2", "idx": 15}, {"begin": 26461, "end": 26499, "n": "4.3.3", "idx": 16}, {"begin": 26954, "end": 26968, "n": "5.", "idx": 17}, {"begin": 27868, "end": 27882, "n": "6.", "idx": 18}], "Paragraph": [{"begin": 50, "end": 1610, "idx": 0}, {"begin": 1637, "end": 2334, "idx": 1}, {"begin": 2335, "end": 3132, "idx": 2}, {"begin": 3133, "end": 3874, "idx": 3}, {"begin": 3875, "end": 4622, "idx": 4}, {"begin": 4623, "end": 6863, "idx": 5}, {"begin": 6864, "end": 6916, "idx": 6}, {"begin": 6917, "end": 7212, "idx": 7}, {"begin": 7213, "end": 7439, "idx": 8}, {"begin": 7440, "end": 7635, "idx": 9}, {"begin": 7653, "end": 9171, "idx": 10}, {"begin": 9172, "end": 10319, "idx": 11}, {"begin": 10320, "end": 10598, "idx": 12}, {"begin": 10640, "end": 10717, "idx": 13}, {"begin": 10782, "end": 10964, "idx": 14}, {"begin": 11143, "end": 11302, "idx": 15}, {"begin": 11303, "end": 11731, "idx": 16}, {"begin": 11765, "end": 12091, "idx": 17}, {"begin": 12152, "end": 12779, "idx": 18}, {"begin": 12825, "end": 13045, "idx": 19}, {"begin": 13046, "end": 13499, "idx": 20}, {"begin": 13521, "end": 13599, "idx": 21}, {"begin": 13667, "end": 13780, "idx": 22}, {"begin": 13995, "end": 14060, "idx": 23}, {"begin": 14114, "end": 14146, "idx": 24}, {"begin": 14215, "end": 14250, "idx": 25}, {"begin": 14315, "end": 14354, "idx": 26}, {"begin": 14409, "end": 14413, "idx": 27}, {"begin": 14531, "end": 14575, "idx": 28}, {"begin": 14653, "end": 14666, "idx": 29}, {"begin": 14723, "end": 14730, "idx": 30}, {"begin": 14769, "end": 15686, "idx": 31}, {"begin": 15710, "end": 16504, "idx": 32}, {"begin": 16544, "end": 17597, "idx": 33}, {"begin": 17638, "end": 19129, "idx": 34}, {"begin": 19154, "end": 20074, "idx": 35}, {"begin": 20091, "end": 21153, "idx": 36}, {"begin": 21182, "end": 22276, "idx": 37}, {"begin": 22332, "end": 22628, "idx": 38}, {"begin": 22629, "end": 23681, "idx": 39}, {"begin": 23682, "end": 24040, "idx": 40}, {"begin": 24068, "end": 25101, "idx": 41}, {"begin": 25123, "end": 25324, "idx": 42}, {"begin": 25358, "end": 25757, "idx": 43}, {"begin": 25803, "end": 26459, "idx": 44}, {"begin": 26500, "end": 26952, "idx": 45}, {"begin": 26969, "end": 27866, "idx": 46}], "ReferenceToBib": [{"begin": 1783, "end": 1786, "target": "#b8", "idx": 0}, {"begin": 1787, "end": 1790, "target": "#b13", "idx": 1}, {"begin": 1809, "end": 1812, "target": "#b5", "idx": 2}, {"begin": 1813, "end": 1816, "target": "#b28", "idx": 3}, {"begin": 1840, "end": 1843, "target": "#b1", "idx": 4}, {"begin": 1844, "end": 1847, "target": "#b24", "idx": 5}, {"begin": 1864, "end": 1867, "target": "#b4", "idx": 6}, {"begin": 1868, "end": 1871, "target": "#b21", "idx": 7}, {"begin": 2154, "end": 2158, "target": "#b27", "idx": 8}, {"begin": 2176, "end": 2180, "target": "#b18", "idx": 9}, {"begin": 2207, "end": 2211, "target": "#b23", "idx": 10}, {"begin": 2297, "end": 2301, "target": "#b11", "idx": 11}, {"begin": 2329, "end": 2333, "target": "#b12", "idx": 12}, {"begin": 2557, "end": 2560, "target": "#b0", "idx": 13}, {"begin": 2561, "end": 2563, "target": "#b7", "idx": 14}, {"begin": 2587, "end": 2591, "target": "#b18", "idx": 15}, {"begin": 2592, "end": 2595, "target": "#b25", "idx": 16}, {"begin": 3327, "end": 3331, "target": "#b17", "idx": 17}, {"begin": 3418, "end": 3422, "target": "#b22", "idx": 18}, {"begin": 3669, "end": 3673, "target": "#b18", "idx": 19}, {"begin": 4006, "end": 4009, "target": "#b3", "idx": 20}, {"begin": 4180, "end": 4184, "target": "#b31", "idx": 21}, {"begin": 4341, "end": 4345, "target": "#b34", "idx": 22}, {"begin": 5331, "end": 5335, "target": "#b18", "idx": 23}, {"begin": 5895, "end": 5898, "target": "#b3", "idx": 24}, {"begin": 6115, "end": 6119, "target": "#b31", "idx": 25}, {"begin": 7476, "end": 7480, "target": "#b15", "idx": 26}, {"begin": 7495, "end": 7499, "target": "#b16", "idx": 27}, {"begin": 7514, "end": 7518, "target": "#b30", "idx": 28}, {"begin": 7530, "end": 7534, "target": "#b32", "idx": 29}, {"begin": 7545, "end": 7549, "target": "#b13", "idx": 30}, {"begin": 7562, "end": 7565, "target": "#b8", "idx": 31}, {"begin": 7910, "end": 7914, "target": "#b17", "idx": 32}, {"begin": 8078, "end": 8082, "target": "#b9", "idx": 33}, {"begin": 8265, "end": 8269, "target": "#b18", "idx": 34}, {"begin": 8494, "end": 8497, "target": "#b2", "idx": 35}, {"begin": 8662, "end": 8666, "target": "#b22", "idx": 36}, {"begin": 8976, "end": 8980, "target": "#b10", "idx": 37}, {"begin": 8981, "end": 8984, "target": "#b25", "idx": 38}, {"begin": 9416, "end": 9420, "target": "#b31", "idx": 39}, {"begin": 9669, "end": 9673, "target": "#b34", "idx": 40}, {"begin": 9817, "end": 9821, "target": "#b29", "idx": 41}, {"begin": 9960, "end": 9964, "target": "#b33", "idx": 42}, {"begin": 15107, "end": 15111, "target": "#b9", "idx": 43}, {"begin": 18553, "end": 18557, "target": "#b10", "idx": 44}, {"begin": 20214, "end": 20218, "target": "#b15", "idx": 45}, {"begin": 20232, "end": 20236, "target": "#b16", "idx": 46}, {"begin": 20251, "end": 20255, "target": "#b30", "idx": 47}, {"begin": 20267, "end": 20271, "target": "#b32", "idx": 48}, {"begin": 20282, "end": 20286, "target": "#b13", "idx": 49}, {"begin": 20299, "end": 20302, "target": "#b8", "idx": 50}, {"begin": 20799, "end": 20802, "target": "#b6", "idx": 51}, {"begin": 21263, "end": 21267, "target": "#b26", "idx": 52}, {"begin": 22248, "end": 22275, "idx": 53}, {"begin": 22514, "end": 22518, "target": "#b17", "idx": 54}, {"begin": 22623, "end": 22627, "target": "#b20", "idx": 55}, {"begin": 23845, "end": 23849, "target": "#b17", "idx": 56}, {"begin": 23896, "end": 23900, "target": "#b35", "idx": 57}, {"begin": 23945, "end": 23949, "target": "#b20", "idx": 58}, {"begin": 23988, "end": 23992, "target": "#b14", "idx": 59}, {"begin": 24033, "end": 24037, "target": "#b18", "idx": 60}], "Sentence": [{"begin": 50, "end": 151, "idx": 0}, {"begin": 152, "end": 259, "idx": 1}, {"begin": 260, "end": 497, "idx": 2}, {"begin": 498, "end": 646, "idx": 3}, {"begin": 647, "end": 798, "idx": 4}, {"begin": 799, "end": 961, "idx": 5}, {"begin": 962, "end": 1065, "idx": 6}, {"begin": 1066, "end": 1221, "idx": 7}, {"begin": 1222, "end": 1377, "idx": 8}, {"begin": 1378, "end": 1533, "idx": 9}, {"begin": 1534, "end": 1610, "idx": 10}, {"begin": 1637, "end": 1877, "idx": 11}, {"begin": 1878, "end": 2059, "idx": 12}, {"begin": 2060, "end": 2217, "idx": 13}, {"begin": 2218, "end": 2334, "idx": 14}, {"begin": 2335, "end": 2464, "idx": 15}, {"begin": 2465, "end": 2596, "idx": 16}, {"begin": 2597, "end": 2689, "idx": 17}, {"begin": 2690, "end": 2756, "idx": 18}, {"begin": 2757, "end": 2911, "idx": 19}, {"begin": 2912, "end": 3025, "idx": 20}, {"begin": 3026, "end": 3132, "idx": 21}, {"begin": 3133, "end": 3240, "idx": 22}, {"begin": 3241, "end": 3303, "idx": 23}, {"begin": 3304, "end": 3406, "idx": 24}, {"begin": 3407, "end": 3540, "idx": 25}, {"begin": 3541, "end": 3631, "idx": 26}, {"begin": 3632, "end": 3665, "idx": 27}, {"begin": 3666, "end": 3779, "idx": 28}, {"begin": 3780, "end": 3874, "idx": 29}, {"begin": 3875, "end": 3959, "idx": 30}, {"begin": 3960, "end": 4010, "idx": 31}, {"begin": 4011, "end": 4089, "idx": 32}, {"begin": 4090, "end": 4166, "idx": 33}, {"begin": 4167, "end": 4328, "idx": 34}, {"begin": 4329, "end": 4467, "idx": 35}, {"begin": 4468, "end": 4622, "idx": 36}, {"begin": 4623, "end": 4715, "idx": 37}, {"begin": 4716, "end": 4826, "idx": 38}, {"begin": 4827, "end": 4897, "idx": 39}, {"begin": 4898, "end": 4986, "idx": 40}, {"begin": 4987, "end": 5138, "idx": 41}, {"begin": 5139, "end": 5270, "idx": 42}, {"begin": 5271, "end": 5418, "idx": 43}, {"begin": 5419, "end": 5507, "idx": 44}, {"begin": 5508, "end": 5632, "idx": 45}, {"begin": 5633, "end": 5785, "idx": 46}, {"begin": 5786, "end": 5834, "idx": 47}, {"begin": 5835, "end": 5972, "idx": 48}, {"begin": 5973, "end": 6031, "idx": 49}, {"begin": 6032, "end": 6187, "idx": 50}, {"begin": 6188, "end": 6444, "idx": 51}, {"begin": 6445, "end": 6526, "idx": 52}, {"begin": 6527, "end": 6662, "idx": 53}, {"begin": 6663, "end": 6667, "idx": 54}, {"begin": 6668, "end": 6863, "idx": 55}, {"begin": 6864, "end": 6916, "idx": 56}, {"begin": 6917, "end": 7119, "idx": 57}, {"begin": 7120, "end": 7212, "idx": 58}, {"begin": 7213, "end": 7278, "idx": 59}, {"begin": 7279, "end": 7439, "idx": 60}, {"begin": 7440, "end": 7635, "idx": 61}, {"begin": 7653, "end": 7688, "idx": 62}, {"begin": 7689, "end": 7766, "idx": 63}, {"begin": 7767, "end": 7886, "idx": 64}, {"begin": 7887, "end": 8067, "idx": 65}, {"begin": 8068, "end": 8261, "idx": 66}, {"begin": 8262, "end": 8439, "idx": 67}, {"begin": 8440, "end": 8583, "idx": 68}, {"begin": 8584, "end": 8820, "idx": 69}, {"begin": 8821, "end": 8972, "idx": 70}, {"begin": 8973, "end": 9171, "idx": 71}, {"begin": 9172, "end": 9207, "idx": 72}, {"begin": 9208, "end": 9329, "idx": 73}, {"begin": 9330, "end": 9414, "idx": 74}, {"begin": 9415, "end": 9532, "idx": 75}, {"begin": 9533, "end": 9656, "idx": 76}, {"begin": 9657, "end": 9813, "idx": 77}, {"begin": 9814, "end": 9947, "idx": 78}, {"begin": 9948, "end": 10122, "idx": 79}, {"begin": 10123, "end": 10319, "idx": 80}, {"begin": 10320, "end": 10389, "idx": 81}, {"begin": 10390, "end": 10598, "idx": 82}, {"begin": 10640, "end": 10676, "idx": 83}, {"begin": 10677, "end": 10717, "idx": 84}, {"begin": 10782, "end": 10938, "idx": 85}, {"begin": 10939, "end": 10964, "idx": 86}, {"begin": 11143, "end": 11249, "idx": 87}, {"begin": 11250, "end": 11286, "idx": 88}, {"begin": 11287, "end": 11302, "idx": 89}, {"begin": 11303, "end": 11387, "idx": 90}, {"begin": 11388, "end": 11410, "idx": 91}, {"begin": 11411, "end": 11529, "idx": 92}, {"begin": 11530, "end": 11625, "idx": 93}, {"begin": 11626, "end": 11731, "idx": 94}, {"begin": 11765, "end": 11875, "idx": 95}, {"begin": 11876, "end": 12091, "idx": 96}, {"begin": 12152, "end": 12245, "idx": 97}, {"begin": 12246, "end": 12302, "idx": 98}, {"begin": 12303, "end": 12408, "idx": 99}, {"begin": 12409, "end": 12498, "idx": 100}, {"begin": 12499, "end": 12616, "idx": 101}, {"begin": 12617, "end": 12720, "idx": 102}, {"begin": 12721, "end": 12779, "idx": 103}, {"begin": 12825, "end": 12871, "idx": 104}, {"begin": 12872, "end": 12985, "idx": 105}, {"begin": 12986, "end": 13045, "idx": 106}, {"begin": 13046, "end": 13198, "idx": 107}, {"begin": 13199, "end": 13376, "idx": 108}, {"begin": 13377, "end": 13499, "idx": 109}, {"begin": 13521, "end": 13599, "idx": 110}, {"begin": 13667, "end": 13744, "idx": 111}, {"begin": 13745, "end": 13780, "idx": 112}, {"begin": 13995, "end": 14022, "idx": 113}, {"begin": 14023, "end": 14060, "idx": 114}, {"begin": 14114, "end": 14146, "idx": 115}, {"begin": 14215, "end": 14250, "idx": 116}, {"begin": 14315, "end": 14354, "idx": 117}, {"begin": 14409, "end": 14413, "idx": 118}, {"begin": 14531, "end": 14575, "idx": 119}, {"begin": 14653, "end": 14666, "idx": 120}, {"begin": 14723, "end": 14730, "idx": 121}, {"begin": 14769, "end": 14822, "idx": 122}, {"begin": 14823, "end": 14962, "idx": 123}, {"begin": 14963, "end": 15033, "idx": 124}, {"begin": 15034, "end": 15112, "idx": 125}, {"begin": 15113, "end": 15332, "idx": 126}, {"begin": 15333, "end": 15490, "idx": 127}, {"begin": 15491, "end": 15586, "idx": 128}, {"begin": 15587, "end": 15686, "idx": 129}, {"begin": 15710, "end": 15861, "idx": 130}, {"begin": 15862, "end": 15922, "idx": 131}, {"begin": 15923, "end": 16057, "idx": 132}, {"begin": 16058, "end": 16195, "idx": 133}, {"begin": 16196, "end": 16386, "idx": 134}, {"begin": 16387, "end": 16444, "idx": 135}, {"begin": 16445, "end": 16504, "idx": 136}, {"begin": 16544, "end": 16794, "idx": 137}, {"begin": 16795, "end": 16893, "idx": 138}, {"begin": 16894, "end": 16932, "idx": 139}, {"begin": 16933, "end": 17023, "idx": 140}, {"begin": 17024, "end": 17077, "idx": 141}, {"begin": 17078, "end": 17302, "idx": 142}, {"begin": 17303, "end": 17398, "idx": 143}, {"begin": 17399, "end": 17541, "idx": 144}, {"begin": 17542, "end": 17597, "idx": 145}, {"begin": 17638, "end": 17684, "idx": 146}, {"begin": 17685, "end": 17912, "idx": 147}, {"begin": 17913, "end": 17987, "idx": 148}, {"begin": 17988, "end": 18008, "idx": 149}, {"begin": 18009, "end": 18089, "idx": 150}, {"begin": 18090, "end": 18223, "idx": 151}, {"begin": 18224, "end": 18368, "idx": 152}, {"begin": 18369, "end": 18558, "idx": 153}, {"begin": 18559, "end": 18625, "idx": 154}, {"begin": 18626, "end": 18745, "idx": 155}, {"begin": 18746, "end": 18891, "idx": 156}, {"begin": 18892, "end": 19055, "idx": 157}, {"begin": 19056, "end": 19129, "idx": 158}, {"begin": 19154, "end": 19358, "idx": 159}, {"begin": 19359, "end": 19399, "idx": 160}, {"begin": 19400, "end": 19496, "idx": 161}, {"begin": 19497, "end": 19562, "idx": 162}, {"begin": 19563, "end": 19686, "idx": 163}, {"begin": 19687, "end": 19802, "idx": 164}, {"begin": 19803, "end": 19909, "idx": 165}, {"begin": 19910, "end": 20074, "idx": 166}, {"begin": 20091, "end": 20329, "idx": 167}, {"begin": 20330, "end": 20474, "idx": 168}, {"begin": 20475, "end": 20543, "idx": 169}, {"begin": 20544, "end": 20691, "idx": 170}, {"begin": 20692, "end": 20737, "idx": 171}, {"begin": 20738, "end": 20930, "idx": 172}, {"begin": 20931, "end": 21153, "idx": 173}, {"begin": 21182, "end": 21197, "idx": 174}, {"begin": 21198, "end": 21410, "idx": 175}, {"begin": 21411, "end": 21566, "idx": 176}, {"begin": 21567, "end": 21860, "idx": 177}, {"begin": 21861, "end": 22042, "idx": 178}, {"begin": 22043, "end": 22092, "idx": 179}, {"begin": 22093, "end": 22199, "idx": 180}, {"begin": 22200, "end": 22276, "idx": 181}, {"begin": 22332, "end": 22458, "idx": 182}, {"begin": 22459, "end": 22519, "idx": 183}, {"begin": 22520, "end": 22628, "idx": 184}, {"begin": 22629, "end": 22636, "idx": 185}, {"begin": 22637, "end": 22697, "idx": 186}, {"begin": 22698, "end": 22785, "idx": 187}, {"begin": 22786, "end": 22815, "idx": 188}, {"begin": 22816, "end": 22912, "idx": 189}, {"begin": 22913, "end": 23085, "idx": 190}, {"begin": 23086, "end": 23122, "idx": 191}, {"begin": 23123, "end": 23209, "idx": 192}, {"begin": 23210, "end": 23282, "idx": 193}, {"begin": 23283, "end": 23382, "idx": 194}, {"begin": 23383, "end": 23460, "idx": 195}, {"begin": 23461, "end": 23508, "idx": 196}, {"begin": 23509, "end": 23565, "idx": 197}, {"begin": 23566, "end": 23681, "idx": 198}, {"begin": 23682, "end": 23692, "idx": 199}, {"begin": 23693, "end": 23870, "idx": 200}, {"begin": 23871, "end": 23883, "idx": 201}, {"begin": 23884, "end": 23921, "idx": 202}, {"begin": 23922, "end": 23934, "idx": 203}, {"begin": 23935, "end": 23943, "idx": 204}, {"begin": 23944, "end": 23970, "idx": 205}, {"begin": 23971, "end": 23983, "idx": 206}, {"begin": 23984, "end": 24013, "idx": 207}, {"begin": 24014, "end": 24026, "idx": 208}, {"begin": 24027, "end": 24040, "idx": 209}, {"begin": 24068, "end": 24206, "idx": 210}, {"begin": 24207, "end": 24370, "idx": 211}, {"begin": 24371, "end": 24403, "idx": 212}, {"begin": 24404, "end": 24591, "idx": 213}, {"begin": 24592, "end": 24707, "idx": 214}, {"begin": 24708, "end": 24767, "idx": 215}, {"begin": 24768, "end": 24852, "idx": 216}, {"begin": 24853, "end": 24882, "idx": 217}, {"begin": 24883, "end": 24942, "idx": 218}, {"begin": 24943, "end": 25013, "idx": 219}, {"begin": 25014, "end": 25101, "idx": 220}, {"begin": 25123, "end": 25241, "idx": 221}, {"begin": 25242, "end": 25324, "idx": 222}, {"begin": 25358, "end": 25478, "idx": 223}, {"begin": 25479, "end": 25546, "idx": 224}, {"begin": 25547, "end": 25621, "idx": 225}, {"begin": 25622, "end": 25757, "idx": 226}, {"begin": 25803, "end": 25916, "idx": 227}, {"begin": 25917, "end": 25981, "idx": 228}, {"begin": 25982, "end": 26052, "idx": 229}, {"begin": 26053, "end": 26092, "idx": 230}, {"begin": 26093, "end": 26112, "idx": 231}, {"begin": 26113, "end": 26159, "idx": 232}, {"begin": 26160, "end": 26178, "idx": 233}, {"begin": 26179, "end": 26459, "idx": 234}, {"begin": 26500, "end": 26642, "idx": 235}, {"begin": 26643, "end": 26782, "idx": 236}, {"begin": 26783, "end": 26877, "idx": 237}, {"begin": 26878, "end": 26952, "idx": 238}, {"begin": 26969, "end": 27131, "idx": 239}, {"begin": 27132, "end": 27328, "idx": 240}, {"begin": 27329, "end": 27500, "idx": 241}, {"begin": 27501, "end": 27623, "idx": 242}, {"begin": 27624, "end": 27765, "idx": 243}, {"begin": 27766, "end": 27866, "idx": 244}], "ReferenceToFigure": [{"begin": 5004, "end": 5005, "target": "#fig_0", "idx": 0}, {"begin": 6544, "end": 6545, "target": "#fig_1", "idx": 1}, {"begin": 13223, "end": 13224, "target": "#fig_0", "idx": 2}, {"begin": 18576, "end": 18577, "target": "#fig_1", "idx": 3}, {"begin": 18889, "end": 18890, "target": "#fig_10", "idx": 4}, {"begin": 19894, "end": 19895, "target": "#fig_1", "idx": 5}, {"begin": 21151, "end": 21152, "target": "#fig_11", "idx": 6}, {"begin": 25645, "end": 25646, "target": "#fig_12", "idx": 7}, {"begin": 25808, "end": 25809, "target": "#fig_14", "idx": 8}, {"begin": 26640, "end": 26641, "target": "#fig_15", "idx": 9}], "Abstract": [{"begin": 40, "end": 1610, "idx": 0}], "SectionFootnote": [{"begin": 27885, "end": 27895, "idx": 0}], "ReferenceString": [{"begin": 27912, "end": 28113, "id": "b0", "idx": 0}, {"begin": 28115, "end": 28409, "id": "b1", "idx": 1}, {"begin": 28413, "end": 28640, "id": "b2", "idx": 2}, {"begin": 28644, "end": 28777, "id": "b3", "idx": 3}, {"begin": 28781, "end": 29007, "id": "b4", "idx": 4}, {"begin": 29011, "end": 29250, "id": "b5", "idx": 5}, {"begin": 29254, "end": 29350, "id": "b6", "idx": 6}, {"begin": 29354, "end": 29510, "id": "b7", "idx": 7}, {"begin": 29514, "end": 29712, "id": "b8", "idx": 8}, {"begin": 29716, "end": 29883, "id": "b9", "idx": 9}, {"begin": 29887, "end": 30083, "id": "b10", "idx": 10}, {"begin": 30087, "end": 30216, "id": "b11", "idx": 11}, {"begin": 30220, "end": 30463, "id": "b12", "idx": 12}, {"begin": 30467, "end": 30660, "id": "b13", "idx": 13}, {"begin": 30664, "end": 30794, "id": "b14", "idx": 14}, {"begin": 30798, "end": 30880, "id": "b15", "idx": 15}, {"begin": 30884, "end": 31056, "id": "b16", "idx": 16}, {"begin": 31060, "end": 31210, "id": "b17", "idx": 17}, {"begin": 31214, "end": 31473, "id": "b18", "idx": 18}, {"begin": 31477, "end": 31638, "id": "b19", "idx": 19}, {"begin": 31642, "end": 31938, "id": "b20", "idx": 20}, {"begin": 31942, "end": 32165, "id": "b21", "idx": 21}, {"begin": 32169, "end": 32417, "id": "b22", "idx": 22}, {"begin": 32421, "end": 32607, "id": "b23", "idx": 23}, {"begin": 32611, "end": 32816, "id": "b24", "idx": 24}, {"begin": 32820, "end": 33038, "id": "b25", "idx": 25}, {"begin": 33042, "end": 33231, "id": "b26", "idx": 26}, {"begin": 33235, "end": 33501, "id": "b27", "idx": 27}, {"begin": 33505, "end": 33728, "id": "b28", "idx": 28}, {"begin": 33732, "end": 33904, "id": "b29", "idx": 29}, {"begin": 33908, "end": 34046, "id": "b30", "idx": 30}, {"begin": 34050, "end": 34280, "id": "b31", "idx": 31}, {"begin": 34284, "end": 34559, "id": "b32", "idx": 32}, {"begin": 34563, "end": 34786, "id": "b33", "idx": 33}, {"begin": 34790, "end": 34976, "id": "b34", "idx": 34}, {"begin": 34980, "end": 35215, "id": "b35", "idx": 35}]}}