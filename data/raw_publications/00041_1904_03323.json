{"text": "Publicly Available Clinical BERT Embeddings\n\nAbstract:\nContextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domainspecific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.\n\n\n1 Introduction\nNatural language processing (NLP) has been shaken in recent months with the dramatic successes enabled by transfer learning and contextual word embedding models, such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2018).\nThese models have been primarily explored for general domain text, and, recently, biomedical text with BioBERT (Lee et al., 2019). However, clinical narratives (e.g., physician notes) have known differences in linguistic characteristics from both general text and non-clinical biomedical text, motivating the need for specialized clinical BERT models.\nIn this work, we build and publicly release exactly such an embedding model. 1 Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike.\nIn particular, we make the following contributions:\n1. We train and publicly release BERT-Base and BioBERT-finetuned models trained on both all clinical notes and only discharge summaries. 2\n2. We demonstrate that using clinical specific contextual embeddings improves both upon general domain results and BioBERT results across 2 well established clinical NER tasks and one medical natural language inference task (i2b2 2010 (Uzuner et al., 2011), i2b2 2012 (Sun et al., 2013a,b), and MedNLI (Romanov and Shivade, 2018)). On 2 de-identification (de-ID) tasks, i2b2 2006 (Uzuner et al., 2007) and i2b2 2014 (Stubbs et al., 2015; Stubbs and Uzuner, 2015), general BERT and BioBERT outperform clinical BERT and we argue that fundamental facets of the de-ID context motivate this lack of performance.\n\n2 Related Work\nContextual Embeddings in General Traditional word-level vector representations, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), express all possible meanings of a word as a single vector representation and cannot disambiguate the word senses based on the surrounding context. Over the last two years, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) present strong solutions that can provide contextualized word representations. By pre-training on a large text corpus as a language model, ELMo can create a context-sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More importantly, rather than simply providing word embeddings as features, BERT can be incorporated into a downstream task and gets fine-tuned as an integrated task-specific architecture.\nBERT has, in general, been found to be superior to ELMo and far superior to non-contextual embeddings on a variety of tasks, including those in the clinical domain (Si et al., 2019). For this reason, we only examine BERT here, rather than including ELMo or non-contextual embedding methods.\n\nContextual Clinical & Biomedical Embeddings\nSeveral works have explored the utility of contextual models in the clinical and biomedical domains. BioBERT (Lee et al., 2019) trains a BERT model over a corpus of biomedical research articles sourced from PubMed 3 article abstracts and PubMed Central 4 article full texts. They find the specificity offered by biomedical texts translated to improved performance on several biomedical NLP tasks, and fully release their pre-trained BERT model.\nOn clinical text, (Khin et al., 2018) uses a general-domain pretrained ELMo model towards the task of clinical text de-identification, reporting near state-of-the-art performance on the i2b2 2014 task (Stubbs and Uzuner, 2015; Stubbs et al., 2015) and state of the art performance on several axes of the HIPAA PHI dataset.\nTwo works that we know of train contextual embedding models on clinical corpora.  (Zhu et al., 2018) trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task (Uzuner et al., 2011). They release a pre-trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful contextual embeddings.  (Si et al., 2019), released in late February 2019, train a clinical note corpus BERT language model and uses complex task-specific models to yield improvements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks (Sun et al., 2013b,a) and the SemEval 2014 task 7 (Pradhan et al., 2014) and 2015 task 14 (Elhadad et al.) tasks, establishing new state-of-theart results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance opportunities offered by fine-tuning BioBERT with clinical text or by training note-type specific embedding models, as we do.\n\n3 Methods\nIn this section, we first describe our clinical text dataset, the details of the BERT training procedure, and finally the specific tasks we examine.\n\n3.1 Data\nWe use clinical text from the approximately 2 million notes in the MIMIC-III v1.4 database (Johnson et al., 2016). Details of our text pre-processing procedure can be found in Appendix A. Note that while some of our tasks use a small subset of MIMIC notes in their corpora, we do not try to filter these notes out of our BERT pre-training procedure. We expect the bias this induces is negligible given the relative sizes of the two corpora.\nWe train two varieties of BERT on MIMIC notes: Clinical BERT, which uses text from all note types, and Discharge Summary BERT, which uses only discharge summaries in an effort to tailor the corpus to downstream tasks (which often largely use discharge summaries).\nNote that we train our clinical BERT instantiations on all notes of the appropriate type(s), without regard for whether or not any individual note appeared in any of the train/test sets for the various tasks we use (two of which use a small subset of MIMIC notes either partially or completely as their backing corpora). We feel this has a negligible impact given the dramatically larger size of the entire MIMIC corpus relative to the various task corpora.\n\n3.2 BERT Training\nIn this work, we aim to provide the pre-trained embeddings as a community resource, rather than demonstrate technical novelty in the training procedure, and accordingly our BERT training procedure is completely standard. As such, we have relegated specifics of the training procedure to Appendix B.\nWe trained two BERT models on clinical text: 1) Clinical BERT, initialized from BERT-Base, and 2) Clinical BioBERT, initialized from BioBERT. For all downstream tasks, BERT models were allowed to be fine-tuned, then the output BERT embedding was passed through a single linear layer for classification, either at a per-token level for NER or de-ID tasks or applied to the sentinel \"begin sentence\" token for MedNLI. Note that this is a substantially lower capacity model than, for example, the Bi-LSTM layer used in (Si et al., 2019). This reduced capacity potentially limits performance on downstream tasks, but is in line with our goal of demonstrating the efficacy of clinical-specific embeddings and releasing pretrained BERT model for these embeddings. We did not experiment with more complex representations as our goal is not to necessarily surpass stateof-the-art performances on these tasks.\nComputational Cost Pre-processing and training BERT on MIMIC notes took significant computational resources. We estimate that our entire embedding model procedure took roughly 17 -18 days of computational runtime using a single GeForce GTX TITAN X 12 GB GPU (and significant CPU power and memory for pre-processing tasks). This is not including any time required to download or setup MIMIC or to train any final downstream tasks. 18 days of continuous runtime is a significant investment and may be beyond the reach of some labs or institutions. This is precisely why we believe that releasing our pretrained model will be useful to the community.\n\n3.3 Tasks\nThe Clinical BERT and Clinical BioBERT models were applied to the MedNLI natural language inference task (Romanov and Shivade, 2018) and four i2b2 named entity recognition (NER) tasks, all in IOB format (Ramshaw and Marcus, 1995) : i2b2 2006 1B de-identification (Uzuner et al., 2007), i2b2 2010 concept extraction (Uzuner et al., 2011). Exact F1 requires that the text span and label be an exact match to be considered correct.\ntion challenge (Sun et al., 2013a,b), i2b2 2014 7A de-identification challenge (Stubbs and Uzuner, 2015; Stubbs et al., 2015). Details of the IOB format can be seen in the appendix, section C. All task dataset sizes, evaluation metrics, and number of classes are shown in Table 1. Note that our two de-identification (de-ID) datasets present synthetically-masked PHI in their texts-e.g., they replace instances of real names, hospitals, etc., with synthetic, but consistent and realistic, names, hospitals, etc. As a result, they present significantly different text distributions than traditionally de-identified text (such as MIMIC notes) which will instead present sentinel \"PHI\" symbols at locations where PHI was removed.\n\n4 Results & Discussions\nIn this section, we will first describe quantitative comparisons of the various BERT models on the clinical NLP tasks we considered, and second describe qualitative evaluations of the differences between Clinical-and Bio-BERT.\nClinical NLP Tasks Full results are shown in Table 2. On three of the five tasks (MedNLI, i2b2 2010, and i2b2 2012), clinically fine-tuned BioBERT shows improvements over BioBERT or general BERT. Notably, on MedNLI, clinical BERT actually yields a new state of the art, yielding a performance of 82.7% accuracy as compared to the prior state of the art of 73.5% (Romanov and Shivade, 2018) obtained via the InferSent model (Conneau et al., 2017). However, on our two de-ID tasks, i2b2 2006 and i2b2 2014, clinical BERT offers no improvements over Bio-or general BERT. This is actually not surprising, and is instead, we argue, a direct consequence of the na- ture of de-ID challenges. De-ID challenge data presents a different data distribution than MIMIC text. In MIMIC, PHI is identified and replaced with sentinel PHI markers, whereas in the de-ID task, PHI is masked with synthetic, but realistic PHI. This data drift would be problematic for any embedding model, but will be especially damaging to contextual embedding models like BERT because the underlying sentence structure will have changed: in raw MIMIC, sentences with PHI will universally have a sentinel PHI token. In contrast, in the de-ID corpus, all such sentences will have different synthetic masks, meaning that a canonical, nearly constant sentence structure present during BERT's training will be non-existent at task-time. For these reasons, we think it is sensible that clinical BERT is not successful on the de-ID corpora. Furthermore, this is a good example for the community given how prevalent the assumption is that contextual embedding models trained on task-like corpora will offer dramatic improvements.\nOverall, we feel our results demonstrates the utility of using domain-specific contextual embeddings for non de-ID clinical NLP tasks. Additionally, on one task Discharge Summary BERT offers performance improvements over Clinical BERT, so it may be that adding greater specificity to the underlying corpus is helpful in some cases. We release both models with this work for public use.  3 shows the nearest neighbors for 3 words each from 3 categories under BioBERT and Clinical BERT. These lists suggest that Clinical BERT retains greater cohesion around medical or clinicoperations relevant terms than does BioBERT. For example, the word \"Discharge\" is most closely associated with \"admission,\" \"wave,\" and \"sight\" under BioBERT, yet only the former seems relevant to clinical operations. In contrast, under Clinical BERT, the associated words all are meaningful in a clinical operations context.\n\nQualitative Embedding Comparisons Table\nLimitations & Future Work This work has several notable limitations. First, we do not experiment with any more advanced model architectures atop our embeddings. This likely hurts our performance. Second, MIMIC only contains notes from the intensive care unit of a single healthcare institution (BIDMC). Differences in care practices across institutions are significant, and using notes from multiple institutions could offer significant gains. Lastly, our model shows no improvements for either de-ID task we explored. If our hypothesis is correct as to its cause, a possible solution could entail introducing synthetic de-ID into the source clinical text and using that as the source for de-ID tasks going forward.\nIn this work, we pretrain and release clinically oriented BERT models, some trained solely on clinical text, and others fine-tuned atop BioBERT. We find robust evidence that our clinical embeddings are superior to general domain or BioBERT specific embeddings for non de-ID tasks, and that using note-type specific corpora can induce further selective performance benefits. To the best of our knowledge, our work is the first to release clinically trained BERT models. Our hope is that all clinical NLP researchers will be able to benefit from these embeddings without the necessity of the significant computational resources required to train these models over the MIMIC corpus.\n\nA MIMIC Notes\nMIMIC notes are distributed among 15 note types (Figure 1). Many note types are semi-structured, with section headers separating free text paragraphs. To process these notes, we split all notes into sections, then used Scispacy (Neumann et al., 2019)\n\nB BERT Training Details\nFor all pre-training experiments, we leverage the tensorflow implementation of BERT (Devlin et al., 2018). 5\n\nB.1 Pre-training\nWe used a batch size of 32, a maximum sequence length of 128, and a learning rate of 5 \u2022 10 \u22125 for pre-training our models. Models were trained for 150,000 steps. We experimented with models pretrained for 300,000 steps, but we found no significant differences in downstream task performance with these models. The dup factor for duplicating input data with different masks was set to 5. All other default parameters were used (specifically, masked language model probability = 0.15 and max predictions per sequence = 20).\n\nB.2 Fine-tuning\nFor all downstream tasks, we explored the following hyperparameters: learning rate \u2208 {2 \u2022 10 \u22125 , 3 \u2022 5 https://github.com/google-research/bert 10 \u22125 , 5 \u2022 10 \u22125 }, batch size \u2208 {16, 32}, and epochs \u2208 {3, 4}. For the NER tasks, we also tried epoch \u2208 {2}. The maximum sequence length was 150 across all tasks. Due to time constraints, only 2 epochs were run for the i2b2 2014 task.\n\nC IOB Format\nThe IOB (Inside-Outside-Beginning) format (Ramshaw and Marcus, 1995) is a method of encoding span-based NER tasks to add more granularity to the label space over span positions, specifically re-classifying each class as having three subclasses:\nInside (I-) This label is used to specify words within a span for this class.\n\nOutside (O)\nThis label is used to specify words outside any span for this class. This label will be shared across all classes and will replace the \"no class\" label applied to extraneous words.\nBeginning (B-) This label is used to specify words at the beginning of a span for this class.\nFor example, if the input text, with span labels is given as \"The patient is very sick.\" with NER labels \"Null Null Null Problem Problem\" we could convert this into IOB format via \"O O O B-Problem I-Problem\"\n\nFootnotes:\n1: github.com/EmilyAlsentzer/clinicalBERT\n2: Discharge summaries are commonly used in downstream tasks.\n3: https://www.ncbi.nlm.nih.gov/pubmed/\n4: https://www.ncbi.nlm.nih.gov/pmc/\n\nReferences:\n\n- Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.- Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Su- pervised Learning of Universal Sentence Repre- sentations from Natural Language Inference Data. arXiv:1705.02364 [cs]. ArXiv: 1705.02364.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. ArXiv: 1810.04805.\n\n- Noemie Elhadad, Sameer Pradhan, Sharon Gorman, Suresh Manandhar, Wendy Chapman, and Guergana Savova. SemEval-2015 Task 14: Analysis of Clini- cal Text. page 8.\n\n- Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. arXiv:1801.06146 [cs, stat]. ArXiv: 1801.06146.\n\n- Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li- wei H. Lehman, Mengling Feng, Mohammad Ghas- semi, Benjamin Moody, Peter Szolovits, Leo An- thony Celi, and Roger G. Mark. 2016. MIMIC-III, a freely accessible critical care database. Scientific Data, 3:160035.\n\n- Kaung Khin, Philipp Burckhardt, and Rema Pad- man. 2018. A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation. arXiv:1810.01570 [cs]. ArXiv: 1810.01570.\n\n- Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. arXiv:1901.08746 [cs].\n\n- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.\n\n- Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and Robust Mod- els for Biomedical Natural Language Processing. arXiv:1902.07669 [cs]. ArXiv: 1902.07669.\n\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532-1543.\n\n- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv:1802.05365 [cs]. ArXiv: 1802.05365.\n\n- Sameer Pradhan, Nomie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova. 2014. SemEval-2014 Task 7: Analysis of Clinical Text. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54-62, Dublin, Ireland. Association for Computational Lin- guistics and Dublin City University.\n\n- Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking using Transformation-Based Learning. arXiv:cmp-lg/9505040. ArXiv: cmp-lg/9505040.\n\n- Alexey Romanov and Chaitanya Shivade. 2018. Lessons from Natural Language Inference in the Clinical Domain. arXiv:1808.06752 [cs]. ArXiv: 1808.06752.\n\n- Yuqi Si, Jingqi Wang, Hua Xu, and Kirk Roberts. 2019. Enhancing Clinical Concept Extraction with Con- textual Embedding. arXiv:1902.08691 [cs]. ArXiv: 1902.08691.\n\n- Amber Stubbs, Christopher Kotfila, and zlem Uzuner. 2015. Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/UTHealth shared task Track 1. Journal of Biomedical Informatics, 58 Suppl:S11-19.\n\n- Amber Stubbs and zlem Uzuner. 2015. Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus. Journal of Biomedical Informatics, 58 Suppl:S20-29.\n\n- Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a. Annotating temporal information in clinical narratives. Journal of Biomedical Informatics, 46 Suppl:S5-12.\n\n- Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b. Evaluating temporal relations in clini- cal text: 2012 i2b2 Challenge. Journal of the American Medical Informatics Association: JAMIA, 20(5):806-813.\n\n- Ozlem Uzuner, Yuan Luo, and Peter Szolovits. 2007. Evaluating the state-of-the-art in automatic de- identification. Journal of the American Medical In- formatics Association: JAMIA, 14(5):550-563.\n\n- zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association : JAMIA, 18(5):552-556.\n\n- Henghui Zhu, Ioannis Ch Paschalidis, and Amir Tah- masebi. 2018. Clinical Concept Extraction with Contextual Word Embedding. arXiv:1810.10566 [cs]. ArXiv: 1810.10566.\n\n", "annotations": {"Abstract": [{"begin": 45, "end": 1011, "idx": 0}], "Head": [{"begin": 1014, "end": 1028, "n": "1", "idx": 0}, {"begin": 2657, "end": 2671, "n": "2", "idx": 1}, {"begin": 3953, "end": 3996, "idx": 2}, {"begin": 5899, "end": 5908, "n": "3", "idx": 3}, {"begin": 6059, "end": 6067, "n": "3.1", "idx": 4}, {"begin": 7232, "end": 7249, "n": "3.2", "idx": 5}, {"begin": 9099, "end": 9108, "n": "3.3", "idx": 6}, {"begin": 10266, "end": 10289, "n": "4", "idx": 7}, {"begin": 13103, "end": 13142, "idx": 8}, {"begin": 14540, "end": 14553, "idx": 9}, {"begin": 14806, "end": 14829, "idx": 10}, {"begin": 14940, "end": 14956, "idx": 11}, {"begin": 15481, "end": 15496, "idx": 12}, {"begin": 15879, "end": 15891, "idx": 13}, {"begin": 16216, "end": 16227, "idx": 14}], "ReferenceToBib": [{"begin": 101, "end": 122, "target": "#b11", "idx": 0}, {"begin": 132, "end": 153, "target": "#b2", "idx": 1}, {"begin": 1204, "end": 1225, "target": "#b11", "idx": 2}, {"begin": 1234, "end": 1258, "target": "#b4", "idx": 3}, {"begin": 1269, "end": 1290, "target": "#b2", "idx": 4}, {"begin": 1403, "end": 1421, "target": "#b7", "idx": 5}, {"begin": 2284, "end": 2305, "target": "#b21", "idx": 6}, {"begin": 2317, "end": 2338, "idx": 7}, {"begin": 2351, "end": 2378, "target": "#b14", "idx": 8}, {"begin": 2429, "end": 2450, "target": "#b20", "idx": 9}, {"begin": 2465, "end": 2486, "idx": 10}, {"begin": 2487, "end": 2511, "idx": 11}, {"begin": 2769, "end": 2791, "target": "#b8", "idx": 12}, {"begin": 2799, "end": 2824, "target": "#b10", "idx": 13}, {"begin": 2839, "end": 2864, "target": "#b0", "idx": 14}, {"begin": 3044, "end": 3065, "target": "#b11", "idx": 15}, {"begin": 3075, "end": 3096, "target": "#b2", "idx": 16}, {"begin": 3825, "end": 3842, "target": "#b15", "idx": 17}, {"begin": 4106, "end": 4123, "target": "#b7", "idx": 18}, {"begin": 4460, "end": 4479, "target": "#b6", "idx": 19}, {"begin": 4643, "end": 4668, "idx": 20}, {"begin": 4669, "end": 4689, "idx": 21}, {"begin": 4847, "end": 4864, "target": "#b22", "idx": 22}, {"begin": 5069, "end": 5090, "target": "#b21", "idx": 23}, {"begin": 5244, "end": 5261, "target": "#b15", "idx": 24}, {"begin": 5489, "end": 5510, "idx": 25}, {"begin": 5539, "end": 5561, "target": "#b12", "idx": 26}, {"begin": 6159, "end": 6181, "target": "#b5", "idx": 27}, {"begin": 8065, "end": 8082, "target": "#b15", "idx": 28}, {"begin": 9214, "end": 9241, "target": "#b14", "idx": 29}, {"begin": 9312, "end": 9338, "target": "#b13", "idx": 30}, {"begin": 9372, "end": 9393, "target": "#b20", "idx": 31}, {"begin": 9424, "end": 9445, "target": "#b21", "idx": 32}, {"begin": 9553, "end": 9574, "idx": 33}, {"begin": 9617, "end": 9642, "idx": 34}, {"begin": 9643, "end": 9663, "idx": 35}, {"begin": 10879, "end": 10906, "target": "#b14", "idx": 36}, {"begin": 10940, "end": 10962, "target": "#b1", "idx": 37}, {"begin": 14782, "end": 14804, "target": "#b9", "idx": 38}, {"begin": 14914, "end": 14935, "target": "#b2", "idx": 39}, {"begin": 15934, "end": 15960, "target": "#b13", "idx": 40}], "ReferenceToFootnote": [{"begin": 1721, "end": 1722, "target": "#foot_0", "idx": 0}, {"begin": 2047, "end": 2048, "target": "#foot_1", "idx": 1}, {"begin": 4211, "end": 4212, "target": "#foot_2", "idx": 2}, {"begin": 4250, "end": 4251, "target": "#foot_3", "idx": 3}], "SectionFootnote": [{"begin": 16712, "end": 16903, "idx": 0}], "ReferenceString": [{"begin": 16920, "end": 17117, "id": "b0", "idx": 0}, {"begin": 17119, "end": 17346, "id": "b1", "idx": 1}, {"begin": 17350, "end": 17545, "id": "b2", "idx": 2}, {"begin": 17549, "end": 17708, "id": "b3", "idx": 3}, {"begin": 17712, "end": 17862, "id": "b4", "idx": 4}, {"begin": 17866, "end": 18129, "id": "b5", "idx": 5}, {"begin": 18133, "end": 18331, "id": "b6", "idx": 6}, {"begin": 18335, "end": 18551, "id": "b7", "idx": 7}, {"begin": 18555, "end": 18786, "id": "b8", "idx": 8}, {"begin": 18790, "end": 18973, "id": "b9", "idx": 9}, {"begin": 18977, "end": 19210, "id": "b10", "idx": 10}, {"begin": 19214, "end": 19416, "id": "b11", "idx": 11}, {"begin": 19420, "end": 19748, "id": "b12", "idx": 12}, {"begin": 19752, "end": 19894, "id": "b13", "idx": 13}, {"begin": 19898, "end": 20047, "id": "b14", "idx": 14}, {"begin": 20051, "end": 20213, "id": "b15", "idx": 15}, {"begin": 20217, "end": 20459, "id": "b16", "idx": 16}, {"begin": 20463, "end": 20648, "id": "b17", "idx": 17}, {"begin": 20652, "end": 20810, "id": "b18", "idx": 18}, {"begin": 20814, "end": 21015, "id": "b19", "idx": 19}, {"begin": 21019, "end": 21215, "id": "b20", "idx": 20}, {"begin": 21219, "end": 21446, "id": "b21", "idx": 21}, {"begin": 21450, "end": 21616, "id": "b22", "idx": 22}], "ReferenceToTable": [{"begin": 9816, "end": 9817, "target": "#tab_0", "idx": 0}, {"begin": 10568, "end": 10569, "target": "#tab_1", "idx": 1}, {"begin": 12590, "end": 12591, "target": "#tab_2", "idx": 2}], "Footnote": [{"begin": 16723, "end": 16764, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 16765, "end": 16826, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 16827, "end": 16866, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 16867, "end": 16903, "id": "foot_3", "n": "4", "idx": 3}], "Paragraph": [{"begin": 55, "end": 1011, "idx": 0}, {"begin": 1029, "end": 1291, "idx": 1}, {"begin": 1292, "end": 1643, "idx": 2}, {"begin": 1644, "end": 1857, "idx": 3}, {"begin": 1858, "end": 1909, "idx": 4}, {"begin": 1910, "end": 2048, "idx": 5}, {"begin": 2049, "end": 2655, "idx": 6}, {"begin": 2672, "end": 3660, "idx": 7}, {"begin": 3661, "end": 3951, "idx": 8}, {"begin": 3997, "end": 4441, "idx": 9}, {"begin": 4442, "end": 4764, "idx": 10}, {"begin": 4765, "end": 5897, "idx": 11}, {"begin": 5909, "end": 6057, "idx": 12}, {"begin": 6068, "end": 6508, "idx": 13}, {"begin": 6509, "end": 6772, "idx": 14}, {"begin": 6773, "end": 7230, "idx": 15}, {"begin": 7250, "end": 7548, "idx": 16}, {"begin": 7549, "end": 8449, "idx": 17}, {"begin": 8450, "end": 9097, "idx": 18}, {"begin": 9109, "end": 9537, "idx": 19}, {"begin": 9538, "end": 10264, "idx": 20}, {"begin": 10290, "end": 10516, "idx": 21}, {"begin": 10517, "end": 12202, "idx": 22}, {"begin": 12203, "end": 13101, "idx": 23}, {"begin": 13143, "end": 13858, "idx": 24}, {"begin": 13859, "end": 14538, "idx": 25}, {"begin": 14554, "end": 14804, "idx": 26}, {"begin": 14830, "end": 14938, "idx": 27}, {"begin": 14957, "end": 15479, "idx": 28}, {"begin": 15497, "end": 15877, "idx": 29}, {"begin": 15892, "end": 16136, "idx": 30}, {"begin": 16137, "end": 16214, "idx": 31}, {"begin": 16228, "end": 16408, "idx": 32}, {"begin": 16409, "end": 16502, "idx": 33}, {"begin": 16503, "end": 16710, "idx": 34}], "SectionHeader": [{"begin": 0, "end": 1011, "idx": 0}], "SectionReference": [{"begin": 16905, "end": 21618, "idx": 0}], "Sentence": [{"begin": 55, "end": 259, "idx": 0}, {"begin": 260, "end": 446, "idx": 1}, {"begin": 447, "end": 619, "idx": 2}, {"begin": 620, "end": 775, "idx": 3}, {"begin": 776, "end": 1011, "idx": 4}, {"begin": 1029, "end": 1291, "idx": 5}, {"begin": 1292, "end": 1422, "idx": 6}, {"begin": 1423, "end": 1643, "idx": 7}, {"begin": 1644, "end": 1857, "idx": 8}, {"begin": 1858, "end": 1909, "idx": 9}, {"begin": 1910, "end": 2048, "idx": 10}, {"begin": 2049, "end": 2380, "idx": 11}, {"begin": 2381, "end": 2655, "idx": 12}, {"begin": 2672, "end": 3013, "idx": 13}, {"begin": 3014, "end": 3175, "idx": 14}, {"begin": 3176, "end": 3357, "idx": 15}, {"begin": 3358, "end": 3471, "idx": 16}, {"begin": 3472, "end": 3660, "idx": 17}, {"begin": 3661, "end": 3843, "idx": 18}, {"begin": 3844, "end": 3951, "idx": 19}, {"begin": 3997, "end": 4097, "idx": 20}, {"begin": 4098, "end": 4271, "idx": 21}, {"begin": 4272, "end": 4441, "idx": 22}, {"begin": 4442, "end": 4764, "idx": 23}, {"begin": 4765, "end": 4845, "idx": 24}, {"begin": 4846, "end": 5091, "idx": 25}, {"begin": 5092, "end": 5242, "idx": 26}, {"begin": 5243, "end": 5664, "idx": 27}, {"begin": 5665, "end": 5897, "idx": 28}, {"begin": 5909, "end": 6057, "idx": 29}, {"begin": 6068, "end": 6182, "idx": 30}, {"begin": 6183, "end": 6417, "idx": 31}, {"begin": 6418, "end": 6508, "idx": 32}, {"begin": 6509, "end": 6772, "idx": 33}, {"begin": 6773, "end": 7093, "idx": 34}, {"begin": 7094, "end": 7230, "idx": 35}, {"begin": 7250, "end": 7470, "idx": 36}, {"begin": 7471, "end": 7548, "idx": 37}, {"begin": 7549, "end": 7690, "idx": 38}, {"begin": 7691, "end": 7964, "idx": 39}, {"begin": 7965, "end": 8083, "idx": 40}, {"begin": 8084, "end": 8306, "idx": 41}, {"begin": 8307, "end": 8449, "idx": 42}, {"begin": 8450, "end": 8558, "idx": 43}, {"begin": 8559, "end": 8772, "idx": 44}, {"begin": 8773, "end": 8879, "idx": 45}, {"begin": 8880, "end": 8995, "idx": 46}, {"begin": 8996, "end": 9097, "idx": 47}, {"begin": 9109, "end": 9446, "idx": 48}, {"begin": 9447, "end": 9537, "idx": 49}, {"begin": 9538, "end": 9664, "idx": 50}, {"begin": 9665, "end": 9818, "idx": 51}, {"begin": 9819, "end": 10049, "idx": 52}, {"begin": 10050, "end": 10264, "idx": 53}, {"begin": 10290, "end": 10516, "idx": 54}, {"begin": 10517, "end": 10712, "idx": 55}, {"begin": 10713, "end": 10963, "idx": 56}, {"begin": 10964, "end": 11084, "idx": 57}, {"begin": 11085, "end": 11201, "idx": 58}, {"begin": 11202, "end": 11278, "idx": 59}, {"begin": 11279, "end": 11422, "idx": 60}, {"begin": 11423, "end": 11695, "idx": 61}, {"begin": 11696, "end": 11912, "idx": 62}, {"begin": 11913, "end": 12014, "idx": 63}, {"begin": 12015, "end": 12202, "idx": 64}, {"begin": 12203, "end": 12337, "idx": 65}, {"begin": 12338, "end": 12534, "idx": 66}, {"begin": 12535, "end": 12588, "idx": 67}, {"begin": 12589, "end": 12687, "idx": 68}, {"begin": 12688, "end": 12820, "idx": 69}, {"begin": 12821, "end": 12993, "idx": 70}, {"begin": 12994, "end": 13101, "idx": 71}, {"begin": 13143, "end": 13211, "idx": 72}, {"begin": 13212, "end": 13303, "idx": 73}, {"begin": 13304, "end": 13338, "idx": 74}, {"begin": 13339, "end": 13445, "idx": 75}, {"begin": 13446, "end": 13586, "idx": 76}, {"begin": 13587, "end": 13661, "idx": 77}, {"begin": 13662, "end": 13858, "idx": 78}, {"begin": 13859, "end": 14003, "idx": 79}, {"begin": 14004, "end": 14232, "idx": 80}, {"begin": 14233, "end": 14327, "idx": 81}, {"begin": 14328, "end": 14538, "idx": 82}, {"begin": 14554, "end": 14613, "idx": 83}, {"begin": 14614, "end": 14704, "idx": 84}, {"begin": 14705, "end": 14804, "idx": 85}, {"begin": 14830, "end": 14938, "idx": 86}, {"begin": 14957, "end": 15080, "idx": 87}, {"begin": 15081, "end": 15119, "idx": 88}, {"begin": 15120, "end": 15267, "idx": 89}, {"begin": 15268, "end": 15479, "idx": 90}, {"begin": 15497, "end": 15640, "idx": 91}, {"begin": 15641, "end": 15705, "idx": 92}, {"begin": 15706, "end": 15751, "idx": 93}, {"begin": 15752, "end": 15805, "idx": 94}, {"begin": 15806, "end": 15877, "idx": 95}, {"begin": 15892, "end": 16136, "idx": 96}, {"begin": 16137, "end": 16214, "idx": 97}, {"begin": 16228, "end": 16296, "idx": 98}, {"begin": 16297, "end": 16408, "idx": 99}, {"begin": 16409, "end": 16502, "idx": 100}, {"begin": 16503, "end": 16710, "idx": 101}], "ReferenceToFigure": [{"begin": 14610, "end": 14611, "target": "#fig_0", "idx": 0}], "Div": [{"begin": 55, "end": 1011, "idx": 0}, {"begin": 1014, "end": 2655, "idx": 1}, {"begin": 2657, "end": 3951, "idx": 2}, {"begin": 3953, "end": 5897, "idx": 3}, {"begin": 5899, "end": 6057, "idx": 4}, {"begin": 6059, "end": 7230, "idx": 5}, {"begin": 7232, "end": 9097, "idx": 6}, {"begin": 9099, "end": 10264, "idx": 7}, {"begin": 10266, "end": 13101, "idx": 8}, {"begin": 13103, "end": 14538, "idx": 9}, {"begin": 14540, "end": 14804, "idx": 10}, {"begin": 14806, "end": 14938, "idx": 11}, {"begin": 14940, "end": 15479, "idx": 12}, {"begin": 15481, "end": 15877, "idx": 13}, {"begin": 15879, "end": 16214, "idx": 14}, {"begin": 16216, "end": 16710, "idx": 15}], "SectionMain": [{"begin": 1011, "end": 16710, "idx": 0}]}}