{"text": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\n\nAbstract:\nOpen-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to \"back-off\" to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.\n\n\n1 Introduction\nOpen-domain QA (ODQA) systems usually have access to a background corpus that can be used to answer questions. Models which explicitly exploit this corpus are commonly referred to as Open-book models (Roberts et al., 2020). They typically index the whole corpus, and then retrieve-and-read documents in order to answer questions on-the-fly (Chen et al., 2017; Lee et al., 2019a, inter alia).\nA second class of models, closed-book question answering (CBQA) models, have recently been proposed. They learn to directly map questions to answers from training question-answer (QA) pairs without access to a background corpus (Roberts et al., 2020; Ye et al., 2021). These models usually take the form of pretrained seq2seq models such as T5 (Raffel et al., 2020) or BART (Lewis et al., 2019a), fine-tuned on QA-pairs. It has recently been shown that current closed-book models mostly memorise training QA-pairs, and can struggle to answer questions that do not overlap with training data (Lewis et al., 2020b).\nModels which explicitly retrieve (training) QApairs, rather than memorizing them in parameters, have been shown to perform competitively with CBQA models (Lewis et al., 2020b; Xiao et al., 2020). These models have a number of useful properties, such as fast inference, interpretable outputs (by inspecting retrieved QA-pairs), and the ability to update the model's knowledge at test time by adding or removing QA-pairs. However, CBQA and QA-pair retriever models are currently not competitive with retrieve-and-read systems in terms of accuracy, largely because the training QA-pairs they operate on cover substantially less knowledge than background corpora like Wikipedia. In this paper, we explore whether massively expanding the coverage of QA-pairs enables CBQA and QA-pair retriever models which are competitive with retrieve-and-read models.\nWe present Probably Asked Questions (PAQ), a semi-structured Knowledge Base (KB) of 65M natural language QA-pairs, which models can memorise and/or learn to retrieve from. PAQ differs from traditional KBs in that questions and answers are stored in natural language, and that questions are generated such that they are likely to appear in ODQA datasets. PAQ is automatically constructed using a question generation model and Wikipedia. To ensure generated questions are not only answerable given the passage they are generated from, we employ a global filtering post-processing step employing a state-of-the-art ODQA system. This greatly reduces the amount of wrong and ambiguous questions compared other approaches (Fang et al., 2020; Alberti et al., 2019), and is critical for high-accuracy, downstream QA models.\nTo complement PAQ we develop RePAQ, a question answering model based on question retrieval/matching models, using dense Maximum Inner Product Search-based retrieval, and optionally, re-ranking. We show that PAQ and RePAQ provide accurate ODQA predictions, at the level of relatively recent large-scale retrieve-and-read systems such as RAG (Lewis et al., 2020a) on Nat-uralQuestions (Kwiatkowski et al., 2019a) and Triv-iaQA (Joshi et al., 2017). PAQ instances are annotated with scores that reflect how likely we expect questions to appear, which can be used to control the memory footprint of RePAQ by filtering the KB accordingly. As a result, RePAQ is extremely flexible, allowing us to configure QA systems with near state-of-the-art results, very small memory size, or inference speeds of over 1,000 questions per second. Memory-optimised configurations of RePAQ won two of the four tracks of the 2020 Efficien-tQA NeurIPS competition (Min et al., 2020a), with system sizes of 336MB and 29MB, respectively.\nWe also show that PAQ is a useful source of training data for CBQA models. BART models trained on PAQ outperform baselines trained on standard data by 5%. However, these models struggle to effectively memorise all the knowledge in PAQ, lagging behind RePAQ by 15%. This demonstrates the effectiveness of RePAQ at leveraging PAQ.\nFinally, we show that since RePAQ's question matching score correlates well with QA accuracy, it effectively \"knows when it doesn't know\", allowing for selective question answering (Rodriguez et al., 2019) where QA systems may abstain from answering if confidence is too low. Whilst answer abstaining is important in its own right, it also enables an elegant \"back-off\" approach where we can defer to a more accurate but expensive QA system when answer confidence is low. This enables us to make use of the best of both speed and accuracy.\nIn summary, we make the following contribu-tions: i) introduce PAQ, 65M QA-pairs automatically generated from Wikipedia, and demonstrate the importance of global filtering for high quality ii) introduce RePAQ, a QA system designed to utilize PAQ and demonstrate how it can be optimised for memory, speed or accuracy iii) investigate the utility of PAQ for CBQA models, improving by 5% but note significant headroom to RePAQ iv) demonstrate RePAQ's strength on selective QA, enabling us to combine RePAQ with a state-of-the-art QA model, making it both more accurate and 2x faster 1 2 Open-Domain Question Answering ODQA is the task of answering natural language factoid question from an open set of domains. A typical question might be \"when was the last year astronauts landed on the moon?\", with a target answer \"1972\". The goal of ODQA is to develop an answer function m : Q \u2192 A, where Q and A respectively are the sets of all possible questions and answers. We assume there is a distribution P (q, a) of QA-pairs, defined over Q \u00d7 A. A good answer function will minimise the expected error over P (q, a) with respect to some loss function, such as answer string match. In practice, we do not have access to P (q, a), and instead rely on an empirical sample of QA-pairs K drawn from P , and measure the empirical loss of answer functions on K. Our goal in this work is to implicitly model P (q, a) so that we can draw a large sample of QApairs, PAQ, which we can train on and/or retrieve from. Drawing a sufficiently large sample will overlap with K, essentially pre-empting and caching questions that humans may ask at test-time. This allows us to shift computation from test-time to train-time compared to retrieve-and-read methods.\n\n3 Generating Question-Answer Pairs\nIn this section, we describe the process for generating PAQ. Given a large background corpus C, our QA-pair generation process consists of the following components:\n1. A passage selection model p s (c), to identify passages which humans are likely to ask questions about.\n2. An answer extraction model p a (a | c), for identifying spans in a passage that are more likely to be answers to a question. 3. A question generator model p q (q | a, c) that, given a passage and an answer, generates a question.\n4. A filtering QA model p f (a | q, C) that generates an answer for a given question. If an answer generated by p f does not match the answer a question was generated from, the question is discarded. This ensures generated questions are consistent (Alberti et al., 2019).\nAs shown in Fig. 1, these models are applied sequentially to generate QA-pairs for PAQ, in a similar manner to related work in contextual QA generation (Alberti et al., 2019; Lewis et al., 2019b). First a passage c is selected with a high probability according to p s . Next, candidate answers a are extracted from c using p a , and questions q are generated for each answer in the passage using p q . Lastly, p f generates a new answer a for the question. If the source answer a matches a , then (q, a) is deemed consistent and is added to PAQ. In the following, we describe each component in detail.\n\n3.1 Passage Selection, p s\nThe passage selection model p s is used to find passages which are likely to contain information that humans may ask about, and would thus be good candidates to generate questions from. We learn p s using a similar method to Karpukhin et al. (2020b). Concretely, we assume access to a set of positive passages C + \u2282 C, which we obtain from answercontaining passages from an ODQA training set. Since we do not have a set of labelled negative passages, we sample negatives from the corpus, ei-ther randomly or using heuristics. We then train a model to minimise negative log-likelihood of positive passages relative to negatives.\nWe implement p s with RoBERTa (Liu et al., 2019a) and obtain positive passages from Natural Questions (NQ, Kwiatkowski et al., 2019b). We sample easy negatives at random from Wikipedia, and hard negatives from the same Wikipedia article as the positive passage. Easy negatives help the model to learn topics of interest, and hard negatives help to differentiate between interesting and non-interesting passages from the same article. We evaluate by measuring how highly positive validation passages are ranked amongst negatives.\n\n3.2 Answer Extraction, p a\nGiven a passage, this component identifies spans that are likely to be answers to questions. We consider two alternatives: an off-the-shelf Named Entity Recogniser (NER) or training a BERT (Devlin et al., 2019) answer extraction model on NQ.\nThe NER answer extractor simply extracts all named entities from a passage. 2 The majority of questions in ODQA datasets consist of entity mentions (Kwiatkowski et al., 2019a; Joshi et al., 2017), so this approach can achieve high answer coverage. However, as we extract all entity mentions in a passage, we may extract unsuitable mentions, or miss answers that do not conform to the NER system's annotation schema. The trained answer span extractor aims to address these issues.\nBERT answer span extraction is typically performed by modelling answer start and end independently, obtaining answer probabilities via p a (a | c) = p(a start | c) \u00d7 p(a end | c) (Devlin et al., 2019). We found this approach be sub-optimal for modelling multiple span occurrences in a passage. We instead use an approach that breaks the conditional independence of answer spans by directly predicting p a (a | c) = p([a start , a end ] | c). This model first feeds a passage through BERT, before concatenating the start and end token representations of all possible spans of up to length 30, before feeding them into a MLP to compute p a (a | c). At generation time, the answer extraction component extracts a constant number of spans from each passage, ranked by their extraction probabilities.\n\n3.3 Question Generation, p q\nGiven a passage and an answer, this component generates likely questions with that answer. To indicate the answer and its occurrence in the passage, we prepend the answer to the passage and label the answer span with surrounding special tokens. We construct the dataset from NQ, TriviaQA, and SQuAD, and perform standard fine-tuning of BART-base (Lewis et al., 2019a) to obtain p q .\n\n3.4 Filtering, p f\nThe filtering model p f improves the quality of generated questions, by ensuring that they are consistent: that the answer they were generated is likely to be a valid answer to the question. Previous work (Alberti et al., 2019; Fang et al., 2020) has employed a machine reading comprehension (MRC) QA model for this purpose, p f (a | q, c), which produces an answer when supplied with a question and the passage it was generated from. We refer to this as local filtering. However, local filtering will not remove questions which are ambiguous (Min et al., 2020b), and can only be answered correctly with access to the source passage. Thus, we use an ODQA model for filtering, p f (a | q, C), supplied with only the generated question, and not the source passage. We refer to this as global filtering, and later show that it is vital for strong downstream results. We use FiD-base with 50 retrieved passages, trained on NQ (Izacard and Grave, 2020).\n\n4 Question Answering using PAQ\nWe consider two uses of PAQ for building QA models. The first is to use PAQ as a source of training QA-pairs for CBQA models. The second treats PAQ as a KB, which models learn to directly retrieve from. These use-cases are related, as CBQA models have been shown to memorise the train data in their parameters, latently retrieving from them at test time (Lewis et al., 2020b; Domingos, 2020).\n\n4.1 PAQ for Closed-Book QA\nWe fine-tune a BART-large (Lewis et al., 2019a) with QA-pairs from the concatenation of the training data and PAQ, using a similar training procedure to Roberts et al. (2020). We use early stopping on the validation set and a batch size of 512, and note learning is slow, requiring 70 epochs on PAQ. Following recent best practices (Alberti et al., 2019; Yang et al., 2019), we then fine-tune on the training QA-pairs only, using validation Exact Match score for early stopping (Rajpurkar et al., 2018).\nWe note that an effective CBQA model must be able to understand the semantics of questions and how to generate answers, in addition to being able to store a large number of facts in its parameters. This model thus represents a kind of combined parametric knowledgebase and retrieval system (Petroni et al., 2020a). The model proposed in the next section, RePAQ, represents an explicit non-parametric instantiation of this idea.\n\n4.2 RePAQ\nRePAQ is a retrieval model which operates on KBs of QA-pairs, such as PAQ. RePAQ extends recently proposed nearest neighbour QA-pair retriever models (Lewis et al., 2020b; Xiao et al., 2020). These models assume access to a KB of N QA-pairs K = {(q 1 , a 1 )...(q N , a N )}. These models provide an answer to a test question q by finding the most relevant QA-pair (q , a ) in K, using a scalable relevance function, then returning a as the answer to q. Such a function could be implemented using standard information retrieval techniques, (e.g. TF-IDF) or learnt from training data. RePAQ is learnt from ODQA data and consists of a neural dense retriever, optionally followed by a neural reranker.\n\n4.2.1 RePAQ Retriever\nOur retriever adopts the dense Maximum Inner Product Search (MIPS) retriever paradigm, that has recently been shown to obtain state-of-the-art results in a number of settings (Karpukhin et al., 2020b; Lee et al., 2021, inter alia). Our goal is to embed queries q and indexed items d into a representation space via embedding functions g q and g d , so that the inner product g q (q) g d (d) is maximised for items relevant to q. In our case, queries are questions and indexed items are QA-pairs (q , a ). We make our retriever symmetric by embedding q rather than (q , a ), meaning that only one embedding function g q is required, which maps questions to embeddings. This applies a useful inductive bias, and we find that it aids stability during training.\nLearning the embedding function g q is complicated by the lack of labelled question pair paraphrases in ODQA datasets. We propose a latent variable approach similar to retrieval-augmented generation (RAG, Lewis et al., 2020b), 3 where we we index training QA-pairs rather than documents. For an input question q, the top K QApairs (q , a ) are retrieved by a retriever p ret where p ret (q|q ) \u221d exp(g q (q) g q (q )). These are then fed into a seq2seq model p gen which generates an answer for each retrieved QA-pair, before a final answer is produced by marginalising,p(a|q) = (a ,q )\u2208top-k pret(\u2022|q) p gen (a|q, q , a )p ret (q |q),\nAs p gen generates answers token-by-token, credit can be given for retrieving helpful QA-pairs which do not exactly match the target answer. For example, for the question \"when was the last time anyone was on the moon\" and target answer \"December 1972\", retrieving \"when was the last year astronauts landed on the moon\" with answer \"1972\" will help to generate the target answer, despite the answers having different granularity. After training, we discard p ret 4, retaining only the question embedder g. We implement p ret with ALBERT (Lan et al., 2020) with an output dimension of 768, and p gen with BART-large (Lewis et al., 2019a). We train using the top 100 retrieved QA-pairs, and refresh the embedding index every 5 training steps.\nOnce the embedder g q is trained, we build a test-time QA system by embedding and indexing a QA KB such as PAQ. Answering is achieved by retrieving the most similar stored question, and returning its answer. The matched QA-pair can be displayed to the user, providing a mechanism for more interpretable answers than CBQA models and many retrieve-and-read generators which consume thousands of tokens to generate an answer. Efficient MIPS libraries such as FAISS (Johnson et al., 2017) enable RePAQ's retriever to answer 100s to 1000s of questions per second (see Section 5.2.3). We use a KB for RePAQ consisting of training set QA-pairs concatenated with QA-pairs from PAQ.\n\n4.2.2 RePAQ Reranker\nThe accuracy of RePAQ can be improved using a reranker on the top-K QA-pairs from the retriever. The reranker uses cross-encoding (Humeau et al., 2020), and includes the retrieved answer in the scoring function for richer featurisation. We concatenate the input question q, the retrieved question q and its answer a with separator tokens, and feed it through ALBERT. We obtain training data in the following manner: For a training QA-pair, we first retrieve the top 2K QA-pairs from PAQ using RePAQ's retriever. If one of the retrieved QA-pairs has the correct answer, we treat that QA-pair as a positive, and randomly sample K-1 of the incorrect retrieved questions as negatives. We train by minimising negative log likelihood of positives relative to 10 negatives, and rerank 50 retrieved pairs at test time. The reranker improves accuracy at the expense of some speed. However, as QA-pairs consist of fewer tokens than passages, the reranker is still faster than retrieve-and-read models, even when using architectures such as ALBERT-xxlarge.\n\n5 Results\nWe first examine the PAQ resource in general, before exploring how both CBQA models and RePAQ perform using PAQ, comparing to recently published systems. We use the Natural Questions (NQ, Kwiatkowski et al., 2019b) and TriviaQA (Joshi et al., 2017) datasets to assess performance, evaluating with the standard Exact Match (EM) score.\n\n5.1 Examining PAQ\nWe generate PAQ by applying the pipeline described in Section 3 to the Wikipedia dump from Karpukhin et al. (2020a), which splits Wikipedia into 100-word passages. We use passage selection model p s to rank all 21M passages, and generate from the top 10M, before applying global filtering. 5 e are interested in understanding the effectiveness of different answer extractors, and whether generating more questions per answer span results leads to better results. To address these questions, we create three versions of PAQ, described below. PAQ L uses the learnt answer extractor, and a question generator trained on NQ and TriviaQA. We extract 8 answers per passage and use a beam size of 4 for question generation. In PAQ L,1 we only use the top scoring question in the beam, whereas in PAQ L,4 we use all four questions from the beam, allowing for several questions to be generated from one answer in a passage. PAQ N E,1 uses the NER answer extractor, and a generator trained only on NQ. PAQ N E,1 allow us assess whether diversity in the form of answer extractors and question generators leads to better results. The final KB, referred to as just \"PAQ\", is the union of PAQ L and PAQ N E . As shown in Table 1, PAQ consists of 65M filtered QA pairs. 6 This was obtained by extracting 165M answer spans and generating 279M unique questions before applying global filtering. Table 1 shows that the PAQ L pipeline is more efficient than PAQ N E , with 24.4% of QA-pairs surviving filtering, compared to 18%.\n\nPAQ Answer\nCoverage To evaluate answer extractors, we calculate how many answers in the validation sets of TriviaQA and NQ also occur in PAQ's filtered QA-pairs. 7 Table 1 shows that the answer coverage of PAQ is very high -over 90% for both TriviaQA and NQ. Comparing PAQ L with PAQ N E shows that the learnt extractor achieves higher coverage, but the union of the two leads to the highest coverage overall. Comparing PAQ L,1 and PAQ L,4 indicates that using more questions from the beam also results in higher coverage.\n\nPAQ Question Generation Quality\nIllustrative examples from PAQ can be seen in Table 2. Manual inspection of 50 questions from PAQ reveals that 82% of questions accurately capture information from the passage and contain sufficient details to locate the answer. 16% of questions confuse the semantics of certain answer types, either by conflating similar entities in the passage or by misinterpreting rare phrases (see examples 7 and 8 in Table 2). Finally, we find small numbers of grammar errors (such as example 5) and mismatched wh-words (5% and 2% respectively). 8\nOther observations PAQ often contains several paraphrases of the same QA-pair. This redundancy reflects how information is distributed in Wikipedia, with facts often mentioned on several different pages. Generating several questions per answer span also increases redundancy. Whilst this means that PAQ could be more information-dense if a de-duplication step was applied, we later show that RePAQ always improves with more questions in its KB (section 5.2.1). This suggests that it is worth increasing redundancy for greater coverage.\n\n5.2 Question Answering Results\nIn this section, we shall compare how the PAQleveraging models proposed in section 4 compare to existing approaches. We primarily compare to a state-of-the-art retrieve-and-read model, Fusionin-Decoder (FiD, Izacard and Grave, 2020). FiD uses DPR (Karpukhin et al., 2020b) to retrieve relevant passages from Wikipedia, and feeds them into T5 (Raffel et al., 2020) to generate a final answer.\nTable 3 shows the highest-accuracy configurations of our models alongside recent state-of-the-art approaches. We make the following observations: Comparing rows 2 and 7 shows that a CBQA BART model trained with PAQ outperforms a comparable NQ-only model by 5%, and only 3% behind T5-11B (row 1) which has 27x more parameters. Second, we note strong results for RePAQ on NQ (47.7%, row 9), outperforming recent retrieve-andread systems such as RAG by over 3% (row 4).\nMulti-task training RePAQ on NQ and TriviaQA improves TriviaQA results by 1-2% (comparing rows 8-9 with 10-11). RePAQ does not perform quite as strongly on TriviaQA (see section 5.2.6), but is within 5% of RAG, and outperforms concurrent work on real-time QA, DensePhrases (row 6, Lee et al., 2021). Lastly, row 12 shows that combining RePAQ and FiD-large into a combined system is 0.9% more accurate than FiD-large (see Section 5.2.4 for more details).\n\n5.2.1 Ablating PAQ using RePAQ\nTable 4 shows RePAQ's accuracy when using different PAQ variants. To establish the effect of filtering, we evaluate RePAQ with unfiltered, locally-filtered and globally-filtered QA-pairs on PAQ L,1 . Comparing rows 2, 3 and 4 shows that global filtering is crucial, leading to a 9% and 14% improvement over locally-filtered and unfiltered datasets respectively. We also note a general trend in Table 4 that adding more globally-filtered questions improves accuracy. Rows 4 and 5 show that using four questions per answer span is better than generating one (+0.9%), and Rows 5,6 and 7 show that combining PAQ N E and PAQ L results in a further 1.2% improvement. Empirically we did not observe any cases where increasing the number of globally filtered QA-pairs reduced accuracy, even when there were millions of QA-pairs already.\n\n5.2.2 System Size vs Accuracy\nPAQ's QA-pairs are accompanied by scores of how likely they are to be asked. These scores can be used to filter the KB and reduce the RePAQ system size. A similar procedure can be used to fil- ter the background corpus for a retrieve-and-read model (Izacard et al., 2020). We compare the system size of a FiD-large system and RePAQ as the number of items (passages and QA-pairs respectively) in their indexes are reduced. We select which passages and QA-pairs are included using the passage selection model p s . 9 Further experimental details can be found in appendix A.4. shows the that both system sizes can be reduced several-fold with only a small drop in accuracy, demonstrating the effectiveness of p s . FiD can achieve a higher accuracy, but requires larger system sizes. RePAQ can be reduced to a smaller size before a significant accuracy drop, driven primarily by the higher information density of QA-pairs relative to passages, and fewer model parameters used by RePAQ compared to FiD. Highly-optimized RePAQ models won the \"500MB\" and \"Smallest System\" tracks of the EfficientQA NeurIPS competition with disk images of 336MB and 29MB respectively. For further details on EfficientQA, the reader is referred to Min et al. (2020a).\n\n5.2.3 Inference Speed vs Accuracy\nWe train a variety of differently-sized RePAQ models to explore the relationship between accuracy and inference speed. We use a fast Hierarchical Navigable Small World (HNSW) index in FAISS (Malkov and Yashunin, 2018; Johnson et al., 2017) 10 and measure the time required to evaluate the NQ test set on a system with access to one GPU. 11 Table 5 shows these results. Some retrieveronly RePAQ models can answer over 1000 questions per second, and are relatively insensitive to model size, with ALBERT-base only scoring 0.5% lower than ALBERT-xlarge. They also outperform retrieve-and-read models like REALM (40.4%, Guu et al., 2020) and recent real-time QA models like DensePhrases (40.9%, Lee et al., 2021). We find that larger, slower RePAQ rerankers achieve higher accuracy. However, even the slowest RePAQ is 3x faster than FiD-base, whilst only being 0.8% less accurate, and 12x faster than FiD-large. 10 The HNSW index has negligible (\u223c0.1%) drop in retriever accuracy compared to a flat index 11\n\n5.2.4 Selective Question Answering\nQA systems should not just be able to answer accurately, but also \"know when they don't know\", and abstain from answering when they are unlikely to produce good answers. This task is challenging for current systems (Asai and Choi, 2020; Jiang et al., 2020b), and has been approached in MRC by training on unanswerable questions (Rajpurkar et al., 2018) and for trivia systems by leveraging incremental QA formats (Rodriguez et al., 2019).\nWe find that RePAQ's retrieval and reranking scores are well-correlated with answering correctly. This allows RePAQ to be used for selective question answering by abstaining when the score is below a certain threshold. Figure 3 shows a risk-coverage plot (Wang et al., 2018) for RePAQ and FiD, where we use FiD's answer log probability for its answer confidence. 12 The plot shows the accuracy on the top N% highest confidence answers for NQ. If we require models to answer 75% of user questions, RePAQ's accuracy on the questions it does answer is 59%, whereas FiD, which has poorer calibration, scores only 55%. This difference is even more pronounced with stricter thresholds -with coverage of 50%, RePAQ outperforms FiD by over 10%. FiD only outperforms RePAQ when we require systems to answer more than 85% of questions.\nWhilst RePAQ's selective QA is useful in its own right, it also allows us to combine the slow but accurate FiD with the fast and precise RePAQ, which we refer to as backoff. We first try to answer with RePAQ, and if the confidence is below a threshold determined on validation data, we pass the question onto FiD. For NQ, the combined system is 2.1x faster than FiD-large, with RePAQ answering 57% of the questions, and the overall accuracy is 1% higher than FiD-large (see table 3).\nIf inference speed is a priority, the threshold can be decreased so that RePAQ answers 80% of the questions, which retains the same overall accuracy as FiD, with a 4.6x speedup. For TriviaQA, the combined system backs off to FiD earlier, due to the stronger relative performance of FiD. Additional details can be found in appendix A.6\n\n5.2.5 Analysing RePAQ's Predictions\nSome examples of top retrieved questions are shown Table 6. When RePAQ answers correctly, the retrieved question is a paraphrase of the test question from PAQ in 89% of cases. As such, there is high (80.8 ROUGE-L) similarity between correctly-answered test questions and the top retrieved questions. 9% of test questions even exist verbatim in PAQ, and are thus trivial to answer. The reranker primarily improves over the retriever for ambiguous cases, and cases where the top retrieved answer does not have the right granularity. In 32% of cases, RePAQ does not retrieve the correct answer in the top 50 QA-pairs, suggesting a lack of coverage may be a significant source of error. In these cases, retrieved questions are much less similar to the test question than for correctly answered questions, dropping by 20 ROUGE-L. We also observe cases where retrieved questions match the test question, but the retrieved answer does not match the desired answer. This is usually due to different answer granularity, but in a small number of cases was due to factually incorrect answers.\n\n5.2.6 Does the Filtering Model Limit\nRePAQ's Accuracy?\nAs RePAQ relies on retrieving paraphrases of test questions, we may expect that the ODQA filtering model places an upper bound on it's performance. For example, if a valid QA-pair is generated which overlaps with a test QA-pair, but the filter cannot  answer it correctly, that QA-pair will not be added to PAQ, and RePAQ cannot use it to answer the test question. The NQ FiD-base-50-passage model used for filtering scores 46.1% and 53.1% for NQ and TriviaQA respectively. RePAQ actually outperforms the filter model on NQ by 1.6%. This is possible because generated questions may be phrased in such a way that they are easier to answer, e.g. being less ambiguous (Min et al., 2020b). RePAQ can then retrieve the paraphrased QA-pair and answer correctly, even if the filter could not answer the test question directly. The filtering model's weaker scores on TriviaQA helps explain why RePAQ is not as strong on this dataset. We speculate that using a stronger filtering model for TriviaQA would in turn improve RePAQ's results.\n\n5.3 Closed-book QA vs RePAQ\nTable 7 shows results on test set splits which measure how effectively models memorise QA-pairs from the NQ train set (\"Q overlap\"), and generalise to novel questions (\"A overlap only\" and \"No overlap\"). 13 Comparing CBQA models trained on NQ vs those trained on NQ and PAQ show that models trained with PAQ answer more questions correctly from the \"A-only overlap\" and \"No overlap\" categories, indicating they have learnt facts not present in the NQ train set. Applying additional NQ finetuning on the PAQ CBQA model improves scores on \"Q overlap\" (indicating greater memorisation of NQ), but scores on the other categories drop (indicating reduced memorisation of PAQ). However, RePAQ, which explicitly retrieves from PAQ rather than memorising it in parameters, strongly outperforms the CBQA model in all categories, demonstrating that the CBQA model struggles to memorise enough facts from PAQ. CBQA models with more parameters may be better able to memorise PAQ, but have downsides in terms of system resources. Future work should address how to better store PAQ in CBQA model parameters.\n\n6 Related Work\nODQA has received much attention in both for its practical applications, and as a benchmark for how NLP models store and access knowledge (Chen and Yih, 2020; Petroni et al., 2020b).\nKBQA A number of early approaches in ODQA focused on using structured KBs (Berant et al., 2013) such as Freebase (Bollacker et al., 2007), with recent examples from F\u00e9vry et al. (2020) and Verga et al. (2020). This approach often has high precision but suffers when KB doesn't match user requirements, or where the schema limits what knowledge can be stored. We populate our knowledgebase with semi-structured QA pairs which are specifically likely to be relevant at test time mitigating both of these drawbacks, and sharing many of the benefits, such as precision and extensibility.\nOpen Information Extraction Our work touches on automatic KB construction and open information extraction (OpenIE) (Angeli et al., 2015). Here, the goal is to mine facts from free text into structured or semi-structured forms, typically (subject, relation, object) triples for use in tasks such as slot-filling (Surdeanu, 2013). We generate natural language QA-pairs rather than OpenIE triples, and we do not attempt to extract all possible facts in a corpus, focusing only those 13 See Lewis et al. (2020b) for further details.\nthat are likely to be asked. QA-pairs have also been used in semantic role labelling, such as QA-SRL (FitzGerald et al., 2018).\nReal-time ODQA Systems that prioritise fast runtimes over accuracy are sometimes referred to as real-time QA systems (Seo et al., 2018). Den-SPI (Seo et al., 2019) and a contemporary work, DensePhrases (Lee et al., 2021), approach this by indexing all possible phrases in a background corpus, and learn mappings from questions to passagephrase pairs. We also build an index for fast answering, but generate and index globally-answerable questions. Indexing QA-pairs can be considered as indexing summaries of important facts from the corpus, rather than indexing the corpus itself. We also generate and store multiple questions per passageanswer pair, relieving information bottlenecks from encoding a passage-answer pair into a single vector.\nQuestion Generation for QA Question generation has been used for various purposes, such as data augmentation (Alberti et al., 2019; Lewis et al., 2019b; Lee et al., 2021), improved retrieval (Nogueira et al., 2019), generative modelling for contextual QA (Lewis and Fan, 2018), as well as being studied in its own right (Du et al., 2017; Hosking and Riedel, 2019).  Serban et al. (2016) generate large numbers of questions from Freebase, but do not address how to use them for QA. Closest to our work is the recently-proposed OceanQA (Fang et al., 2020). OceanQA first generates contextual QA-pairs from Wikipedia. At test-time, a document retrieval system is used to retrieve the most relevant passage for a question and the closest pre-generated QA-pair from that passage is then selected. In contrast, we focusing on generating a large KB of non-contextual, globallyconsistent ODQA questions and exploring what QA systems are facilitated by such a resource.\n\n7 Conclusion\nWe have introduced PAQ, a dataset of 65M QApairs, and explored how it could be used to improve ODQA models. We demonstrated the effectiveness of RePAQ, a system which retrieves from PAQ, in terms of accuracy, speed, space efficiency and selective QA. Generating PAQ is computationally intensive due to its large scale, but should be a useful, re-usable resource for more accurate, smaller and faster QA models. Nevertheless, future work should be carried out to improve the efficiency of generation in order to expand PAQ's coverage. We also demonstrated PAQ's utility for improved CBQA models, but note a large accuracy gap between our CBQA models and RePAQ. Exploring the trade-offs between storing and retrieving knowledge parametrically or non-parametrically is a topic of great current interest (Lewis et al., 2020a; De Cao et al., 2020), and PAQ should be a useful testbed for probing this relationship further. We also note that PAQ could be used as general dataaugmentation when training any open-domain QA model or retriever. Whilst we consider such work out-of-scope for this paper, leveraging PAQ to improve retrieve-and-read and other systems systems should be explored in future work.\n\nA Appendices\nA.1 Dataset splits For Natural Questions (NQ, Kwiatkowski et al., 2019b), we use the standard Open-Domain splits in our experiments, consisting of 79,168 train, 8,757 development, and 3,610 test question-answer pairs.\nFor TriviaQA, We use the open-domain train-test splits, which correspond to the unfiltered-train and unfiltered-dev reading comprehension splits (Lee et al., 2019b; Min et al., 2019; Karpukhin et al., 2020a).\n\nA.2 Futher details on Passage selection\nOur passage selection model is based on RoBERTa BASE (Liu et al., 2019b). We feed each passage we select into the model as input and use an MLP on top of RoBERTa's [CLS] token to predict if it is positive or negative. We then use this model to perform inference and obtain a score for every single passage in the whole wikipedia corpus. The top N passages ranked by its score are served as the candidate pool we generate answers from. The model is optimized for a higher recall, such that the positive passages should be identified with a high probability. Our model achieved 84.7% recall on the Natural Questions dev set.\n\nA.3 Further Details on Question Quality\nFor NQ, we find that the retrieved questions are paraphrases of the test questions in the majority of cases. The test questions in TriviaQA are mostly very specific, and whilst retrieved questions in PAQ contain less details, they still usually have correct answers. To better evaluate the quality of the questions, we conduct human evaluation on 50 random sampled questions generated from the wikipedia passage pool. We have the following observations: 1) the majority (82%) questions accurately capture the context of the answer in the passage, and contain relevant and informative details to locate the answer. 2) 16% questions fail to understand the semantics of certain types of answers. The failure can be traced to: Mistaking extremely similar entities. For example, given the sentence \"The Kerch Peninsula is ... located at the eastern end of the Crimean Peninsula\" and the answer \"the Crimean Peninsula\", the generated question is \"what is the eastern end of the kerch peninsula\"; Generalisation to rare phrases. The digit combinations appearing in passages mostly stand for date or timestamp. However, it is not applied to all the cases and the model fails to capture that. For example for \"under a 109-124 loss to the Milwaukee Bucks\", the question is generated as \"when did ... play for the toronto raptors\". 3) Very few (2%) questions mismatch question type words (Wh-types) with answers when the answers are words rarely asked (e.g. first).\n\nA.4 Further details on System Size vs Accuracy\nThe system experiment described in Section 5.2.2 measures the bytes required to store the models, the text of the documents/QA-pairs, and a dense index. For figure 2, We assume models are stored at fp16 precision, the text has been compressed using LZMA 14, and the indexes both use 768d vectors, and Product Quantization. These are realistic defaults, with usage in the question answering literature (Izacard et al., 2020; Min et al., 2020a). The RePAQ model used in this figure consists of an ALBERT-base retriever and ALBERTxxlarge reranker, and the FID system consists of DPR (Karpukhin et al., 2020b) (itself consisting of two BERT-base retrievers) and a T5-large reader (Raffel et al., 2020). Using a different system setup (for example using full precision to store the models, no text compression, and fp16 index quantization, shown in figure 4) shifts the relative position of the curves in Figure 2, but not the qualitative observation that RePAQ models can be compressed to smaller sizes before significant drop in accuracy.\n\nA.5 Further details on Inference speed\nThe system used for inference speed benchmarking is a machine learning workstation with 80 CPU cores, 512GB of CPU RAM and access to one 32GB NVIDIA V100 GPU. Inference is carried out at mixed precision for all systems, and questions are allowed to be answered in parallel. Models are implemented in Pytorch (Paszke et al., 2019) using Transformers (Wolf et al., 2020). Measurements are repeated 3 times and the mean time is reported, rounded to an appropriate significant figure. The HNSW index used in this experiment indexes all 65M PAQ QA-pairs with vectors of dimension 768 dimensions, uses an ef construction of 80, ef search of 32, and store n of 256, and performs up to 2048 index searches in parallel. This index occupies 220GB, but could be considerably compressed with techniques including scalar or product quantization, or training retrievers with smaller projection dimensions. The reader is referred to Izacard et al. (2020) and Lewis et al. (2020a, Appendix C) for experiments demonstrating index compression with little-to-no drop in accuracy.\n\nA.6 Further details on Selective Question Answering\nFigure 5 shows the Risk-Coverage plot for Trivi-aQA. The results are qualitatively similar as those for NQ (see Figure 3 in the main paper), although FiD's stronger overall performance on TriviaQA shifts its risk-coverage curve up the accuracy axis relative to RePAQ. FiD also seems a little better calibrated on TriviaQA than NQ, indicated by greater gradient. However, RePAQ remains better calibrated than FiD, and outperforms it for answer coverages below 50%. We also investigate ways to improve the calibration of FiD-large on NQ, using post-hoc calibration, using an approach similar to Jiang et al. son). Using RePAQ's answer confidence scores to calibrate FiD leads to the best results for FiD (2020a). We train a Gradient Boosting Machine (GBM, Friedman, 2001) on the dev set of NQ, which attempts to predict whether FID-large has answered correctly or not. The GBM is given FID's answer loss, answer log probability and the retrieval score of the top 100 retrieved documents from DPR. Figure 6 shows these results. We first note that FiD-Large's answer loss and answer log probabilities perform similarly, and both struggle to calibrate FiD, as mentioned in the main paper. The GBM does improve calibration, especially at lower coverages, but still lags behind RePAQ by 7% EM at 50% coverage. We also note a surprising finding that we can acutally use RePAQ's scores to calibrate FiD. Here, we use FiD's predicted answer, but RePAQ's confidence score to decide whether to answer or not. This result is also plotted in Figure 6, and results in the best risk-coverage curve for FiD. However, this is still not as well-calibrated as simply using RePAQ, highlighting the strength of RePAQ for selective QA further.\n\nA.7 Additional Model training details\nRePAQ models were trained for up to 3 days on a machine with 8 NVIDIA 32GB V100 GPUs. Validation Exact match score was used to determine when to stop training in all cases. RePAQ retrievers were trained using Fairseq (Ott et al., 2019), and rerankers were trained in Transformers (Wolf et al., 2020) in Pytorch. The PAQ BART CBQA models were trained in Fairseq for 6 days on 8 V100s, after which validation accuracy had plateaued. Standard BART hyperparameters were used, apart from batch size and learning rate, which were tuned to try to promote faster learning, but learning became unstable with learning rates greater than 0.0001.\n\nFootnotes:\n1: The PAQ data, models and code will be made available at https://github.com/facebookresearch/PAQ\n2: We use the sPaCy (Honnibal and Montani, 2017) NER system, trained on OntoNotes (Hovy et al., 2006).\n3: Other methods, such as heuristically constructing paraphrase pairs assuming that questions with the same answer are paraphrases, and training with sampled negatives would also be valid, but were not competitive in our early experiments\n4: 4  We could use pgen as a reranker/aggregator for QA, but in practice find it both slower and less accurate than the reranker described in Section 4.2.2\n5: Generation was stopped when downstream performance with RePAQ did not significantly improve with more questions.\n6: Each question only has one answer due to global filtering\n7: performed using standard answer normalisation (Rajpurkar et al., 2016)\n9: Here, we use PAQL1, which is 5x smaller than the full PAQ, but retains most of the accuracy (see Table4)\n12: We also investigate improving FiD's calibration using an auxiliary model, see Appendix A.6. We find that the most effective way to calibrate FiD is to use RePAQ's confidences\n14: https://tukaani.org/xz/\n\nReferences:\n\n- Chris Alberti, Daniel Andor, Emily Pitler, Jacob De- vlin, and Michael Collins. 2019. Synthetic QA Cor- pora Generation with Roundtrip Consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168-6173, Florence, Italy. Association for Compu- tational Linguistics.- Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging Lin- guistic Structure For Open Domain Information Ex- traction. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguis- tics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pages 344-354, Beijing, China. Association for Computational Linguistics.\n\n- Akari Asai and Eunsol Choi. 2020. Challenges in Information Seeking QA:Unanswerable Questions and Paragraph Retrieval. arXiv:2010.11915 [cs].\n\n- Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP, pages 1533- 1544. ACL.\n\n- Kurt D. Bollacker, Robert P. Cook, and Patrick Tufts. 2007. Freebase: A shared database of structured general human knowledge. In AAAI, pages 1962- 1963. AAAI Press.\n\n- Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open- Domain Questions. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870- 1879, Vancouver, Canada. Association for Computa- tional Linguistics.\n\n- Danqi Chen and Wen-tau Yih. 2020. Open-domain question answering. In ACL (tutorial), pages 34-37. Association for Computational Linguistics.\n\n- Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive Entity Retrieval. arXiv:2010.00904 [cs, stat]. ArXiv: 2010.00904.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\n- Pedro Domingos. 2020. Every Model Learned by Gra- dient Descent Is Approximately a Kernel Machine. arXiv:2012.00152 [cs, stat]. ArXiv: 2012.00152.\n\n- Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to Ask: Neural Question Generation for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1342-1352, Vancouver, Canada. Association for Computational Linguistics.\n\n- Yuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun, and Jingjing Liu. 2020. Accelerating Real- Time Question Answering via Question Generation. arXiv:2009.05167 [cs]. ArXiv: 2009.05167.\n\n- Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer. 2018. Large-Scale QA-SRL Pars- ing. arXiv:1805.05377 [cs]. ArXiv: 1805.05377.\n\n- Jerome H. Friedman. 2001. Greedy function approx- imation: A gradient boosting machine. Annals of Statistics, 29(5):1189-1232. Publisher: Institute of Mathematical Statistics.\n\n- Thibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGer- ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En- tities as Experts: Sparse Memory Access with En- tity Supervision. arXiv:2004.07202 [cs]. ArXiv: 2004.07202.\n\n- Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-Augmented Language Model Pre- Training. arXiv:2002.08909 [cs].\n\n- Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed- dings, convolutional neural networks and incremen- tal parsing. To appear.\n\n- Tom Hosking and Sebastian Riedel. 2019. Eval- uating Rewards for Question Generation Models. arXiv:1902.11049 [cs]. ArXiv: 1902.11049.\n\n- Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of the Human Lan- guage Technology Conference of the NAACL, Com- panion Volume: Short Papers, NAACL-Short '06, pages 57-60, USA. Association for Computational Linguistics.\n\n- Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Trans- former Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. arXiv:1905.01969 [cs]. ArXiv: 1905.01969.\n\n- Gautier Izacard and Edouard Grave. 2020. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. arXiv:2007.01282 [cs]. ArXiv: 2007.01282.\n\n- Gautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Sebastian Riedel, and Edouard Grave. 2020. A Memory Efficient Baseline for Open Do- main Question Answering. arXiv:2012.15156 [cs].\n\n- Zhengbao Jiang, Jun Araki, Haibo Ding, and Gra- ham Neubig. 2020a. How Can We Know When Language Models Know? arXiv:2012.00955 [cs].\n\n- Zhengbao Jiang, Wei Xu, Jun Araki, and Gra- ham Neubig. 2020b. Generalizing Natural Lan- guage Analysis through Span-relation Representa- tions. arXiv:1911.03822 [cs]. ArXiv: 1911.03822.\n\n- Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with GPUs. arXiv:1702.08734 [cs]. ArXiv: 1702.08734.\n\n- Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Dis- tantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.\n\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020a. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 6769-6781. Association for Computational Linguistics.\n\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020b. Dense Passage Retrieval for Open-Domain Question Answering. arXiv:2004.04906 [cs]. ArXiv: 2004.04906.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019a. Natu- ral Questions: a Benchmark for Question Answering Research. Transactions of the Association of Com- putational Linguistics.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur P. Parikh, Chris Al- berti, Danielle Epstein, Illia Polosukhin, Jacob De- vlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019b. Natural questions: a benchmark for question answer- ing research. Trans. Assoc. Comput. Linguistics, 7:452-466.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Con- ference on Learning Representations.\n\n- Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2021. Learning Dense Representations of Phrases at Scale. arXiv:2012.12624 [cs]. ArXiv: 2012.12624.\n\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019a. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, Flo- rence, Italy. Association for Computational Linguis- tics.\n\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019b. Latent retrieval for weakly supervised open domain question answering. In ACL (1), pages 6086-6096. Association for Computational Linguis- tics.\n\n- Mike Lewis and Angela Fan. 2018. Generative Ques- tion Answering: Learning to Answer the Whole Question. In International Conference on Learning Representations.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019a. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. arXiv:1910.13461 [cs, stat]. ArXiv: 1910.13461.\n\n- Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019b. Unsupervised Question Answering by Cloze Translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 4896-4910, Florence, Italy. Associa- tion for Computational Linguistics.\n\n- Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020a. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs]. ArXiv: 2005.11401.\n\n- Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020b. Question and Answer Test-Train Over- lap in Open-Domain Question Answering Datasets. arXiv:2008.02637 [cs]. ArXiv: 2008.02637.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach. arXiv:1907.11692 [cs]. ArXiv: 1907.11692.\n\n- Yu A. Malkov and D. A. Yashunin. 2018. Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. arXiv:1603.09320 [cs]. ArXiv: 1603.09320.\n\n- Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jenni- maria Palomaki, Colin Raffel, Adam Roberts, Tom Kwiatkowski, Patrick Lewis, Yuxiang Wu, Hein- rich K\u00fcttler, Linqing Liu, Pasquale Minervini, Pon- tus Stenetorp, Sebastian Riedel, Sohee Yang, Min- joon Seo, Gautier Izacard, Fabio Petroni, Lu- cas Hosseini, Nicola De Cao, Edouard Grave, Ikuya Yamada, Sonse Shimaoka, Masatoshi Suzuki, Shumpei Miyawaki, Shun Sato, Ryo Takahashi, Jun Suzuki, Martin Fajcik, Martin Docekal, Karel On- drej, Pavel Smrz, Hao Cheng, Yelong Shen, Xi- aodong Liu, Pengcheng He, Weizhu Chen, Jian- feng Gao, Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Wen-tau Yih. 2020a. NeurIPS 2020 Ef- ficientQA Competition: Systems, Analyses and Lessons Learned. arXiv:2101.00133 [cs]. ArXiv: 2101.00133.\n\n- Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM ap- proach for weakly supervised question answering. In EMNLP/IJCNLP (1), pages 2851-2864. Associ- ation for Computational Linguistics.\n\n- Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020b. AmbigQA: Answering Ambiguous Open-domain Questions. arXiv:2004.10645 [cs]. ArXiv: 2004.10645.\n\n- Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by Query Prediction. arXiv:1904.08375 [cs]. ArXiv: 1904.08375. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. arXiv:1904.01038 [cs]. ArXiv: 1904.01038.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Te- jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learn- ing Library. arXiv:1912.01703 [cs, stat]. ArXiv: 1912.01703.\n\n- Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020a. How Context Affects Lan- guage Models' Factual Predictions. In Automated Knowledge Base Construction.\n\n- Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2020b. KILT: a Benchmark for Knowledge Intensive Language Tasks. arXiv:2009.02252 [cs]. ArXiv: 2009.02252.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text- to-Text Transformer. Journal of Machine Learning Research, 21(140):1-67.\n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don't Know: Unanswerable Ques- tions for SQuAD. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784- 789, Melbourne, Australia. Association for Compu- tational Linguistics.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Lin- guistics.\n\n- Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack Into the Pa- rameters of a Language Model? arXiv:2002.08910 [cs, stat]. ArXiv: 2002.08910.\n\n- Pedro Rodriguez, Shi Feng, Mohit Iyyer, He He, and Jordan Boyd-Graber. 2019. Quizbowl: The Case for Incremental Question Answering. arXiv:1904.04792 [cs]. ArXiv: 1904.04792. Minjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2018. Phrase- Indexed Question Answering: A New Challenge for Scalable Document Comprehension. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 559-564, Brussels, Belgium. Association for Computational Linguistics.\n\n- Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 4430-4441, Florence, Italy. Association for Computational Linguistics.\n\n- Iulian Vlad Serban, Alberto Garc\u00eda-Dur\u00e1n, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. 2016. Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 588-598, Berlin, Germany. Associa- tion for Computational Linguistics.\n\n- M. Surdeanu. 2013. Overview of the TAC2013 Knowl- edge Base Population Evaluation: English Slot Fill- ing and Temporal Slot Filling. TAC.\n\n- Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. 2020. Facts as Experts: Adapt- able and Interpretable Neural Memory over Sym- bolic Knowledge. arXiv:2007.00849 [cs]. ArXiv: 2007.00849.\n\n- William Wang, Angelina Wang, Aviv Tamar, Xi Chen, and Pieter Abbeel. 2018. Safer Classification by Synthesis. arXiv:1711.08534 [cs, stat]. ArXiv: 1711.08534.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. HuggingFace's Transformers: State-of-the-art Nat- ural Language Processing. arXiv:1910.03771 [cs].\n\n- Jinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung Bui, Tong Sun, and Jiawei Han. 2020. Open- Domain Question Answering with Pre-Constructed Question Spaces. arXiv:2006.08337 [cs]. ArXiv: 2006.08337.\n\n- Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. Data Augmenta- tion for BERT Fine-Tuning in Open-Domain Ques- tion Answering. arXiv:1904.06652 [cs]. ArXiv: 1904.06652.\n\n- Qinyuan Ye, Belinda Z. Li, Sinong Wang, Ben- jamin Bolte, Hao Ma, Wen-tau Yih, Xiang Ren, and Madian Khabsa. 2021. Studying Strategi- cally: Learning to Mask for Closed-book QA. arXiv:2012.15856 [cs]. ArXiv: 2012.15856.\n\n", "annotations": {"Abstract": [{"begin": 72, "end": 1621, "idx": 0}], "Head": [{"begin": 1624, "end": 1638, "n": "1", "idx": 0}, {"begin": 7931, "end": 7965, "n": "3", "idx": 1}, {"begin": 9345, "end": 9371, "n": "3.1", "idx": 2}, {"begin": 10530, "end": 10556, "n": "3.2", "idx": 3}, {"begin": 12076, "end": 12104, "n": "3.3", "idx": 4}, {"begin": 12490, "end": 12508, "n": "3.4", "idx": 5}, {"begin": 13459, "end": 13489, "n": "4", "idx": 6}, {"begin": 13884, "end": 13910, "n": "4.1", "idx": 7}, {"begin": 14844, "end": 14853, "n": "4.2", "idx": 8}, {"begin": 15554, "end": 15575, "n": "4.2.1", "idx": 9}, {"begin": 18386, "end": 18406, "n": "4.2.2", "idx": 10}, {"begin": 19454, "end": 19463, "n": "5", "idx": 11}, {"begin": 19799, "end": 19816, "n": "5.1", "idx": 12}, {"begin": 21328, "end": 21338, "idx": 13}, {"begin": 21852, "end": 21883, "idx": 14}, {"begin": 22958, "end": 22988, "n": "5.2", "idx": 15}, {"begin": 24303, "end": 24333, "n": "5.2.1", "idx": 16}, {"begin": 25164, "end": 25193, "n": "5.2.2", "idx": 17}, {"begin": 26439, "end": 26472, "n": "5.2.3", "idx": 18}, {"begin": 27478, "end": 27512, "n": "5.2.4", "idx": 19}, {"begin": 29598, "end": 29633, "n": "5.2.5", "idx": 20}, {"begin": 30717, "end": 30753, "n": "5.2.6", "idx": 21}, {"begin": 31802, "end": 31829, "n": "5.3", "idx": 22}, {"begin": 32925, "end": 32939, "n": "6", "idx": 23}, {"begin": 36070, "end": 36082, "n": "7", "idx": 24}, {"begin": 37282, "end": 37294, "idx": 25}, {"begin": 37723, "end": 37762, "idx": 26}, {"begin": 38387, "end": 38426, "idx": 27}, {"begin": 39883, "end": 39929, "idx": 28}, {"begin": 40967, "end": 41005, "idx": 29}, {"begin": 42068, "end": 42119, "idx": 30}, {"begin": 43842, "end": 43879, "idx": 31}], "ReferenceToBib": [{"begin": 1839, "end": 1861, "target": "#b52", "idx": 0}, {"begin": 1979, "end": 1998, "target": "#b5", "idx": 1}, {"begin": 1999, "end": 2029, "idx": 2}, {"begin": 2259, "end": 2281, "target": "#b52", "idx": 3}, {"begin": 2282, "end": 2298, "target": "#b62", "idx": 4}, {"begin": 2375, "end": 2396, "target": "#b49", "idx": 5}, {"begin": 2405, "end": 2426, "target": "#b35", "idx": 6}, {"begin": 2622, "end": 2643, "target": "#b38", "idx": 7}, {"begin": 2799, "end": 2820, "target": "#b38", "idx": 8}, {"begin": 2821, "end": 2839, "target": "#b60", "idx": 9}, {"begin": 4210, "end": 4229, "target": "#b11", "idx": 10}, {"begin": 4230, "end": 4251, "target": "#b0", "idx": 11}, {"begin": 4650, "end": 4671, "target": "#b37", "idx": 12}, {"begin": 4693, "end": 4720, "target": "#b28", "idx": 13}, {"begin": 4735, "end": 4755, "target": "#b25", "idx": 14}, {"begin": 5251, "end": 5270, "target": "#b42", "idx": 15}, {"begin": 5833, "end": 5857, "target": "#b53", "idx": 16}, {"begin": 8718, "end": 8740, "target": "#b0", "idx": 17}, {"begin": 8894, "end": 8916, "target": "#b0", "idx": 18}, {"begin": 8917, "end": 8937, "target": "#b36", "idx": 19}, {"begin": 9597, "end": 9621, "target": "#b27", "idx": 20}, {"begin": 10030, "end": 10049, "target": "#b39", "idx": 21}, {"begin": 10107, "end": 10133, "target": "#b29", "idx": 22}, {"begin": 10746, "end": 10767, "target": "#b8", "idx": 23}, {"begin": 10947, "end": 10974, "target": "#b28", "idx": 24}, {"begin": 10975, "end": 10994, "target": "#b25", "idx": 25}, {"begin": 11458, "end": 11479, "target": "#b8", "idx": 26}, {"begin": 12451, "end": 12472, "target": "#b35", "idx": 27}, {"begin": 12714, "end": 12736, "target": "#b0", "idx": 28}, {"begin": 12737, "end": 12755, "target": "#b11", "idx": 29}, {"begin": 13052, "end": 13071, "target": "#b44", "idx": 30}, {"begin": 13431, "end": 13456, "idx": 31}, {"begin": 13844, "end": 13865, "target": "#b38", "idx": 32}, {"begin": 13866, "end": 13881, "target": "#b9", "idx": 33}, {"begin": 13937, "end": 13958, "target": "#b35", "idx": 34}, {"begin": 14064, "end": 14085, "target": "#b52", "idx": 35}, {"begin": 14243, "end": 14265, "target": "#b0", "idx": 36}, {"begin": 14266, "end": 14284, "target": "#b61", "idx": 37}, {"begin": 14389, "end": 14413, "target": "#b50", "idx": 38}, {"begin": 14705, "end": 14728, "target": "#b47", "idx": 39}, {"begin": 15004, "end": 15025, "target": "#b38", "idx": 40}, {"begin": 15026, "end": 15044, "target": "#b60", "idx": 41}, {"begin": 15751, "end": 15776, "target": "#b27", "idx": 42}, {"begin": 15777, "end": 15806, "idx": 43}, {"begin": 16539, "end": 16559, "target": "#b38", "idx": 44}, {"begin": 17507, "end": 17525, "target": "#b30", "idx": 45}, {"begin": 17585, "end": 17606, "target": "#b35", "idx": 46}, {"begin": 18173, "end": 18195, "target": "#b24", "idx": 47}, {"begin": 18537, "end": 18558, "target": "#b19", "idx": 48}, {"begin": 19652, "end": 19678, "target": "#b29", "idx": 49}, {"begin": 19692, "end": 19712, "target": "#b25", "idx": 50}, {"begin": 19908, "end": 19932, "target": "#b26", "idx": 51}, {"begin": 23197, "end": 23221, "idx": 52}, {"begin": 23236, "end": 23261, "target": "#b27", "idx": 53}, {"begin": 23331, "end": 23352, "target": "#b49", "idx": 54}, {"begin": 24129, "end": 24146, "target": "#b31", "idx": 55}, {"begin": 25443, "end": 25465, "idx": 56}, {"begin": 26418, "end": 26436, "target": "#b42", "idx": 57}, {"begin": 26663, "end": 26690, "target": "#b41", "idx": 58}, {"begin": 26691, "end": 26712, "target": "#b24", "idx": 59}, {"begin": 27089, "end": 27106, "target": "#b15", "idx": 60}, {"begin": 27164, "end": 27181, "target": "#b31", "idx": 61}, {"begin": 27728, "end": 27749, "target": "#b2", "idx": 62}, {"begin": 27750, "end": 27770, "target": "#b23", "idx": 63}, {"begin": 27841, "end": 27865, "target": "#b50", "idx": 64}, {"begin": 27926, "end": 27950, "target": "#b53", "idx": 65}, {"begin": 28207, "end": 28226, "target": "#b58", "idx": 66}, {"begin": 31437, "end": 31456, "target": "#b44", "idx": 67}, {"begin": 33078, "end": 33098, "target": "#b6", "idx": 68}, {"begin": 33099, "end": 33121, "target": "#b48", "idx": 69}, {"begin": 33197, "end": 33218, "target": "#b3", "idx": 70}, {"begin": 33236, "end": 33260, "target": "#b4", "idx": 71}, {"begin": 33288, "end": 33307, "target": "#b14", "idx": 72}, {"begin": 33312, "end": 33331, "target": "#b57", "idx": 73}, {"begin": 33822, "end": 33843, "target": "#b1", "idx": 74}, {"begin": 34018, "end": 34034, "target": "#b56", "idx": 75}, {"begin": 34194, "end": 34214, "target": "#b38", "idx": 76}, {"begin": 34337, "end": 34362, "target": "#b12", "idx": 77}, {"begin": 34481, "end": 34499, "target": "#b53", "idx": 78}, {"begin": 34509, "end": 34527, "target": "#b54", "idx": 79}, {"begin": 34566, "end": 34584, "target": "#b31", "idx": 80}, {"begin": 35217, "end": 35239, "target": "#b0", "idx": 81}, {"begin": 35240, "end": 35260, "target": "#b36", "idx": 82}, {"begin": 35261, "end": 35278, "target": "#b31", "idx": 83}, {"begin": 35299, "end": 35322, "target": "#b45", "idx": 84}, {"begin": 35363, "end": 35384, "target": "#b34", "idx": 85}, {"begin": 35428, "end": 35445, "target": "#b10", "idx": 86}, {"begin": 35446, "end": 35471, "target": "#b17", "idx": 87}, {"begin": 35474, "end": 35494, "target": "#b55", "idx": 88}, {"begin": 35642, "end": 35661, "target": "#b11", "idx": 89}, {"begin": 36883, "end": 36904, "target": "#b37", "idx": 90}, {"begin": 36905, "end": 36925, "idx": 91}, {"begin": 37341, "end": 37367, "target": "#b29", "idx": 92}, {"begin": 37658, "end": 37677, "target": "#b33", "idx": 93}, {"begin": 37678, "end": 37695, "target": "#b43", "idx": 94}, {"begin": 37696, "end": 37720, "target": "#b26", "idx": 95}, {"begin": 37816, "end": 37835, "target": "#b40", "idx": 96}, {"begin": 40331, "end": 40353, "idx": 97}, {"begin": 40354, "end": 40372, "target": "#b42", "idx": 98}, {"begin": 40510, "end": 40534, "target": "#b27", "idx": 99}, {"begin": 40606, "end": 40627, "target": "#b49", "idx": 100}, {"begin": 41314, "end": 41335, "target": "#b46", "idx": 101}, {"begin": 41355, "end": 41374, "target": "#b59", "idx": 102}, {"begin": 41924, "end": 41945, "idx": 103}, {"begin": 41950, "end": 41982, "idx": 104}, {"begin": 42726, "end": 42730, "idx": 105}, {"begin": 42874, "end": 42889, "target": "#b13", "idx": 106}, {"begin": 44097, "end": 44115, "target": "#b45", "idx": 107}, {"begin": 44160, "end": 44179, "target": "#b59", "idx": 108}, {"begin": 44646, "end": 44673, "target": "#b16", "idx": 109}, {"begin": 44708, "end": 44727, "target": "#b18", "idx": 110}, {"begin": 45350, "end": 45374, "target": "#b51", "idx": 111}], "ReferenceToFootnote": [{"begin": 6772, "end": 6773, "target": "#foot_0", "idx": 0}, {"begin": 10875, "end": 10876, "target": "#foot_1", "idx": 1}, {"begin": 16561, "end": 16562, "target": "#foot_2", "idx": 2}, {"begin": 17433, "end": 17434, "target": "#foot_3", "idx": 3}, {"begin": 20107, "end": 20108, "target": "#foot_4", "idx": 4}, {"begin": 21072, "end": 21073, "target": "#foot_5", "idx": 5}, {"begin": 21490, "end": 21491, "target": "#foot_6", "idx": 6}, {"begin": 25707, "end": 25708, "target": "#foot_7", "idx": 7}, {"begin": 28315, "end": 28317, "target": "#foot_8", "idx": 8}, {"begin": 40184, "end": 40186, "target": "#foot_9", "idx": 9}], "SectionFootnote": [{"begin": 44516, "end": 45689, "idx": 0}], "ReferenceString": [{"begin": 45706, "end": 46024, "id": "b0", "idx": 0}, {"begin": 46026, "end": 46450, "id": "b1", "idx": 1}, {"begin": 46454, "end": 46595, "id": "b2", "idx": 2}, {"begin": 46599, "end": 46754, "id": "b3", "idx": 3}, {"begin": 46758, "end": 46923, "id": "b4", "idx": 4}, {"begin": 46927, "end": 47242, "id": "b5", "idx": 5}, {"begin": 47246, "end": 47386, "id": "b6", "idx": 6}, {"begin": 47390, "end": 47545, "id": "b7", "idx": 7}, {"begin": 47549, "end": 47971, "id": "b8", "idx": 8}, {"begin": 47975, "end": 48121, "id": "b9", "idx": 9}, {"begin": 48125, "end": 48441, "id": "b10", "idx": 10}, {"begin": 48445, "end": 48624, "id": "b11", "idx": 11}, {"begin": 48628, "end": 48775, "id": "b12", "idx": 12}, {"begin": 48779, "end": 48954, "id": "b13", "idx": 13}, {"begin": 48958, "end": 49171, "id": "b14", "idx": 14}, {"begin": 49175, "end": 49333, "id": "b15", "idx": 15}, {"begin": 49337, "end": 49510, "id": "b16", "idx": 16}, {"begin": 49514, "end": 49648, "id": "b17", "idx": 17}, {"begin": 49652, "end": 49955, "id": "b18", "idx": 18}, {"begin": 49959, "end": 50190, "id": "b19", "idx": 19}, {"begin": 50194, "end": 50364, "id": "b20", "idx": 20}, {"begin": 50368, "end": 50562, "id": "b21", "idx": 21}, {"begin": 50566, "end": 50698, "id": "b22", "idx": 22}, {"begin": 50702, "end": 50888, "id": "b23", "idx": 23}, {"begin": 50892, "end": 51029, "id": "b24", "idx": 24}, {"begin": 51033, "end": 51389, "id": "b25", "idx": 25}, {"begin": 51393, "end": 51653, "id": "b26", "idx": 26}, {"begin": 51657, "end": 51879, "id": "b27", "idx": 27}, {"begin": 51883, "end": 52307, "id": "b28", "idx": 28}, {"begin": 52311, "end": 52724, "id": "b29", "idx": 29}, {"begin": 52728, "end": 52964, "id": "b30", "idx": 30}, {"begin": 52968, "end": 53122, "id": "b31", "idx": 31}, {"begin": 53126, "end": 53428, "id": "b32", "idx": 32}, {"begin": 53432, "end": 53635, "id": "b33", "idx": 33}, {"begin": 53639, "end": 53800, "id": "b34", "idx": 34}, {"begin": 53804, "end": 54102, "id": "b35", "idx": 35}, {"begin": 54106, "end": 54393, "id": "b36", "idx": 36}, {"begin": 54397, "end": 54702, "id": "b37", "idx": 37}, {"begin": 54706, "end": 54894, "id": "b38", "idx": 38}, {"begin": 54898, "end": 55134, "id": "b39", "idx": 39}, {"begin": 55138, "end": 55383, "id": "b40", "idx": 40}, {"begin": 55387, "end": 55573, "id": "b41", "idx": 41}, {"begin": 55577, "end": 56523, "id": "b42", "idx": 42}, {"begin": 56527, "end": 56753, "id": "b43", "idx": 43}, {"begin": 56757, "end": 56927, "id": "b44", "idx": 44}, {"begin": 56931, "end": 57292, "id": "b45", "idx": 45}, {"begin": 57296, "end": 57737, "id": "b46", "idx": 46}, {"begin": 57741, "end": 57970, "id": "b47", "idx": 47}, {"begin": 57974, "end": 58261, "id": "b48", "idx": 48}, {"begin": 58265, "end": 58534, "id": "b49", "idx": 49}, {"begin": 58538, "end": 58853, "id": "b50", "idx": 50}, {"begin": 58857, "end": 59160, "id": "b51", "idx": 51}, {"begin": 59164, "end": 59338, "id": "b52", "idx": 52}, {"begin": 59342, "end": 59862, "id": "b53", "idx": 53}, {"begin": 59866, "end": 60210, "id": "b54", "idx": 54}, {"begin": 60214, "end": 60641, "id": "b55", "idx": 55}, {"begin": 60645, "end": 60782, "id": "b56", "idx": 56}, {"begin": 60786, "end": 60990, "id": "b57", "idx": 57}, {"begin": 60994, "end": 61151, "id": "b58", "idx": 58}, {"begin": 61155, "end": 61590, "id": "b59", "idx": 59}, {"begin": 61594, "end": 61794, "id": "b60", "idx": 60}, {"begin": 61798, "end": 61992, "id": "b61", "idx": 61}, {"begin": 61996, "end": 62215, "id": "b62", "idx": 62}], "ReferenceToTable": [{"begin": 21030, "end": 21031, "target": "#tab_0", "idx": 0}, {"begin": 21201, "end": 21202, "target": "#tab_0", "idx": 1}, {"begin": 21498, "end": 21499, "target": "#tab_0", "idx": 2}, {"begin": 21936, "end": 21937, "target": "#tab_1", "idx": 3}, {"begin": 22296, "end": 22297, "target": "#tab_1", "idx": 4}, {"begin": 23387, "end": 23388, "target": "#tab_2", "idx": 5}, {"begin": 24340, "end": 24341, "target": "#tab_3", "idx": 6}, {"begin": 24734, "end": 24735, "target": "#tab_3", "idx": 7}, {"begin": 26819, "end": 26820, "target": "#tab_4", "idx": 8}, {"begin": 29258, "end": 29259, "target": "#tab_2", "idx": 9}, {"begin": 29691, "end": 29692, "target": "#tab_5", "idx": 10}, {"begin": 31836, "end": 31837, "target": "#tab_6", "idx": 11}], "Footnote": [{"begin": 44527, "end": 44625, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 44626, "end": 44728, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 44729, "end": 44967, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 44968, "end": 45123, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 45124, "end": 45239, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 45240, "end": 45300, "id": "foot_5", "n": "6", "idx": 5}, {"begin": 45301, "end": 45374, "id": "foot_6", "n": "7", "idx": 6}, {"begin": 45375, "end": 45482, "id": "foot_7", "n": "9", "idx": 7}, {"begin": 45483, "end": 45661, "id": "foot_8", "n": "12", "idx": 8}, {"begin": 45662, "end": 45689, "id": "foot_9", "n": "14", "idx": 9}], "Paragraph": [{"begin": 82, "end": 1621, "idx": 0}, {"begin": 1639, "end": 2030, "idx": 1}, {"begin": 2031, "end": 2644, "idx": 2}, {"begin": 2645, "end": 3493, "idx": 3}, {"begin": 3494, "end": 4309, "idx": 4}, {"begin": 4310, "end": 5322, "idx": 5}, {"begin": 5323, "end": 5651, "idx": 6}, {"begin": 5652, "end": 6191, "idx": 7}, {"begin": 6192, "end": 7929, "idx": 8}, {"begin": 7966, "end": 8130, "idx": 9}, {"begin": 8131, "end": 8237, "idx": 10}, {"begin": 8238, "end": 8469, "idx": 11}, {"begin": 8470, "end": 8741, "idx": 12}, {"begin": 8742, "end": 9343, "idx": 13}, {"begin": 9372, "end": 9999, "idx": 14}, {"begin": 10000, "end": 10528, "idx": 15}, {"begin": 10557, "end": 10798, "idx": 16}, {"begin": 10799, "end": 11278, "idx": 17}, {"begin": 11279, "end": 12074, "idx": 18}, {"begin": 12105, "end": 12488, "idx": 19}, {"begin": 12509, "end": 13457, "idx": 20}, {"begin": 13490, "end": 13882, "idx": 21}, {"begin": 13911, "end": 14414, "idx": 22}, {"begin": 14415, "end": 14842, "idx": 23}, {"begin": 14854, "end": 15552, "idx": 24}, {"begin": 15576, "end": 16333, "idx": 25}, {"begin": 16334, "end": 16904, "idx": 26}, {"begin": 16970, "end": 17710, "idx": 27}, {"begin": 17711, "end": 18384, "idx": 28}, {"begin": 18407, "end": 19452, "idx": 29}, {"begin": 19464, "end": 19797, "idx": 30}, {"begin": 19817, "end": 21326, "idx": 31}, {"begin": 21339, "end": 21850, "idx": 32}, {"begin": 21884, "end": 22420, "idx": 33}, {"begin": 22421, "end": 22956, "idx": 34}, {"begin": 22989, "end": 23380, "idx": 35}, {"begin": 23381, "end": 23847, "idx": 36}, {"begin": 23848, "end": 24301, "idx": 37}, {"begin": 24334, "end": 25162, "idx": 38}, {"begin": 25194, "end": 26437, "idx": 39}, {"begin": 26473, "end": 27476, "idx": 40}, {"begin": 27513, "end": 27951, "idx": 41}, {"begin": 27952, "end": 28777, "idx": 42}, {"begin": 28778, "end": 29261, "idx": 43}, {"begin": 29262, "end": 29596, "idx": 44}, {"begin": 29634, "end": 30715, "idx": 45}, {"begin": 30754, "end": 30771, "idx": 46}, {"begin": 30772, "end": 31800, "idx": 47}, {"begin": 31830, "end": 32923, "idx": 48}, {"begin": 32940, "end": 33122, "idx": 49}, {"begin": 33123, "end": 33706, "idx": 50}, {"begin": 33707, "end": 34235, "idx": 51}, {"begin": 34236, "end": 34363, "idx": 52}, {"begin": 34364, "end": 35107, "idx": 53}, {"begin": 35108, "end": 36068, "idx": 54}, {"begin": 36083, "end": 37280, "idx": 55}, {"begin": 37295, "end": 37512, "idx": 56}, {"begin": 37513, "end": 37721, "idx": 57}, {"begin": 37763, "end": 38385, "idx": 58}, {"begin": 38427, "end": 39881, "idx": 59}, {"begin": 39930, "end": 40965, "idx": 60}, {"begin": 41006, "end": 42066, "idx": 61}, {"begin": 42120, "end": 43840, "idx": 62}, {"begin": 43880, "end": 44514, "idx": 63}], "SectionHeader": [{"begin": 0, "end": 1621, "idx": 0}], "SectionReference": [{"begin": 45691, "end": 62217, "idx": 0}], "Sentence": [{"begin": 82, "end": 349, "idx": 0}, {"begin": 350, "end": 487, "idx": 1}, {"begin": 488, "end": 667, "idx": 2}, {"begin": 668, "end": 814, "idx": 3}, {"begin": 815, "end": 878, "idx": 4}, {"begin": 879, "end": 1039, "idx": 5}, {"begin": 1040, "end": 1197, "idx": 6}, {"begin": 1198, "end": 1316, "idx": 7}, {"begin": 1317, "end": 1434, "idx": 8}, {"begin": 1435, "end": 1621, "idx": 9}, {"begin": 1639, "end": 1749, "idx": 10}, {"begin": 1750, "end": 1862, "idx": 11}, {"begin": 1863, "end": 2030, "idx": 12}, {"begin": 2031, "end": 2131, "idx": 13}, {"begin": 2132, "end": 2299, "idx": 14}, {"begin": 2300, "end": 2451, "idx": 15}, {"begin": 2452, "end": 2644, "idx": 16}, {"begin": 2645, "end": 2840, "idx": 17}, {"begin": 2841, "end": 3064, "idx": 18}, {"begin": 3065, "end": 3319, "idx": 19}, {"begin": 3320, "end": 3493, "idx": 20}, {"begin": 3494, "end": 3665, "idx": 21}, {"begin": 3666, "end": 3847, "idx": 22}, {"begin": 3848, "end": 3929, "idx": 23}, {"begin": 3930, "end": 4118, "idx": 24}, {"begin": 4119, "end": 4309, "idx": 25}, {"begin": 4310, "end": 4503, "idx": 26}, {"begin": 4504, "end": 4756, "idx": 27}, {"begin": 4757, "end": 4943, "idx": 28}, {"begin": 4944, "end": 5137, "idx": 29}, {"begin": 5138, "end": 5322, "idx": 30}, {"begin": 5323, "end": 5397, "idx": 31}, {"begin": 5398, "end": 5477, "idx": 32}, {"begin": 5478, "end": 5587, "idx": 33}, {"begin": 5588, "end": 5651, "idx": 34}, {"begin": 5652, "end": 5927, "idx": 35}, {"begin": 5928, "end": 6123, "idx": 36}, {"begin": 6124, "end": 6191, "idx": 37}, {"begin": 6192, "end": 6899, "idx": 38}, {"begin": 6900, "end": 7013, "idx": 39}, {"begin": 7014, "end": 7153, "idx": 40}, {"begin": 7154, "end": 7364, "idx": 41}, {"begin": 7365, "end": 7688, "idx": 42}, {"begin": 7689, "end": 7825, "idx": 43}, {"begin": 7826, "end": 7929, "idx": 44}, {"begin": 7966, "end": 8026, "idx": 45}, {"begin": 8027, "end": 8130, "idx": 46}, {"begin": 8131, "end": 8133, "idx": 47}, {"begin": 8134, "end": 8237, "idx": 48}, {"begin": 8238, "end": 8365, "idx": 49}, {"begin": 8366, "end": 8469, "idx": 50}, {"begin": 8470, "end": 8555, "idx": 51}, {"begin": 8556, "end": 8669, "idx": 52}, {"begin": 8670, "end": 8741, "idx": 53}, {"begin": 8742, "end": 8938, "idx": 54}, {"begin": 8939, "end": 9011, "idx": 55}, {"begin": 9012, "end": 9143, "idx": 56}, {"begin": 9144, "end": 9198, "idx": 57}, {"begin": 9199, "end": 9287, "idx": 58}, {"begin": 9288, "end": 9343, "idx": 59}, {"begin": 9372, "end": 9557, "idx": 60}, {"begin": 9558, "end": 9622, "idx": 61}, {"begin": 9623, "end": 9764, "idx": 62}, {"begin": 9765, "end": 9897, "idx": 63}, {"begin": 9898, "end": 9999, "idx": 64}, {"begin": 10000, "end": 10134, "idx": 65}, {"begin": 10135, "end": 10261, "idx": 66}, {"begin": 10262, "end": 10433, "idx": 67}, {"begin": 10434, "end": 10528, "idx": 68}, {"begin": 10557, "end": 10649, "idx": 69}, {"begin": 10650, "end": 10798, "idx": 70}, {"begin": 10799, "end": 11046, "idx": 71}, {"begin": 11047, "end": 11214, "idx": 72}, {"begin": 11215, "end": 11278, "idx": 73}, {"begin": 11279, "end": 11480, "idx": 74}, {"begin": 11481, "end": 11572, "idx": 75}, {"begin": 11573, "end": 11720, "idx": 76}, {"begin": 11721, "end": 11925, "idx": 77}, {"begin": 11926, "end": 12074, "idx": 78}, {"begin": 12105, "end": 12195, "idx": 79}, {"begin": 12196, "end": 12349, "idx": 80}, {"begin": 12350, "end": 12488, "idx": 81}, {"begin": 12509, "end": 12699, "idx": 82}, {"begin": 12700, "end": 12943, "idx": 83}, {"begin": 12944, "end": 12980, "idx": 84}, {"begin": 12981, "end": 13142, "idx": 85}, {"begin": 13143, "end": 13271, "idx": 86}, {"begin": 13272, "end": 13372, "idx": 87}, {"begin": 13373, "end": 13457, "idx": 88}, {"begin": 13490, "end": 13541, "idx": 89}, {"begin": 13542, "end": 13615, "idx": 90}, {"begin": 13616, "end": 13692, "idx": 91}, {"begin": 13693, "end": 13882, "idx": 92}, {"begin": 13911, "end": 14086, "idx": 93}, {"begin": 14087, "end": 14210, "idx": 94}, {"begin": 14211, "end": 14414, "idx": 95}, {"begin": 14415, "end": 14612, "idx": 96}, {"begin": 14613, "end": 14729, "idx": 97}, {"begin": 14730, "end": 14842, "idx": 98}, {"begin": 14854, "end": 14928, "idx": 99}, {"begin": 14929, "end": 15045, "idx": 100}, {"begin": 15046, "end": 15129, "idx": 101}, {"begin": 15130, "end": 15307, "idx": 102}, {"begin": 15308, "end": 15399, "idx": 103}, {"begin": 15400, "end": 15437, "idx": 104}, {"begin": 15438, "end": 15552, "idx": 105}, {"begin": 15576, "end": 15807, "idx": 106}, {"begin": 15808, "end": 16004, "idx": 107}, {"begin": 16005, "end": 16080, "idx": 108}, {"begin": 16081, "end": 16243, "idx": 109}, {"begin": 16244, "end": 16333, "idx": 110}, {"begin": 16334, "end": 16452, "idx": 111}, {"begin": 16453, "end": 16621, "idx": 112}, {"begin": 16622, "end": 16752, "idx": 113}, {"begin": 16753, "end": 16904, "idx": 114}, {"begin": 16970, "end": 17110, "idx": 115}, {"begin": 17111, "end": 17399, "idx": 116}, {"begin": 17400, "end": 17475, "idx": 117}, {"begin": 17476, "end": 17607, "idx": 118}, {"begin": 17608, "end": 17710, "idx": 119}, {"begin": 17711, "end": 17822, "idx": 120}, {"begin": 17823, "end": 17918, "idx": 121}, {"begin": 17919, "end": 18133, "idx": 122}, {"begin": 18134, "end": 18289, "idx": 123}, {"begin": 18290, "end": 18384, "idx": 124}, {"begin": 18407, "end": 18503, "idx": 125}, {"begin": 18504, "end": 18643, "idx": 126}, {"begin": 18644, "end": 18773, "idx": 127}, {"begin": 18774, "end": 18918, "idx": 128}, {"begin": 18919, "end": 19087, "idx": 129}, {"begin": 19088, "end": 19217, "idx": 130}, {"begin": 19218, "end": 19278, "idx": 131}, {"begin": 19279, "end": 19452, "idx": 132}, {"begin": 19464, "end": 19617, "idx": 133}, {"begin": 19618, "end": 19797, "idx": 134}, {"begin": 19817, "end": 19980, "idx": 135}, {"begin": 19981, "end": 20108, "idx": 136}, {"begin": 20109, "end": 20279, "idx": 137}, {"begin": 20280, "end": 20357, "idx": 138}, {"begin": 20358, "end": 20450, "idx": 139}, {"begin": 20451, "end": 20533, "idx": 140}, {"begin": 20534, "end": 20731, "idx": 141}, {"begin": 20732, "end": 20808, "idx": 142}, {"begin": 20809, "end": 20934, "idx": 143}, {"begin": 20935, "end": 21011, "idx": 144}, {"begin": 21012, "end": 21073, "idx": 145}, {"begin": 21074, "end": 21194, "idx": 146}, {"begin": 21195, "end": 21326, "idx": 147}, {"begin": 21339, "end": 21491, "idx": 148}, {"begin": 21492, "end": 21586, "idx": 149}, {"begin": 21587, "end": 21737, "idx": 150}, {"begin": 21738, "end": 21850, "idx": 151}, {"begin": 21884, "end": 22112, "idx": 152}, {"begin": 22113, "end": 22299, "idx": 153}, {"begin": 22300, "end": 22420, "idx": 154}, {"begin": 22421, "end": 22499, "idx": 155}, {"begin": 22500, "end": 22624, "idx": 156}, {"begin": 22625, "end": 22696, "idx": 157}, {"begin": 22697, "end": 22881, "idx": 158}, {"begin": 22882, "end": 22956, "idx": 159}, {"begin": 22989, "end": 23105, "idx": 160}, {"begin": 23106, "end": 23222, "idx": 161}, {"begin": 23223, "end": 23380, "idx": 162}, {"begin": 23381, "end": 23490, "idx": 163}, {"begin": 23491, "end": 23706, "idx": 164}, {"begin": 23707, "end": 23847, "idx": 165}, {"begin": 23848, "end": 23959, "idx": 166}, {"begin": 23960, "end": 24147, "idx": 167}, {"begin": 24148, "end": 24301, "idx": 168}, {"begin": 24334, "end": 24399, "idx": 169}, {"begin": 24400, "end": 24533, "idx": 170}, {"begin": 24534, "end": 24695, "idx": 171}, {"begin": 24696, "end": 24799, "idx": 172}, {"begin": 24800, "end": 24898, "idx": 173}, {"begin": 24899, "end": 24994, "idx": 174}, {"begin": 24995, "end": 25162, "idx": 175}, {"begin": 25194, "end": 25270, "idx": 176}, {"begin": 25271, "end": 25346, "idx": 177}, {"begin": 25347, "end": 25466, "idx": 178}, {"begin": 25467, "end": 25615, "idx": 179}, {"begin": 25616, "end": 25708, "idx": 180}, {"begin": 25709, "end": 25905, "idx": 181}, {"begin": 25906, "end": 25974, "idx": 182}, {"begin": 25975, "end": 26192, "idx": 183}, {"begin": 26193, "end": 26355, "idx": 184}, {"begin": 26356, "end": 26437, "idx": 185}, {"begin": 26473, "end": 26591, "idx": 186}, {"begin": 26592, "end": 26812, "idx": 187}, {"begin": 26813, "end": 26841, "idx": 188}, {"begin": 26842, "end": 27023, "idx": 189}, {"begin": 27024, "end": 27182, "idx": 190}, {"begin": 27183, "end": 27251, "idx": 191}, {"begin": 27252, "end": 27383, "idx": 192}, {"begin": 27384, "end": 27476, "idx": 193}, {"begin": 27513, "end": 27682, "idx": 194}, {"begin": 27683, "end": 27951, "idx": 195}, {"begin": 27952, "end": 28049, "idx": 196}, {"begin": 28050, "end": 28170, "idx": 197}, {"begin": 28171, "end": 28317, "idx": 198}, {"begin": 28318, "end": 28394, "idx": 199}, {"begin": 28395, "end": 28565, "idx": 200}, {"begin": 28566, "end": 28688, "idx": 201}, {"begin": 28689, "end": 28777, "idx": 202}, {"begin": 28778, "end": 28951, "idx": 203}, {"begin": 28952, "end": 29091, "idx": 204}, {"begin": 29092, "end": 29261, "idx": 205}, {"begin": 29262, "end": 29439, "idx": 206}, {"begin": 29440, "end": 29548, "idx": 207}, {"begin": 29549, "end": 29596, "idx": 208}, {"begin": 29634, "end": 29693, "idx": 209}, {"begin": 29694, "end": 29809, "idx": 210}, {"begin": 29810, "end": 29838, "idx": 211}, {"begin": 29839, "end": 29933, "idx": 212}, {"begin": 29934, "end": 30014, "idx": 213}, {"begin": 30015, "end": 30164, "idx": 214}, {"begin": 30165, "end": 30316, "idx": 215}, {"begin": 30317, "end": 30458, "idx": 216}, {"begin": 30459, "end": 30591, "idx": 217}, {"begin": 30592, "end": 30715, "idx": 218}, {"begin": 30754, "end": 30771, "idx": 219}, {"begin": 30772, "end": 30919, "idx": 220}, {"begin": 30920, "end": 31136, "idx": 221}, {"begin": 31137, "end": 31245, "idx": 222}, {"begin": 31246, "end": 31304, "idx": 223}, {"begin": 31305, "end": 31415, "idx": 224}, {"begin": 31416, "end": 31457, "idx": 225}, {"begin": 31458, "end": 31591, "idx": 226}, {"begin": 31592, "end": 31697, "idx": 227}, {"begin": 31698, "end": 31800, "idx": 228}, {"begin": 31830, "end": 32036, "idx": 229}, {"begin": 32037, "end": 32291, "idx": 230}, {"begin": 32292, "end": 32501, "idx": 231}, {"begin": 32502, "end": 32846, "idx": 232}, {"begin": 32847, "end": 32923, "idx": 233}, {"begin": 32940, "end": 33122, "idx": 234}, {"begin": 33123, "end": 33332, "idx": 235}, {"begin": 33333, "end": 33481, "idx": 236}, {"begin": 33482, "end": 33706, "idx": 237}, {"begin": 33707, "end": 33844, "idx": 238}, {"begin": 33845, "end": 34035, "idx": 239}, {"begin": 34036, "end": 34235, "idx": 240}, {"begin": 34236, "end": 34264, "idx": 241}, {"begin": 34265, "end": 34363, "idx": 242}, {"begin": 34364, "end": 34500, "idx": 243}, {"begin": 34501, "end": 34714, "idx": 244}, {"begin": 34715, "end": 34811, "idx": 245}, {"begin": 34812, "end": 34945, "idx": 246}, {"begin": 34946, "end": 35107, "idx": 247}, {"begin": 35108, "end": 35472, "idx": 248}, {"begin": 35473, "end": 35588, "idx": 249}, {"begin": 35589, "end": 35662, "idx": 250}, {"begin": 35663, "end": 35722, "idx": 251}, {"begin": 35723, "end": 35899, "idx": 252}, {"begin": 35900, "end": 36068, "idx": 253}, {"begin": 36083, "end": 36190, "idx": 254}, {"begin": 36191, "end": 36333, "idx": 255}, {"begin": 36334, "end": 36493, "idx": 256}, {"begin": 36494, "end": 36616, "idx": 257}, {"begin": 36617, "end": 36742, "idx": 258}, {"begin": 36743, "end": 37000, "idx": 259}, {"begin": 37001, "end": 37117, "idx": 260}, {"begin": 37118, "end": 37280, "idx": 261}, {"begin": 37295, "end": 37512, "idx": 262}, {"begin": 37513, "end": 37721, "idx": 263}, {"begin": 37763, "end": 37836, "idx": 264}, {"begin": 37837, "end": 37980, "idx": 265}, {"begin": 37981, "end": 38099, "idx": 266}, {"begin": 38100, "end": 38197, "idx": 267}, {"begin": 38198, "end": 38319, "idx": 268}, {"begin": 38320, "end": 38385, "idx": 269}, {"begin": 38427, "end": 38535, "idx": 270}, {"begin": 38536, "end": 38693, "idx": 271}, {"begin": 38694, "end": 38844, "idx": 272}, {"begin": 38845, "end": 39040, "idx": 273}, {"begin": 39041, "end": 39119, "idx": 274}, {"begin": 39120, "end": 39187, "idx": 275}, {"begin": 39188, "end": 39448, "idx": 276}, {"begin": 39449, "end": 39529, "idx": 277}, {"begin": 39530, "end": 39610, "idx": 278}, {"begin": 39611, "end": 39747, "idx": 279}, {"begin": 39748, "end": 39873, "idx": 280}, {"begin": 39874, "end": 39881, "idx": 281}, {"begin": 39930, "end": 40082, "idx": 282}, {"begin": 40083, "end": 40252, "idx": 283}, {"begin": 40253, "end": 40373, "idx": 284}, {"begin": 40374, "end": 40628, "idx": 285}, {"begin": 40629, "end": 40965, "idx": 286}, {"begin": 41006, "end": 41164, "idx": 287}, {"begin": 41165, "end": 41279, "idx": 288}, {"begin": 41280, "end": 41375, "idx": 289}, {"begin": 41376, "end": 41486, "idx": 290}, {"begin": 41487, "end": 41716, "idx": 291}, {"begin": 41717, "end": 41897, "idx": 292}, {"begin": 41898, "end": 42066, "idx": 293}, {"begin": 42120, "end": 42172, "idx": 294}, {"begin": 42173, "end": 42387, "idx": 295}, {"begin": 42388, "end": 42481, "idx": 296}, {"begin": 42482, "end": 42583, "idx": 297}, {"begin": 42584, "end": 42731, "idx": 298}, {"begin": 42732, "end": 42830, "idx": 299}, {"begin": 42831, "end": 42986, "idx": 300}, {"begin": 42987, "end": 43144, "idx": 301}, {"begin": 43145, "end": 43303, "idx": 302}, {"begin": 43304, "end": 43422, "idx": 303}, {"begin": 43423, "end": 43514, "idx": 304}, {"begin": 43515, "end": 43616, "idx": 305}, {"begin": 43617, "end": 43710, "idx": 306}, {"begin": 43711, "end": 43840, "idx": 307}, {"begin": 43880, "end": 43965, "idx": 308}, {"begin": 43966, "end": 44052, "idx": 309}, {"begin": 44053, "end": 44191, "idx": 310}, {"begin": 44192, "end": 44310, "idx": 311}, {"begin": 44311, "end": 44514, "idx": 312}], "ReferenceToFigure": [{"begin": 8759, "end": 8760, "target": "#fig_0", "idx": 0}, {"begin": 28178, "end": 28179, "target": "#fig_2", "idx": 1}, {"begin": 40094, "end": 40095, "target": "#fig_1", "idx": 2}, {"begin": 40781, "end": 40782, "idx": 3}, {"begin": 40837, "end": 40838, "target": "#fig_1", "idx": 4}, {"begin": 42127, "end": 42128, "target": "#fig_5", "idx": 5}, {"begin": 42239, "end": 42240, "target": "#fig_2", "idx": 6}, {"begin": 43122, "end": 43123, "target": "#fig_7", "idx": 7}, {"begin": 43655, "end": 43656, "target": "#fig_7", "idx": 8}], "Div": [{"begin": 82, "end": 1621, "idx": 0}, {"begin": 1624, "end": 7929, "idx": 1}, {"begin": 7931, "end": 9343, "idx": 2}, {"begin": 9345, "end": 10528, "idx": 3}, {"begin": 10530, "end": 12074, "idx": 4}, {"begin": 12076, "end": 12488, "idx": 5}, {"begin": 12490, "end": 13457, "idx": 6}, {"begin": 13459, "end": 13882, "idx": 7}, {"begin": 13884, "end": 14842, "idx": 8}, {"begin": 14844, "end": 15552, "idx": 9}, {"begin": 15554, "end": 18384, "idx": 10}, {"begin": 18386, "end": 19452, "idx": 11}, {"begin": 19454, "end": 19797, "idx": 12}, {"begin": 19799, "end": 21326, "idx": 13}, {"begin": 21328, "end": 21850, "idx": 14}, {"begin": 21852, "end": 22956, "idx": 15}, {"begin": 22958, "end": 24301, "idx": 16}, {"begin": 24303, "end": 25162, "idx": 17}, {"begin": 25164, "end": 26437, "idx": 18}, {"begin": 26439, "end": 27476, "idx": 19}, {"begin": 27478, "end": 29596, "idx": 20}, {"begin": 29598, "end": 30715, "idx": 21}, {"begin": 30717, "end": 31800, "idx": 22}, {"begin": 31802, "end": 32923, "idx": 23}, {"begin": 32925, "end": 36068, "idx": 24}, {"begin": 36070, "end": 37280, "idx": 25}, {"begin": 37282, "end": 37721, "idx": 26}, {"begin": 37723, "end": 38385, "idx": 27}, {"begin": 38387, "end": 39881, "idx": 28}, {"begin": 39883, "end": 40965, "idx": 29}, {"begin": 40967, "end": 42066, "idx": 30}, {"begin": 42068, "end": 43840, "idx": 31}, {"begin": 43842, "end": 44514, "idx": 32}], "SectionMain": [{"begin": 1621, "end": 44514, "idx": 0}]}}