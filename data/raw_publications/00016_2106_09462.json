{"text": "pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks\n\nAbstract:\nExtracting opinions from texts has gathered a lot of interest in the last years, as we are experiencing an unprecedented volume of usergenerated content in social networks and other places. A problem that social researchers find in using opinion mining tools is that they are usually behind commercial APIs and unavailable for other languages than English. To address these issues, we present pysentimiento, a multilingual Python toolkit for Sentiment Analysis and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish and English in a black-box fashion, allowing researchers to easily access these techniques.\n\n\n1 Introduction\nExtracting opinions and states-of-mind from user-generated context has drawn a lot of attention in the past years, particularly since the eclosion of Social Networks. Applications for these techniques come in many flavors, such as commercial uses, political campaigns, and even studying the changing patterns of emotions during the COVID-19 pandemics [8].\nAn issue that inhibits users of these opinion-mining technologies is that there is not a big spectrum of libraries for these tasks; mainly, one has to resort paid APIs provided by companies, or rely on models that are very out-of-the-date or even unavailable for a given language different from English.\nIn order to foster research using sentiment and other opinion-mining analysis as black-box tools, we present pysentimiento, a multilingual toolkit for these tasks. pysentimiento provides state-of-the-art transformed-based models for Sentiment Analysis and Emotion Analysis in an out-of-the-box fashion, with current support of Spanish and English. This library is released as free and open-source software 4 for anyone interested in using it for research purposes. We now describe the datasets we used, the models we trained, the results, and the selected default models in our library.\n\n1.1 Previous Work\nAppart from the commercial APIs, there are not many options of libraries providing out-of-the-box models for Sentiment Analysis. VADER [7] is a lexicon and rule-based library for Sentiment Analysis in English, specially crafted for Social Media. It provides multilingual support through translation from the target language to English.\nTextblob is a library providing pretrained models for many NLP tasks using classic machine learning models such as Naive Bayes. Support is primary in English, with some adapters for other languages.\nTransformers [14] is a library providing pretrained language models such as BERT [3] for classification and language-generation tasks. It also provides pipelines for Sentiment Analysis and other NLP tasks but -as far as we knowonly English models are provided.\n\n2 Data\nWe tackled two tasks: Sentiment Analysis and Emotion Analysis, both on Twitter datasets. For the former, we used two datasets: TASS 2020 Task 1 [4] and SemEval 2017 Task 4 Subtask 1 [12]. Both datasets were labeled with general polarity using positive, negative and neutral outcomes. We merged together the Spanish subsets for each dialect, summing up to 6,000 tweets (CHECK THIS!). The English dataset has around 50k tweets for training and around 12k for testing.\nRegarding Emotion Analysis, we used EmoEvent [1], a multilingual emotion dataset labelled with the six Ekman's basic emotions (anger, disgust, fear, joy, sadness, surprise) and also a \"neutral\" emotion.\nTable 1 summarizes this info along the number of instances used for train and test.\n\n3 Method\nTransformer-based models have become state-of-the-art in NLP, both for classification and generation tasks, displacing models based on recurrent networks. BERT [3] and GPT [11] are flagships of these models. We performed experiments with several models. For English, we tested BERT base [3], RoBERTa base [9], BERTweet [10] and multilingual models, namely Dis-tilBERT [13] and mBERT [3]. Spanish has lesser availability of models: we used BETO [2], a Spanish-trained version of BERT, and the aforementioned multilingual models.\nModels were trained in a fairly standard way, using small triangular learning rates (\u223c 10 \u22125 ) [6] for 5 epochs (10 in the case of SemEval dataset as it is big enough). For the emotion datasets we used class weights in the cross-entropy loss as it is somehow imbalanced.\nTraining (and posterior delivering) is done using transformers [14] library.\n\n4 Results\nTable 2 contains the results of the experiments for both tasks and languages. In English, the best performing model is BerTweet but just with a slight difference with RoBERTa. Both are powerful models known to have better performance than BERT ; particularly BERTweet is very suited to these SocialNLP tasks as it is entirely trained on tweets. We can observe that in Spanish BETO yields the best results for a large margin (around 6 F1 points) against mbert and distilbert, something expected as it is well known that monolingual models greatly outperform multilingual ones. Although there are models similar to BERTweet in Spanish [5] we were not able to test them as they were not available in huggingface's model hub.\n\n5 Conclusion and future work\nIn this work we presented pysentimiento, a multilingual toolkit for Sentiment Analysis and Emotion Analysis. We provide state-of-the-art models and an easyto-use interface in Python, expecting this will help researchers interested in opinion mining from social networks.\nFor the time being, we support these two tasks. We plan to expand to more tasks, such as hate speech detection, irony detection, and others. Also, we plan to provide more powerful pretrained models for other languages (mainly Spanish) to enhance performance in other languages than English.\n\nFootnotes:\n4: Code is available at http://github.com/pysentimiento/pysentimiento\n\nReferences:\n\n- Plaza del Arco, F.M., Strapparava, C., Urena Lopez, L.A., Martin, M.: Emo- Event: A multilingual emotion corpus based on different events. In: Proceed- ings of The 12th Language Resources and Evaluation Conference. pp. 1492- 1498. European Language Resources Association, Marseille, France (May 2020), https://www.aclweb.org/anthology/2020.lrec-1.186- Canete, J., Chaperon, G., Fuentes, R., P\u00e9rez, J.: Spanish pre-trained bert model and evaluation data. PML4DC at ICLR 2020 (2020)\n\n- Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\n\n- Garc\u00eda-Vegaa, M., D\u00edaz-Galianoa, M.C., Garc\u00eda-Cumbrerasa, M. \u00c1., del Arcoa, F.M.P., Montejo-R\u00e1eza, A., Jim\u00e9nez-Zafraa, S.M., C\u00e1marab, E.M., Aguilarc, C.A., Antonio, M., Cabezudod, S., et al.: Overview of tass 2020: introducing emotion de- tection (2020)\n\n- Gonzalez, J.A., Hurtado, L.F., Pla, F.: Twilbert: Pre-trained deep bidirectional transformers for spanish twitter. Neurocomputing 426, 58-69 (2021)\n\n- Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146 (2018)\n\n- Hutto, C., Gilbert, E.: Vader: A parsimonious rule-based model for sentiment anal- ysis of social media text. In: Proceedings of the International AAAI Conference on Web and Social Media. vol. 8 (2014)\n\n- Kaur, S., Kaul, P., Zadeh, P.M.: Monitoring the dynamics of emotions during covid-19 using twitter data. Procedia Computer Science 177, 423-430 (2020)\n\n- Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)\n\n- Nguyen, D.Q., Vu, T., Nguyen, A.T.: Bertweet: A pre-trained language model for english tweets. arXiv preprint arXiv:2005.10200 (2020)\n\n- Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language un- derstanding by generative pre-training (2018)\n\n- Rosenthal, S., Farra, N., Nakov, P.: Semeval-2017 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.00741 (2019)\n\n- Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019)\n\n- Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface's transformers: State-of- the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019)\n\n", "annotations": {"Abstract": [{"begin": 76, "end": 734, "idx": 0}], "Head": [{"begin": 737, "end": 751, "n": "1", "idx": 0}, {"begin": 2000, "end": 2017, "n": "1.1", "idx": 1}, {"begin": 2815, "end": 2821, "n": "2", "idx": 2}, {"begin": 3576, "end": 3584, "n": "3", "idx": 3}, {"begin": 4462, "end": 4471, "n": "4", "idx": 4}, {"begin": 5195, "end": 5223, "n": "5", "idx": 5}], "ReferenceToBib": [{"begin": 1103, "end": 1106, "target": "#b7", "idx": 0}, {"begin": 2153, "end": 2156, "target": "#b6", "idx": 1}, {"begin": 2566, "end": 2570, "target": "#b13", "idx": 2}, {"begin": 2634, "end": 2637, "target": "#b2", "idx": 3}, {"begin": 2966, "end": 2969, "target": "#b3", "idx": 4}, {"begin": 3004, "end": 3008, "target": "#b11", "idx": 5}, {"begin": 3333, "end": 3336, "target": "#b0", "idx": 6}, {"begin": 3745, "end": 3748, "target": "#b2", "idx": 7}, {"begin": 3757, "end": 3761, "target": "#b10", "idx": 8}, {"begin": 3872, "end": 3875, "target": "#b2", "idx": 9}, {"begin": 3890, "end": 3893, "target": "#b8", "idx": 10}, {"begin": 3904, "end": 3908, "target": "#b9", "idx": 11}, {"begin": 3953, "end": 3957, "target": "#b12", "idx": 12}, {"begin": 3968, "end": 3971, "target": "#b2", "idx": 13}, {"begin": 4029, "end": 4032, "target": "#b1", "idx": 14}, {"begin": 4208, "end": 4211, "target": "#b5", "idx": 15}, {"begin": 4447, "end": 4451, "target": "#b13", "idx": 16}, {"begin": 5105, "end": 5108, "target": "#b4", "idx": 17}], "ReferenceToFootnote": [{"begin": 1818, "end": 1819, "target": "#foot_0", "idx": 0}], "SectionFootnote": [{"begin": 5787, "end": 5867, "idx": 0}], "ReferenceString": [{"begin": 5884, "end": 6234, "id": "b0", "idx": 0}, {"begin": 6236, "end": 6364, "id": "b1", "idx": 1}, {"begin": 6368, "end": 6539, "id": "b2", "idx": 2}, {"begin": 6543, "end": 6796, "id": "b3", "idx": 3}, {"begin": 6800, "end": 6947, "id": "b4", "idx": 4}, {"begin": 6951, "end": 7074, "id": "b5", "idx": 5}, {"begin": 7078, "end": 7279, "id": "b6", "idx": 6}, {"begin": 7283, "end": 7433, "id": "b7", "idx": 7}, {"begin": 7437, "end": 7642, "id": "b8", "idx": 8}, {"begin": 7646, "end": 7779, "id": "b9", "idx": 9}, {"begin": 7783, "end": 7909, "id": "b10", "idx": 10}, {"begin": 7913, "end": 8040, "id": "b11", "idx": 11}, {"begin": 8044, "end": 8206, "id": "b12", "idx": 12}, {"begin": 8210, "end": 8447, "id": "b13", "idx": 13}], "ReferenceToTable": [{"begin": 3497, "end": 3498, "target": "#tab_0", "idx": 0}, {"begin": 4478, "end": 4479, "target": "#tab_1", "idx": 1}], "Footnote": [{"begin": 5798, "end": 5867, "id": "foot_0", "n": "4", "idx": 0}], "Paragraph": [{"begin": 86, "end": 734, "idx": 0}, {"begin": 752, "end": 1107, "idx": 1}, {"begin": 1108, "end": 1411, "idx": 2}, {"begin": 1412, "end": 1998, "idx": 3}, {"begin": 2018, "end": 2353, "idx": 4}, {"begin": 2354, "end": 2552, "idx": 5}, {"begin": 2553, "end": 2813, "idx": 6}, {"begin": 2822, "end": 3287, "idx": 7}, {"begin": 3288, "end": 3490, "idx": 8}, {"begin": 3491, "end": 3574, "idx": 9}, {"begin": 3585, "end": 4112, "idx": 10}, {"begin": 4113, "end": 4383, "idx": 11}, {"begin": 4384, "end": 4460, "idx": 12}, {"begin": 4472, "end": 5193, "idx": 13}, {"begin": 5224, "end": 5494, "idx": 14}, {"begin": 5495, "end": 5785, "idx": 15}], "SectionHeader": [{"begin": 0, "end": 734, "idx": 0}], "SectionReference": [{"begin": 5869, "end": 8449, "idx": 0}], "Sentence": [{"begin": 86, "end": 275, "idx": 0}, {"begin": 276, "end": 442, "idx": 1}, {"begin": 443, "end": 574, "idx": 2}, {"begin": 575, "end": 734, "idx": 3}, {"begin": 752, "end": 918, "idx": 4}, {"begin": 919, "end": 1107, "idx": 5}, {"begin": 1108, "end": 1411, "idx": 6}, {"begin": 1412, "end": 1575, "idx": 7}, {"begin": 1576, "end": 1759, "idx": 8}, {"begin": 1760, "end": 1876, "idx": 9}, {"begin": 1877, "end": 1998, "idx": 10}, {"begin": 2018, "end": 2146, "idx": 11}, {"begin": 2147, "end": 2263, "idx": 12}, {"begin": 2264, "end": 2353, "idx": 13}, {"begin": 2354, "end": 2481, "idx": 14}, {"begin": 2482, "end": 2552, "idx": 15}, {"begin": 2553, "end": 2687, "idx": 16}, {"begin": 2688, "end": 2813, "idx": 17}, {"begin": 2822, "end": 2910, "idx": 18}, {"begin": 2911, "end": 3009, "idx": 19}, {"begin": 3010, "end": 3105, "idx": 20}, {"begin": 3106, "end": 3204, "idx": 21}, {"begin": 3205, "end": 3287, "idx": 22}, {"begin": 3288, "end": 3490, "idx": 23}, {"begin": 3491, "end": 3574, "idx": 24}, {"begin": 3585, "end": 3739, "idx": 25}, {"begin": 3740, "end": 3792, "idx": 26}, {"begin": 3793, "end": 3838, "idx": 27}, {"begin": 3839, "end": 3972, "idx": 28}, {"begin": 3973, "end": 4112, "idx": 29}, {"begin": 4113, "end": 4281, "idx": 30}, {"begin": 4282, "end": 4383, "idx": 31}, {"begin": 4384, "end": 4460, "idx": 32}, {"begin": 4472, "end": 4549, "idx": 33}, {"begin": 4550, "end": 4647, "idx": 34}, {"begin": 4648, "end": 4816, "idx": 35}, {"begin": 4817, "end": 5047, "idx": 36}, {"begin": 5048, "end": 5193, "idx": 37}, {"begin": 5224, "end": 5332, "idx": 38}, {"begin": 5333, "end": 5494, "idx": 39}, {"begin": 5495, "end": 5542, "idx": 40}, {"begin": 5543, "end": 5635, "idx": 41}, {"begin": 5636, "end": 5785, "idx": 42}], "Div": [{"begin": 86, "end": 734, "idx": 0}, {"begin": 737, "end": 1998, "idx": 1}, {"begin": 2000, "end": 2813, "idx": 2}, {"begin": 2815, "end": 3574, "idx": 3}, {"begin": 3576, "end": 4460, "idx": 4}, {"begin": 4462, "end": 5193, "idx": 5}, {"begin": 5195, "end": 5785, "idx": 6}], "SectionMain": [{"begin": 734, "end": 5785, "idx": 0}]}}