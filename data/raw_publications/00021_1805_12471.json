{"text": "Neural Network Acceptability Judgments\n\nAbstract:\nThis paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.'s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.\n\n\n1 Introduction\nArtificial neural networks (ANNs) achieve a high degree of competence on many applied natural language understanding (NLU) tasks, but this does not entail that they have knowledge of grammar. A key property of a human's linguistic competence is the ability to identify in one's native language, without formal training in grammar, a contrast in acceptability 1 between pairs of sentences like those in (1). Acceptability judgments like these are the primary behavioral measure that generative linguists use to observe humans' grammatical knowledge (Chomsky, 1957; Sch\u00fctze, 1996).\n(1) a. What did Betsy paint a picture of? b. *What was a picture of painted by Betsy?\nWe train neural networks to perform acceptability judgments-following work by Lawrence et al. (2000), Lau et al. (2016), and others-in order to evaluate their acquisition of the kinds of grammatical concepts linguists identify as central to human linguistic competence. This contributes to a growing effort to test ANNs' ability to make finegrained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018). This research program seeks to provide new informative ways to evaluate ANN models popular with engineers. Furthermore, it has the potential to address foundational questions in theoretical linguistics by investigating how well unbiased learners can acquire grammatical knowledge.\nIn this paper we make four concrete contributions: (i) We introduce the Corpus of Linguistic Acceptability (CoLA), a collection of sentences from the linguistics literature with expert acceptability labels which, at over 10k examples, is by far the largest of its kind. (ii) We train several semi-supervised neural sequence models to do acceptability classification on CoLA and compare their performance with unsupervised models from Lau et al. (2016). Our best model outperforms unsupervised baselines, but falls short of human performance on CoLA by a wide margin. (iii) We analyze the impact of supervised training on acceptability classifiers by varying the domain and quantity of training data. (iv) We assess our models' performance on acceptability classification of specific linguistic phenomena. These experiments illustrate how acceptability classification and CoLA can give detailed insights into what grammatical knowledge typical neural network models can acquire. We find that our models do not show evidence of learning non-local dependencies related to agreement and questions, but do appear to acquire knowledge about basic subject-verb-object word order and verbal argument structure. Resources CoLA can be downloaded from the corpus website. 2 The code for training our baselines is available as well. 3 There are also two competition sites for evaluating acceptability classifiers on CoLA's in-domain 4 and out-of-domain 5 test sets (unlabeled). Finally, CoLA is included in the GLUE benchmark 6 (Wang et al., 2018), which also hosts CoLA training data, (unlabeled) test data, and a leaderboard.\n2 Acceptability Judgments\n\n2.1 In Linguistics\nOur investigation of acceptability classification builds on decades of established scientific knowledge in generative linguistics, where acceptability judgments are studied extensively. In his foundational work on generative syntax, Chomsky (1957) defines an empirically adequate grammar of a language L as one which generates all and only those strings of L which native speakers of L judge to be acceptable. Evaluating grammatical theories against native speaker judgments has been the dominant paradigm for research in generative syntax over the last sixty years (Sch\u00fctze, 1996). Linguists generally provide evidence in the text of their papers in the form of constructed example sentences annotated with Boolean acceptability judgments from themselves or native speakers.\n\n2.2 The Acceptability Classification Task\nWhile acceptability classification has been explored previously in computational linguistics, there is no standard approach to this task. Following common practice in generative linguistics our study focuses on the Boolean acceptability classification task. This approach is also taken in earlier computational work on this task (Lawrence et al., 2000; Wagner et al., 2009; Linzen et al., 2016). By contrast, other computational work aims to model gradient acceptability judgments (Heilman et al., 2014; Lau et al., 2016). Datasets for acceptability classification require a source of unacceptable sentences, which are not generally found in naturalistic speech or writing by native speakers. The sentences in CoLA consist entirely of examples from the linguistics literature.  Lawrence et al. (2000) and Lau et al. (2016) build datasets similar in this respect. However, at over 10k sentences, CoLA is by far the largest dataset of this kind, and represents the widest range of sources. Prior work in this area also obtains unacceptable sentences by programmatically generating fake sentences that are unlikely to be acceptable.  Wagner et al. (2009) distort real sentences by, for example, deleting words, inserting words, or altering verbal inflection. Lau et al. ( 2016) use round-trip machine-translation from English into various languages and back. We also generate fake sentences to pre-train our baselines before further training on CoLA.\nWe see several advantages in using linguistics example sentences. First, they are labeled for acceptability by the authors, thereby simplifying the annotation process. Second, because linguists present examples to motivate arguments, these sentences isolate a particular grammatical construction while minimizing superfluous content. Hence, unacceptable sentences in CoLA tend to be maximally similar to acceptable sentences and are unacceptable for a single identifiable reason.\nWe note that Gibson and Fedorenko (2010) express concern about standard practices around acceptability judgments and call for theoretical linguists to quantitatively measure the reliability of the judgments they report, sparking an ongoing dialog about the validity and reproducibility of these judgments (Sprouse and Almeida, 2012, 2017; Sprouse et al., 2013; Mahowald et al., 2016). We take no position on this general question, but perform a small human evaluation to gauge the reproducibility of the judgments in CoLA (Section 3).\n\n2.3 The Role of Minimal Pairs\nAcceptability judgments can alternatively be framed as a forced choice between minimal pairs, i.e. pairs of minimally different sentences contrasting in acceptability as in (1), where the classifier or subject selects the sentence with greater (predicted) acceptability. This kind of judgment has been taken as a standard for replicability of reported judgments in syntax articles (Sprouse and Almeida, 2012; Sprouse et al., 2013; Linzen and Oseki, 2018). It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018 Wilcox et al., , 2019)). This task is often employed to evaluate language models because the outputted probabilities for a pair of minimally different sentences are directly comparable, while the output for a single sentence cannot be taken as a measure of acceptability without some kind of normalization (Lau et al., 2016).\nWe leave a comparison of this methodology with our own for future work. We settle on the single-sentence judgment task because it is directly comparable with methodology in generative linguistics. While some work in theoretical linguists presents acceptability judgments as a ranking of two or more sentences (Sch\u00fctze, 1996, pp. 77-81), Boolean judgments are still the norm, and the dominant current theories still make Boolean predictions about whether a sentence is or is not grammatical (Chomsky, 1995, pp. 12-16). Accordingly, CoLA, but not datasets based solely on preferences between minimal pairs, may be used to evaluate models' ability to make judgments that align with both native speaker judgments and the predictions of generative theories. Nonce Words Examples like (g) illustrate impossible affixation or lexical gaps. Since these words will not appear in the vocabularies of typical word-level NLP models, they will be impossible for these models to judge.\n\n2.4 Defining (Un)acceptability\nThe acceptability judgment task as we define it still requires identifying challenging grammatical contrasts. A successful model needs to recognize (a) morphological anomalies such as mismatches in verbal inflection, (b) syntactic anomalies such as wh-movement out of extraction islands, and (c) semantic anomalies such as violations of animacy requirements of verbal arguments.\n\n3 CoLA\nThis paper introduces the Corpus of Linguistic Acceptability (CoLA), a set of example sentences from the linguistics literature labeled for acceptability. CoLA is available online, alongside source code for our baseline models, and a leaderboard showing model performance on test data using privately-held labels (see footnotes 2-6 for links).\nSources We compile CoLA with the aim of representing a wide variety of phenomena of interest in theoretical linguistics. We draw examples from linguistics publications spanning a wide time period, a broad set of topics, and a range of target audiences. In some cases, we change the content of examples slightly. To avoid irrelevant complications from out-of-vocabulary words, we restrict CoLA to the 100k most frequent words in the British National Corpus, and edit sentences as needed to remove words outside that set. For example, That new handle unscrews easily is replaced with That new handle detaches easily to avoid the out-ofvocabulary word unscrews. We make these alterations manually to preserve the author's stated intent, in this case selecting another verb that undergoes the middle voice alternation.\nFinally, we define acceptability classification as a sentence classification task. To ensure that all examples in CoLA are sentences, we augment fragmentary examples, replacing, for example, *The Bill's book with *The Bill's book has a red cover.\nSplitting the Data In addition to the train/development/test split used to control overfitting in standard benchmark datasets, CoLA is further divided into an in-domain set and an out-of-domain set, as specified in Table 2. The out-of-domain set is constructed to be about 10% the size of CoLA and to include sources of varying sizes, degrees of domain specificity, and time period. 7 The in-domain set is split three ways into training (8551 examples), development (527), and test sets (530), all drawn from the same 17 sources. The out-of-domain set is split into development (516) and a test sets (533), drawn from another 6 sources. We split CoLA in this way in order to monitor two types of overfitting during training: overfitting to the specific sentences in the training set (in-domain), and overfitting to the specific sources and phenomena represented in the training set (out-of-domain).\nPhenomena in CoLA CoLA has wide coverage of syntactic and semantic phenomena. To quantify the distribution of phenomena represented, we annotate the entire CoLA development set for the presence of constructions falling into 15 broad classes, of which 8 are discussed here, for brevity. 8 Briefly, simple labels sentences with no marked syntactic structures; adjunct labels sentences that contain adjuncts of nouns and verb phrases; comp clause labels sentences with embedded or complement clauses; to-VP labels sentences with non-\nThe more books I ask to whom he will give, the more he reads. Culicover and Jackendoff (1999) I said that my father, he was tight as a hoot-owl.\nRoss (1967) The jeweller inscribed the ring with the name.\nLevin  Figure 1 shows the frequency of these 8 features in the development set. Argument alternations are the best represented phenomenon and appear in over 40% of sentences in this sample. This is due both to the high frequency of these constructions as well as the inclusion of several sources directly addressing this topic (Levin, 1993; Collins, 2005; Rappaport Hovav and Levin, 2008). Most other constructions appear in about 10-20% of sentences, indicating that CoLA is fairly balanced according to this annotation scheme. There are likely biases in CoLA that other annotation schemes could detect. However, it is open to debate what a balanced dataset for acceptability judgments should look like. There is no agreed upon set of key phenomena in linguistics and any attempt to create one is likely to be controversial and overly simplistic. Furthermore, if such a set of phenomena did exist, the builders of a balanced dataset must decide whether it should be balanced equally across phenomena, or weighted by either the frequency in broad coverage corpora of English or the number of distinguishing syntactic contrasts associated with each phenomenon. We assume that CoLA skews towards the latter, as a major goal of linguistics articles is to document key unique facts about some phenomenon without excessive repetition.\nHuman Performance We measure human performance on a subset of CoLA to set an approximate upper bound for machine performance on acceptability classification and to estimate the reproducibility of the judgments in CoLA. We have five linguistics PhD students, all native English speakers, perform a forced-choice single-sentence acceptability judgment task on 200 sentences from CoLA, divided evenly between the in-domain and out-of-domain development sets. These human judgments are available alongside on the corpus site.\nResults appear in Table 4. Average annotator agreement with CoLA is 86.1%, and average Matthews Correlation Coefficient (MCC) 9 is 0.697. Selecting the majority decision from our annotators gives us a rough upper bound on human performance. These judgments agreed with CoLA's ratings on 87% of sentences with an MCC of 0.713. In other words, 13% of the labels in CoLA contradict the observed majority judgment.\nWe identify several reasons for disagreements between our annotators and CoLA. Errors in character recognition in the source PDFs may produce artifacts which alter the acceptability of the sentence or omit the original judgment. Based on these 200 sampled sentences, we estimate such errors occur in 1-2% of CoLA sentences. Ascribing two percentage points of disagreement to such errors, the remaining eleven points can be ascribed to a lack of context or genuine variation between the dialect spoken by the original author and that spoken by the annotator. 10 We also measure our individual annotators' agreement with the aggregate rating, yielding an average pairwise agreement of 93%, and an average MCC of 0.852.\n\n4 Experiments\nWe train several semi-supervised neural network models to do acceptability classification on CoLA. At 10k sentences, CoLA is likely too small to train a low-bias learner like a recurrent neural network without additional prior knowledge. In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017). Our best model uses a transfer learning approach in which a large sentence encoder is trained on an unsupervised real/fake discrimination objective, and a lightweight multilayer perceptron classifier is trained on top to do acceptability classification over CoLA. It also uses contexualized word embeddings inspired by ELMo (Peters et al., 2018).\nWe compare our models to a continuous bag of words (CBOW) baseline, the unsupervised models proposed by (Lau et al., 2016), and human performance. To make these comparisons more meaningful, we avoid giving our models distinct advantages over human learners by limiting the training data in two ways: (i) Aside from acceptability labels, our training has no grammatical annotation. (ii) Our large sentence encoders are limited to 100-200 million tokens of training data, which is within a factor of ten of the number of tokens human learners are exposed to during language acquisition (Hart and Risley, 1992). 11 We avoid training models on significantly more data because such models have a distinct advantage over the human learners we aim to match.\n\n4.1 Preliminaries\nLanguage Model We use an LSTM language model (LSTM LM) at various stages in our experiments: (i) Several of our models use word embeddings or hidden states from the LM as input.\n(ii) The LM generates fake data for the real/fake task. (iii) The LM is an integral part of our implementation of the method proposed by (Lau et al., 2016). We train the LM on the 100 million-token British National Corpus (BNC). It learns word embeddings from scratch for the 100k most frequent words in the BNC (with out of vocabulary words replaced by <unk>). We lowercase and tokenize the BNC data using NLTK (Bird and Loper, 2004). The LM achieves a word-level perplexity of 56.1 on the BNC.\nWord Representations We experiment with three styles of word representations: (i) We train a set of conventional fixed word embeddings as part of the training of the LM described above, which we refer to as BNC embeddings. (ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent w i as a linear combination of the hidden states h j i for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained 300-dimensional (6B) GloVe embeddings from Pennington et al. (2014). 12 Real/Fake Auxiliary Task We train sentence encoders on a real/fake task in which the objective is to distinguish real sentences from the BNC and \"fake\" English sentences automatically generated by two strategies: (i) We sample strings, e.g. (2-a), from the LSTM LM. (ii) We manipulate sentences of the BNC, e.g. (2-b), by randomly permuting a subset of the words, keeping the other words in situ. Training data includes the entire BNC and an equal amount of fake data. We lowercase and (2) a. either excessive tenure does not threaten a value to death. b. what happened in to the empire early the traditional roman portrait?\nWe choose this task because arbitrary numbers of labeled fake sentences can be generated without using any explicit knowledge of grammar in the process, and we expect that many of the same features are relevant to both the real/fake task and the downstream acceptability task.\n\n4.2 Baselines\nPooling Classifier Our real/fake classifiers and acceptability classifiers use an architecture we refer to as a pooling classifier which is based on Conneau et al. (2017). As illustrated in Figure 2, the pooling classifier consists of two parts: (i) a sentence encoder which reduces variable-length sequences of tokens into fixed-length sentence embeddings, and (ii) a lightweight classifier which outputs a classification based on the sentence embedding. In the sentence encoder, a deep bidirectional LSTM reads a sequence of word embeddings; then the forward and backward hidden states for each time step are concatenated, and max-pooling over the sequence gives a sentence embedding. In the classifier, the sentence embedding is passed through a sigmoid output layer (optionally preceded by a single hidden layer) giving a scalar representing the probability of a positive classification (either the sentence is real or acceptable, depending on the task).\nWe train several variations of pooling classifiers, as shown in Table 4. First, we train classifiers end-to-end on the real/fake task, varying the style of word embedding. The classifier portion consists only of a single softmax layer. We evaluate these classifiers on CoLA without CoLA training.\nSecond, we train pooling classifiers entirely on CoLA. We test only ELMo-style embeddings here because, unlike BNC and GloVe embeddings, they include robust contextual information about the entire sequence, eliminating the need for training a large LSTM on CoLA alone.\nThird, we transfer features learned from the real/fake task to classifiers trained on CoLA. Specifically, we freeze the weights of the sentence encoder portion of the real/fake classifiers, and train new classifiers on CoLA using the sentence embeddings as input. For these experiments, in addition to a sigmoid layer, the classifier has an additional hidden tanh layer to compensates for the fact that the sentence encoder is not fine-tuned on CoLA.\n\nLau et al. (2016)\nWe compare our models to those of Lau et al. (2016). Their models obtain an acceptability prediction from unsupervised LMs by normalizing the LM output using one of several metrics. Following their recommendation, we use the Word LogProb Min-1 metric. 13 Since this metric produces unbounded scalar scores rather than probabilities or Boolean judgments, we fit a threshold to the outputs in order to use these models as acceptability classifiers. This is done with 10-fold cross-validation on the CoLA test set: We repeatedly find the optimum threshold for 90% of the model outputs and evaluate the remaining 10% with that threshold, until all the data have been evaluated. Following their methods, we train ngram models on the BNC using their published code. 14 In place of their RNN LM, we use the same LSTM LM that we use to generate sentences 13 Where s=sentence, pLM(x) is the probability the LM assigns to string x and pu(x) is the unigram probability of string x: Word LP Min-1 = min \u2212 log p LM (w) log p u (w) , w \u2208 s . Lau et al. also get strong results with the SLOR metric. We also calculate results with SLOR but find them to be slightly worse overall, though not universally. We do not report these results, but they are available upon request. for the real/fake task.\nCBOW For a simple baseline, we train a continuous bag-of-words (CBOW) model directly on CoLA. We pass the sum of BNC word embeddings for the sentence to a multilayer perceptron with a single hidden layer.\n\n4.3 Training details\nAll neural network models are implemented in Py-Torch and optimized using Adam (Kingma and Ba, 2014). We train 20 LSTM LMs with fromscratch embeddings for up to 7 days or until completing 4 epochs without improving in development perplexity and select the best checkpoint. Hyperparameters for each experiment are chosen at random in these ranges: embedding size \u2208 [200, 600], hidden size \u2208 [600, 1200], number of layers \u2208 [1, 4], learning rate \u2208 [3 \u00d7 10 \u22123 , 10 \u22125 ], dropout rate \u2208 {0.2, 0.5}. We select the model with best performance for use in further experiments. We train 20 pooling classifiers end-to-end on real/fake data with BNC embeddings, 20 with GloVe, and 20 with ELMo-style embeddings for up to 7 days or until completing 4 epochs without improving in development MCC. We train 20 pooling classifiers end-to-end on CoLA using ELMo-style embeddings. Hyperparameters are chosen at random in these ranges: embedding size\u2208 [200, 600], hidden size \u2208 [500, 1500], number of layers \u2208 [1, 5], learning rate \u2208 [3\u00d710 \u22123 , 10 \u22125 ], dropout rate \u2208 {0.2, 0.5}.\nFor transfer learning experiments, we extract and freeze the weights from the encoders from the 5 best real/fake classifiers with BNC, GloVe, and ELMo-style embeddings, each. For every encoder, we train 10 classifiers on CoLA until completing 20 epochs without improving in MCC on the development set. Hyperparameters are chosen at random in these ranges: hidden size \u2208 [20, 1200] and learning rate \u2208 [10 \u22122 , 10 \u22125 ], dropout rate \u2208 {0.2, 0.5}.\nFor our single best model-a pooling classifier with ELMo-style embeddings, an encoder with real/fake training, and a classifier with CoLA training-the embedding size (i.e. LM hidden size) is 819 dimensions, the real/fake encoder hidden layer size is 528 dimensions, and the acceptability classifier hidden layer size is 1134.\n\n5 Results and Discussion\nTable 4 shows the results of the best run from each experiment. The best model overall is the real/fake model with ELMo-style embeddings. It achieves the highest MCC and accuracy both in-domain and out-of-domain by a large margin, outperforming even the models with access to GloVe.\nAll models with real/fake encoders and CoLA training perform better than the unsupervised models of Lau et al. ( 2016) on both evaluation metrics on the in-domain test set. Out-of-domain, Lau et al.'s baselines offer the second-best results. Our models consistently perform worse out-of-domain than in-domain, with MCC dropping by as much as 50% in one case. Since Lau et al.'s baselines don't use the training set, they perform similarly in-domain and out-of-domain. Real/fake classifiers without any additional training on CoLA tend to perform significantly worse than their counterparts with CoLA supervision.\nThe sequence models consistently outperform the word order-independent CBOW baseline, indicating that the LSTM models are using word order for acceptability classification in a non-trivial way. In line with Lau et al.'s findings, the n-gram LM baselines are worse than the LSTM LM. This result is expected given that n-gram models, but not LSTMs, have a limited feature window.\n\nDiscussion\nOf the models we have tested, LSTMs are the most effective low-bias learners for acceptability classification. Compared to humans, though, their absolute performance is underwhelming. This indicates to us that while the ANNs we study can acquire substantial knowledge of grammar, their linguistic competence is far from rivaling humans'.\nOur models with unsupervised pretraining have an advantage over similar models without pretraining. This finding aligns with the conclusions of Peters et al. (2018). We see this effect with both the LM pretraining for our ELMo-style embeddings real/fake pretraining for our sentence encoders. Unsurprisingly, the unsupervised Lau et al. models and real/fake classifiers are not as effective as models trained on CoLA. However, they far outperform random guessing and the CBOW baseline, indicating that even purely unsupervised models acquire significant knowledge of grammar.\nThe supervised models universally see a substantial drop in performance from the in-domain test set to the out-of-domain test set. This suggests that they have specialized somewhat to the phenomena in the training set, rather than English grammar in a fully general way as one would hope for. Addressing this problem will likely involve new forms of regularization to mitigate this overfitting and, more importantly, new pretraining strategies that can help the model better learn the fundamental ingredients of grammaticality from unlabeled data.\n\n6 CoLA Design Experiments\nThe results in the previous section highlight the effects of pretraining, but give little insight into how the labeled training data in CoLA impacts classifier performance. To quantify the impact of CoLA training, we conduct two additional experiments: First, we measure how the amount of training data impacts model performance on the CoLA development set. Second, we investigate how the specific contents of the in-domain and out-of-domain sets impact model generalization.\nTraining Set Size In this experiment, we vary the amount of training data seen by our acceptability classifiers. We construct alternate training sets of sizes 100, 300, 1000, and 3000 by randomly downsampling the 8551-example CoLA training set. Then, for each training set we train classifiers with 20 restarts using the best performing ELMo-style real/fake encoder, and evaluate on the entire development set. Figure 3 plots the results. As training data increases from 100 to 8551 sentences, we see approximately log-linear improvements in performance. The small decrease in performance between 1000 and 3000 sentences is likely an artifact of the random downsampling.\nFrom these results we draw two main conclusions: First, it appears that increasing the amount of training data in CoLA by an order of magnitude may significantly benefit our models. Second, much of what our models learn from CoLA can be learned from as few as 300 training examples. This suggests that CoLA training is not teaching our models specific facts about acceptability as much as teaching them to use existing grammatical knowledge from the sentence encoders.\nSplitting CoLA Our results in Table 4 show that our models' performance drops noticeably when tested on out-of-domain sentences from publications not represented in the training data. In this experiment, we investigate different splits of CoLA into in-domain and out-of-domain to test the degree to which the decrease in performance on out-of-domain sentences is a stable property of these models, or simply an artifact of the particular publications represented in the out-of-domain set (as described in section 3).\nThe splits are constructed by randomly select- Table 5 : Results for 5 different splits of CoLA and the original split into in-domain and out-of-domain. All results are averages over 20 restarts. Out N is the number of out-of-domain sentences. Sources are abbreviated by authors' last initial and year; full citations for each source are shown in Table 2. In Table 5, we report the average test performance over 20 restarts. We conclude that the domain difference between two samples of sources in CoLA is generally a meaningful one for these models. This is especially so for the original split, where average in-domain MCC is 0.125 greater than out-of-domain MCC, close to the maximum observed difference of 0.162. By contrast, in one case average out-of-domain performance was actually better. This tells us that the particular nature of the sources in each domain has a large effect on what our models learn.\n\n7 Phenomenon-Specific Analysis\nIn addition to testing the general grammatical knowledge of low-bias learners, acceptability classification can be used to probe models' knowledge of particular linguistic phenomena. We analyze our baselines' performance by phenomenon using two methods: First, we break down their performance on CoLA based on the different constructions present in the target sentences. Second, we evaluate them on controlled test sets targeting specific grammatical contrasts.\n\n7.1 CoLA Performance by Phenomenon\nIn this error analysis, we study performance on CoLA as a function of the syntactic features of the individual sentences, using the 8 features described in Section 3. We train classifiers with 20 restarts using the best performing ELMo-style real/fake encoder. For each feature, we measure the MCC of our models on only those sentences with that feature.\nFigure 4 shows the mean MCC over 20 restarts for each feature. Unsurprisingly, syntactically simple sentences are easier than average, but unexpectedly sentences with adjuncts are as well. Sentences with complement clauses, embedded VPs, and argument alternations are about as hard as the average sentence in CoLA. While these constructions can be complex, they also occur with very high frequency. Sentences with binding and violations, including morphological violations, are among the hardest. We also find that our models perform poorly on sentences with question- like syntax. This difficulty is likely due to longdistance dependencies in these sentences.\n\n7.2 Targeted Test Sets\nHere, we run additional evaluations to probe whether our models can reliably classify sets of sentences which target a single grammatical contrast. This kind of evaluation can give insight into what kinds of grammatical features our models do and do not acquire easily. Using data generation techniques inspired by Ettinger et al. (2016), we build five auxiliary datasets (described below) using simple rewrite grammars which target specific grammatical contrasts.\nUnlike in CoLA, none of these judgments are meant to be difficult or subtle, and we expect that most humans could reach perfect accuracy. We also take care to make the test sentences as simple as possible to reduce classification errors unrelated to the target contrast. Specifically, we limit noun phrases to 1 or 2 words and use semantically related vocabulary items within examples.\nSubject-Verb-Object This test set consists of 100 triples of subject, verb, and object each appearing in five permutations of (SVO, SOV, VSO, VOS, OVS). 15 The set of 100 triples is the Cartesian product product of three sets containing 10 subjects ({John, Bo, ...}), 2 verbs ({read, wrote}), and 5 objects ({the book, the letter, ...}).\n( 3 Used intransitively (6-b), it is the subject (the bubble) that undergoes a change of state and the cause need not be specified (Levin, 1993)\n\nResults\nThe results from these experiments are given in Table 6. Our models' performance on these test sets is mixed. They make some systematic acceptability judgments that reflect correct grammatical generalizations. Some models are very effective at judging violations in gross word order (SVO in Table 6). The pooling classifier with GloVe embeddings achieves near perfect correlation, suggesting that it systematically uses gross word order. However, the remaining tests yield much poorer performance. Our models consistently outperform Lau et al.'s baselines on lexical semantics (Causative), judging more accurately whether a verb can undergo the causative-inchoative alternation. This may be due in part to the fact that our models receive supervision from CoLA, in which argument alternations are well represented (see Figure 1).\nLau et al.'s baseline outperforms our models in some cases. The LSTM LM with the Word LP Min-1 metric is the only model that can reliably identify the non-local dependency between a wh-word and its gap (Wh-extraction). It also performs relatively better on judgments involving agreement (Singular/Pl). All models struggle on the Reflexive examples.\nThe poor performance of our models on contrasts involving agreement (Singular/Pl and Reflexive) is surprising in light of findings by (Linzen et al., 2016) that LSTMs can identify agreement errors easily even without access to sub-word information. We speculate that this is due to underrepresentation of the relevant examples in CoLA. We estimate that morphological violations make up about 6% of examples in CoLA (about half of the Violations in Figure 1).\n\n8 Motivation & Related Work\nWe see two chief motivations that guide work on acceptability classification with ANNs by us and by others: First, more fine-grained evaluation tools may accelerate work on general-purpose neural network modules for sentence understanding. Second, studying the linguistic competence of ANNs bears on foundational questions in linguistics about the learnability of grammar.\n\nFine-Grained Evaluation of ANNs\nThe question of how well ANNs learn fine-grained grammatical distinctions has been the subject of much recent work. One method is to train models to perform probing tasks which target a construction of interest. Examples of such tasks are to determine whether the sentence is in active or passive voice (Shi et al., 2016), whether the subject is singular or plural (Conneau et al., 2018), or whether a given token is under the scope of negation (Ettinger et al., 2018). In each case, the authors use these tasks to compare the performance of reusable sentence embeddings.\nAcceptability classification can be used to target many of the same grammatical constructions as probing tasks. For instance, an acceptability classifier that can reliably distinguish between pairs of sentences as in (9) must have implicit knowledge of the whether the subject of a sentence is singular or plural (in the first case) and whether the token ever is under the scope of negation. These exact experiments have been conducted by Linzen et al. (2016) and Marvin and Linzen (2018), re-spectively, although these works differ from our approach in that they do not evaluate domain general acceptability classifiers on these contrasts.\n(9) a. The key is/*are on the table.\nb. Betsy hasn't/*has ever been to France.\nAcceptability classification also enables certain kinds of investigations not possible with probing tasks. A single acceptability classifier can be trained to identify numerous unrelated contrasts. This is generally not possible with probing tasks, because the classes are tied to specific grammatical concepts. Acceptability classification also encourages direct comparison between ANN and human linguistic competence because, unlike many probing tasks, it can be easily performed by native speakers without linguistic training. Finally acceptability classifiers and generative grammars share a common objective, namely to predict the well-formedness of all and only those strings of the language that are acceptable to native speakers. Accordingly, it is straightforward to draw parallels between acceptability classifiers and established work in generative linguistics.\nThe Poverty of the Stimulus Research on acceptability classification can also be brought to bear on a foundational question in linguistic theory: The extent to which human linguistic competence is learned or innate. The influential argument from the poverty of the stimulus (APS) holds that the extent of human linguistic competence cannot be explained by purely domain general learning mechanisms and that humans must be born with a Universal Grammar which imparts specific knowledge of grammatical universals to the child and makes learning possible (Chomsky, 1965). While the APS has been subject to much criticism (Pullum and Scholz, 2002), it remains a foundation of much of contemporary linguistics.\nIn the setting of machine learning, the APS predicts that any artificial leaner trained with no prior knowledge of the principles of syntax and no more data than a human child sees must fail to make acceptability judgments with human-level accuracy (Clark and Lappin, 2011). If linguistically-uninformed neural network models achieve human-level performance on specific phenomena or on a domain-general dataset like CoLA, this would be clear evidence limiting the scope of phenomena for which the APS can hold.\nHowever, acceptability classification alone can-not evaluate aspects of ANNs' linguistic competence against humans' in every relevant way. For example, Berwick et al. (2011) note that native speakers can easily recognize that, e.g., in Bo is easy to please, Bo is the entity being pleased, while in Bo is eager to please, Bo is the one who does the pleasing. Since the acceptability judgments in CoLA are reading-independent (see Table 1), they cannot be used to probe whether ANNs understand these distinctions.\nWe wish to stress that the success of supervised acceptability classifiers like the ones we train cannot falsify the APS, because unacceptable examples play no apparent role in child language acquisition. While unsupervised acceptability classification could do so, more work is needed to find methods for extracting reliable Boolean acceptability judgments from unsupervised language models. Our approach of fitting a threshold to the models of Lau et al. (2016) gives encouraging results, but these models are ultimately not as effective as supervised models. An alternative adopted by Linzen et al. (2016) and Marvin and Linzen (2018) is to evaluate whether language models' assign higher probability to the acceptable sentence in a minimal pair. However, this forced choice minimal pair task, as discussed in Section 2.3, cannot be applied to CoLA, which does not exclusively contain minimal pairs. Still, we maintain that our approach is a valuable step in the direction of evaluating the APS. Our results strongly suggest that grammatically unbiased sentence embeddings and contextualized word embeddings have non-trivial implicit knowledge of grammar before supervised training on CoLA. As our experiments in Section 6 show, a significant portion of what these models learn from CoLA can be learned from relatively little acceptability judgment data (as few as 300 sentences, of which fewer than 100 are unacceptable). Furthermore, the real/fake encoders and ELMo-style embeddings are trained on a quantity of data comparable to what human learners are exposed to. Given the rapid pace of development of new robust sentence embeddings, we expect to see increasingly human-like acceptability judgments from powerful neural networks in coming years, though with an eye towards evaluating the APS, future work should continue to investigate acceptability classifiers with unsupervised methods and restricted training resources. This work offers resources and baselines for the study of semi-supervised machine learning for acceptability judgments. Most centrally, we introduce CoLA, the first large-scale corpus of acceptability judgments, making it possible to train and evaluate modern neural networks on this task.\nIn baseline experiments, we find that a network trained on our artificial real/fake task, combined with ELMo-style word representations, outperforms other available models, but remains far from human performance.\nMuch work remains to be done to implement the agenda described in Section 8. There is much untapped potential in the acceptability classification task as a fine-grained evaluation tool and as a test of the Poverty of the Stimulus Argument. We hope for future work to test the performance of a broader range of new effective low-bias machine learning models on CoLA, and to investigate further what grammatical principles these models do and do not learn.\n\nFootnotes:\n* Current affiliation. This work was completed when the author was at New York University.1 Following terminological conventions in linguistics, a sentence's grammaticality is determined by a grammatical formalism, while its acceptability is determined by introspective judgments of native speakers (Sch\u00fctze, 1996).\n2: https://nyu-mll.github.io/CoLA/\n3: https://github.com/nyu-mll/\n7: In Section 6 we consider several alternate splits of CoLA.\n8: The annotated data also includes 63 fine-grained features. The annotated data is available for download on the CoLA website alongside detailed annotation guidelines.\n9: MCC (Matthews, 1975) is an evaluation metric for unbalanced binary classifiers. It is a special case of Pearson's r for Boolean variables, i.e. it measures correlation of two Boolean distributions, giving a value between -1 and 1. On average, any two unrelated distributions will have an MCC of 0, regardless of class imbalance. By contrast, accuracy and F1 favor classifiers with a majority-class bias.\n10: We observe greater disagreement between human annotators and published judgments than Sprouse et al. (2013) do. As a reviewer points out, this may be due to the fact that Sprouse et al. measure agreement with minimal pairs of sentences using a forced choice task, which is more constrained and arguably easier than single sentence judgments.\n11:  Hart and Risley (1992) find that children in affluent families are exposed to about 45 million tokens by age 4.\n12: 12  Results with models that use these GloVe embeddings are less immediately comparable with human performance results, since GloVe is trained on several orders of magnitude more text than humans see during language acquisition.\n15: OSV is excluded because it does not yield a clear acceptability rating. Examples such as \"The book John read\", can be interpreted as marginally acceptable sentences with topicalized subjects, or as acceptable noun phrases (rather than sentences) with relative clause modifiers.\n\nReferences:\n\n- David Adger. 2003. Core Syntax: A Minimalist Approach. Oxford University Press Oxford.- Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In Proceedings of ICLR Con- ference Track. Toulon, France.\n\n- Mark R. Baltin. 1982. A landing site theory of movement rules. Linguistic Inquiry, 13(1):1- 38.\n\n- Mark R. Baltin and Chris Collins, editors. 2001. Handbook of Contemporary Syntactic Theory. Blackwell Publishing Ltd.\n\n- Robert C Berwick, Paul Pietroski, Beracah Yankama, and Noam Chomsky. 2011. Poverty of the stimulus revisited. Cognitive Science, 35(7):1207-1242.\n\n- Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstra- tion sessions, page 31. Association for Compu- tational Linguistics.\n\n- Joan W. Bresnan. 1973. Syntax of the compara- tive clause construction in English. Linguistic Inquiry, 4(3):275-343.\n\n- Andrew Carnie. 2013. Syntax: A Generative In- troduction. John Wiley & Sons.\n\n- Noam Chomsky. 1957. Syntactic Structures.\n\n- Mouton. Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.\n\n- Noam Chomsky. 1995. The Minimalist Program. MIT press.\n\n- Sandra Chung, William A. Ladusaw, and James McCloskey. 1995. Sluicing and logical form. Natural Language Semantics, 3(3):239-282.\n\n- Alexander Clark and Shalom Lappin. 2011. Lin- guistic Nativism and the Poverty of the Stimulus. John Wiley & Sons.\n\n- Chris Collins. 2005. A smuggling approach to the passive in English. Syntax, 8(2):81-120.\n\n- Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Su- pervised learning of universal sentence repre- sentations from natural language inference data. In Proceedings of the 2017 Conference on Em- pirical Methods in Natural Language Process- ing, pages 670-680.\n\n- Alexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00efc Barrault, and Marco Baroni.\n\n- What you can cram into a single &!#* vector: Probing sentence embeddings for lin- guistic properties. In ACL 2018-56th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 2126-2136. Asso- ciation for Computational Linguistics.\n\n- Peter W. Culicover and Ray Jackendoff. 1999. The view from the periphery: The English comparative correlative. Linguistic Inquiry, 30(4):543-571.\n\n- Veneeta Dayal. 1998. Any as inherently modal. Linguistics and Philosophy, 21(5):433-476.\n\n- Allyson Ettinger, Ahmed Elgohary, Colin Phillips, and Philip Resnik. 2018. Assessing composi- tion in sentence vector representations. In Pro- ceedings of the 27th International Conference on Computational Linguistics, pages 1790- 1801. Association for Computational Linguis- tics.\n\n- Allyson Ettinger, Ahmed Elgohary, and Philip Resnik. 2016. Probing for semantic evidence of composition by means of simple classifica- tion tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134-139.\n\n- Richard Futrell, Ethan Wilcox, Takashi Morita, and Roger Levy. 2018. RNNs as psycholinguis- tic subjects: Syntactic state and grammatical de- pendency. arXiv preprint arXiv:1809.01329.\n\n- Gerald Gazdar. 1981. Unbounded dependencies and coordinate structure. In The Formal Com- plexity of Natural Language, pages 183-226. Springer.\n\n- Edward Gibson and Evelina Fedorenko. 2010. Weak quantitative standards in linguistics research. Trends in Cognitive Sciences, 14(6):233-234.\n\n- Adele E. Goldberg and Ray Jackendoff. 2004. The English resultative as a family of constructions. Language, 80(3):532-568.\n\n- Betty Hart and Todd R. Risley. 1992. American parenting of language-learning children: Per- sisting differences in family-child interactions observed in natural home environments. Devel- opmental Psychology, 28(6):1096.\n\n- Michael Heilman, Aoife Cahill, Nitin Madnani, Melissa Lopez, Matthew Mulholland, and Joel Tetreault. 2014. Predicting grammaticality on an ordinal scale. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers), volume 2, pages 174-180.\n\n- Ray S. Jackendoff. 1971. Gapping and related rules. Linguistic Inquiry, 2(1):21-35.\n\n- Nirit Kadmon and Fred Landman. 1993. Any. Linguistics and Philosophy, 16(4):353-422.\n\n- Jong-Bok Kim and Peter Sells. 2008. English Syn- tax: An Introduction. CSLI Publications.\n\n- Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Pro- ceedings of the 3rd International Conference on Learning Representations.\n\n- Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. 2015. Skip-thought vec- tors. In Advances in Neural Information Pro- cessing Systems, pages 3294-3302.\n\n- Jey Han Lau, Alexander Clark, and Shalom Lap- pin. 2016. Grammaticality, acceptability, and probability: A probabilistic view of linguis- tic knowledge. Cognitive Science, 41(5):1202- 1241.\n\n- Steve Lawrence, C. Lee Giles, and Sandiway Fong. 2000. Natural language grammatical in- ference with recurrent neural networks. IEEE Transactions on Knowledge and Data Engi- neering, 12(1):126-140.\n\n- Beth Levin. 1993. English Verb Classes and Al- ternations: A preliminary investigation. Uni- versity of Chicago Press.\n\n- Tal Linzen, Emmanuel Dupoux, and Yoav Gold- berg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transac- tions of the Association for Computational Lin- guistics, 4:521-535.\n\n- Tal Linzen and Yohei Oseki. 2018. The reliabil- ity of acceptability judgments across languages. Glossa: a journal of general linguistics, 3(1).\n\n- Kyle Mahowald, Peter Graff, Jeremy Hartman, and Edward Gibson. 2016. SNAP judgments: A small n acceptability paradigm (SNAP) for linguistic acceptability judgments. Language, 92(3):619-635.\n\n- Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empir- ical Methods in Natural Language Processing, pages 1192-1202.\n\n- Brian W. Matthews. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451.\n\n- Jim Miller. 2002. An Introduction to English Syn- tax. Edinburgh University Press.\n\n- Jeffrey Pennington, Richard Socher, and Christo- pher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing, pages 1532-1543.\n\n- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextu- alized word representations. In Proceedings of the 2018 Conference of the North Ameri- can Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), volume 1, pages 2227-2237.\n\n- Geoffrey K. Pullum and Barbara C. Scholz. 2002. Empirical assessment of stimulus poverty argu- ments. The Linguistic Review, 18(1-2):9-50.\n\n- Malka Rappaport Hovav and Beth Levin. 2008. The English dative alternation: The case for verb sensitivity. Journal of Linguistics, 44(1):129-167.\n\n- John Robert Ross. 1967. Constraints on Variables in Syntax. Ph.D. thesis, MIT.\n\n- Ivan A. Sag. 1997. English relative clause con- structions. Journal of Linguistics, 33(2):431- 483.\n\n- Ivan A. Sag, Gerald Gazdar, Thomas Wasow, and Steven Weisler. 1985. Coordination and how to distinguish categories. Natural Language & Linguistic Theory, 3(2):117-171.\n\n- Ivan A. Sag, Thomas Wasow, and Emily M. Ben- der. 2003. Syntactic Theory: A Formal Intro- duction, 2 edition. CSLI Publications.\n\n- Carson T. Sch\u00fctze. 1996. The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic Methodology. University of Chicago Press.\n\n- Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural MT learn source syn- tax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro- cessing, pages 1526-1534.\n\n- Dominique Sportiche, Hilda Koopman, and Ed- ward Stabler. 2013. An Introduction to Syntac- tic Analysis and Theory. John Wiley & Sons.\n\n- Jon Sprouse and Diogo Almeida. 2012. Assess- ing the reliability of textbook data in syntax: Adger's Core Syntax. Journal of Linguistics, 48(3):609-652.\n\n- Jon Sprouse and Diogo Almeida. 2017. Set- ting the empirical record straight: Acceptabil- ity judgments appear to be reliable, robust, and replicable. Behavioral and Brain Sciences, 40.\n\n- Jon Sprouse, Carson T. Sch\u00fctze, and Diogo Almeida. 2013. A comparison of informal and formal acceptability judgments using a ran- dom sample from Linguistic Inquiry 2001- 2010. Lingua, 134:219-248.\n\n- Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Ex- periments in sentence classification. CALICO Journal, 26(3):474-490.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language under- standing. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Inter- preting Neural Networks for NLP, pages 353- 355.\n\n- Ethan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018. What do RNN lan- guage models learn about filler-gap dependen- cies? In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Inter- preting Neural Networks for NLP, pages 211- 221.\n\n- Ethan Wilcox, Peng Qian, Richard Futrell, Miguel Ballesteros, and Roger Levy. 2019. Struc- tural supervision improves learning of non-local grammatical dependencies. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3302-3312.\n\n- Edwin Williams. 1980. Predication. Linguistic Inquiry, 11(1):203-238.\n\n", "annotations": {"Abstract": [{"begin": 40, "end": 843, "idx": 0}], "Head": [{"begin": 846, "end": 860, "n": "1", "idx": 0}, {"begin": 3936, "end": 3954, "n": "2.1", "idx": 1}, {"begin": 4732, "end": 4773, "n": "2.2", "idx": 2}, {"begin": 7238, "end": 7267, "n": "2.3", "idx": 3}, {"begin": 9170, "end": 9200, "n": "2.4", "idx": 4}, {"begin": 9581, "end": 9587, "n": "3", "idx": 5}, {"begin": 15609, "end": 15622, "n": "4", "idx": 6}, {"begin": 17108, "end": 17125, "n": "4.1", "idx": 7}, {"begin": 19299, "end": 19312, "n": "4.2", "idx": 8}, {"begin": 21290, "end": 21307, "idx": 9}, {"begin": 22796, "end": 22816, "n": "4.3", "idx": 10}, {"begin": 24653, "end": 24677, "n": "5", "idx": 11}, {"begin": 25953, "end": 25963, "idx": 12}, {"begin": 27427, "end": 27452, "n": "6", "idx": 13}, {"begin": 30500, "end": 30530, "n": "7", "idx": 14}, {"begin": 30994, "end": 31028, "n": "7.1", "idx": 15}, {"begin": 32046, "end": 32068, "n": "7.2", "idx": 16}, {"begin": 33404, "end": 33411, "idx": 17}, {"begin": 35051, "end": 35078, "n": "8", "idx": 18}, {"begin": 35453, "end": 35484, "idx": 19}], "ReferenceToBib": [{"begin": 552, "end": 569, "target": "#b32", "idx": 0}, {"begin": 1409, "end": 1424, "target": "#b8", "idx": 1}, {"begin": 1425, "end": 1439, "target": "#b49", "idx": 2}, {"begin": 1605, "end": 1627, "target": "#b33", "idx": 3}, {"begin": 1629, "end": 1646, "target": "#b32", "idx": 4}, {"begin": 1901, "end": 1922, "target": "#b35", "idx": 5}, {"begin": 1923, "end": 1940, "target": "#b1", "idx": 6}, {"begin": 1941, "end": 1962, "idx": 7}, {"begin": 1963, "end": 1985, "target": "#b19", "idx": 8}, {"begin": 1986, "end": 2010, "target": "#b38", "idx": 9}, {"begin": 2727, "end": 2744, "target": "#b32", "idx": 10}, {"begin": 3809, "end": 3828, "target": "#b56", "idx": 11}, {"begin": 4188, "end": 4202, "target": "#b8", "idx": 12}, {"begin": 4521, "end": 4536, "target": "#b49", "idx": 13}, {"begin": 5103, "end": 5126, "target": "#b33", "idx": 14}, {"begin": 5127, "end": 5147, "target": "#b55", "idx": 15}, {"begin": 5148, "end": 5168, "target": "#b35", "idx": 16}, {"begin": 5255, "end": 5277, "target": "#b26", "idx": 17}, {"begin": 5278, "end": 5295, "target": "#b32", "idx": 18}, {"begin": 5552, "end": 5574, "target": "#b33", "idx": 19}, {"begin": 5579, "end": 5596, "target": "#b32", "idx": 20}, {"begin": 5905, "end": 5925, "target": "#b55", "idx": 21}, {"begin": 6715, "end": 6742, "target": "#b23", "idx": 22}, {"begin": 7007, "end": 7019, "idx": 23}, {"begin": 7020, "end": 7040, "idx": 24}, {"begin": 7041, "end": 7062, "target": "#b54", "idx": 25}, {"begin": 7063, "end": 7085, "target": "#b37", "idx": 26}, {"begin": 7649, "end": 7676, "target": "#b52", "idx": 27}, {"begin": 7677, "end": 7698, "target": "#b54", "idx": 28}, {"begin": 7699, "end": 7722, "target": "#b36", "idx": 29}, {"begin": 7782, "end": 7803, "target": "#b35", "idx": 30}, {"begin": 7804, "end": 7828, "target": "#b38", "idx": 31}, {"begin": 7829, "end": 7850, "target": "#b21", "idx": 32}, {"begin": 7851, "end": 7870, "target": "#b57", "idx": 33}, {"begin": 7871, "end": 7894, "target": "#b58", "idx": 34}, {"begin": 8177, "end": 8195, "target": "#b32", "idx": 35}, {"begin": 8506, "end": 8532, "idx": 36}, {"begin": 8687, "end": 8713, "idx": 37}, {"begin": 12955, "end": 12968, "target": "#b34", "idx": 38}, {"begin": 12969, "end": 12983, "target": "#b13", "idx": 39}, {"begin": 12984, "end": 13016, "target": "#b44", "idx": 40}, {"begin": 15965, "end": 15985, "target": "#b31", "idx": 41}, {"begin": 15986, "end": 16007, "target": "#b14", "idx": 42}, {"begin": 16333, "end": 16354, "target": "#b42", "idx": 43}, {"begin": 16460, "end": 16478, "target": "#b32", "idx": 44}, {"begin": 16940, "end": 16963, "target": "#b25", "idx": 45}, {"begin": 17441, "end": 17459, "target": "#b32", "idx": 46}, {"begin": 17716, "end": 17738, "target": "#b5", "idx": 47}, {"begin": 18102, "end": 18123, "target": "#b42", "idx": 48}, {"begin": 19462, "end": 19483, "target": "#b14", "idx": 49}, {"begin": 21342, "end": 21359, "target": "#b32", "idx": 50}, {"begin": 26446, "end": 26466, "target": "#b42", "idx": 51}, {"begin": 32384, "end": 32406, "target": "#b20", "idx": 52}, {"begin": 33389, "end": 33402, "target": "#b34", "idx": 53}, {"begin": 34725, "end": 34746, "target": "#b35", "idx": 54}, {"begin": 35788, "end": 35806, "target": "#b50", "idx": 55}, {"begin": 35850, "end": 35872, "idx": 56}, {"begin": 35930, "end": 35953, "target": "#b19", "idx": 57}, {"begin": 36496, "end": 36516, "target": "#b35", "idx": 58}, {"begin": 36521, "end": 36545, "target": "#b38", "idx": 59}, {"begin": 38202, "end": 38217, "target": "#b9", "idx": 60}, {"begin": 38268, "end": 38293, "target": "#b43", "idx": 61}, {"begin": 38605, "end": 38629, "target": "#b12", "idx": 62}, {"begin": 39019, "end": 39040, "target": "#b4", "idx": 63}, {"begin": 39826, "end": 39843, "target": "#b32", "idx": 64}, {"begin": 39968, "end": 39988, "target": "#b35", "idx": 65}, {"begin": 39993, "end": 40017, "target": "#b38", "idx": 66}, {"begin": 42581, "end": 42596, "target": "#b49", "idx": 67}, {"begin": 42902, "end": 42918, "target": "#b39", "idx": 68}, {"begin": 43392, "end": 43413, "target": "#b54", "idx": 69}, {"begin": 43653, "end": 43675, "target": "#b25", "idx": 70}], "ReferenceToFootnote": [{"begin": 3554, "end": 3555, "target": "#foot_1", "idx": 0}, {"begin": 3614, "end": 3615, "target": "#foot_2", "idx": 1}, {"begin": 11377, "end": 11378, "target": "#foot_3", "idx": 2}, {"begin": 12179, "end": 12180, "target": "#foot_4", "idx": 3}, {"begin": 15449, "end": 15451, "target": "#foot_6", "idx": 4}, {"begin": 16965, "end": 16967, "target": "#foot_7", "idx": 5}, {"begin": 33073, "end": 33075, "target": "#foot_9", "idx": 6}], "SectionFootnote": [{"begin": 42271, "end": 44279, "idx": 0}], "ReferenceString": [{"begin": 44296, "end": 44382, "id": "b0", "idx": 0}, {"begin": 44384, "end": 44601, "id": "b1", "idx": 1}, {"begin": 44605, "end": 44700, "id": "b2", "idx": 2}, {"begin": 44704, "end": 44821, "id": "b3", "idx": 3}, {"begin": 44825, "end": 44970, "id": "b4", "idx": 4}, {"begin": 44974, "end": 45182, "id": "b5", "idx": 5}, {"begin": 45186, "end": 45302, "id": "b6", "idx": 6}, {"begin": 45306, "end": 45382, "id": "b7", "idx": 7}, {"begin": 45386, "end": 45427, "id": "b8", "idx": 8}, {"begin": 45431, "end": 45502, "id": "b9", "idx": 9}, {"begin": 45506, "end": 45560, "id": "b10", "idx": 10}, {"begin": 45564, "end": 45693, "id": "b11", "idx": 11}, {"begin": 45697, "end": 45811, "id": "b12", "idx": 12}, {"begin": 45815, "end": 45904, "id": "b13", "idx": 13}, {"begin": 45908, "end": 46203, "id": "b14", "idx": 14}, {"begin": 46207, "end": 46292, "id": "b15", "idx": 15}, {"begin": 46296, "end": 46551, "id": "b16", "idx": 16}, {"begin": 46555, "end": 46700, "id": "b17", "idx": 17}, {"begin": 46704, "end": 46792, "id": "b18", "idx": 18}, {"begin": 46796, "end": 47077, "id": "b19", "idx": 19}, {"begin": 47081, "end": 47329, "id": "b20", "idx": 20}, {"begin": 47333, "end": 47517, "id": "b21", "idx": 21}, {"begin": 47521, "end": 47663, "id": "b22", "idx": 22}, {"begin": 47667, "end": 47807, "id": "b23", "idx": 23}, {"begin": 47811, "end": 47933, "id": "b24", "idx": 24}, {"begin": 47937, "end": 48156, "id": "b25", "idx": 25}, {"begin": 48160, "end": 48457, "id": "b26", "idx": 26}, {"begin": 48461, "end": 48544, "id": "b27", "idx": 27}, {"begin": 48548, "end": 48632, "id": "b28", "idx": 28}, {"begin": 48636, "end": 48725, "id": "b29", "idx": 29}, {"begin": 48729, "end": 48893, "id": "b30", "idx": 30}, {"begin": 48897, "end": 49116, "id": "b31", "idx": 31}, {"begin": 49120, "end": 49309, "id": "b32", "idx": 32}, {"begin": 49313, "end": 49510, "id": "b33", "idx": 33}, {"begin": 49514, "end": 49632, "id": "b34", "idx": 34}, {"begin": 49636, "end": 49840, "id": "b35", "idx": 35}, {"begin": 49844, "end": 49988, "id": "b36", "idx": 36}, {"begin": 49992, "end": 50181, "id": "b37", "idx": 37}, {"begin": 50185, "end": 50381, "id": "b38", "idx": 38}, {"begin": 50385, "end": 50563, "id": "b39", "idx": 39}, {"begin": 50567, "end": 50649, "id": "b40", "idx": 40}, {"begin": 50653, "end": 50878, "id": "b41", "idx": 41}, {"begin": 50882, "end": 51245, "id": "b42", "idx": 42}, {"begin": 51249, "end": 51387, "id": "b43", "idx": 43}, {"begin": 51391, "end": 51536, "id": "b44", "idx": 44}, {"begin": 51540, "end": 51618, "id": "b45", "idx": 45}, {"begin": 51622, "end": 51721, "id": "b46", "idx": 46}, {"begin": 51725, "end": 51892, "id": "b47", "idx": 47}, {"begin": 51896, "end": 52024, "id": "b48", "idx": 48}, {"begin": 52028, "end": 52169, "id": "b49", "idx": 49}, {"begin": 52173, "end": 52380, "id": "b50", "idx": 50}, {"begin": 52384, "end": 52518, "id": "b51", "idx": 51}, {"begin": 52522, "end": 52674, "id": "b52", "idx": 52}, {"begin": 52678, "end": 52863, "id": "b53", "idx": 53}, {"begin": 52867, "end": 53064, "id": "b54", "idx": 54}, {"begin": 53068, "end": 53227, "id": "b55", "idx": 55}, {"begin": 53231, "end": 53536, "id": "b56", "idx": 56}, {"begin": 53540, "end": 53802, "id": "b57", "idx": 57}, {"begin": 53806, "end": 54135, "id": "b58", "idx": 58}, {"begin": 54139, "end": 54208, "id": "b59", "idx": 59}], "ReferenceToTable": [{"begin": 11215, "end": 11216, "idx": 0}, {"begin": 14504, "end": 14505, "target": "#tab_4", "idx": 1}, {"begin": 20342, "end": 20343, "target": "#tab_4", "idx": 2}, {"begin": 24684, "end": 24685, "target": "#tab_4", "idx": 3}, {"begin": 29105, "end": 29106, "target": "#tab_4", "idx": 4}, {"begin": 29639, "end": 29640, "idx": 5}, {"begin": 29939, "end": 29940, "idx": 6}, {"begin": 29951, "end": 29952, "idx": 7}, {"begin": 33466, "end": 33467, "target": "#tab_6", "idx": 8}, {"begin": 33709, "end": 33710, "target": "#tab_6", "idx": 9}, {"begin": 39303, "end": 39304, "target": "#tab_0", "idx": 10}], "Footnote": [{"begin": 42282, "end": 42597, "id": "foot_0", "idx": 0}, {"begin": 42598, "end": 42632, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 42633, "end": 42663, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 42664, "end": 42725, "id": "foot_3", "n": "7", "idx": 3}, {"begin": 42726, "end": 42894, "id": "foot_4", "n": "8", "idx": 4}, {"begin": 42895, "end": 43301, "id": "foot_5", "n": "9", "idx": 5}, {"begin": 43302, "end": 43647, "id": "foot_6", "n": "10", "idx": 6}, {"begin": 43648, "end": 43764, "id": "foot_7", "n": "11", "idx": 7}, {"begin": 43765, "end": 43997, "id": "foot_8", "n": "12", "idx": 8}, {"begin": 43998, "end": 44279, "id": "foot_9", "n": "15", "idx": 9}], "ReferenceToFormula": [{"begin": 6043, "end": 6047, "idx": 0}, {"begin": 25074, "end": 25078, "idx": 1}, {"begin": 33260, "end": 33261, "idx": 2}], "Paragraph": [{"begin": 50, "end": 843, "idx": 0}, {"begin": 861, "end": 1440, "idx": 1}, {"begin": 1441, "end": 1526, "idx": 2}, {"begin": 1527, "end": 2292, "idx": 3}, {"begin": 2293, "end": 3908, "idx": 4}, {"begin": 3909, "end": 3934, "idx": 5}, {"begin": 3955, "end": 4730, "idx": 6}, {"begin": 4774, "end": 6221, "idx": 7}, {"begin": 6222, "end": 6701, "idx": 8}, {"begin": 6702, "end": 7236, "idx": 9}, {"begin": 7268, "end": 8196, "idx": 10}, {"begin": 8197, "end": 9168, "idx": 11}, {"begin": 9201, "end": 9579, "idx": 12}, {"begin": 9588, "end": 9931, "idx": 13}, {"begin": 9932, "end": 10746, "idx": 14}, {"begin": 10747, "end": 10993, "idx": 15}, {"begin": 10994, "end": 11892, "idx": 16}, {"begin": 11893, "end": 12423, "idx": 17}, {"begin": 12424, "end": 12568, "idx": 18}, {"begin": 12569, "end": 12627, "idx": 19}, {"begin": 12628, "end": 13957, "idx": 20}, {"begin": 13958, "end": 14479, "idx": 21}, {"begin": 14480, "end": 14890, "idx": 22}, {"begin": 14891, "end": 15607, "idx": 23}, {"begin": 15623, "end": 16355, "idx": 24}, {"begin": 16356, "end": 17106, "idx": 25}, {"begin": 17126, "end": 17303, "idx": 26}, {"begin": 17304, "end": 17799, "idx": 27}, {"begin": 17800, "end": 19020, "idx": 28}, {"begin": 19021, "end": 19297, "idx": 29}, {"begin": 19313, "end": 20271, "idx": 30}, {"begin": 20272, "end": 20568, "idx": 31}, {"begin": 20569, "end": 20837, "idx": 32}, {"begin": 20838, "end": 21288, "idx": 33}, {"begin": 21308, "end": 22589, "idx": 34}, {"begin": 22590, "end": 22794, "idx": 35}, {"begin": 22817, "end": 23749, "idx": 36}, {"begin": 23880, "end": 24325, "idx": 37}, {"begin": 24326, "end": 24651, "idx": 38}, {"begin": 24678, "end": 24960, "idx": 39}, {"begin": 24961, "end": 25573, "idx": 40}, {"begin": 25574, "end": 25951, "idx": 41}, {"begin": 25964, "end": 26301, "idx": 42}, {"begin": 26302, "end": 26877, "idx": 43}, {"begin": 26878, "end": 27425, "idx": 44}, {"begin": 27453, "end": 27928, "idx": 45}, {"begin": 27929, "end": 28599, "idx": 46}, {"begin": 28600, "end": 29068, "idx": 47}, {"begin": 29069, "end": 29585, "idx": 48}, {"begin": 29586, "end": 30498, "idx": 49}, {"begin": 30531, "end": 30992, "idx": 50}, {"begin": 31029, "end": 31383, "idx": 51}, {"begin": 31384, "end": 32044, "idx": 52}, {"begin": 32069, "end": 32533, "idx": 53}, {"begin": 32534, "end": 32919, "idx": 54}, {"begin": 32920, "end": 33257, "idx": 55}, {"begin": 33258, "end": 33402, "idx": 56}, {"begin": 33412, "end": 34241, "idx": 57}, {"begin": 34242, "end": 34590, "idx": 58}, {"begin": 34591, "end": 35049, "idx": 59}, {"begin": 35079, "end": 35451, "idx": 60}, {"begin": 35485, "end": 36056, "idx": 61}, {"begin": 36057, "end": 36697, "idx": 62}, {"begin": 36698, "end": 36734, "idx": 63}, {"begin": 36735, "end": 36776, "idx": 64}, {"begin": 36777, "end": 37649, "idx": 65}, {"begin": 37650, "end": 38355, "idx": 66}, {"begin": 38356, "end": 38866, "idx": 67}, {"begin": 38867, "end": 39379, "idx": 68}, {"begin": 39380, "end": 41601, "idx": 69}, {"begin": 41602, "end": 41814, "idx": 70}, {"begin": 41815, "end": 42269, "idx": 71}], "SectionHeader": [{"begin": 0, "end": 843, "idx": 0}], "SectionReference": [{"begin": 44281, "end": 54210, "idx": 0}], "Sentence": [{"begin": 50, "end": 223, "idx": 0}, {"begin": 224, "end": 396, "idx": 1}, {"begin": 397, "end": 578, "idx": 2}, {"begin": 579, "end": 739, "idx": 3}, {"begin": 740, "end": 843, "idx": 4}, {"begin": 861, "end": 1052, "idx": 5}, {"begin": 1053, "end": 1267, "idx": 6}, {"begin": 1268, "end": 1440, "idx": 7}, {"begin": 1441, "end": 1447, "idx": 8}, {"begin": 1448, "end": 1482, "idx": 9}, {"begin": 1483, "end": 1526, "idx": 10}, {"begin": 1527, "end": 1796, "idx": 11}, {"begin": 1797, "end": 2011, "idx": 12}, {"begin": 2012, "end": 2118, "idx": 13}, {"begin": 2119, "end": 2292, "idx": 14}, {"begin": 2293, "end": 2562, "idx": 15}, {"begin": 2563, "end": 2745, "idx": 16}, {"begin": 2746, "end": 2859, "idx": 17}, {"begin": 2860, "end": 2992, "idx": 18}, {"begin": 2993, "end": 3097, "idx": 19}, {"begin": 3098, "end": 3270, "idx": 20}, {"begin": 3271, "end": 3495, "idx": 21}, {"begin": 3496, "end": 3615, "idx": 22}, {"begin": 3616, "end": 3758, "idx": 23}, {"begin": 3759, "end": 3908, "idx": 24}, {"begin": 3909, "end": 3934, "idx": 25}, {"begin": 3955, "end": 4140, "idx": 26}, {"begin": 4141, "end": 4364, "idx": 27}, {"begin": 4365, "end": 4537, "idx": 28}, {"begin": 4538, "end": 4730, "idx": 29}, {"begin": 4774, "end": 4911, "idx": 30}, {"begin": 4912, "end": 5031, "idx": 31}, {"begin": 5032, "end": 5169, "idx": 32}, {"begin": 5170, "end": 5296, "idx": 33}, {"begin": 5297, "end": 5466, "idx": 34}, {"begin": 5467, "end": 5550, "idx": 35}, {"begin": 5551, "end": 5636, "idx": 36}, {"begin": 5637, "end": 5761, "idx": 37}, {"begin": 5762, "end": 5903, "idx": 38}, {"begin": 5904, "end": 6029, "idx": 39}, {"begin": 6030, "end": 6129, "idx": 40}, {"begin": 6130, "end": 6221, "idx": 41}, {"begin": 6222, "end": 6287, "idx": 42}, {"begin": 6288, "end": 6389, "idx": 43}, {"begin": 6390, "end": 6555, "idx": 44}, {"begin": 6556, "end": 6701, "idx": 45}, {"begin": 6702, "end": 7086, "idx": 46}, {"begin": 7087, "end": 7236, "idx": 47}, {"begin": 7268, "end": 7538, "idx": 48}, {"begin": 7539, "end": 7723, "idx": 49}, {"begin": 7724, "end": 7895, "idx": 50}, {"begin": 7896, "end": 8196, "idx": 51}, {"begin": 8197, "end": 8268, "idx": 52}, {"begin": 8269, "end": 8393, "idx": 53}, {"begin": 8394, "end": 8714, "idx": 54}, {"begin": 8715, "end": 8949, "idx": 55}, {"begin": 8950, "end": 9029, "idx": 56}, {"begin": 9030, "end": 9168, "idx": 57}, {"begin": 9201, "end": 9310, "idx": 58}, {"begin": 9311, "end": 9579, "idx": 59}, {"begin": 9588, "end": 9742, "idx": 60}, {"begin": 9743, "end": 9931, "idx": 61}, {"begin": 9932, "end": 10052, "idx": 62}, {"begin": 10053, "end": 10184, "idx": 63}, {"begin": 10185, "end": 10243, "idx": 64}, {"begin": 10244, "end": 10451, "idx": 65}, {"begin": 10452, "end": 10590, "idx": 66}, {"begin": 10591, "end": 10746, "idx": 67}, {"begin": 10747, "end": 10829, "idx": 68}, {"begin": 10830, "end": 10993, "idx": 69}, {"begin": 10994, "end": 11217, "idx": 70}, {"begin": 11218, "end": 11378, "idx": 71}, {"begin": 11379, "end": 11523, "idx": 72}, {"begin": 11524, "end": 11630, "idx": 73}, {"begin": 11631, "end": 11892, "idx": 74}, {"begin": 11893, "end": 11970, "idx": 75}, {"begin": 11971, "end": 12180, "idx": 76}, {"begin": 12181, "end": 12423, "idx": 77}, {"begin": 12424, "end": 12485, "idx": 78}, {"begin": 12486, "end": 12568, "idx": 79}, {"begin": 12569, "end": 12627, "idx": 80}, {"begin": 12628, "end": 12707, "idx": 81}, {"begin": 12708, "end": 12817, "idx": 82}, {"begin": 12818, "end": 13017, "idx": 83}, {"begin": 13018, "end": 13156, "idx": 84}, {"begin": 13157, "end": 13232, "idx": 85}, {"begin": 13233, "end": 13332, "idx": 86}, {"begin": 13333, "end": 13475, "idx": 87}, {"begin": 13476, "end": 13787, "idx": 88}, {"begin": 13788, "end": 13957, "idx": 89}, {"begin": 13958, "end": 14176, "idx": 90}, {"begin": 14177, "end": 14413, "idx": 91}, {"begin": 14414, "end": 14479, "idx": 92}, {"begin": 14480, "end": 14617, "idx": 93}, {"begin": 14618, "end": 14720, "idx": 94}, {"begin": 14721, "end": 14805, "idx": 95}, {"begin": 14806, "end": 14890, "idx": 96}, {"begin": 14891, "end": 14969, "idx": 97}, {"begin": 14970, "end": 15119, "idx": 98}, {"begin": 15120, "end": 15214, "idx": 99}, {"begin": 15215, "end": 15451, "idx": 100}, {"begin": 15452, "end": 15607, "idx": 101}, {"begin": 15623, "end": 15721, "idx": 102}, {"begin": 15722, "end": 15860, "idx": 103}, {"begin": 15861, "end": 16008, "idx": 104}, {"begin": 16009, "end": 16272, "idx": 105}, {"begin": 16273, "end": 16355, "idx": 106}, {"begin": 16356, "end": 16502, "idx": 107}, {"begin": 16503, "end": 16736, "idx": 108}, {"begin": 16737, "end": 16967, "idx": 109}, {"begin": 16968, "end": 17106, "idx": 110}, {"begin": 17126, "end": 17303, "idx": 111}, {"begin": 17304, "end": 17359, "idx": 112}, {"begin": 17360, "end": 17460, "idx": 113}, {"begin": 17461, "end": 17532, "idx": 114}, {"begin": 17533, "end": 17665, "idx": 115}, {"begin": 17666, "end": 17739, "idx": 116}, {"begin": 17740, "end": 17799, "idx": 117}, {"begin": 17800, "end": 18022, "idx": 118}, {"begin": 18023, "end": 18395, "idx": 119}, {"begin": 18396, "end": 18636, "idx": 120}, {"begin": 18637, "end": 18707, "idx": 121}, {"begin": 18708, "end": 18792, "idx": 122}, {"begin": 18793, "end": 18864, "idx": 123}, {"begin": 18865, "end": 18948, "idx": 124}, {"begin": 18949, "end": 19020, "idx": 125}, {"begin": 19021, "end": 19297, "idx": 126}, {"begin": 19313, "end": 19484, "idx": 127}, {"begin": 19485, "end": 19768, "idx": 128}, {"begin": 19769, "end": 19999, "idx": 129}, {"begin": 20000, "end": 20271, "idx": 130}, {"begin": 20272, "end": 20443, "idx": 131}, {"begin": 20444, "end": 20507, "idx": 132}, {"begin": 20508, "end": 20568, "idx": 133}, {"begin": 20569, "end": 20623, "idx": 134}, {"begin": 20624, "end": 20837, "idx": 135}, {"begin": 20838, "end": 20929, "idx": 136}, {"begin": 20930, "end": 21101, "idx": 137}, {"begin": 21102, "end": 21288, "idx": 138}, {"begin": 21308, "end": 21360, "idx": 139}, {"begin": 21361, "end": 21489, "idx": 140}, {"begin": 21490, "end": 21562, "idx": 141}, {"begin": 21563, "end": 21754, "idx": 142}, {"begin": 21755, "end": 21981, "idx": 143}, {"begin": 21982, "end": 22070, "idx": 144}, {"begin": 22071, "end": 22335, "idx": 145}, {"begin": 22336, "end": 22392, "idx": 146}, {"begin": 22393, "end": 22496, "idx": 147}, {"begin": 22497, "end": 22565, "idx": 148}, {"begin": 22566, "end": 22589, "idx": 149}, {"begin": 22590, "end": 22683, "idx": 150}, {"begin": 22684, "end": 22794, "idx": 151}, {"begin": 22817, "end": 22918, "idx": 152}, {"begin": 22919, "end": 23089, "idx": 153}, {"begin": 23090, "end": 23311, "idx": 154}, {"begin": 23312, "end": 23385, "idx": 155}, {"begin": 23386, "end": 23600, "idx": 156}, {"begin": 23601, "end": 23680, "idx": 157}, {"begin": 23681, "end": 23749, "idx": 158}, {"begin": 23880, "end": 24054, "idx": 159}, {"begin": 24055, "end": 24181, "idx": 160}, {"begin": 24182, "end": 24325, "idx": 161}, {"begin": 24326, "end": 24497, "idx": 162}, {"begin": 24498, "end": 24651, "idx": 163}, {"begin": 24678, "end": 24741, "idx": 164}, {"begin": 24742, "end": 24815, "idx": 165}, {"begin": 24816, "end": 24960, "idx": 166}, {"begin": 24961, "end": 25133, "idx": 167}, {"begin": 25134, "end": 25202, "idx": 168}, {"begin": 25203, "end": 25319, "idx": 169}, {"begin": 25320, "end": 25428, "idx": 170}, {"begin": 25429, "end": 25573, "idx": 171}, {"begin": 25574, "end": 25767, "idx": 172}, {"begin": 25768, "end": 25855, "idx": 173}, {"begin": 25856, "end": 25951, "idx": 174}, {"begin": 25964, "end": 26074, "idx": 175}, {"begin": 26075, "end": 26147, "idx": 176}, {"begin": 26148, "end": 26301, "idx": 177}, {"begin": 26302, "end": 26401, "idx": 178}, {"begin": 26402, "end": 26467, "idx": 179}, {"begin": 26468, "end": 26594, "idx": 180}, {"begin": 26595, "end": 26719, "idx": 181}, {"begin": 26720, "end": 26877, "idx": 182}, {"begin": 26878, "end": 27008, "idx": 183}, {"begin": 27009, "end": 27170, "idx": 184}, {"begin": 27171, "end": 27425, "idx": 185}, {"begin": 27453, "end": 27625, "idx": 186}, {"begin": 27626, "end": 27810, "idx": 187}, {"begin": 27811, "end": 27928, "idx": 188}, {"begin": 27929, "end": 28041, "idx": 189}, {"begin": 28042, "end": 28173, "idx": 190}, {"begin": 28174, "end": 28367, "idx": 191}, {"begin": 28368, "end": 28483, "idx": 192}, {"begin": 28484, "end": 28599, "idx": 193}, {"begin": 28600, "end": 28781, "idx": 194}, {"begin": 28782, "end": 28882, "idx": 195}, {"begin": 28883, "end": 29068, "idx": 196}, {"begin": 29069, "end": 29252, "idx": 197}, {"begin": 29253, "end": 29585, "idx": 198}, {"begin": 29586, "end": 29738, "idx": 199}, {"begin": 29739, "end": 29781, "idx": 200}, {"begin": 29782, "end": 29829, "idx": 201}, {"begin": 29830, "end": 29941, "idx": 202}, {"begin": 29942, "end": 30010, "idx": 203}, {"begin": 30011, "end": 30136, "idx": 204}, {"begin": 30137, "end": 30302, "idx": 205}, {"begin": 30303, "end": 30382, "idx": 206}, {"begin": 30383, "end": 30498, "idx": 207}, {"begin": 30531, "end": 30713, "idx": 208}, {"begin": 30714, "end": 30901, "idx": 209}, {"begin": 30902, "end": 30992, "idx": 210}, {"begin": 31029, "end": 31289, "idx": 211}, {"begin": 31290, "end": 31383, "idx": 212}, {"begin": 31384, "end": 31446, "idx": 213}, {"begin": 31447, "end": 31572, "idx": 214}, {"begin": 31573, "end": 31698, "idx": 215}, {"begin": 31699, "end": 31782, "idx": 216}, {"begin": 31783, "end": 31880, "idx": 217}, {"begin": 31881, "end": 31965, "idx": 218}, {"begin": 31966, "end": 32044, "idx": 219}, {"begin": 32069, "end": 32216, "idx": 220}, {"begin": 32217, "end": 32338, "idx": 221}, {"begin": 32339, "end": 32533, "idx": 222}, {"begin": 32534, "end": 32671, "idx": 223}, {"begin": 32672, "end": 32804, "idx": 224}, {"begin": 32805, "end": 32919, "idx": 225}, {"begin": 32920, "end": 33075, "idx": 226}, {"begin": 33076, "end": 33257, "idx": 227}, {"begin": 33258, "end": 33402, "idx": 228}, {"begin": 33412, "end": 33468, "idx": 229}, {"begin": 33469, "end": 33521, "idx": 230}, {"begin": 33522, "end": 33621, "idx": 231}, {"begin": 33622, "end": 33712, "idx": 232}, {"begin": 33713, "end": 33849, "idx": 233}, {"begin": 33850, "end": 33909, "idx": 234}, {"begin": 33910, "end": 34090, "idx": 235}, {"begin": 34091, "end": 34241, "idx": 236}, {"begin": 34242, "end": 34301, "idx": 237}, {"begin": 34302, "end": 34460, "idx": 238}, {"begin": 34461, "end": 34543, "idx": 239}, {"begin": 34544, "end": 34590, "idx": 240}, {"begin": 34591, "end": 34839, "idx": 241}, {"begin": 34840, "end": 34926, "idx": 242}, {"begin": 34927, "end": 35049, "idx": 243}, {"begin": 35079, "end": 35318, "idx": 244}, {"begin": 35319, "end": 35451, "idx": 245}, {"begin": 35485, "end": 35600, "idx": 246}, {"begin": 35601, "end": 35696, "idx": 247}, {"begin": 35697, "end": 35954, "idx": 248}, {"begin": 35955, "end": 36056, "idx": 249}, {"begin": 36057, "end": 36168, "idx": 250}, {"begin": 36169, "end": 36448, "idx": 251}, {"begin": 36449, "end": 36697, "idx": 252}, {"begin": 36698, "end": 36704, "idx": 253}, {"begin": 36705, "end": 36734, "idx": 254}, {"begin": 36735, "end": 36776, "idx": 255}, {"begin": 36777, "end": 36883, "idx": 256}, {"begin": 36884, "end": 36974, "idx": 257}, {"begin": 36975, "end": 37088, "idx": 258}, {"begin": 37089, "end": 37306, "idx": 259}, {"begin": 37307, "end": 37514, "idx": 260}, {"begin": 37515, "end": 37649, "idx": 261}, {"begin": 37650, "end": 37865, "idx": 262}, {"begin": 37866, "end": 38218, "idx": 263}, {"begin": 38219, "end": 38355, "idx": 264}, {"begin": 38356, "end": 38630, "idx": 265}, {"begin": 38631, "end": 38866, "idx": 266}, {"begin": 38867, "end": 39005, "idx": 267}, {"begin": 39006, "end": 39225, "idx": 268}, {"begin": 39226, "end": 39379, "idx": 269}, {"begin": 39380, "end": 39584, "idx": 270}, {"begin": 39585, "end": 39772, "idx": 271}, {"begin": 39773, "end": 39941, "idx": 272}, {"begin": 39942, "end": 40129, "idx": 273}, {"begin": 40130, "end": 40282, "idx": 274}, {"begin": 40283, "end": 40378, "idx": 275}, {"begin": 40379, "end": 40573, "idx": 276}, {"begin": 40574, "end": 40805, "idx": 277}, {"begin": 40806, "end": 40951, "idx": 278}, {"begin": 40952, "end": 41311, "idx": 279}, {"begin": 41312, "end": 41431, "idx": 280}, {"begin": 41432, "end": 41601, "idx": 281}, {"begin": 41602, "end": 41814, "idx": 282}, {"begin": 41815, "end": 41891, "idx": 283}, {"begin": 41892, "end": 42054, "idx": 284}, {"begin": 42055, "end": 42269, "idx": 285}], "ReferenceToFigure": [{"begin": 12642, "end": 12643, "target": "#fig_1", "idx": 0}, {"begin": 19510, "end": 19511, "target": "#fig_2", "idx": 1}, {"begin": 28347, "end": 28348, "target": "#fig_3", "idx": 2}, {"begin": 31391, "end": 31392, "target": "#fig_4", "idx": 3}, {"begin": 34238, "end": 34239, "target": "#fig_1", "idx": 4}, {"begin": 35046, "end": 35047, "target": "#fig_1", "idx": 5}], "Div": [{"begin": 50, "end": 843, "idx": 0}, {"begin": 846, "end": 3934, "idx": 1}, {"begin": 3936, "end": 4730, "idx": 2}, {"begin": 4732, "end": 7236, "idx": 3}, {"begin": 7238, "end": 9168, "idx": 4}, {"begin": 9170, "end": 9579, "idx": 5}, {"begin": 9581, "end": 15607, "idx": 6}, {"begin": 15609, "end": 17106, "idx": 7}, {"begin": 17108, "end": 19297, "idx": 8}, {"begin": 19299, "end": 21288, "idx": 9}, {"begin": 21290, "end": 22794, "idx": 10}, {"begin": 22796, "end": 24651, "idx": 11}, {"begin": 24653, "end": 25951, "idx": 12}, {"begin": 25953, "end": 27425, "idx": 13}, {"begin": 27427, "end": 30498, "idx": 14}, {"begin": 30500, "end": 30992, "idx": 15}, {"begin": 30994, "end": 32044, "idx": 16}, {"begin": 32046, "end": 33402, "idx": 17}, {"begin": 33404, "end": 35049, "idx": 18}, {"begin": 35051, "end": 35451, "idx": 19}, {"begin": 35453, "end": 42269, "idx": 20}], "SectionMain": [{"begin": 843, "end": 42269, "idx": 0}]}}