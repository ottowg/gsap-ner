{"text": "KLUE: Korean Language Understanding Evaluation\n\nAbstract:\nWe introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of 8 Korean natural language understanding (NLU) tasks, including Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We build all of the tasks from scratch from diverse source corpora while respecting copyrights, to ensure accessibility for anyone without any restrictions. With ethical considerations in mind, we carefully design annotation protocols. Along with the benchmark tasks and data, we provide suitable evaluation metrics and fine-tuning recipes for pretrained language models for each task. We furthermore release the pretrained language models (PLM), KLUE-BERT and KLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby facilitate future research. We make a few interesting observations from the preliminary experiments using the proposed KLUE benchmark suite, already demonstrating the usefulness of this new benchmark suite. First, we find KLUE-RoBERTa LARGE outperforms other baselines, including multilingual PLMs and existing open-source Korean PLMs. Second, we see minimal degradation in performance even when we replace personally identifiable information from the pretraining corpus, suggesting that privacy and NLU capability are not at odds with each other. Lastly, we find that using BPE tokenization in combination with morpheme-level pre-tokenization is effective in tasks involving morpheme-level tagging, detection and generation. In addition to accelerating Korean NLP research, our comprehensive documentation on creating KLUE will facilitate creating similar resources for other languages in the future. KLUE is available at https://klue-benchmark.com/.\n\n\n1 Introduction\nA major factor behind recent success of pretrained language models, such as BERT [30] and its variants [82, 22, 49] as well as GPT-3 [110] and its variants [111, 76, 9], has been the availability of well-designed benchmark suites for evaluating their effectiveness in natural language understanding (NLU). GLUE [133] and SuperGLUE [132] are representative examples of such suites and were designed to evaluate diverse aspects of NLU, including syntax, semantics and pragmatics. The research community has embraced GLUE and SuperGLUE, and has made rapid progress in developing better model architectures as well as learning algorithms for NLU.\nThe success of GLUE and SuperGLUE has sparked interest in building such a standardized benchmark suite for other languages, in order to better measure the progress in NLU in languages beyond English. Such efforts have been pursued along two directions. First, various groups in the world have independently created language-specific benchmark suites; a Chinese version of GLUE (CLUE [142]), a French version of GLUE (FLUE [72]), an Indonesian variant [137], an Indic version [57] and a Russian variant of SuperGLUE [125]. On the other hand, some have relied on both machine and human translation of existing benchmark suites for building multilingual version of the benchmark suites which were often created initially in English. These include for instance XGLUE [78] and XTREME [54]. Although the latter approach scales much better than the former does, the latter often fails to capture societal aspects of NLU and also introduces various artifacts arising from translation.\nTo this end, we build a new benchmark suite for evaluating NLU in Korean which is the 13-th most used language in the world according to [34] but lacks a unified benchmark suite for NLU. Instead of starting from existing benchmark tasks or corpora, we build this benchmark suite from ground up by determining and collecting base corpora, identifying a set of benchmark tasks, designing appropriate annotation protocols and finally validating collected annotation. This allows us to preemptively address and avoid properties that may have undesirable consequences, such as copyright infringement, annotation artifacts, social biases and privacy violations.\nIn the rest of this section, we summarize a series of decisions and principles that went behind creating KLUE.\n\n1.1 Summary\nIn designing the Korean Language Understanding Evaluation (KLUE), we aim to make KLUE; 1) cover diverse tasks and corpora, 2) accessible to everyone without any restriction, 3) include accurate and unambiguous annotations, 4) mitigate AI ethical issues. KLUE is safe to use for both building and evaluating systems, because KLUE has proactively addressed potential ethical issues. Here, we describe more in detail how these principles have guided creating KLUE from task selection, corpus selection, annotation protocols, determining evaluation metrics to baseline construction.\nDesign Principles First, let us describe each design principle in detail:\n\u2022 Covering diverse tasks and corpora: To cover diverse aspects of language understanding, we choose eight tasks that cover diverse domain, including news, encyclopedia, user review, smart home queries and task-oriented dialogue, and diverse style, both formal and colloquial. \u2022 Accessible to everyone without any restriction: It is critical for a benchmark suite to be accessible by everyone for it to serve as a true guideline in evaluating and improving NLU systems. We thus use only corpora and resources that can be freely copied, redistributed, remixed and transformed for the purpose of benchmarking NLU systems. \u2022 Obtaining accurate and unambiguous annotations: Ambiguity in benchmark tasks leads to ambiguity in evaluation, which often leads to the discrepancy between the quality of an NLU system measured by the benchmark and its true quality. In order to minimize such discrepancy, we carefully design annotation guidelines of all tasks and improve them over multiple iterations, to avoid accurate annotations. \u2022 Mitigating AI ethical issues: It has been repeatedly observed that large-scale language models can and often do amplify social biases embedded in text used to train them [95]. In order to disincentivize such behaviors, we proactively remove examples, from both unlabeled and labeled corpora, that reflect social biases, contain toxic content and have personally identifiable information (PII), both manually and automatically. Social biases are defined as overgeneralized judgment on certain individuals or group based on social attributes (e.g., gender, ethnicity, religion). Toxic contents include insults, sexual harassment and offensive expressions.\nDiverse Task Selection We carefully choose the following eight NLU tasks with two goals; 1) to cover as diverse aspects of NLU in Korean, and 2) to minimize redundancy among the tasks. See Table 1 for their formats, evaluation granularity and other properties:\n\u2022 Topic Classification (TC): classify a single sentence into a single class.\n\u2022 Semantic Textual Similarity (STS): judge the semantic similarity between two sentences.\n\u2022 Natural Language Inference (NLI): classify whether the first sentence entails the second one.\n\u2022 Named Entity Recognition (NER): extract entities from a sentence.\n\u2022 Relation Extraction (RE): predict the relationship between two entities within a sentence.\n\u2022 Dependency Parsing (DP): predict the syntactic structure of a sentence.\n\u2022 Machine Reading Comprehension (MRC): identify an answer span within a paragraph given a question.\n\u2022 Dialogue State Tracking (DST): track the state of a goal-oriented dialogue.\nSource Corpora Collection We have actively sought corpora that are accessible, cover diverse domains and topics and are written in modern Korean. This active search has ended up with the following ten source corpora from which we derive task-specific corpora. These ten sources are released under CC BY(-SA) license or not considered as copyrighted work, permitting 1) derivative work, 2) redistribution, and 3) commercial use:\n\u2022 News Headlines from Yonhap News Agency\n\u2022 Wikipedia\n\u2022 Wikinews\n\u2022 Wikitree\n\u2022 Policy News\n\u2022 ParaKQC\n\u2022 Airbnb Reviews\n\u2022 NAVER Sentiment Movie Corpus\n\u2022 The Korea Economics Daily News\n\n\u2022 Acrofan News\nBefore sending a subset for annotation, we filter them to remove noisy, toxic or socially biased content, as well as PII. This is done automatically using predefined rules and machine learning models.\n\nConsiderations in Annotation\nFor each task, we annotate a subset from the source corpora. In doing so, we take into account three major considerations below:\n\u2022 Better reflection of linguistic characteristics of Korean: Many existing Korean datasets were constructed as a part of multilingually aligned benchmarks, and they do not fully reflect linguistic characteristics of Korean such as agglutinative nature in named entity recognition (NER) [100], or tagset in part-of-speech (POS) tagging and dependency parsing [86, 46]. We write and revise annotation guidelines more appropriately to the linguistic property of Korean.\n\u2022 Obtaining accurate annotations: We provide crowdworkers or select participants with carefully designed annotation guidelines and improve them over multiple iterations, in order to reduce the ambiguity of annotation process as well as to mitigate known artifact issues. In particular, we often filter out examples for which annotators cannot easily agree with each other.\n\u2022 Mitigating harmful social bias and removing PII: To disincentivize socially biased NLU systems [7], we explicitly instruct annotators as well as inspectors to manually mark and/or exclude examples that are unacceptable according to our principle of ethics. Our definitions of bias and hate speech follow Moon et al. [92]. We denote bias as an overgeneralized prejudice on certain groups or individuals based on the following traits: gender, race, background, nationality, ethnic group, political stance, skin color, religion, disability, age, appearance, (socio-)economic status, and occupations. In the case of hate speech, we include offensive, aggressive, insulting, or sarcastic contents. We identify a list of personally identifiable information (PII) following KISA (Korea Internet and Security Agency) guideline, 1 whose information is related to a living individual based on personal information protection act of Korea. 2 e do not consider public figure's name as personal information.\n\nColloquial\nEvaluation Metrics The diversity of tasks in KLUE implies that we must choose a proper set of evaluation metrics for each task carefully and separately. Here, we list the tasks and describe how we choose the evaluation metrics for each of these tasks.\n\u2022 KLUE-TC (Yonhap News Agency Topic Classification (YNAT)): We formulate KLUE-TC as a multi-class classification problem with seven classes. Because the headline alone is often not enough to precisely identify the proper class to which it belongs, we manually annotate and keep 70,000 headlines, for each of which there was a majority consensus on the class by the annotators. We then use the consensus classes as ground-truth classes and use macro F1 score as an evaluation metric.\n\u2022 KLUE-STS: In KLUE-STS the similarity between each pair of sentences is annotated with the average (real-valued) similarity rating (between 0 and 5). We measure the quality of an NLU model in two different ways. First, we use the Pearson correlation coefficient between the real-valued target and prediction. Second, we compute the F1 score after binarizing the real-valued similarity rating as in paraphrase detection.\n\u2022 KLUE-NLI: Similar to existing NLI datasets, such as SNLI [8] and MNLI [138], we use classification accuracy, and this is appropriate, as we create KLUE-NLI dev/test set to have a balanced class distribution.\n\u2022 KLUE-NER: In KLUE-NER, a named entity recognizer is expected to output BIO tags and also categorize each detected entity into one of six types; person, location, organization, date, time and quantity. To account for rich morphology in Korean, we use entity-level and character-level F1 score to evaluate the quality of the detection to evaluate the recognizer's ability in determining the type of each entity.\n\u2022 KLUE-RE: KLUE-RE is designed as a sentence classification task in which the input is a single sentence with two marked entities and the output is their relationship out of 30 types. We use two evaluation metrics. The first one is micro F1 score, considering only meaningful types (excluding no relationship), which allows us to evaluate the NLU system's ability to identify a fine-grained relationship between a pair of entities. The second one is the area under the precision-recall curve (AUPRC), which gives us a holistic view into the quality of the relation extraction model in question.\n\u2022 KLUE-DP: Following standard practice in dependency parsing, we use both unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate a dependency parser. We annotate and use both formal and informal text (subsets from the news corpora and colloquial review corpora, respectively), which allows us to perform fine-grained analysis across multiple domains. \u2022 KLUE-MRC: Similarly to KLUE-NER, KLUE-MRC is framed as a span prediction problem. We keep character-level exact match (EM) for comparison against existing datasets, while we propose to use ROUGE-W which measures the F1 score based on the longest common consecutive subsequence (LCCS) between the ground-truth and predicted answer spans. The latter handles rich morphology of Korean as well as the former does while being more interpretable. \u2022 KLUE-DST (Wizard of Seoul, WoS): We formulate KLUE-DST as a multiple-sentence slot-value prediction task, and evaluate an NLU system using two metrics. The first metric is the joint goal accuracy which measures whether all the slots were correctly predicted, while the other metric is average F1 score. Because the former treats all examples for which not all slots were correctly filled in, it often fails to distinguish similarly performing NLU systems. We address this shortcoming by reporting both the joint goal accuracy and slot F1 score. We furthermore build it using multiple domains in order to facilitate finer-grain analysis.\nBaselines In addition to creating a benchmark suite, we also build and publicly release a set of strong baselines based on large-scale pretrained language models. In due course, we pretrain and release large-scale language models for Korean ourselves, which will reduce the burden of retraining these large-scale models from individual researchers. We also use several existing multilingual pretrained language models and open-source Korean-specific models in addition to our own models, to gain further insights into the proposed KLUE benchmark. We present all the results in Table 32 and summarize a few interesting observations here. First, Korean-specific language models generally outperform multilingual models. Second, different models perform best on different tasks when controlled for their sizes; KLUE-BERT performs best for YNAT and WoS, KLUE-RoBERTa for KLUE-RE and KLUE-MRC, and KoELECTRA BASE for KLUE-STS and KLUE-NLI. Third, as we increase the model size, KLUE-RoBERTa LARGE ends up outperforming all the other models in all the tasks other than KLUE-NER. Lastly, we observe that removing PII has minimal effect on the downstream task performances, and our tokenization scheme, morpheme-based subword tokenization, is effective in tasks involving tagging, detection and even generation at the morpheme level.\nTask Overview In Table 1, we summarize the resulting eight KLUE tasks, listing important properties, such as type, format, evaluation metrics and annotated data sizes. In the rest of the paper, we will walk through the process by which each and every one of these tasks was constructed much more in detail.\n\n2 Source Corpora\nWe build KLUE from scratch, instead of putting together existing datasets, which has been a common practice in setting up benchmarks. We investigate available textual resources, and document the process in order to provide better understanding on how and why we select some corpora but not others. We adopt the recently proposed documentation frameworks; datasheets [41] and data statements [6]. Based on these frameworks, we document and provide more information to carefully describe our protocol.\n\n2.1 Corpora Selection Criteria\nWe consider two criteria when sourcing a set of corpora to build a source corpus from which task-specific corpora are derived and annotated. The first criterion is accessibility. As the main purpose of KLUE is to facilitate future NLP research and development, we ensure KLUE comes with data that can be used and shared as freely as possible to all. The second criterion is the quality and diversity. We ensure each example with these corpora is of certain quality by removing low-quality text and also the balance is met between formal and colloquial text within these corpora.\nAccessibility Unlike Wang et al. [132], Hu et al. [54], Kakwani et al. [57], we design KLUE to reach as broad and diverse researchers as possible by avoiding any restriction on affiliations of users as well as the purpose of its use. Furthermore, we acknowledge the rapid pace of advances in the field and allow users to reproduce and redistribute KLUE to prolong its usability as a standard benchmark of NLU. To do so, we build and release the source corpus with CC BY-SA. 4 he source corpus, or a set of source corpora, satisfies the following conditions:\n\u2022 No restriction on the use: We allow both non-commercial and commercial use of KLUE, in order to accommodate the recent trend of fundamental research from industry labs.\n\u2022 Derivatives: We allow users to freely refurbish any part of KLUE to first address any shortcomings, such as unanticipated artifacts, ethical issues and annotation mistakes, and second derive more challenging benchmarks for the future. This is similar to what has been done with SQuAD 2.0 [113] which was created to include SQuAD 1.1 [112].\n\u2022 Redistributable: We allow KLUE benchmark datasets to be distributed by anyone via any channel as long as the proper attribution is given to the original creators of KLUE. We deliberately make this decision to avoid situations where only a limited and select group of researchers have a monopoly on resources, ultimately hindering the progress overall. This is in reaction to some of the existing Korean corpora which come together with restrictive policies, often preventing derivatives as well as redistribution, and are only accessible by researchers in Korea after acquiring permissions from the corpus publishers who are often public institutions in Korea. KLUE avoids such preventive policies in order to maximally facilitate the progress in Korean NLP.\nBecause most of the existing datasets do not meet these conditions, we curate the source corpus from scratch by considering only those resources that either come with one of the following licenses: CC0, 5 CC BY, 6 CC BY-SA, 7 and other similar licenses such as KOGL Type 1, 8 are not protected by the copyright act according to the latest copyright act in Korea, 9 or have been explicitly provided to us by copyright holders under contracts. We end up 20 candidate corpora in total, of which subset is selected to form a source corpus set of KLUE. They are listed in Table 2.\nQuality and Diversity Among these 20 source corpora, we select a subset of ten corpora to form the source corpus and to build the KLUE benchmark. In doing so, we consider the following criteria; 1) the corpus should not be specific to narrow domains (diversity), 2) the corpus must be written in contemporary Korean (quality), 3) the corpus should not be dominated by contents that have privacy or toxicity concerns (quality) and 4) the corpus must be amenable to annotation for at least one of the eight benchmark tasks. Furthermore, we select the subset of corpora to cover both formal and colloquial uses. The Final Source Corpora Based on these criteria and decisions, we choose News Headlines, Wikipedia, Wikinews, Policy News, The Korea Economics Daily News, and Acrofan News for (relatively) formal text. 10 For more colloquial text, we use ParaKQC, Airbnb Reviews, and NAVER Sentiment Movie Corpus. These are marked bold in Table 2.\n\n2.2 Selected Corpora\nHere, we describe in more detail general characteristics and potential concerns of each source corpus. We document the collection mechanisms, timeframe, domain, style, license, and background of each corpus as well.\nNews Headlines from Yonhap News Agency (YNA).\nYNA is a dataset of news headlines from Yonhap News Agency, one of the representative news agencies in South Korea. Using news headlines does not infringe on copyrights, unlike the actual contents of news articles. We include YNA from 2016 to 2020 with a main purpose of using it for a single sentence classification task.\n\nWikipedia (WIKIPEDIA)\nWIKIPEDIA is an open encyclopedia written in a formal style and has been widely used for language modeling and dataset construction across many languages, because of its high-quality and well-curated text. The Wikipedia articles in Korean are released under CC BY-SA. We use the dump of Korean Wikipedia released on December 1st, 2020.\nWikinews (WIKINEWS) WIKINEWS implements collective journalism and provides news articles for free under CC BY, both of which are rare for news articles. Due to these properties, we include it in the source corpora despite its limited number of articles (approximately 500 of them).\nWikitree (WIKITREE) WIKITREE is a dataset of news articles derived from Wikitree, the first Korean social media-based news platform that started in 2010. Although there are concerns that the articles on Wikitree are in many cases advertisement-in-disguise or click-bait headlines and express undesirable biases, we include WIKITREE, as it is the only large-scale source of news articles that are freely distributed under CC BY-SA, to the best of our knowledge. It also covers a broad spectrum of topics, including politics, economics, culture and life. We use the articles published between 2016 and 2020. We conduct more thorough manual inspection of WIKITREE is more thoroughly conducted. See Section 2.2.1 for more details.\nPolicy News (POLICY) POLICY is a dataset of various articles distributed by ministries, national offices, and national commissions of South Korea. It covers statements, notices, or media notes reported by the government agencies. POLICY is protected under the Korea Open Government License (KOGL) Type 1, which permits users to share and remix even for commercial purposes, if attribution is properly done. We include articles released up to the end of 2020.\nParaKQC (PARAKQC) PARAKQC is a dataset of 10,000 utterances aimed at smart home devices, consisting of 1,000 intents of 10 similar queries [18]. It covers various topics which are probable when interacting with smart home speakers, such as scheduling an appointment and asking about the weather. PARAKQC is available under CC BY-SA.\nAirbnb Reviews (AIRBNB) AIRBNB is a review dataset sourced from the publicly accessible portion of the Airbnb website. More specifically, we start from the existing multilingual Airbnb reviews collected and preprocessed by Inside Airbnb. 11 We identify a subset of reviews written in Korean from this multilingual Airbnb corpus, using regular expressions. Reviews are from hosts and guests who have completed their stays. AIRBNB is available under CC0.\nNAVER Sentiment Movie Corpus (NSMC) NSMC is a movie review dataset scraped from NAVER Movies. 12 The reviews are written by online users. Each review comes with both the textual content and the binary sentiment label. There are 200,000 reviews in total. The numbers of positive and negative reviewers are balanced. NSMC is available under CC0.\nAcrofan News (ACROFAN) ACROFAN is a corpus consisting of news articles released by ACROFAN. Most articles are press release-like in that they often introduce new products or events of companies. The formats and styles are quite templated, although the articles cover a broad set of categories including automobiles, IT, startups, big companies, energy, beauty and fashion. We obtain the permission and use of the articles from ACROFAN for KLUE. We include news articles published between Dec 2020 and Jan 2021.\nThe Korea Economics Daily News (The Korea Economy Daily) The Korea Economy Daily is a news corpus consisting of articles from the Korea Economics Daily owned by Hankyung corporation. Korea Economics Daily is a newspaper that mainly covers economic issues, but also publishes various topics such as politics, culture and IT topics. The owner of the Korea Economics Daily and we have entered a contract to use news articles published between Jan 2013 and Dec 2015, provided by the Hankyung corporation, as a part of KLUE. This allows us to ensure high-quality, well-curated news articles are included in KLUE. We release The Korea Economy Daily under CC BY-SA, with the condition that these articles are used for the purpose of machine learning research.\n\n2.2.1 Potential Concerns\nBased on the ten selected corpora above, we list up and discuss some of the concerns here. Some concerns are focused on the quality of data, while the others are more societal and ethical.\nToxic Content Although news articles, such as those from YNA, WIKINEWS, WIKITREE, POLICY, ACROFAN, and The Korea Economy Daily are better written and curated than user-generated contents, such as online reviews, these articles nevertheless may reflect some of the biases possessed by journalists and editors. In particular, our manual inspection has revealed that WIKITREE contains more of potentially problematic patterns than the other news sources, due to the incentive structure that incentivizes articles that are more widely shared and clicked more on social media. This is especially true with headlines of these articles, and we thus refrain from using the headlines from WIKITREE when constructing TC. We also do not use the article contents from WIKITREE for MRC, as articles in whole often exaggerate and emphasize sensational aspects of stories. We however use sentences sampled from WIKITREE when building other task-specific corpora, as they are often complete and well-formed. We discard any problematic sentences via annotation.\nUnlike news articles, online reviews have higher potential to contain toxic content, although such tendency varies from one corpus to another. Due to its peer-reviewing system, AIRBNB rarely contains reviews that are deemed toxic. NSMC on the other hand contains comments that could be considered offensive toward movies, their casts, and their directors.\nAs there is a Korean hate speech dataset on review domains [92], we first filter out toxic content with a detector trained on the dataset. Then we discard problematic sentences via the annotation procedure.\nAll utterances of PARAKQC are carefully created based on a pre-defined annotation guideline [18]. This largely prevents toxic content from entering the corpus.\nPersonally Identifiable Information (PII) Private information is any information that can be used to identify an individual who is not considered a public figure. 13 It includes for instance names, social security numbers, telephone numbers and bank account numbers.\nIn the case of news articles, due to their nature of describing social events, they often contain PII such as names and addresses. This is less so with online reviews, as they are often about public figures, such as actors, actresses and directors, as we observe in NSMC. We however notice that the reviews in AIRBNB contain the names of hosts and/or guests as well as their addresses, which must be carefully handled.\nSome of the artificially generated utterances in PARAKQC do contain names. It is however our understanding that these are mostly fictional, meaning that they are unlikely to be truly private information.\n\n2.3 Preprocessing\nBecause these source corpora come from various sources with varying levels of quality and curation, we carefully preprocess them even before deriving a subset for each downstream task. In this section, we describe our preprocessing routines which are applied after splitting each document within these corpora into sentences using the Korean Sentence Splitter (KSS) v2.2.0.2. 14 The proprocessing routines below are in addition to manual inspection and filtering during the annotation stage of each KLUE task.\n\nNoise Filtering\nWe remove noisy and/or non-Korean text from the selected source corpora. We first remove hashtags (e.g., #JMT), HTML tags (e.g., <br>), bad characters (e.g., U+200B (zero-width space), U+FEFF (byte order mark)), empty parenthesis (e.g., ()), and consecutive blanks. We then filter out sentences with more than 10 Chinese or Japanese characters. For the corpora derived from news articles, we remove information about reporters and press, images, source tags as well as copyright tags (e.g., copyright by \u00a9).\nToxic Content Removal In order to avoid introducing undesire contents and biases into KLUE, we use a number of automatic tools to remove various undesirable sentences from the source corpora. Using the Korean hate speech dataset [92], we train a gender bias 15 and a hate speech detector. 16 We discard a sentence which was predicted to exhibit gender bias with the predictive score of at least 0.5. We also discard a sentence if it was deemed to be hate speech, with the predictive score of 0.9 or above. The thresholds are manually determined for each corpus. This approach work well for online text, such as reviews, because the Korean hate speech dataset was constructed using online reviews. It however does not work well for more formal text, such as found in news articles, based on which we decide against using this strategy on The Korea Economy Daily, ACROFAN, and YNA.\n\nPII Removal\nTo mitigate potential privacy issues, we get rid of sentences that contain private information. We detect such sentences using regular expressions that match email addresses, URL and user-mentioning keywords, such as '@gildong'.\n\n2.4 Task Assignment\nWe use these source corpora to build the datasets for the seven KLUE tasks, except for the DST. DST is built from simulated dialogues by crowdworkers and does not require access to offline text. For each downstream task, we use a subset of the source corpora, as described below:\n\u2022 Topic Classification (TC): We use YNA, which has been widely studied for a single sentence topic classification task.\n\u2022 Semantic Textual Similarity (STS): We use AIRBNB, POLICY, and PARAKQC to include diverse semantic contexts. Intent queries and topic information of PARAKQC are useful when generating semantically related sentence pairs. \u2022 Natural Language Inference (NLI): Following MNLI [138], we use multiple sources to construct NLI. We use WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC and AIRBNB. \u2022 Named Entity Recognition (NER): Due to the nature of NER, we must build a corpus in which (named) entities frequently appear. We thus use WIKITREE and NSMC, which enables us to include both formal and informal writing styles. \u2022 Relation Extraction (RE): We use WIKIPEDIA, WIKITREE and POLICY. These corpora tend to have long complete sentences with the names of public figures and their relationships to various organizations. \u2022 Dependency Parsing (DP): We balance formal and colloquial writing styles, while ensuring most of sentences from selected corpora are complete. We end up using WIKITREE and AIRBNB. We choose AIRBNB over NSMC, because the former has better-formed sentences. \u2022 Machine Reading Comprehension (MRC): To provide informative passages, we use WIKIPEDIA, The Korea Economy Daily, and ACROFAN.\n\n3 KLUE Benchmark\nThe goal of KLUE is to provide high quality evaluation datasets and suitable automatic metrics to test a system's ability to understand Korean language. We provide comprehensive details on how we construct our 8 benchmark datasets. We document 1) background of source corpus selection, 2) annotation protocol, 3) annotation process, 4) dataset split strategy, and 5) design process of the metrics. In the annotation process, we guide workers to identify texts containing potential ethical issues. See Section 1.1 for our definitions on bias, hate, and PII.\n\n3.1 Topic Classification (TC)\nIn topic classification (TC), the goal is to train a classifier to predict the topic of a given text snippet. Topic classification datasets typically consist of news or Wikipedia articles and their predefined categories, because the categories often represent topics [151].\nWe include TC in our KLUE benchmark, as inferring the topic of a text is a key capability that should be possessed by a language understanding system. As a typical single sentence classification task, other NLU benchmarks such as CLUE [142] and IndicGLUE [57] also contain TNEWS and News Category Classification. For Korean, no dataset has been proposed for the task, which motivates us to construct the first Korean topic classification benchmark.\nIn this task, given a news headline, a text classifier must predict a topic which is one of {politics, economy, society, culture, world, IT/science, sports}. We formulate TC as single sentence classification task following previous works and use macro-F1 score as an evaluation metric.\n\n3.1.1 Dataset Construction\nOur TC benchmark is constructed in three stages. First, we collect headlines and their corresponding categories, then we annotate the topics without looking at the categories and we finalize the dataset by defining its split into training, development and test splits considering the publication date and term appearances.\nSource Corpora We collect news headlines from online articles distributed by Yonhap News Agency (YNA), the largest news agency in Korea. Specifically, we collect the headlines of the published articles from January 2016 to December 2020 from Naver News. 17 These articles belong to one of the following seven sections: politics, economy, society, culture, world, IT/science, and sports. To balance the data across the different sections, we randomly sample 10,000 articles from each section, except for the sports and IT/science section. We collect 9,000 sports articles and 11,000 IT/science articles.\nUnlike other benchmarks such as TNEWS in CLUE [142] or AG News [151], we exclude contents of the articles to avoid infringement of copyright. Since the contents are protected as copyrighted work, we cannot freely use them without permission. Headlines, on the other hand, are not considered copyrighted work based on a legal precedent [23].\nAnnotation Protocol The headline of each article may not reflect all of the main content, such that the topic of the headline may be different from the original news section of the article. To address this gap between the headline and the corresponding article, we manually annotate the topics of the headlines.\nWe use SelectStar, 18 a crowdsourcing platform in Korea, to annotate topics of the headlines. For each headline, three annotators label topics independently from each other. Each annotator picks at most three topics in the order of relevance among the seven categories. For precise annotation, we also present key terms of each topic to annotators. The terms are subsections of corresponding topics in NAVER news platform as shown in Table 3.\nAn annotator may choose unable-to-decide if the headline does not contain sufficient information to identify the appropriate categories. Such an example is \"Youngsoo Kim awards an appreciation plaque\". There is no clue about who \"Youngsoo Kim\" is nor why he is awarding the appreciation plaque, in this headline.\nWe request the workers to report any headline that includes personally identifiable information (PII), expresses social bias, or is hate speech. We discard the reported headline after manually reviewing them.\n\nAnnotation Process\nWe run a pilot study to select workers, before commencing the main annotation process. We exclude workers who have continuously failed to assign a topic or have failed to agree with the other workers during the pilot stage. As a result, 13 workers have passed this stage of pilot study. In the main annotation, the 13 selected workers labeled topics for all 70,000 headlines. During the annotation, they reported 650 headlines are including potential PIIs (0.93%), 194 toxic contents (0.28%), and 2,515 unable-to-decides (3.59%). We first exclude such invalid 2,953 headlines. The sum of the three type of problematic headlines are larger than the total value because of the intersection among them. After filtering them, 67,047 headlines remain.\nWe look at agreements between three annotators in valid headlines. We consider each of the first relevant topics chosen by three annotators. In 40,359 (60.5%) headlines, all three annotators agree to a single topic. 23,353 (34.8%) had two majority votes, and the other 3,155 (4.7%) did not reached to agreement. To make the headlines classified to a single topic, we remove the others, leaving 63,892 headlines.\nWe examine the second and third relevant topics within an annotator. For 48,885 (69.8%) of headlines, three annotators did not choose any second and third most relevant topic. Only 5,088 (7.3%) of headlines have the second topic in three annotators. We thus assume that headlines are sufficiently represented by the first relevant topics within an annotator.\nWe thus keep only a single topic for each headline, selected by at least two annotators out of three. The annotator agreement on the resulting 63,892 headlines is fairly high (Krippendorff's \u03b1 = 0.713) [67].\nFinal Dataset We partition the final dataset, named YNAT (Yonhap News Agency dataset for Topic classification), into train, development, and test sets. We split the dataset based on the publication date. We include headlines published after 2020 in the development and test sets, while those published before 2020 in the training set. To prevent TC models attending specific keyword to classify the headlines, we also include headlines containing terms that have not appeared in the train set in the development and test set. As shown in the\n\n3.1.2 Evaluation Metric\nThe evaluation metric for YNAT is macro F1 score. Macro F1 score is defined as the mean of topic-wise F1 scores, giving the same importance to each topic. Topic-wise F1 score weights recall and precision equally.\n\n3.1.3 Related Work\nAlthough many topic classification datasets have been proposed in various languages, we are not aware of any public TC benchmark in Korea. AG News [151], a widely used benchmark for topic classification in English, consists of more than a million of news articles collected from the news search engine ComeToMyHead, 19 and categorizes articles into four sections: world, sports, business, and science/technology. More recently, a number of TC benchmark datasets in languages other than English were proposed. IndicGLUE [57] includes News Genre Classification in Indian languages, in which the goal is to classify a news article or news headline into seven categories; entertainment, sports, business, lifestyle, technology, politics, and crime. TNEWS from CLUE [142] is a news topic classification task in Mandarin and consists of 73K titles with 15 news categories, published in Toutiao.\nSince a large language model fine-tuned on TC benchmark can closely reach 100% accuracy as in IndicGLUE [57], some researchers focus on making challenging TC benchmark to leave a room for improvement. CLUE [142] filters easy examples in TNEWS by using 4-fold cross-validation, and then randomly shuffle and split the dataset. Instead of designing our benchmark artificially more difficult, we reflect how topic classification is done in practice even a baseline model reaches to good performance with relatively easy examples in our benchmark.\n\n3.1.4 Conclusion\nWe introduce YNAT, the first Korean topic classification benchmark. The benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories. We assume each headline has only a single topic, but it could be formulated as multi-label classification. We thus open the second and third relevant topic annotations. Also, URLs for each headlines are accompanied for future work if metadata is needed. If some of them requires permission to use, one should contact to the agency. We expect YNAT to serve as a simple and basic NLU task compared to others in KLUE.\n\n3.2 Semantic Textual Similarity (STS)\nSemantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences. We include STS in our benchmark because it is essential to other NLP tasks such as machine translation, summarization, and question answering. Like STS [13] in GLUE [133], many NLU benchmarks include comparing semantic similarity of text snippets such as semantic similarity [142], paraphrase detection [133, 57], or word sense disambiguation [125, 72].\nWe formulate STS as a sentence pair regression task which predicts the semantic similarity of two input sentences as a real value from 0 (no meaning overlap) to 5 (meaning equivalence). A model performance is measured by Pearson's correlation coefficient following the evaluation scheme of STS-b [13]. We additionally binarize the real numbers into two classes with a threshold score 3.0 (paraphrased or not), and use F1 score to evaluate the model.\n\n3.2.1 Dataset Construction\nSource Corpora To diversify domain and style of source corpora, we collect sentences from AIRBNB (colloquial review), POLICY (formal news), and PARAKQC [18] (smart home utterances). We carefully match them to sentence pairs.\nFor each corpus, we design a sampling strategy of sentence pairs to uniformly cover all range of the similarity scores. Without a sophisticated strategy, simple random sampling and matching sentence to pairs would result in a majority of the score zero. To alleviate this skewness, potentially similar and less similar sentences are separately paired by using various methods. For instance, if two descriptions are depicting the same image or headlines referring to the same event, they are likely to be similar because of the additional information. Otherwise, they would not be similar [2].\nInspired from these, we use available additional information to pair sentences as similar or not. If not available, we use round-trip translation (RTT) to obtain the similar pairs and greedy sentence matching for the less similar pairs.\nWe specify the strategy for PARAKQC where the intent of each sentence is available. All sentences are queries for a smart home domain and their intent are shared among some queries. For example, \"How's the weather today in Seoul?\" and \"You know what the weather is like in Seoul today?\" share the same intent which is asking \"The weather of Seoul today\". We pair two sentences with the same intent as similar pairs and different intent as the less similar. Note that even the less similar pairs share topic to avoid making too many mutually dissimilar pairs.\nFor AIRBNB and POLICY, we cannot find meaningful metadata to estimate similarity between sentences. So we adopt RTT technique using NAVER Papago 20 to generate the similar sentence pairs, since RTT is known to yield sentences with slightly different lexical representation while preserving the core meaning of the original sentence. We set English as an intermediate language. We choose a honorific option when translating back to Korean because the option tends to preserve the meaning of the sentences empirically. For less similar pairs, we first compute ROUGE [80] of all possible sentence pairs, by assuming the higher score correlates with higher semantic similarity. 21 Then we draw a pair with the largest score from all possible pairs and the draw is repeated over remaining pairs until all of sentences are matched. As it progresses, the score declines as the number of remaining pairs becomes smaller, producing less similar pairs. We summarize this process as greedy sentence matching (GSM), as presented in Algorithm 1. [2]. It suggests chunking both sentences and compares similarity in chunk-level (e.g., NP, verb chain, PP, etc.). Then an annotator should sum up their judgement to sentence-level similarity. However, we could not directly apply the guide because chunking is highly challenging in Korean. In chunking, tokenization and morpheme-level decomposition of words are required, but they are difficult and even not deterministic in some cases [103]. We thus guide an annotator to evaluate the similarity without chunking and stick to sentence-level comparison.\nWe give crowdworkers additional cues what is important or unimportant for sentence-level similarity evaluation. Important content indicates the main idea in a sentence. If it is a declarative sentence, its providing facts, explanation, or information is the main idea. For an interrogative and imperative sentence, conveying a request or command is important. In exclamatory sentence, feelings or opinion is the main content [4]. Other components than these important contents are regarded as unimportant. For example, they are auxiliary verbs or function words which affect its nuance or politeness. An annotator should score the similarity as follows:\n\u2022 5: Two sentences are equivalent in terms of important and unimportant content.\n\u2022 4: Two sentences are closely equivalent. Some unimportant content differ.\n\u2022 3: Two sentences are roughly equivalent. Important content are similar to each other, but difference between unimportant content is not ignorable.\n\u2022 2: Two sentences are not equivalent. Important content are not similar to each other, only sharing some unimportant contents.\n\u2022 1: Two sentences are not equivalent. Important and unimportant content are not similar to each other. Two sentences only share their topics.\n\u2022 0: Two sentences are not equivalent. They are not sharing any important and unimportant contents and even topics.\nWe also guide crowdworkers to consider the context of sentences. If it significantly affects distinguishing the meaning of two sentences, the score should be low. For example, let two sentences contain important information 'check-in' such as \"Check-in was done by someone other than the host.\" and \"Check-in was done by someone.\" In the latter sentence, 'someone' might be the host. Since we lose information by dropping 'other than the host' from the former, difference of meaning between the two sentence is not ignorable. We score this pair to 3. Furthermore, if the former sentence is compared to 'Check-out was done by someone other than the host.', important information differ so we give score 2.\nAnnotation Process We recruit workers from SelectStar, 22 a crowdsourcing platform in Korea and familiarize them to our annotation protocol. We run pilot annotation to select qualified workers. If a crowdworker's judgement is frequently disagreed against that of other workers, the person is excluded from the main annotation process. As a result, 19 out of the initial 20 workers participate in the main annotation. After removing the sentence pairs used in the pilot, we use 14,869 pairs for the main annotation, consisting of 7,375 for AIRBNB, 2,956 for POLICY, and 4,538 for PARAKQC. 7 different workers labeled all sentence pairs independently.\nWe average 7 labels for each sentence pair and remove outliers following Agirre et al. [3], Cer et al. [13]. First, we filter out annotators showing Pearson's correlation < 0.80 or Krippendorff's alpha < 0.20 (nominal) [67] with others' annotations. We exclude two annotators with this criteria so all sentence pairs have annotations from at least five people. Lastly, similarity score is rounded up to the first decimal place.\nA few more filtering schemes are applied. First, we drop 14 pairs whose annotations are showing larger than 2 standard deviation. Those pairs might contain ambiguous expressions interpreted in various ways, or misannotations. Second, we ask workers to report the sentences including translation error or misinformation caused by RTT. We inspect the reported sentences and remove 418 sentence pairs. Third, we drop sentences involving ethical issues. Workers report the pairs if they are including any kind of hate speech, social bias, and potential personally identifiable information (PII). 1,213 sentence pairs were additionally removed after inspection. As a result, we have 13,224 sentence pairs in total. We report inter-annotator agreement (IAA) by using Krippendorff's alpha instead of Pearson's correlation because 7 annotators (or less) differ by pairs. The annotator agreed to each other's annotations. (Krippendorff's alpha (interval) = 0.85).\nWe observe the distribution of similarity score annotations differ between the potentially similar sentence pairs and the less similar pairs. Figure 1 illustrates label distributions generated by RTT (top) and GSM (bottom) in AIRBNB. As expected, RTT pairs tend to show high similarity (from 3 to 5) while GSM pairs are considered less similar (from 0 to 3). Note that the number of GSM pairs scored 0 is high even we employ similarity-based matching. Similar tendencies are observed in POLICY and PARAKQC. By combining two distributions, we manage to obtain various sentence pairs in terms of similarity scores.\nFinal Dataset We collect 13,224 sentence pairs and corresponding similarity scores. We split them to training, development, and test sets, considering the distribution of the scores. Even if we carefully sampled the pairs, the overall score distribution is not uniform across 0\u22125 as shown in Figure 1. However, we prefer uniform distribution at least in evaluation (development and test) set, in order to prevent evaluation bias toward a specific score. We therefore construct  We also consider word overlap between sentences in each pair for evaluation set. Since larger word overlap might indicate higher semantic similarity, we try to reduce pairs satisfying such tendency to prevent the model from predicting similarity simply using word overlap. The overlap is measured by morpheme-level Jaccard distance by using MeCab [68]. We choose the pairs with the least word overlap from score 3\u22125, and the pairs with most word overlap from the rest. Such pairs are prioritized to be included to every bins in the dev and the test sets.\nWe split the evaluation set with 1:2 ratio to construct the dev and the test sets, resulting in 519 and 1,037 pairs, respectively. The rest 11,668 pairs comprise the train set. Detailed numbers for each corpus are presented in Table 4. For all the sets, we balance the ratio between source corpora with that of the original pairs. Additionally, the scores are binarized with a threshold 3.0 same as paraphrase detection task.\n\n3.2.2 Evaluation Metrics\nThe evaluation metrics for KLUE-STS is 1) Pearson's correlation coefficient (Pearson' r), and 2) F1 score. Pearson's r is a measure of linear correlation between human-labeled sentence-similarity scores and model predicted scores, adopted in STS-b [13]. Since our dev and test set have a balanced score distribution, the coefficient correctly gives the magnitude of the relationship. F1 score is adopted to measure binarized results (paraphrased / not paraphrased). Specifically, our F1 reports results for the paraphrased class. Measuring similarity between sentences is a fundamental natural language understanding problem so that closely related to various NLP applications. Because of its importance, STS is included in various NLU benchmarks [133, 142]. To facilitate research in this area, many shared tasks have been held and annotated corpora are released [2, 3, 13]. Typically, they cover multiple text domains such as question pairs, image descriptions, news headlines, annotated with a real value from 0 (no meaning overlap) to 5 (meaning equivalence).\nRecently, Ham et al. [45] introduces a machine-translated Korean STS benchmark. This is a translation of [13] in GLUE, which contains around 8,600 sentence pairs in total. All examples are solely relying on machine translation, and sentence pairs in evaluation (dev and test) set are further post-edited by human. However, corresponding labels were not adjusted to translated meanings. Lack of re-labeling process would be problematic because Korean speakers would judge the similarity between them differently.\nIf similarity labels are binarized by a certain threshold, STS also could be seen as paraphrase detection task such as Microsoft Research Paraphrase Corpus (MRPC) [32], Quora Question Pairs (QQP) [133], or PAWS [152] and PAWS-X [144]. Thus we additionally binarize our ground truths and predictions, reporting binary classification performance to see how well a model performs in paraphrase detection.\nIn paraphrase detection, Cho et al. [18] presents a benchmark that includes the human-generated queries for smart home, where ten paraphrase sentences are grouped together to make up a total of 1,000 groups. The granularity of scale is from 0 to 5, but the semantic similarity is judged only with attributes such as topic (smart home, weather, etc.) and speech act (question, prohibition, etc.), which does not consider other details such as nuance and syntactic structure because it lacks direct human judgement of similarity. PAWS-X [144] provides a translated version of PAWS [152] of Korean. Like KorSTS, the train split is machine-translated and its dev and test splits are human-translated, and corresponding labels are preserved without human inspection. There are also paraphrase corpora provided by government-funded institutions such as National Institute of Korean Language (NIKL) [98], but it simply provides human-generated and machine-paraphrased sentences with limited accessibility.\n\n3.2.4 Conclusion\nWe create the first human-annotated Korean STS benchmark, KLUE-STS, that covers multiple domains and styles with free accessibility to everyone. The similarity score annotation process is specially designed to capture the characteristics of the Korean language. Covering the expressions from various domains, our benchmark is expected to be a useful resource for further research, beyond serving as a benchmark. Our benchmark helps to develop numerous models established on STS resources, such as SentenceBERT [115].\n\n3.3 Natural Language Inference (NLI)\nThe goal of natural language inference (NLI) is to train a model to infer the relationship between the hypothesis sentence and the premise sentence. Given a premise, an NLI model determines if hypothesis is true (entailment), false (contradiction), or undetermined (neutral). The task is also known as recognizing textual entailment (RTE) [27].\nUnderstanding entailment and contradiction between sentences is fundamental to NLU. NLI datasets are also included in various NLU benchmarks such as GLUE [133] and superGLUE [132], and they are valuable as training data for other NLU tasks [24, 107, 115].\nWe formulate NLI as a classification task where an NLI model reads each pair of premise and hypothesis sentences and predicts whether the relationship is entailment, contradiction, or neutral. We use the classification accuracy to measure the model performance.\n\n3.3.1 Dataset Construction\nWe construct KLUE-NLI by using a collection method similar to that of SNLI [8] and MNLI [138]. First, we collect premise sentences from existing corpora. Then for each premise sentence, we ask one annotator to generate three new hypothesis sentences, one for each of the three relationship classes. Then for each pair of premise and hypothesis sentences, we ask four additional annotators to label the relationship for validation. We follow the criteria proposed by Williams et al. [138] to describe the three labels to the annotators. For both hypothesis generation and pair validation, we recruit workers from SelectStar, 23 a Korean crowdsourcing platform.\nSource Corpora for Premise Sentences We use six corpora for the set of premise sentences: WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC and AIRBNB. They cover diverse topics and writing styles of contemporary Korean. WIKITREE, POLICY and WIKINEWS are news articles and WIKIPEDIA is a crowd-sourced encyclopedia, all of which are written in formal Korean. NSMC and AIRBNB consist of colloquial reviews in the domains of movies and travel, respectively.\nFrom the six corpora, we extract 10,000 premises with which we elicit hypotheses. A valid premise should satisfy three conditions. First, premise is a proposition, a declarative sentence to which we can assign a truth value, excluding mathematical formulae and lists. Second, a premise must include at least one predicate, and the predicate can be of diverse types such as states (e.g., be, believe, know), activities (e.g., play, smile, walk), achievements (e.g. realize, reach, break), and accomplishments (e.g. eat, build, paint). Third, the length of a premise should be from 20 to 90 characters including whitespace.\nAnnotation Protocol for Hypothesis Generation We show annotators a premise and ask them to write three hypotheses that correspond to each label. This allows us to collect nearly equal number of the (premise, hypothesis) pairs for each labels. We maintain the outline of the criteria as follows:\n\u2022 ENTAILMENT: The hypothesis is necessarily true given the premise is true \u2022 CONTRADICTION: The hypothesis is necessarily false given the premise is true \u2022 NEUTRAL: The hypothesis may or may not be true given the premise is true\nWe are aware of the annotation artifacts coming from human writing-based hypothesis generation. Sentence length and explicit lexical patterns are highly associated with certain classes. Neutral sentences tend to be the longest among all classes, since workers can produce neutral hypothesis simply by introducing additional phrase or clause not stated in the premise. Negations such as \"no\", \"never\" and \"nothing\" are often accompanied with the class CONTRADICTION [44, 108].\nDespite the concerns of such artifacts, we stick to such a writing-based annotation procedure. Compared to automatic pipelines to collect hypotheses, human writing yields higher quality data and is still an effective protocol [131]. We focus on ways to encourage annotators to avoid injecting trivial patterns. We prepare guidelines with specific Dos and Don'ts, and rigorously train the workers in advance. To minimize annotation artifacts, we instruct the annotators to write sentences with similar lengths across the classes, refrain from inserting certain lexical items repeatedly, and use as diverse strategies as possible when making inferences.\nSpecifically, we provide detailed guidelines for hypothesis generation together with examples. We encourage annotators to create hypotheses that exhibit diverse linguistic phenomena, in terms of 1) lexical choice, 2) syntactic structures and 3) world knowledge. In the case of lexical choice, our guideline suggests annotators use synonyms/antonyms, [89, 42]. We also encourage using expressions that reflect world knowledge such as time, quantity and geography in order to create a dataset grounded to the real world.\nThere are a few more details in the guideline. We instruct annotators to maintain the writing style of the premise to create a balanced dataset in terms of the style as well. We also instruct them to skip sentences that are difficult to understand either due to the ungrammaticality or the complexity of the content. They are also instructed to skip and report sentences that contain ethical issues such as hate speech, social bias, or personally identifiable information. We examine all reported sentences and make final decisions whether to include the sentences in the dataset.\nAnnotation Protocol for Label Validation Crowdworkers annotate the relations of the resulting premise-hypothesis pairs for validation. For each of the pairs created, we ask four crowdworkers to supply a single label among (EN-TAILMENT, CONTRADICTION, NEUTRAL). This yields a total of five labels per pair, including the initial label intended by the annotator who wrote the hypothesis sentence. For each validated sentence pair, we assign a gold label representing the majority of three or more votes out of five.\nAnnotation Process For hypothesis generation, we go through a pilot phase where we iteratively update the guidelines and train the workers. During the pilot, we find writing a semantically unacceptable sentence or introducing a demonstrative pronoun not used in the premise could be potential problems. Since they might alter the intended label, we ask workers to avoid writing such sentences. The number of workers for this part of the annotation process is 11.\nWe then validate the relation labels for every pair. We go through a pilot phase, starting with 2,604 applicants in the pilot, then select 684 who passed the test to participate in the validation step. With 138 workers dropping out, the final number of workers is 546.\nValidation results are summarized in Table 5. They suggest that our writing protocol is effective in producing a high quality corpus. The rate of unanimous gold labeled examples in KLUE-NLI is 18% higher than SNLI and MNLI. The higher the rate of such examples, the clearer the relationship between the generated hypothesis sentences and the original premise sentences. Individual annotator's agreement with the gold label and the author's label are also higher than SNLI and MNLI, and almost all pairs receive the gold label. Only a few sentence pairs (0.53%) lack the gold label, and we remove those before finalizing our dataset.\n\nFinal Dataset\nThe final dataset consists of 30,998 sentence pairs that are divided into train/development/test sets. Table 6 shows the basic statistics of the dataset. As observed in SNLI and MNLI, our premise sentences also tend to be longer than the corresponding hypothesis sentences. This is because workers generally use partial information of a premise to write a hypothesis.\nNote that we deliberately form the development and test sets in a way to 1) contain balanced source styles and 2) disincentivize models exploiting annotation artifacts. The development and the test set each contains 3,000 sentence pairs.\nTo maintain consistency of style in development and test sets, we include in each set 60% formal and 40% colloquial sentences. We sample 450 sentences each from formal text WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, and 600 sentences each from colloquial text NSMC, AIRBNB. To prevent our NLI benchmark from incentivizing a model that predicts a label using a spurious cue in the hypothesis, we first fine-tune the KLUE-RoBERTa-base model using only the hypothesis sentences with their corresponding labels.\nIf the model finds no clue between the hypothesis and the label, the predicted probability scores for each label should be uniform (i.e., one-third ( 1 3 ) when classified 3-way). Assuming that such score distribution is ideal, we prefer the pairs for development/test sets whose hypothesis-only model's predictions are closest to the ideal. We compute the distance between the prediction and the ideal using cross entropy. To preserve the intact sets of a premise and its three hypotheses, we calculate the mean distance of each set. We extract the sets whose mean distance is among the lowest 20%, and randomly split them into dev and test sets.\nOur idea can be viewed as an extension of pointwise mutual information (PMI). PMI between each hypothesis word (w) and class label (c) has been used to discover the association of the word with each class [44, 131]. If PMI is expanded to the sentence-level association, the metric provides a similar measure to the hypothesis-only model prediction probability as below. PMI(w, c) = log P (w, c) P (w)P (c) = log P (c|w)P (w)P (w)P (c) = log P (c|w) P (c) \u221d P (c|w)\nTo measure human performance and examine whether KLUE-NLI test set improves upon KorNLI [45] test set, a machine-translation of the XNLI [25] test set, we conduct a round of human evaluation. We employ four native Korean undergraduates who major in Korean linguistics and did not participate in the KLUE-NLI construction process. We randomly sample 100 sentence pairs from KLUE-NLI test set and ask the workers to annotate them. We check the agreement of their annotations with the given gold label. We do the same on the subset of the KorNLI test set, to examine whether the human-elicited dataset improves the quality of the dataset. The results are shown in Table 7.\nFor KorNLI, 38% of the sentence pairs have responses from all four annotators that match with the gold labels. There are 18%, 18%, and 16% of sentences, respectively, when three, and two, and one response match with the gold label. 10 pairs do not match with the gold label. On the other hand, KLUE-NLI shows much higher agreement with the given gold label. All annotators agree with the gold label in 71% of the pairs, and 95% obtain at least three agreements. Furthermore, only 258 out of 400 (64.50%) individual annotations are the same as the gold label in KorNLI. Again, KLUE-NLI shows better agreement with the gold labels. 360 (91.00%) annotations are the same as the gold label.\nThese numbers in annotation quality of KLUE-NLI are better than KorNLI as well as SNLI and MLNI. In KorNLI, annotators often report that they do not quite understand at least one of the two sentences or choose NEUTRAL because it is difficult to distinguish the semantic relationships of the sentences. Although the distribution of the gold label is uniform (respectively 33, 33, and 34% of entailment, contradiction, and neutral sentences), the label chosen most frequently by the annotators is NEUTRAL (56.75% on average). There are 26% of cases where the gold labels are different from the majority vote by the annotator. These results suggest that the annotators struggle to grasp the logical semantic relationship of KorNLI sentences.\nOn the other hand, for KLUE-NLI, there is no case where none of the four responses matches the gold label. Considering the cases where more than two of the responses match the gold label, there is a 98% chance of the gold label to be re-selected as the majority tag. Compared to KorNLI, we can see that KLUE-NLI is a much more reliable dataset. This result also confirms that the headroom of our current best model (accuracy: 89.77%) is still there, given that the human accuracy, represented by the majority tag, is 98%.\n\n3.3.2 Evaluation Metric\nThe evaluation metric for KLUE-NLI is accuracy, following SNLI [8] and MNLI [138]. Accuracy measures how well a classifier correctly identifies the results. The class labels are almost equally distributed, thus higher accuracy will correctly represent performances of a model.\n\n3.3.3 Related Work\nRecognizing Textual Entailment (RTE) [27] is a task similar to NLI and was introduced in a series of textual entailment challenges. In the RTE task, two sentences are given, and the model decides whether the meaning of one sentence can be entailed from the other sentence. In earlier RTE 1-3, the task is binary, 'ENTAILMENT' and 'NO ENTAILMENT'.\nIn RTE 4-5, a new class 'UNKNOWN' is introduced, and the task is formulated as a three-way classification.\nTwo major datasets for NLI in English are Stanford Natural Language Inference (SNLI) [8] and Multi-Genre Natural Language Inference (MNLI) [138]. Hypothesis sentences in SNLI and MNLI are labeled ENTAILMENT, CONTRA-DICTION, or NEUTRAL. SNLI is two orders of magnitude larger than the RTE corpora, made from 570,152 image captions in Flickr30k [148]. MNLI premise sentences are derived from 10 different sources, covering a wider range of styles, degrees of formality, and topics.\nMost of the existing NLI datasets are in English, including SNLI and MNLI, and one common approach for constructing NLI datasets in other languages is to translate the existing English corpora to the language of interest. Conneau et al. [25] provides XNLI (Cross-lingual natural language inference) by employing professional translators to translate the development and test sets of MNLI into 15 languages. One main concern of the translation-based approach is whether the relation of the original sentence pair is maintained in the process. Conneau et al. [25] find some translated pairs lose the initial semantic relationship, validated by human annotators who re-annotate a sample of the dataset. The result demonstrates that human translations cause 2% misannotations given the 85% correct examples in the MNLI and 83% in XNLI.\nMotivated by the fact that Korean is not included in XNLI, KorNLI [45] is introduced. KorNLI [45] is a translation of existing English corpora whose train set is created through machine translation of training sets of SNLI and MNLI, and the development and test sets through machine translation of development and tests sets of XNLI and post-editing by professional translators. Although Ham et al. [45] also investigate the data manually and acknowledge some incorrect examples after the translation, no human validation process is performed to quantify the observation and leave analyzing such errors to future work. Moreover, even with post-editing, there are some sentences that are either unnatural in terms of syntactic structure or word choice.\nMany studies have been proposed based on SNLI and MNLI; however, SNLI and MNLI are known to have annotation artifacts [44, 108]. Annotation artifacts are the product of certain types of annotation strategies and heuristics naturally arising from the crowdsourcing process. Such artifacts are problematic as they may lead models to adopt heuristics rather than to actually learn the relationship.\nThere have been some efforts to reduce annotation artifacts in NLI. Vania et al. [131] experiment with two fully automated protocols for creating premise-hypothesis pairs, but find that the methods yield poor-quality data and mixed results on annotation artifacts. OCNLI [53] enhance writing-base protocol with some interventions to control the bias: encouraging writers to use diverse ways of making inference, and putting constraints on overused words. Despite partial effects on reducing negators, the explicit constraint gives rise to other words of correlation, and the final OCNLI dataset exhibit similar level of hypothesis-only test scores to most benchmark NLI datasets.\n\n3.3.4 Conclusion\nOur new dataset, KLUE-NLI, is the first resource constructed upon naturally occurring Korean sentences. KLUE-NLI represents diverse linguistic phenomena, writing style, degree of formality and contents that are most natural and suitable for Korean. The premise sentences of our dataset come from six Korean corpora, and the hypothesis sentences are written by well-trained workers.\nBy keeping the writing-based protocol and thoroughly training workers based on detailed guidelines, we improve upon the existing NLI datasets in the reliability of the labels. KLUE-NLI shows much higher inter-annotator agreement rate than both the MNLI and the translation-based Korean dataset, KorNLI. The gap between the human performance scores of KLUE-NLI and KorNLI also provides evidence that KLUE-NLI is currently the optimal Korean NLI dataset.\nBeyond its main purpose as an NLI benchmark dataset, we hope KLUE-NLI will be a useful resource for future NLU research, as English dataset such as MNLI and SNLI are extended [24, 107, 115].\n\n3.4 Named Entity Recognition (NER)\nThe goal of named entity recognition (NER) is to detect the boundaries of named entities in unstructured text and classify the types. An entity can be series of words that refers to the person, location, organization, time expressions, quantities, monetary values.\nSince NER is an important for application fields like syntax analysis, goal-oriented dialog system, question and answering chatbot and information extraction, various NLU benchmarks contains NER datasets [137, 57, 78, 54]. Despite the rise of necessity of NER datasets in various domains and styles, there are few existing Korean NER datasets to cover such need. Therefore, we annotate corpora including web texts that can be applied to real-word applications.\nIn KLUE-NER, a model should detect the spans and classify the types of entities included in an input sentence. The six entity types used in KLUE-NER are person, location, organization, date, time, and quantity. They are tagged via character-level BIO (Begin-Inside-Outside) tagging scheme, and thus we evaluate a model's performance using entity-level and character-level F1 score.\n\n3.4.1 Dataset Construction\nSource Corpora To incorporate both formal and informal writing styles, we use two corpora, WIKITREE and NSMC for annotation. WIKITREE is a news article corpus and thus contains formal sentences with many entity types, which suits well as a source corpus for NER. NSMC includes colloquial reviews of movies or TV shows. Since the texts in NSMC are user-generated comments, they contain errata and non-normalized expressions, along with emojis and slang. Such a noisy dataset will help broaden the application field of NER models.\nThe preprocessing of the two corpora is performed differently considering the characteristics of each corpus. For WIKITREE, since the news articles are mainly composed of well-written sentences, we simply split the articles into sentences. In contrast, the web texts from NSMC are written in the style of spoken language with blurry sentence boundaries. As each review is generally quite short and the sentences consisting it are on the same topic, we use each review as a single unit of input. In addition, the sentences that contain hate speech or socially biased terms are removed manually. For both corpora, we remove sentences longer than 400 characters.\nFor efficient annotation, we perform pseudo-labeling with a pretrained model. The model is trained with BERT-CRF using a publicly available dataset KMOU-NER corpus, 24 to support fast and accurate entity tagging for annotators. We also filter out the sentences with no pseudo-labeled entity assuming they do not include any of the entities. Remaining sentences account for about 80% in WIKITREE and 41% in NSMC, leaving a total of 36,515 sentences.\nAnnotation Protocol We use six entity types for KLUE-NER annotation: PS (Person), LC (Location), OG (Organization), DT (Date), TI (Time), and QT (Quantity). The description of each entity type is as follows.\n\u2022 We employ the above sets following the convention of two existing tag sets: Korean Telecommunications Technology Association (TTA) NER guidelines 25 and MUC-7 [16]. TTA guideline is a standardized NER tagging scheme for Korean language and we follow the names and the definitions of its entity types. Among the 15 entity types of TTA, we select our six types that correspond with tagsets used in MUC-7 (DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON and TIME). As MONEY and PERCENT types are included in QT (QUANTITY) type from TTA set, we instead adopt an entity type QT.\nIn the case of entities with multiple possible entity types, instead of assigning a unique tag for all use cases, we determine their tags based on the context. One example is Cine21, which, in Korean, can either refer to the name of a magazine or the publisher of the magazine. In a sentence like \"'I bought a Cine21 from a bookstore and read it page by page,\" 'Buy something from a bookstore' and 'read page by page' are properties regarding media (magazine), rather than an organization; thus we do not assign an OG tag. We guide crowdworkers to report if the text for annotation does not meet certain conditions. For example, texts consisting of multiple sentences, texts that are not in a sentence form, a fragment, and a simple sequence of nouns are discarded. Workers are also required to report sentences that include hate speech and various biases in tagging process.\nIn terms of personally identifiable information, we cannot simply drop or pseudonymize the information because the very task of NER often requires the specific information of proper nouns such as person names (PS). In order to minimize the loss of sentences, we inspect through the sentences after the annotation process. We investigate the sentences that include PS tags, and keep the ones that contain the name of public figures that appear in Korean search engines. 26 Other sentences are removed if it has potential privacy issues.\nAnnotation Process 51 qualified crowdworkers recruited by a Korean crowdsourcing platform, DeepNatural 27 participate in the annotation process. The qualification is given when passing a pilot entity tagging test. Then two linguists check whether the crowdworkers' annotations are correct or not. We find some erroneous annotations remaining even after validation. Therefore, six NLP researchers manually correct the annotation errors.\nDuring the annotation process, 5,354 sentences are dropped by workers due to their inadequacy. 118 sentences are dropped due to the privacy issue, and 35 sentences are removed after the inspection by the researchers because all annotations are false positives. A total of 5,507 sentences are dropped in the inspection process, resulting in 31,008 sentences.\nFinal Dataset The resulting corpus is split into train/dev/test sets, each consisting of 21,008, 5,000, and 5,000 sentences (Table 8). The entity-wise statistics is provided in Table 9. We design the test set to include unseen entities to check the robustness of the models in terms of domain transitions and generalization.\nThe finalized entity types are tagged in the character level BIO tagging scheme (Figure 3). In most English and Korean NER datasets, the entities are tagged with the word-level BIO scheme, following CoNLL 2003 dataset [129]. In Korean, however, it is difficult to adhere to the word level tagging scheme based on whitespace for two reasons. First, whitespace-split units (eojeols) are often not a single word and are a composite of content words and functional words (e.g., '\ub2f4\uc8fc\uac00 (the next week is)' = '\ub2f4\uc8fc (the next week)' + '\uac00 (is)') [46]. Second, many compound words in Korean contain whitespaces. Therefore, we choose to tag in character level.\n\n3.4.2 Evaluation Metrics\nThe evaluation metrics for KLUE-NER are 1) entity-level macro F1 (Entity F1) and 2) character-level macro F1 (Char F1) scores. Entity F1 score measures how many predicted entities and types are exactly matched with the ground truths\n\n3.4.3 Related Work\nCoNLL2003 [129] is the most widely used NER benchmark which covers texts from Reuters newswire articles. It handles English and German and is annotated with four named entity types (persons, locations, organizations, and miscellaneous entities). Another dataset on news articles from the Wall Street Journal, MUC (Message Understanding Conference) [43], presents an extended tag set, including temporal and numerical entities. The resulting materials, e.g., MUC-6 [43] and MUC-7 [16], which include six and seven classes of entities, respectively, are adopted as a training source for developing the Stanford NER parser [40].\nTo handle more informal and less sentence-like documents, WNUT16 [127] is proposed. It deals with English Twitter texts which are first suggested in TwitterNER [117]. A total of 15 types of entities are labeled, more subdivided than CoNLL03 and MUC.\nFor Korean, there are four existing NER datasets which are published by Korea Maritime & Ocean University (KMOU), Changwon University, National Institute of Korean Language (NIKL) and Electronics and Telecommunications Research Institute (ETRI). All of them follow the tagging schema of Telecommunications Technology Association (TTA). 28 TTA provides a standardized named entity tagging scheme that serves as an integrated guideline for NER research in Korean. It incorporates 15 named entity tags with 146 subcategories, and provides the definition and the examples regarding each tag with the instructions on the tagging procedure.\nNo existing Korean NER dataset is both freely accessible and covers diverse text domains. According to Cho et al. [19], the NER dataset provided by Korea Maritime & Ocean University (KMOU) and the dataset constructed by Changwon University are publicly accessible. The datasets provided by ETRI and NIKL are not fully public, and the usage is also restricted to domestic researchers. We overcome this issue by making KLUE NER freely available to anyone. None of the aforementioned datasets cover sentences from noisy user generated web texts, which helps model trained on those to be more robust and generalizable. Moreover, except for the KMOU dataset, all the above datasets are tagged in word level, which often conflicts with the morphological characteristics of Korean. In comparison, KLUE-NER uses web texts as source corpora and the entities are annotated in character level, thus being more practical and useful.\n\n3.4.4 Conclusion\nWe construct a new Korean NER benchmark that covers broad domains and styles, which is freely accessible to anyone. The entity types are annotated so that a model has to use both morphological and contextual cues. The character-level entity tagging and evaluation method reflects the characteristics of Korean morphology. Since KLUE-NER dataset covers both formal news articles and informal user-generated web texts, we hope that our benchmark helps develop NER models that can be used in a wide a range of domains, and serve as a resource for developing advanced models for Information Extraction.\n\n3.5 Relation Extraction (RE)\nRelation extraction (RE) identifies semantic relations between entity pairs in a text. The relation is defined between an entity pair consisting of subject entity (e subj ) and object entity (e obj ). For example, in a sentence 'Kierkegaard was born to an affluent family in Copenhagen', the subject entity is 'Kierkegaard' and the object entity is 'Copenhagen'. The goal is then to pick an appropriate relationship between these two entities; 'place_of_birth'.\nRE is a task suitable for evaluating whether a model correctly understands the relationships between entities. In order to ensure KLUE-RE captures this aspect of language understanding, we include a large-scale RE benchmark. Because there is no large-scale RE benchmark publicly available in Korean, we collect and annotate our own dataset.\nWe formulate RE as a single sentence classification task. A model picks one of predefined relation classes describing the relation between two entities within a given sentence. In other words, the RE model predicts an appropriate relation r of entity pair (e subj , e obj ) in a sentence s, where e subj is the subject entity and e obj is the object entity. We refer to (e subj , r, e obj ) as a relation triplet. The entities are marked as corresponding spans in each sentence s. There are 30 relation classes that consist of 18 person-related relations, 11 organization-related relations, and no_relation. Detailed explanation of these classes are presented in Table 10. We evaluate a model using micro F1 score, computed after excluding no_relation, and area under the precision-recall curve including all 30 classes.\n\n3.5.1 Data Construction\nDistant supervision [91] detected by an NER model simultaneously, it is added to the dataset with relation label r by assuming any sentence which contains the pair will express that relation. This approach does not require expensive human annotation, thus allowing us to build a large-scale RE benchmark in a cost-effective way.\nDespite this advantage, distant supervision often ends up with incorrect relation labels when the assumption is not satisfied. In particular, it only considers pairs of entities which are related to each other, which results in an RE model trained on such corpus to over-predict the existence of some relationship between any given pair of entities. In other words, the predicted relation class distribution from such predictors is not realistic [116]. Zhang et al. [153] and Nam et al. [93] thus propose to employ crowdworkers to alleviate erroneous relations extracted by distant supervision. Riedel et al. [116] furthermore intentionally collect irrelevant entity pairs to prevent RE models from overly predicting false positives relations.\nOverview We modify the original strategy of distant supervision above, to address this weakness and to better fit our situation. First, we collect triplets (e subj , r, e obj ) from a small Korean KB 29 and build additional ones by parsing the infoboxes in WIKIPEDIA and NAMUWIKI 30 to enlarge the pool of the candidate triplets. We then ask crowdworkers to select the correct relation class of each candidate triplet within a sentence, compared to distant supervision which directly uses automatically generated relation labels. In addition, we randomly sample entity pairs in s to obtain more realistic relation class distribution in our benchmark. Those examples would include unseen entities in existing KB as well as have higher chance to be irrelevant (no_relation).\nThis procedure can be divided into five steps; (1) candidate sentence collection, (2) relation schema definition, (3) entity detection, (4) entity pair selection and (5) relation annotation. We elaborate each step in the rest of this section.\n\n1. Collect Candidate Sentences\nWe sample candidate sentences from WIKIPEDIA, WIKITREE and POLICY corpora to cover a diverse set of named entities and relational facts. Since our task deals with single sentences, we exploit individual sentences split by Korean Sentence Splitter 31 at the preprocessing step. We filter out sentences that contain undesirable social bias and are considered hate speech, using a classifier trained on the Korean hate speech dataset [92]. The date when the specified organization was dissolved org:founded\n\n2. Define Relation Schema\nThe date when the specified organization was founded org:place_of_headquarters\nThe place which the headquarters of the specified organization are located in org:alternate_names\nAlternative names called instead of the official name to refer to the specified organization org:member_of\nOrganizations to which the specified organization belongs org:members\nOrganizations which belong to the specified organization org:political/religious_affiliation Political/religious groups which the specified organization is affiliated in org:product Products or merchandise produced by the specified organization org:founded_by\nThe person or organization that founded the specified organization org:top_members/employees\nThe representative(s) or members of the specified organization org:number_of_employees/members The total number of members that are affiliated in the specified organization per:date_of_birth\nThe date when the specified person was born per:date_of_death\nThe date when the specified person died per:place_of_birth\nThe place where the specified person was born per:place_of_death\nThe place where the specified person died per:place_of_residence\nThe place where the specified person lives per:origin\nThe origins or the nationality of the specified person per:employee_of\nThe organization where the specified person works per:schools_attended A school where the specified person attended per:alternate_names\nAlternative names called instead of the official name to refer to the specified person per:parents\nThe parents of the specified person per:children\nThe children of the specified person per:siblings\nThe brothers and sisters of the specified person per:spouse\nThe spouse(s) of the specified person per:other_family Family members of the specified person other than parents, children, siblings, and spouse(s) per:colleagues People who work together with the specified person per:product Products or artworks produced by the specified person per:religion\nThe religion in which the specified person believes per:title Official or unofficial names that represent the occupational position of the specified person\nWe remove rarely appearing relation classes in our corpus such as org:website, per:shareholders, per:cause_of_death, per:charges, and per:age. For the same reason, we incorporate org:parents into org:member_of and org:subsidiaries into org:members. Since the taxonomy of TAC-KBP does not precisely reflect the regional hierarchy of Korea, we integrate the prefixes country_of, city_of, and stateorprovince_of into place_of. We introduce additional classes frequently appearing in our corpus such as org:product, per:product and per:colleague:\n\u2022 org:product: A product or merchandise produced by an organization. This includes intangible goods such as an event hosted and a business launched by the organization.\n\u2022 per:product: A product produced by a person. Artworks (e.g. book, music, movie) or contribution to producing them.\n\u2022 per:colleague: A person could be a colleague of someone if they work together. Two people in the same group such as political party or alliance are colleagues as well.\n\n3. Detect Entities\nWe automatically detect named entities in all candidate sentences. We fine-tune a pre-trained ELECTRA for Korean 32 to build two named entity recognition (NER) models on two existing Korean NER resources respectively. One is provided by National Institute of Korean Language [98], and the other is built by Korea Maritime & Ocean University. 33 We modify the named entity types defined in these resources to be compatible with our own entity types previously defined in the schema. We take the union of both models' predictions to extract as many entities as possible. We use crowdsourcing to correct incorrect boundaries of the detected entities, as described later.\n\n4. Select Entity Pairs\nWe select two entities from the entity set E of a given sentence s to make an entity pair (e subj , e obj ). In doing so, we take two distinct approaches; (1) KB-based sampling and (2) uniform sampling.\nFor the first approach, we only consider the subset of entities such that each entity pair (e subj , e obj ) appears in the pool of triplets (e subj , r, e obj ). We collect these triplets from two sources. First, we create the initial pool of triplets, using a Korean KB. 34 Because the number of triplets (\u223c800k) from the Korean KB is small compared to, for instance, that of Freebase (\u223c2b), we enlarge this pool of triplets by gathering and then parsing infoboxes in WIKIPEDIA and Namuwiki.\nIn order to avoid over-inclusion of frequent entities, such as the President of Korea, we set an upper bound to the number of co-occurrence between (e subj , e obj ) during sampling [153].\nIn the second approach, (e subj , e obj ) is uniformly sampled from the entire entity set E of a given sentence s, at random. Because there is no cue whether a sampled pair has any relation between them, the pair is highly likely to be irrelevant (no_relation). Irrelevant pairs will account for a large portion of realistic relation distribution between two arbitrary entities. Therefore, this approach helps to set up real-world scenario. Such a pair is also likely to contain entities that are not selected in the first approach. This leads to capturing entity pairs and their relations independent of KBs.\n\n5. Annotate Relations\nWe ask workers recruited by DeepNatural, 35 a Korean crowdsourcing platform, to annotate each entity pair (e subj , e obj ) with a relation label r. We instruct workers to focus on the current relationship, not ones from the past. For instance, if a person described in a sentence is a former member of a certain organization, workers are asked not to choose the relation per:employee_of. We also ask them to avoid relying on external knowledge, or common sense, to infer the relation from the context solely within a given sentence. Workers report examples that contain hate speech, biased expressions, or personally identifiable information. In addition, they are asked to report sentences with incorrect entity boundaries.\nWe employ 163 qualified workers, each of which correctly labelled at least 4 out of 5 questions during the pilot annotation phase. After the pilot phase, 3 workers are assigned to each example independently to label the relation. Figure 3.5.1 shows the annotation tool for crowdsourcing. To reduce cognitive burden of annotators, we provide a small number of candidate relations at first. The candidates consist of relations that can be defined between types of entity pair predicted by the NER models. If one cannot find appropriate r in the candidates, they are expanded to all relation classes. We take majority-voted labels as gold labels. For each example without a majority label, the top 30 annotators select the final label from the annotated labels. We do not include examples reported as hate speech, biased, or to have privacy issues. The inter-annotator agreement (Krippendorff's \u03b1) on the annotated dataset is 0.701 [67].Final\n\n3.5.2 Evaluation Metrics\nThe evaluation metrics for KLUE-RE are 1) micro F1 score on relation existing cases, and 2) area under the precisionrecall curve (AUPRC) on all classes. Micro F1 score is a harmonic mean of micro-precision and micro-recall. It measures the F1 score of the aggregated contributions of all classes. It gives each sample the same importance, thus naturally weighting more on the majority class. We remove the dominant class (no_relation) for this metric to not incentivize the model that focus more on predicting negative class. AUPRC is an averaged area under the precision-recall curves whose x-axis is recall and y-axis is the precision of all relation classes. It is a useful metric for this imbalanced data setting where important positive examples are rarely occurred.\n\n3.5.3 Related Work\nMany researchers attempt to build KBs from unstructured text through automatically identifying relational facts between entity pairs in plain text by applying machine learning techniques. Doddington et al. [31] and Hendrickx et al. [52] construct English datasets to train such models, including a relatively small number of relation classes for general domain text. Mintz et al. [91] further propose distant supervision to automatically annotate plain text by aligning it to the schema of KBs. This allows researchers to scale up the size of RE datasets [116, 153, 48, 147, 149]. Among these recent studies, TACRED [153] is the most widely used dataset, built based on the popular relation schema TAC-KBP [87] which mainly focuses on person and organization entities. Specifically, TACRED contains 106,264 examples annotated with the 42 relation classes. Yu et al. [149] also proposes a dialogue-based RE task by refining TAC-KBP to obtain 36 relation classes adapted for the dialogue domain. We also follow TAC-KBP to build the relation schema and modify them suitable for our situation.\nIn the cases of languages other than English, there are only a few existing benchmarks, including one in Chinese [141], one in German [121], and one in French [55]. Nam et al. [93] propose an RE dataset in Korean using distant supervision to automatically generate and annotate examples. It however has a relatively small test set (\u223c3k), making it difficult to evaluate performance for the total 49 relation classes properly. Moreover, since there is no negative class (no_relation in ours), it is likely to encourage models to overly predict false positives [153]. We thus consider KLUE-RE as a standard large-scale RE benchmark to properly evaluate Korean language models.\n\n3.5.4 Conclusion\nWe propose KLUE-RE, a large-scale human-annotated RE benchmark for Korean. To overcome the lack of large-scale and up-to-date Korean KBs, we design an efficient candidate collection method, coupled with an effective annotation scheme. KLUE-RE can not only be used for online information extraction but also contribute to building a large-scale knowledge graph from unstructured texts. We therefore expect KLUE-RE to be a starting point for building a large-scale, ever-growing public KB in Korean, as well as a valuable Korean NLU benchmark.\n\n3.6 Dependency Parsing (DP)\nDependency parsing (DP) is an NLP task that aims at finding relational information among words. It has been an important component in many NLP systems, because of its ability to capture the syntactic feature of a sentence. We include DP in KLUE to evaluate the representational power of language models in terms of syntactic features.\nFormally, a dependency parser predicts a graph structure of an input sentence based on the dependency grammar [29, 28]. In general, a parsed tree consists of dependency arcs, connecting dependents to their heads, and the dependency labels attached to the arcs that represent the relations between dependents and their heads. For example, Figure 5 shows a parsed result of the example sentence: \"\ucca0\uc218\uac00 \uc0ac\uacfc\ub97c \uba39\uc5c8\ub2e4 (Chul-Soo ate an apple.)\".\nIn the tree, arrows depart from head and point to their dependents. Thus '\ucca0\uc218\uac00 (Chul-Soo)' and '\uc0ac\uacfc\ub97c (an apple)' are dependents of '\uba39\uc5c8\ub2e4(ate)' and '\uba39\uc5c8\ub2e4(ate)' is the head of '\ucca0\uc218\uac00 (Chul-Soo)' and '\uc0ac\uacfc\ub97c (an apple)'. Also, '\ucca0\uc218\uac00 (Chul-Soo)' is dependent on '\uba39\uc5c8\ub2e4 (ate)' with a \"Subject\" relation. This dependency relation label is called DEPREL. For DEPREL, we follow the TTA Dependency annotation scheme 36 consisting of a combination of 9 syntax tags and 6 function tags.\nSince each word in a sentence has a pair of dependency information (HEAD, DEPREL), DP is conventionally formulated as a word-level sequence tagging task. We evaluate a model's performance using unlabeled attachment score (UAS) and labeled attachment score (LAS). During the evaluation, labels with a cumulative frequency of 1% from the bottom are grouped into the OTHERS label to compensate for the negative impact of lower frequency labels on LAS.\nFigure 6 : A demonstration of the KLUE-DP output format using a sentence that translates to \"Chul-Soo ate an apple.\"\nWe represent the output in a CoNLL-like format, as shown in Figure 6. This format consists of 6 columns, each column contains a word index (ID), word form (FORM), lemma of word form (LEMMA), part-of-speech tag (POS), head of the current word (HEAD), and dependency relation (DEPREL).\n\n3.6.1 Dataset Construction\nSource Corpora To build our corpus as a suitable dataset for a general-usage DP model, we take into account both formal and informal texts. We use WIKITREE and AIRBNB as source corpora. WIKITREE consists of news articles and represents grammatically sound formal texts. On the other hand, AIRBNB mostly consists of user-generated reviews containing web texts, thus showing frequent omission of components and free word order. We collect the same rate of data from both WIKITREE and AIRBNB so that our dataset represents both refined written sentences and noisy colloquial texts.\nAnnotation Protocol Since the part-of-speech indicates how the word functions grammatically in a sentence, it is highly related to the dependency relations. To utilize POS information as an additional syntactic feature, we first annotate POS on the corpus prior to dependency relation annotation. To this end, we follow TTA POS tagging guideline. 37 We use annotated POS information when constructing the DP corpus. Next, we modify the original TTA DP guideline for dependency relation annotation. We add guides for spoken and web data, since the original guideline only contains instruction for annotating written data.\nWe follow TTA DP tagset, which is a standard in Korean. As shown in Table 12, it is made up of a combination of 9 syntax tags and 6 function tags. The syntax tag indicates the POS for token, and there are NP (noun phrase), VP (verb phrase), AP (adverb phrase), VNP (copula phrase), DP (adnoun phrase), IP (interjection phrase), X (pseudo phrase), L (left parenthesis and quotation mark) and R (right parenthesis and quotation mark). The function tag indicates what function the token performs in relation to the head, and there are SBJ (subject), OBJ (object), MOD (noun modifier), AJT (predicate modifier), CMP (complement), and CNJ (conjunction). The TTA DP combines a syntax tag and a function tag into a single tag, such as NP_SBJ (noun phrase subject) and VP_AJT (verb phrase adverb).\nAnnotation Process To build highly reliable corpora for both POS and DP annotation, we use an annotation tool provided by DeepNatural, 38 a Korean crowdsourcing platform, which asks the annotators to annotate cross-reference.\nAfter POS annotation is completed, dependency relation annotation proceeds on the same sentences by using POS information. Both POS and DP are annotated by ten PhD students in Korean linguistics, who previously contributed to constructing MODU 39 [98] opened by the National Institute of the Korean Language. Prior to annotation, we instruct them with our guideline. During annotation, we respond to questions from annotators in real-time, and accordingly, the guidelines are iteratively updated. Annotators report sentences containing hate speech or bias during the annotation. In addition, sentences containing personal information (e.g., name, address, phone number, etc.) are reported. The reported sentences are removed from the dataset after our inspection.\nAnnotations are validated in two steps. First, each annotator reviews and corrects POS and DP labels annotated by other annotators. Then we finally review all data and revise remaining mislabeled sentences.\nFinal Dataset KLUE-DP consists of a total of 14,500 sentences, including 7,250 sentences for WIKINEWS and 7,250 sentences for AIRBNB. POS annotation is given in the 4th column of CoNLL-like format, along with HEAD and DEPREL information in next columns. We set the train/dev/test split as 10,000, 2,000, 2,500 sentences. Tables 13 and  14 show detailed statistics of KLUE-DP.\n\n3.6.2 Evaluation Metrics\nThe evaluation metrics for KLUE-DP are 1) unlabeled attachment score (UAS) and 2) labeled attachment score (LAS), which are popular metrics for DP. Given the goal of DP is to predict head indices (HEAD) and dependency relation classes (DEPREL), UAS only counts HEAD prediction while LAS counts both HEAD and DEPREL. Specifically, UAS calculates macro F1 score on HEAD prediction, while LAS calculates macro F1 score on DEPREL whose HEAD prediction is correct. Both scores give the same importance to all classes. For LAS, since DEPREL distribution is highly skewed, we combine the predictions on the labels with a cumulative frequency of 1% from the bottom into a single label (OTHERS) and then calculate F1 score. The less-appeared labels are referred in Table 14.\n\n3.6.3 Related Work\nThe Penn Treebank [84] is a constituency parsed dataset created from 1989 to 1996. It has a size of about 3 million words, including IBM computer manuals, nursing notes, Wall Street Journal articles, phone conversations. A total of 48 POS tags and 18 syntax tags are used by combining meta tags such as symbols. The Penn Treebank was the best-known parsing data set before dependency parsing became prevalent. Later studies convert the Penn Treebank to dependency parsing [86, 20].\nA representative DP corpus is the Universal Dependencies(UD) dataset. 40 UD is de facto standard of DP data, aiming for a unified treebank annotation in various languages. Google Universal POS [106] composed of 12 tags, was developed and a corpus was built that applied it to 25 different languages. Also de Marneffe and Manning [28] studied guidelines for dependency parsing markers used in Stanford parsers [65]. The Universal Dependency Treebank Project in 2013 attempted to combine the two studies above to have a consistent annotation system for multiple languages. UD started by modifying and supplementing this. UD first started in 2015 and Nivre et al. [97] and 10 corpora in a total of 10 languages were released through the website. As of 2021, UD (UD 2.7v) offers 104 languages and 183 corpora.\nThe corpus constructed with UD scheme is made according to a certain structure called CoNLL-U format. Since UD aims to study general linguistic universality by using the same tag and annotation system in different languages, a unified format for integrating and managing each corpus is needed. The CoNLL-U format is a modified version of this CoNLL format so that it can well represent Universal dependency parsing. CoNLL-U format consists of 10 columns, each column display a word index (ID), word form (FORM), lemma of word form (LEMMA), universal part-of-speech tag (UPOS), language-specific part-It represents of-speech tag (XPOS), list of morphological features (FEATS), head of the current word (HEAD), dependency relation (DEPREL), enhanced dependency graph (DEPS), any other annotation (MISC). In this format, each word stands on a line along with different associated features (word form, lemma, POS tag, etc.) and we adopt this format in our final dataset.\nIn Korean, DP corpus is divided into those that follow the UD scheme and those that do not. Among the former are The Google Korean Universal Dependency Treebank (GKT), The KAIST Korean Universal Dependency Treebank (KTB), and The Penn Korean Universal Dependency Treebank (PKT). These three datasets are converted from The Google Korean Treebank [86], The Kaist Treebank [128], and The Penn Korean treebank [47] according to the UD scheme, respectively [20]. These were first automatically converted according to the head-finding rule and then heuristically modified. These are composed of 6k, 27k, and 5k sentences, respectively, and include the genres of blog, newswire, literature, academic, and manuscript. Among them, PKT was revised by changing the analysis unit and several rules to further reveal the characteristics of Korean [99].\nCorpora that do not follow the UD scheme include the TTA DP Corpus built by Electronics and Telecommunications Research Institute (ETRI) and the Modu Corpus [98] built by the National Institute of the Korean Language (NIKL). Both corpora follow the CoNLL format, and use their own tagset, which was developed from the 21st century Sejong Plan corpus. The syntactic analysis corpus of the 21st century Sejong Plan is constructed according to the constituency grammar, following a scheme similar to the Penn Treebank. Unlike the dependency grammar, which grasps only the dominant relationship between two words, the constituency grammar identifies the relationship between words hierarchically. However, since Korean has relatively free word order, dependency parsing is more suitable than phrasestructure parsing. Studies on converting the 21st century Sejong Plan corpus into DP format have been conducted, and since then, corpora that use the 21st century Sejong Plan's tagset but follow dependency parsing have been constructed. The size of TTA DP corpus is about 27k, and the Modu corpus about 2000k. Unlike UD, which emphasizes general linguistic characteristics, better represents the characteristics of Korean as an individual language, and serves as a national standard for Korean DP tagging. Also, there are already corpus annotated according to the TTA scheme, so we consider compatibility between them and our benchmark. For this reason, we constructed the KLUE-DP using the TTA tagset.\n\n3.6.4 Conclusion\nWe build a Korean DP benchmark KLUE-DP consisting of formal news and informal user-generated web data. KLUE-DP is helpful for developing a DP model that can be used in multiple domains. POS tagging is performed together to improve DP performance, and the tagset and guideline for DP and POS tagging are applied by revising the existing TTA dataset. This guideline is customized to reflect the characteristics of Korean (agglutinative, free word order, etc.), and it also tackles omission of predicates in web data or errors in spacing. We hope that our benchmarks will help in the development of Korean DP models and other natural language processing.\n\n3.7 Machine Reading Comprehension (MRC)\nMachine reading comprehension (MRC) is a task designed to evaluate models' abilities to read a given text passage and then answer a question about the passage, that is, its ability of comprehension.\nMost of existing, widely-used MRC benchmarks are largely in English [21, 56, 60, 112, 113, 145, 150]. Those resources are widely used in evaluating pre-trained language models since it is one of the most intuitive methods for measuring text comprehension. SQuAD 1.1 [112] and SQuAD 2.0 [113] are popular evaluation tasks along with GLUE [30, 82, 22, 71]. BoolQ [21], ReCoRD [150], and MultiRC [60] are selected as a member of SuperGLUE for rigorous evaluation of language models. Recently, open-domain QA task which can be viewed as an MRC task without a given text passage [69, 56, 145, 38], is included in a knowledge-intensive NLP task benchmark [105].\nMotivated by those datasets, MRC has become an essential task in NLU benchmarks for various languages such as Indonesian [137], Chinese [142], and Russian [125]. In Korean, however, an appropriate MRC benchmark is not available because existing Korean MRC datasets are either less challenging, limited in access, or simply machinetranslated from an English dataset [79, 1, 74]. We therefore include MRC in KLUE and create a new challenging Korean MRC benchmark (KLUE-MRC) with the following contributions:\n\u2022 Providing multiple question types: In order to evaluate different aspects of MRC capability of models, we provide three question types: paraphrase, multi-sentence reasoning, and unanswerable. We collect questions by following strict guidelines with specific sets of rules for each type.\n\u2022 Preventing reasoning shortcuts: We prevent MRC models from exploiting reasoning shortcuts with simple wordmatching by enforcing lexical and syntactic variations when workers generate questions. Also, we aim to generate questions which can be answered by considering the full query sentence.\n\u2022 Multiple passage domains accessible to everyone : We include news domain passages as well as Wikipedia. To guarantee CC BY-SA license of KLUE-MRC, we made signed contracts with corresponding news providers.\nWe formulate MRC as a task of predicting the answer span of the question from the given text passage. The input is a concatenated sequence of the question and the passage separated with a delimiter. The output is the start and end positions of the predicted answer span within the passage.\nWe evaluate models with two metrics: 1) exact match (EM) and 2) character-level ROUGE-W. Note that character-level ROUGE-W is different from the character-level F1 score used in the previous Korean MRC datasets. If the question is unanswerable within the given passage, the model should predict the empty answer string. The motivation of our metrics are described in Section 3.7.2.\n\n3.7.1 Dataset Construction\nSource Corpora First, we collect passages from Korean WIKIPEDIA and news articles provided by The Korea Economy Daily and ACROFAN. WIKIPEDIA articles are one of the most commonly used resources for creating MRC datasets. We additionally include news articles reporting contemporary social issues to enhance diversity of passages. They are provided by The Korea Economy Daily and ACROFAN. As news articles are generally copyrighted work, we sign a contract with the news providers to use and redistribute the articles under CC BY-SA license only for building a dataset for machine learning purposes. We believe multi-domain corpus can help MRC models enhance their generalizability.\nWe preprocess the corpus to collect passages. For WIKIPEDIA articles, we remove duplicates in other existing Korean MRC benchmarks (e.g., KorQuAD) for precise evaluation of models. Then, we split each article by its sections to obtain passages. For the news articles, we filter out political articles and articles belonging to categories which have less than 100 articles. We finally gather all preprocessed passages whose length is longer than 512 and shorter than 2048 in characters.\nAnnotation Protocol We annotate questions and answers by giving passages to crowdworkers. We provide a detailed tutorial session to introduce our guidelines. 60 out of 80 workers are selected after a pilot test of creating 15 questionanswer pairs with a given passage. The selected workers generate questions and label corresponding answers spans (for Types 1 and 2) or fake answers spans (for Type 3). We use Tagtog annotation toolkit 41 for the annotation. We assign three inspectors for each question type to validate the generated questions and answers following common and type-specific guidelines. If the generated question-answer pair fails to pass the inspection, the worker refines it based on the feedback given by the inspector.\n\n1. Common Guidelines\nWe first build common annotation guidelines for crowdworkers. The workers are required to follow the instructions during corresponding question generation and answer span annotation for all three question types as below.\nA question should:\n\u2022 Be natural as a web search query: Trying to generate challenging questions can lead to the use of unnatural expressions. We guide workers to make natural questions as if they are for web search queries. We care about the generalizability of questions to extend the task to open-domain QA tasks in future work.\n\u2022 Avoid omission: Pronoun-dropping is prevalent in Korean, hence it tends to omit the subject or object. The omission can result in ambiguity for finding answers. We explicitly guide workers to keep all grammatical components in generated questions.\n\u2022 Not copy a phrase in the passage: Questions might have similar meaning to some phrases in the passage but should not contain exactly the same phrase. This mitigates high word-overlap between questions and the passage as reported in previous work [122].\n\u2022 Not refer to external knowledge: Questions need to be fully understood without any external knowledge. We do not allow workers using their background knowledge or any world knowledge to generate a question. Questions should be derived solely based on the given passage.\n\u2022 Be meaningful in every part for finding the answer: Answers should not be found with only a small fraction of the question. We encourage workers to generate questions that require understanding of the whole question text to find an answer. We do not allow the use of expressions that appear only once in the passage because models can easily infer answers without understanding the whole question.\nAn answer should:\n\u2022 Be a unique entity within a passage: To clarify what to ask, only a single answer should be inferred from the question. When answers can be represented in various lexical forms, workers should mark all answer spans (e.g. Television, TV).\n\u2022 Not be the main topic or title: We aim to prevent a known artifact which the most frequently appeared words within a given passage are likely to be the answer [66].\n\n2. Type-Specific Guidelines\nHere we elaborate question type-specific guidelines. These guidelines are additionally presented to workers along with the common guideline above.\n\n2.1. Question Paraphrasing (Type 1)\nType 1 examples focus on paraphrasing the passage sentences when generating questions to reduce word overlap between them. The paraphrasing enables us to validate whether the model can correctly understand the semantics of the paraphrased question and infer the answer [122].\n\nCuesnon River\nIn our annotation guide for paraphrased questions, we further prevent workers from generating them by simply shifting the order of subsequent phrases or changing the functional particles. Specifically, workers create questions by following principles:\n\u2022 Either syntactic or lexical variation should be applied to text snippets in the passage.\n-For syntactic variation, reconstructing the structure of the original sentence is preferred. Minimal changes such as swapping order between the nearby phrases is not allowed. -For lexical variation, changing verbs or modifiers is required, and variation of noun phrases is recommended.\n\u2022 More than half of the words in the question should not overlap with the corresponding part (sentence) of the passage.\nThen we comprehensively check whether the question has low word overlap with the passage. As shown in Table 15, where Cuesnon River is the answer, the question paraphrases the given evidence sentence (\"the boundary of Britanny and Normandy\") by providing a new sentence structure. The structure is changed from a wh-question with copula to a question with an intransitive verb. Also, a new term \"distinguishes\" is introduced which replaces the phrase \"is the boundary of\".\n\n2.2. Multiple-Sentence Reasoning (Type 2)\nType 2 examples focus on making questions requiring multiple-sentence reasoning. Multiple-sentence reasoning requires models to derive answers from the questions by reasoning over at least two sentences in the passage. We focus on evaluating whether an MRC model can infer the answer span by comprehensively aggregating the information spread across the passage. We carefully design annotation guidelines for multiple-sentence reasoning examples. Since Min et al. [90] report multiple-sentence reasoning questions can easily fall back to single-sentence reasoning, we aim to avoid such cases by guiding workers to follow the steps below:\n\u2022 (Step 1.) Find at least two statements in the given passage that share some properties.\n\u2022 (Step 2.) Select one as the answer among the entities regarding the shared properties.\n\u2022 (Step 3.) Generate a question with the selected entity. When was the area of the Roman Empire twothirds of that of the current U.S.?\nAnswer\ud2b8\ub77c\uc57c\ub204\uc2a4 \ud669\uc81c \uc2dc\ub300(98\ub144-117\ub144) / \ud2b8\ub77c\uc57c\ub204\uc2a4 \ud669 \uc81c \uc2dc\ub300 / 98\ub144-117\ub144\nTrajan(AD 98-AD 117) / Trajan / AD 98-AD 117\nThe example in Table 16 follows the steps described above. Shared properties between the two statements (A: \"In terms of the area today, it is estimated to represent two-thirds of the current U.S. area\", B: \"Under Trajan(AD 98-AD 117), Roman Empire reached its territorial peak.\") are \"Roman Empire\" and \"(Under) Trajan(AD 98 -AD 117)\". Assume we pick \"Trajan(AD 98-AD 117)\" as the answer and generate a question \"When was the area of the Roman Empire two-thirds of that of the current U.S.?\". Neither of the two statements is enough to single out the answer alone. When given the question and statement A alone, it is not possible to resolve an answer between \"Trajan(AD 98-AD 117)\" and \"the 5th century\" which both represent time period in the passage. Since statement B does not include specific information on the territory size, it is also not sufficient to narrow down the answer by itself. Therefore, statement A and B both needs to be aggregated to correctly answer the question.\n\n2.3. Unanswerable Questions (Type 3)\nType 3 examples are questions unable to be answered within the given passage. We name these as 'unanswerable' questions. In the real world, a question is often unanswerable if a passage is not available. If a model is built upon the premise that an answer always exists within the passage, it would not effectively handle such cases as SQuAD 2.0 [113] pointed out. We therefore add unanswerable questions in our benchmark to incentivize models to identify whether a question is answerable or not.\nIn the annotation guideline, we present the following principles to generate desirable unanswerable questions. The unanswerable questions should:\n\u2022 Be relevant to the passage: An entity appearing in the given passage should be included in the question. The entity makes the question relevant to the passage. Otherwise the question would be too easy to be determined as unanswerable.\n\u2022 Have fake answers within the passage: A fake answer is plausible but incorrect regarding the corresponding question. The fake answer should exist in the passage as a distractor. \u2022 Not have correct answers within the passage: Despite the existence of fake answers, the correct answer for the generated question must not exist in the passage.\nIn our setting, a model is likely to fail to identify the question's unanswerability based on the word overlap between the question and the passage. Our question includes entities from the passage which increases the overlap. In that case, an MRC model would choose plausible fake answers that exist in the passage. Table 17 shows an example of the Type 3 problem. The question includes \"bandwidth\" which also exists in the given passage. The question is asking for the bandwidth, so \"1 to 5 Gigabit Ethernet\" would be a plausible but incorrect answer. There is no cue about the correct answer to \"bandwidth-hungry server\" from the question in the passage. The model should predict the empty string using the out of the context span range, such as using the [CLS] token from BERT.  18 and 19 show the detailed statistics of KLUE-MRC.\n\n3.7.2 Evaluation Metrics\nThe evaluation metrics for KLUE-MRC are 1) exact match (EM) and 2) character-level ROUGE-W (ROUGE) [80], which can be viewed as longest common consecutive subsequence (LCCS)-based F1 score. EM is the most commonly used metric for MRC tasks, which measures the equality of ground truth and predicted answer string. If there are multiple gold labels, a model can earn score when at least one prediction is matched. In contrast, ROUGE gives a partial score although a model fails to predict exactly matched answer. Due to the characteristics of Korean, an answer span can be located inside a single word, hence subword-level span should be considered. ROUGE calculates F1 score of the length ratio of LCCS to a prediction and the length ratio of LCCS to the ground truth string. In the case when multiple ground truth answer spans have the same meaning but different lexical variations (e.g. TV, Television), we use the maximum ROUGE score among the combinations of answers and the prediction. We do not adopt character-level F1 score (char F1), which is used in all the previous Korean MRC datasets, since it measures character overlap regardless of the order. When a model predicts \"\ud55c\uad6d\uc758 \uc704\uc778\ub4e4 (great people in Korea)\" and an answer is \"\uad6d\ud55c\ub41c \ubc94\uc704 (limited scope)\", a metric should give a low score. ROUGE scores 15.38, whereas char F1 gives 54.55 due to the overlap of \"\ud55c\", \"\uad6d\", and \"\uc704\".\n\n3.7.3 Analysis\nWe investigate KLUE-MRC by comparing with other Korean MRC datasets. KorQuAD 1.0 [79] and 2.0 [63] are commonly used in Korean MRC research, released with training and evaluation sets separately. However, KorQuAD 2.0 shows a quite different composition in contents compared to ours and KorQUAD 1.0, in specific, HTML tags and tables. Therefore, we conduct comparison only with KorQuAD 1.0 dataset. KorQuAD 1.0 dev set was leveraged because its test set is not available.\n\nQuestion Difficulty of KLUE-MRC Evaluation Set\nAs we aim to make KLUE-MRC challenging, it is necessary to check the difficulty of test sets. We compare the performance on the evaluation set of ours and KorQuAD 1.0 with the model trained with both datasets in Table 20. We fine-tune the model 43 with a collection of the train set from both datasets and test the model on each evaluation set.\nKorQuAD 1.0 train examples (60,407) are almost four times larger than KLUE-MRC (17, 554), and it may have resulted in the higher performance for KorQuAD 1.0 dev set. We additionally conduct fine-tuning with a collection of the same amount of both train datasets for fair comparison. We adjust KorQuAD 1.0 train set to fit the size of KLUE-MRC train set by random sampling. Table 20 shows consistently lower scores of KLUE-MRC compared to KorQuAD 1.0. Thus, our dataset is more challenging regardless of the size of train set.\nLexical Overlap As high lexical overlap between question and passage can cause shortcut reasoning, reducing lexical overlap is important to build a challenging dataset. To investigate the effects of the proposed strict guidelines and challenging question types, we calculate the lexical overlap of ours and KorQuAD 1.0. The lexical overlap ratio is calculated by dividing the number of common components between question and passage into the number of components in the question. We exclude functional particles such as postposition(\uc870\uc0ac, josa) and ending components (\uc5b4\ubbf8, eomi) when computing the overlap ratio via an open-sourced Korean POS tagger. 44 We observe our lexical overlap ratio is almost 10%p lower than that of KorQuAD dataset (70%). For each questions types, Types 1 and 3 show similar ratio in range from 55% to 59%. Type 2 exhibits 68% overlap ratio.\n\nHuman Evaluation\nWe evaluate human performance on our KLUE-MRC to measure its difficulty concerning human reading comprehension capabilities. We randomly sample 1,000 examples from our test set and hire three workers to solve them. We select the score of the top scoring worker as human performance. Table 21 reports the comparison between human performance and base model.\n\n3.7.4 Related Work\nIn recent years, significant progress has been achieved in English MRC research with challenging datasets of various question types, including but not limited to: paraphrase, multi-sentence, and unanswerable.\nParaphrased questions have low word overlap between the question and reading passage, which prevents MRC models from exploiting simple word-matching. Trischler et al. [130] create a NewsQA dataset by generating questions from news headlines and summarizing them via crowdsourcing. They reduce the word overlap by annotating answers on the main articles which are not given during question generation. Saha et al. [120] leverage pairs of plot summaries for the same movies from Wikipedia and IMDb. 45 They generate questions from the shorter plots and annotate answers on the longer ones to obtain naturally paraphrased questions. Sen and Saffari [122] report that datasets with low question-passage overlap will enhance the generalizability of MRC models.\nMulti-sentence questions require reasoning over multiple sentences. As a result, they are more difficult compared to single-sentence questions. Joshi et al. [56] introduce TriviaQA, a dataset of questions from trivia websites. Since they gather evidence passages from various sources (e.g., Wikipedia and the Web), multiple sentences are naturally required for answering the given question. Khashabi et al. [60] explicitly generate multi-sentence questions on various texts through crowdsourcing and release the MultiRC dataset. The SuperGLUE [132] benchmark adopts the MultiRC dataset as one of its tasks.\nSeveral MRC datasets have incorporated unanswerable questions [143, 130, 96, 113, 69]. Rajpurkar et al. [113] report performance drop in MRC models when unanswerable questions are included in the dataset.\nCompared to MRC research on English, Korean MRC research stands on a small number of existing datasets. The primary benchmark for Korean MRC has been KorQuAD [79, 63], which adopts the same data collection process as SQuAD 1.0 [112]. However, the model performance on KorQuAD has already exceeded human performance in a short period, leaving little headroom for further research. Moreover, unlike SQuAD, KorQuAD is under CC BY-ND license and does not allow derivative works (e.g., adding unanswerable questions). AI Hub MRC dataset [1] is based on newspapers and includes unanswerable questions. However, its access is strictly limited to native Korean researchers, prohibiting collaboration even with international researchers residing in Korea. K-QuAD [74] leverage Google Translate 46 to translate SQuAD 1.0 [112] into Korean. Since the K-QuAD dataset does not get updated over time, its quality depends on the machine translator's performance at the time of release. Our KLUE-MRC is different from the existing Korean MRC benchmarks in terms of accessibility-enhance license and more challenging difficulty.\n\n3.7.5 Conclusion\nWe create a new challenging Korean MRC benchmark named (KLUE-MRC). In order to evaluate different aspects of MRC capabilities, KLUE-MRC includes multi-domain passages and three types of questions: paraphrase, multisentence reasoning, and unanswerable. KLUE-MRC shows improvements in question type diversity, difficulty, and lexical overlap compared to existing Korean MRC datasets.\n\n3.8 Dialogue State Tracking (DST)\nBuilding a human-computer conversation system has been increasingly attracting attention, and a task-oriented dialogue system is one type of the dialogue systems [14]. A core module of task-oriented dialogue systems, namely, Dialogue State Tracking (DST) is about predicting the dialogue states from a given dialogue context. As illustrated in Table 22, dialogue states are sets of slot and value pairs that are relevant categories (e.g. hotel type) and their possible values (e.g. guest house, hotel, motel), respectively.\nSeveral recent works have considered task-oriented dialogue (TOD) as an important problem of natural language understanding. For instance, DecaNLP [85] includes a DST, which is a key component of TOD, into one of their benchmark tasks, while DialoGLUE [88] releases the first task-oriented dialogue benchmark containing various sub-tasks including DST. In light of such, we include DST as a part of the KLUE benchmark.\nThe task of dialogue state tracking is to predict slot and value pairs after each user utterance, and the potential pairs are predefined by a task schema and knowledge base (KB), tied to the choice of a scenario. For evaluation, we use joint goal accuracy (JGA) and slot micro F1 score. The JGA checks if all of the predicted slot-value pairs are exactly matched with the ground-truths for every turn, while the slot micro F1 computes f1 score for each slot-value pair independently. 47 (Right, I forgot an important thing. I would like to book for two days from Sunday.)\nHotel-area: center Hotel-type: hotel Hotel-internet: yes Hotel-book people: 8 Hotel-price range: dontcare Hotel-book day: Sunday Hotel-book stay: 2\n\n3.8.1 Dataset Construction\nOur dataset construction protocol is a modified version of the Wizard-of-Oz framework (WOZ) [59], which is a widely used paradigm for building dialogue datasets. The WOZ setting is a particular type of human-to-human dialogue collection, which employs two people that either takes a user and a system role. However, it is arguably time-consuming, complex, and expensive [11]. To overcome the limitations, we adopt 'Self-dialog' scheme which requests a single worker to play both user and system roles [11]. In addition, we introduce a new design choice to obtain a more precise dialogue dataset.\nOverview We construct WoS by following five steps: 1) define a task schema, 2) create knowledge base (KB), 3) design an annotation system, 4) collect and annotate a dataset, and 5) finalize the dataset. Different from the other datasets in our benchmarks, WoS is not following an ordinary protocol: collecting raw corpus followed by annotating labels. Rather, we collect dialogues (raw corpus) and their corresponding dialogue states (labels) at the same time.\n\n1. Defining Task Schema\nWe first define a task schema which expresses the scenario of task-oriented dialogue. Our task schema consists of slots across five domains (hotel, restaurant, attraction, taxi, and metro) as shown in Table 23.\nTypically, slots are categorized into informable slots and requestable slots. The informable slots cover properties which can constrain a user goal 48 such as \"price range\", \"area\", and \"booking day\". The requestable slots provide additional information that a user may ask, but not necessarily need to be specified as a user goal constraint. A typical example of a requestable slot is \"phone number\", which a user may ask for, but would not work to narrow down the probable candidates of the goal [50, 51].\nBased on this schema, we include additional attributes to the informable and requestable slots to provide an easy-tooperate annotation system and easy-to-follow guidelines. A slot could have one or more attributes among whether it is 1) boolean type, 2) required or not (Required), 3) related to booking (Booking-related), and 4) only available after booking is confirmed (Requestable after booking. e.g., reference number). The boolean type slots can have either yes or no as their values, such as \"Parking (availability)\" and \"(has) swimming pool\". Such boolean type values do not appear in the dialogue context explicitly. In other words, they have abstractive properties. A model which understands abstractive properties is desirable, so we have much more boolean type slots than MultiWOZ [10]; WoS has 20 boolean slots across the domains while MultiWOZ includes only 2. Meanwhile, the required slots have to be specified with values in order to fill out a user intent. This helps us to simulate an actual service scenario in which an agent is not allowed to take the next steps without specifying required values [114].\n\n2. Creating Knowledge Base\nWe construct a knowledge base (KB) based on the task schema of each domain to obtain a set of predefined realization candidates of a user's goal. For the hotel and restaurant domains, we manually create virtual instances, whereas for the attraction and metro domains, we leverage real names (e.g. Gangnam Station or Namsan Tower) collected from the web. On the other hand, for the taxi domain, we do not define instances in advance, but dynamically generate the instances during the dialogue collection as in MultiWOZ [10]. With ethical considerations in mind, any personally identifiable information (PII, e.g., phone number, address) is replaced with randomly generated instances using faker. 49 Table 25 shows the KB statistics for each domain.\n\n3. Designing the Annotation System\nIn this section, we describe the annotation platform we used for collecting data from both the User-side and System-side.\n3.1. User Side We provide a goal instruction for a user side role. The instruction includes descriptions of a user's specific goal with corresponding slot values in natural language. It also contains the user's context including persona for variety of dialogues. A user is asked to generate utterances following the instruction. An example is shown Table 24.\nTo devise a multi-domain dialogue scenario where slots are shared across multiple domains, we include domain transition in the instruction [10, 154]. For example, in case of a user aiming to book a hotel, the user might seek information about transportation (taxi, metro, etc.) to get there. In this dialogue, initial domain is changed to another (hotel to taxi/metro). It is more challenging compared with single domain in terms of dialogue state tracking. Because user could express their goal implicitly where values should be inferred by co-referencing other values of preceding domains.\nThe goal instructions are realized by the templates containing placeholders for goal constraining slots and their values. We design diverse templates for each domain, in order to cover various scenarios of the dialogues. Like MultiWOZ, the goal templates include a series of subgoals with corresponding slots. We carefully design the sentences to promote lexical entailment or co-referencing during conversation, which can be naturally observed in the user context or during the domain transition. When filling a template to complete instructions, we randomly assign the instances from KB built upon the domain-specific task schema to the given placeholders. The values of the instruction should be specifically mentioned by a user during a conversation. Each trackable slot either has valid values, None or Dontcare. 50 o properly evaluate a model's generalization ability, we further add counterfactual goals and introduce unseen KB instances during the process [77]. To add new goal instruction based on the current slot distribution, we keep monitoring the slot value frequency and co-occurrence over slots during the construction. Specifically, we add new goal instructions which cover infrequent slot values or rarely co-occurring combination among slots. For example, when \"(hotel-parking, no)\" is infrequent pair in the as-is distribution, we promote it to appear in dialogues by designing goal instruction including it as constraint. Moreover, we differentiate KB instances between particular subset of dataset (train and dev/test set) to simulate realistic scenario regarding unseen slot values in the test time.\n\n3.2. System Side\nThe role of a system side worker (wizard) is to 1) annotate dialogue states of user utterances while 2) generate responses by accessing to the KB if necessary, for every turn. First, the wizard is asked to fill in appropriate slot values inferred from the current dialogue context. If the word uttered by the user is not clear to directly map to a specific slot value, the wizard should clarify the meaning of the word first and then fill the slot when the word has the same meaning as the value. The annotation of a dialogue state is an explicit action of understanding a user request to fully focus on providing the required information. Then, the wizard generates response either to request or to convey information. If the values of required slots are absent, the system side worker is allowed to ask for the missing values to the user. Otherwise, the system provides the user with the adequate information. We enable the system to query the external knowledge base, if needed. When there are more than three search results, the system worker could request more details or recommend one among them.\nTo support the wizard to perform such complicate work effectively and efficiently, we provide a graphical web interface with a newly introduced feature: dropdown components (Figure 3.8.1). Dropdown interface enables the system side worker can choose a value from a list of pre-populated candidates. We present most probable value candidates based on the goal instruction and domain-specific knowledge base, since a dropdown might become worthless when too many options are presented to workers. This procedure naturally prevents several type of annotation errors, such as multi-annotations, mis-annotations, typos, and value canonicalization reported in MutiWOZ 2.1 [37].\n\n4. Dataset Construction\nWe adapt 'Self-dialog' scheme inspired by taskmaster-1 [11] to efficiently collect diverse dialogue dataset while reducing the cost and time. Self-dialog is effective to collect various dialogue data. By having both roles, a worker freely controls a flow of dialogue such as the order of slot occurrence in user utterances and recommendation of system responses. This also leads workers to speak their own styles naturally such that different personas are included. However, we found annotation errors in pilot phase such as early-markup (system pre-fills the values before receiving the values from the user) and delayed-markup (the system fills the values behindhand the proper turn) errors. We further improve the scheme by utilizing explicit turn-switching between user and system roles with providing error correction interface.\nTo elaborate, we train and select trustworthy workers to participate in the main collection process. Prior to the main phase, we conduct several pilot studies with crowdworkers to avoid aforementioned early-markups and delayed-markup errors, and to generate more realistic dialogues including some miscommunications. We also implement an explicit turn-switching between the two roles, in order to immerse themselves in a user/system role, which mitigates overly reduced miscommunications as well. Throughout the pilot, we finally employ 15 selected workers who can effectively handle such issues.\n\n3.8.2 Evaluation Metrics\nFor evaluation metrics for WoS is 1) joint goal accuracy (JGA) and 2) slot micro F1 score. JGA measures the proportion of exactly matched dialogue state which consists of a set of slot-value pairs with the ground truth dialogue state among the total number of dialogue turns. Slot micro F1 score is an average of micro F1 scores in each turn. For each turn, micro F1 score is defined as the harmonic mean of precision and recall in terms of predicted slot-value pairs and ground-truth pairs. Note that the slot micro F1 score ignores when value of the ground truth is \"None\".\n\n3.8.3 Analysis\nWhen splitting the train and dev/test set based on the counterfactual goals and unseen KB instances, like Li et al. [77], we observe performance drop as shown in Table 27. This demonstrates that the counterfactual goal makes WoS more challenging.\n\n3.8.4 Related Work\nWizard-of-Oz (WOZ) [59] is a popular scheme in dialogue collection. In fact, conventional WOZ setting allows to collect various type of dialogues by employing role-playing of two humans. Each human should choose a role between the two: user and system. As taking a role, dialogues are collected by turn-taking generation of utterances with background information provided in advance. In the case of building a task-oriented dialogue, goal is given to a user while knowledge base is allowed to be accessed to system. The system can use its knowledge base when responding to user's request.\nMany dialogue datasets closely follow WOZ settings [135, 35, 36], however, it costs a lot of time and money because two crowdworkers must be matched at the same time and successfully play each role, which prevents collecting dialogues at scale. We refer this limitation to 'worker coexistence constraints'. To overcome the limitation, MultiWOZ [10] slightly change this conventional WOZ to asynchronously collect dialogues turn-by-turn from crowdworkers, which allows different workers to play the same user or system in a single dialogue. This approach costs less but error-prone because every worker must adapt to dialogue already progressed so that their response might be incoherent to the previous context [37]. Recently, CrossWOZ and RiSAWOZ thus pair only selected trustworthy workers to collect dialogues in synchronous manner as suggested in the WOZ settings to keep the annotation quality despite the high construction cost [154, 109].\nByrne et al. [11] also argue that conventional WOZ settings are time consuming, complex and expensive, requiring considerable technical implementation as well as administrative procedures to train and manage both agents and crowdsourced workers, accordingly suggesting Self-dialog as an alternative. Self-dialog is a collection scheme in which workers write the entire dialogue playing both user and system roles. To demonstrate their idea, Byrne et al. [11] build a large-scale dialogue datase, Taskmaster-1, which is built upon the Self-dialog which stands on the WOZ schemes: 1) two people playing user and system roles (conventional WOZ setting) and 2) one person playing both roles (Self-dialog). As a result, Self-dialog can remedy the cost of 'worker coexistence constraint' effectively with avoiding incoherent dialogue generation caused by asynchronous dialogue collection. However, there is a tendency that little miscommunication occurs in the dialogue which compared to the real world conversations because the same person produces utterances in both roles, which might lead to a gap from reality.\nSome researchers further tries to employ only machines to create such dialogues to maximize cost-efficiency. It builds the dialogues on top of a simulator which is able to create utterances by turn automatically from elaborately designed rules and given task schema. The simulator first generates psuedo-dialogue, then adopts crowdsourcing to paraphrase them to the natural utterances [124, 114]. It requires much less human effort, however, heavily relies on the simulator.\nMeanwhile, previous works exists addressing robust evaluation of DST models. According to CoCo [77], state-of-theart DST models are not robust to realistic scenarios since they scarcely appear in the train data. As the name CoCo (controllable counterfactuals) suggests, it generates infrequent but realistic dialogues based on the predefined slot-value pairs. They show that even a state-of-the-art DST model's performance drops significantly when they are evaluated on such dialogues including counterfactual goals. It means the current TOD benchmarks should be improved in terms of robustness to unseen but realistic scenarios.\nAs for Korean, there is a task-oriented dialog dataset is provided by National Information Society Agency (NIA). It covers about 10 domains related to the civil complaints and consists of more than 500k dialogues. The utterances are divided into four types: 1) a main question that a user asks, 2) a sub question that a system could ask for clarification, 3) a user answer, and 4) a system answer. Additionally, user intents are annotated and entities are extracted from each utterance. We find that this dataset does not follow any of the aforementioned settings; there is no dialogue state represented as slot-value pairs, only regarding single turn judgement. It also lacks information about task schema and redistribution is restricted which does not satisfy our accessibility principle, which motivates us to newly create a DST benchmark.\n\n3.8.5 Conclusion\nWe introduce Wizard-of-Seoul (WoS), the first large-scale Korean multi-domain task-oriented dialogue dataset that simulates conversations between Seoul tourists and travel agents. We adapt 'Self-dialog' for efficiently scaling up of dialogue collection scheme. In addition, consideration on annotation interfaces (drop-down menu and turn-switching) mitigates erroneous cases and diverse goal instructions including counterfactual ones promote each conversation to be more natural and challenging. We hope that WoS sparks various future dialogue research in Korean and also offers valuable insights to pushing forward end-to-end dialogue modeling.\n\n4 Pretrained Language Models\nIn order to facilitate further research using KLUE, we provide strong baselines for all the benchmark tasks within it. As a part of this effort, we pretrain and release large-scale language models for Korean, which we hope would reduce the burden of retraining large-scale language models from individual researchers. More specifically, We pretrain language models (PLM), including BERT [30] and RoBERTa [82], from scratch.\n\n4.1 Language Models\nWe pretrain multiple Korean language models while varying training configuration. This enables us to explore effective settings for pretraining Korean models and further establish simple yet effective baseline models for KLUE. We train KLUE-BERT and KLUE-RoBERTa. We vary the choice of a pretraining corpus, preprocessing procedure, tokenization strategy, and other training configurations. Pretraining Corpora We gather the following five publicly available Korean corpora from diverse sources to cover a broad set of topics and many different styles. We combine these corpora to build the final pretraining corpus of size approximately 62GB. See Table 28 for overall statistics:\n\u2022 MODU : Modu 51 Corpus [98] is a collection of Korean corpora distributed by National Institute of Korean Languages. 52 It includes both formal articles (news and books) and colloquial text (dialogues).\n\u2022 CC-100-Kor : CC-100 53 is the large-scale multilingual web crawled corpora by using CC-Net [136]. This is used for training XLM-R [26]. We use the Korean portion from this corpora.\n\u2022 NAMUWIKI : NAMUWIKI is a Korean web-based encyclopedia, similar to Wikipedia, but known to be less formal. Specifically, we download the dump created on March 2nd, 2020. 54 NEWSCRAWL : NEWSCRAWL consists of 12,800,000 news articles published from 2011 to 2020, collected from a news aggregation platform.\n\u2022 PETITION : Petition is a collection of public petitions posted to the Blue House asking for administrative actions on social issues. We use the articles in the Blue House National Petition 55 published from August 2017 to March 2019. 56 reprocessing We filter noisy text and non-Korean text using the same methods from Section 2.3. Each document in the corpus is split into sentences using C++ implementation (v1.3.1.) of rule-based Korean Sentence Splitter (KSS). 57 or CC-100-Kor and NEWSCRAWL, we keep sentences of length greater than equal to 200 characters, as a heuristics to keep well-formed sentences. We then remove sentences included in our benchmark task datasets, using BM25 as a sentence similarity metric [118].\nEthical Considerations Because we collect and use as much publicly available data as possible for pretraining, these corpora often contain undesirable social biases. Furthermore, we noticed earlier quite a bit of PII in these corpora, although they were all publicly available. Both of these are problematic. Social biases in the corpus may result in a language model that learns such biases. PII in the corpus may be memorized by a language model and can subsequently be retrieved by adversarial attacks [12]. We do not filter out socially biased contents nor hate speech for three reasons. First, manual inspection is infeasible for this large-scale pretraining corpora. Second, it is a challenging problem on its own to automatically detect socially biased contents or hate space, as both of these highly depend on the context in which they appear [62]. Lastly, being blind to such harmful contents prevents the future use of a language model for detecting and correcting these harmful contents, such as using it as an anti-expert [81]. We expect future research on the pretrained language models we release to focus on how to detect and correct biases encoded in these models and on how to debias them, as has been recently demonstrated by Cheng et al. [15].\nIn contrast, we pseudonymize PII in our corpora as much as possible. We detect 16 personal data types using regular expressions based on the guideline from the Korea Internet and Security Agency (KISA). 58 It is relatively easy to pseudonymize PII while keeping linguistic patterns, since the selected PII has standardized pattern. We then replace the original information, using either the faker library 59 or random generation based on the pattern. As a result, we pseudonymize 1.2% of the pretraining corpora. Details are illustrated in Table 29.\nTokenization We design and use a new tokenization method, morpheme-based subword tokenization. When building a vocabulary, we pre-tokenize a raw text into morphemes using a morphological analyzer, and then we apply byte pair encoding (BPE) [123] to get the final vocabulary. For morpheme segmentation, we use Mecab-ko, 60 MeCab [68] adapted for Korean, and for BPE segmentation, we use the wordpiece tokenizer from Huggingface Tokenizers library. 61 We specify the vocabulary size to 32k. After building the vocabulary, we only use the BPE model during inference, which allows us to tokenize a word sequence by reflecting morphemes without a morphological analyzer. This improves both usability and speed. Examples are presented in Table 30.\nThe motivation behind this method is that Korean is an agglutinative language, which is to say, a word is a constitution of morphemes -stems and affixes. The morphemes tend to remain unchange on different unions, and the boundary is generally clear. Although BPE has been widely used across many languages due to its effectiveness, it struggles to identify morphemes correctly as demonstrated in Table 30.\nTable 30 : An input text \"\uc870\uacbd\ud604\uc740 \uc778\uacf5\uc9c0\ub2a5 \ubd84\uc57c\uc758 \uc800\uba85\ud55c \uc5f0\uad6c\uc790\uc774\ub2e4. (Kyunghyun Cho is a prominent AI researcher.)\" is segmented with various tokenization strategies. We denote slash (/) as a token separator. The mBERT tokenizer [30] splits the input text into nearly in characters resulting in a longer sequence than monolingual tokenizers. BPE tokenizer generates tokens spanning multiple morphemes (##\ud604\uc740, ##\uba85\ud55c). Morpheme-based subword tokenizer, on the other hand, better splits text into morphemes (##\uc740, ##\uc758).\n\nTokenization Tokenized Sequence\nRaw Text \uc870\uacbd\ud604\uc740 \uc778\uacf5\uc9c0\ub2a5 \ubd84\uc57c\uc758 \uc800\uba85\ud55c \uc5f0\uad6c\uc790\uc774\ub2e4. Training Configurations We choose BERT [30] and RoBERTa [82] architectures for our language models. Table 31 describes the implementation details. All models take sequences of at most 512 tokens long each and are pretrained with a static or dynamic masking strategy following the original training procedure. When masking tokens, we use whole word masking (WWM) which masks all of the the tokens that form a single word. BERT also performs next sentence prediction (NSP). Other hyperparameters not specified in Table 31 nor in the pretraining procedure details are same as the original configurations from [30, 82]. Due to the resource constraints, we could increase batch size only up to 2,048, unlike Liu et al. [82] who use the batch size of 8k. We decrease the learning rate accordingly. We fix the learning rate to 10 \u22124 for both BERT and RoBERTa.BPE (Multilingual) \uc870 / ##\uacbd / ##\ud604 / ##\uc740 / \uc778 / ##\uacf5 / ##\uc9c0 / ##\ub2a5 / \ubd84 / ##\uc57c / ##\uc758 / \uc800 / ##\uba85\ud55c / \uc5f0\uad6c / ##\uc790 / ##\uc774\ub2e4 / . BPE \uc870\uacbd / ##\ud604\uc740 / \uc778\uacf5\uc9c0\ub2a5 / \ubd84\uc57c\uc758 / \uc800 / ##\uba85\ud55c / \uc5f0\uad6c / ##\uc790\uc774 / ##\ub2e4 / . Morpheme \uc870\uacbd\ud604 / \uc740 / \uc778\uacf5\uc9c0\ub2a5 / \ubd84\uc57c / \uc758 / \uc800\uba85 / \ud55c / \uc5f0\uad6c\uc790 / \uc774 / \ub2e4 / . Morpheme-based Subword \uc870\uacbd / ##\ud604 / ##\uc740 / \uc778\uacf5\uc9c0\ub2a5 / \ubd84\uc57c / ##\uc758 / \uc800\uba85 / ##\ud55c / \uc5f0\uad6c\uc790 / ##\uc774\ub2e4 / .\n\n4.2 Existing Language Models\nIn addition to our own language models, we evaluate the following two existing multilingual language models and two Korean monolingual language models on our benchmark:\n\u2022 mBERT [30] : A multilingual BERT introduced and released by Devlin et al. [30]. It is trained with the MLM and NSP objectives on a multilingual corpus covering 104 languages including Korean. \u2022 XLM-R [26] : A RoBERTa [82] trained on a large multilingual corpus by using the MLM objective.\n\u2022 KR-BERT [75] : An open-sourced character-level Korean language model based on BERT. We use the KR-BERT character WordPiece which uses a vocabulary of 16,424 unique tokens. \u2022 KoELECTRA [102] : An open-source Korean language model trained with the MLM and replaced token detection objectives, as was done by Clark et al. [22]. For training corpora, Park [102] uses own crawled news data and MODU corpus [98].\n5 Fine-tuning Language Models We use x i to refer to the i-th token of input and h i \u2208 R H to its corresponding final hidden state from a pretrained language models (PLM), where H is the hidden dimensionality. Following the conventional fine-tuning setup [30], we\n\n5.1.1 Single Sentence Classification\nIn the single sentence classification task, such as TC and RE, a classifier classifies a single sentence into a set of predefined labels. Following the convention, the last hidden state of [CLS] token h 0 is linearly mapped to the number of labels (K) with W \u2208 R K\u00d7H and the entire model is trained to minimize the cross entropy loss.\nYNAT is a single sentence classification task where K is 7 for predefined topic labels, and does not require any special treatment of each input. KLUE-RE on the other hand requires a special procedure to indicate entities within the input sentence. We use <subj>, </subj>, <obj>, and </obj> to mark the beginnings and the ends of subject and object entities, respectively, following Baldini Soares et al. [5]. We expand the embedding matrix to add these four extra tokens.\n\n5.1.2 Sentence Pair Classification / Regression\nIn the sentence pair classification / regression task, a model is asked to determine the relationship between two sentences. A pair of input sentences are concatenated with a special separator token, often [SEP], in-between.\nIn KLUE-STS, each sentence pair is annotated with a real-valued similarity [0, 5]. The model is thus trained to map from the final hidden state of [CLS] to a real number, by minimizing the mean squared error (MSE). In the case of KLUE-NLI, each sentence pair, consisting of a premise and hypothesis, is coupled with one of three classes. The model thus maps the hidden state of [CLS] token to a three-dimensional real-valued vectors and is trained to minimize the cross-entropy loss.\n\n5.1.3 Multiple-Sentence Slot-Value Prediction\nWoS is a slot-value prediction task for a given dialogue context, where the prediction should be considered across multiple turns instead of a single utterance. We employ an encoder-decoder model following the architecture of TRADE [140], which consists of an utterance encoder, a state generator, and a slot gate classifier (Figure 5.1.3). In our implementation, we change the utterance encoder from GRU [17] to PLM to get better representations. Thus, the state generator takes the final hidden state of [CLS] token h 0 as the first decoder hidden state. We also modify the slot gate classifier to predict additional two slot gate labels (yes, no), since WoS contains relatively more Boolean type slots than MultiWOZ [10]. We jointly minimize the cross-entropy loss of the state generator and slot gate classifier.\n\n5.1.4 Sequence Tagging\nKLUE-NER is a token-level tagging task, where each character is assigned a label. This requires a care in using tokenization, as the labels from the characters within each subword token must be aggregated, and the predicted label of each subword token must be properly distributed across the characters within it. See Figure 9 for an example. We linearly map each of the final hidden states from the encoder h \u2208 R |x|\u00d7H into a 12-dimensional real-valued vectors, corresponding to the 12 named-entity categories. We then minimize the cross-entropy loss summed over all the tokens. KLUE-MRC is a span prediction task in which a model tags the beginning and end tokens of the answer span within a passage, given a question. The input to the model is the concatenation of a tokenized passage and an associated question (separated by [SEP]). The final hidden state of each token in the passage is linearly projected to a 2-dimensional real-valued vector. The dimensions in this vector correspond to the logits of two binary classifiers, the start and end token classifiers. The special token [CLS] is considered as both the correct start and end tokens, when given question is unanswerable. We minimize the cross-entropy loss to train the model.\nWe frame KLUE-DP as a sequence tagging problem. Each token within an input sentence is tagged twice, once with its head token and the other with the type of the arc connecting the head and the current token. Our baseline architecture follows the model proposed by Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez [39], except word representation and attention mechanisms. Similarly to KLUE-NER, we must be careful in handling subword tokens, as the annotation is done at the word level. In our approach, we use a pretrained language model (to be fine-tuned) to extract subword representations and concatenate the first and last subword token representations of each word, to form word vector representations. Each of these word representations is optionally concatenated with the part-of-speech embedding. For the attention layers, we use biaffine attention [33] to predict HEAD, and bilinear attention [64] to predict arc type (DEPREL) for each word. Just like KLUE-NER and KLUE-MRC, we minimize the cross-entropy loss to fine-tune the entire model. See Figure 10 for a graphical illustration of the model architecture.\n\n5.2 Fine-Tuning Configurations\nFor all the experiments, we use Huggingface Transformers [139] and PyTorch-Lightning. 62 We use AdamW optimizer [83] with the learning rate selected from {10 \u22125 , 2 \u00d7 10 \u22125 , 3 \u00d7 10 \u22125 , 5 \u00d7 10 \u22125 }, the warm-up ratio from {0., 0.1, 0.2, 0.6} and the weight decay coefficient from {0.0, 0.01}. We choose the batch size from {8, 16, 32} and the number of epochs from {3, 4, 5, 10}. We use the maximum sequence length of 512 for KLUE-MRC and WoS, and 128 for all the other tasks. We report the score obtained from the best hyperparameter configuration based on the dev set performance.\n\n5.3 Evaluation Results\nIn this section, we present the evaluation results including our KLUE-PLMs and existing PLMs on the KLUE benchmark, in Table 32. 63 Different from other NLU benchmarks, we do not average the scores over tasks, since simple averaging of scores of different scales and interpretations could be highly misleading. Rather, we describe and discuss the result of each task separately. Within the Korean BASE models, KLUE-BERT BASE performs best for YNAT and WoS, KLUE-RoBERTa BASE for KLUE-RE and KLUE-MRC, and KoELECTRA BASE for KLUE-STS and KLUE-NLI.\nWe make two major observations. First, we see that KLUE-RoBERTa LARGE , which is the largest model among the baseline PLM's we tested, outperforms all the other models across all the tasks except for KLUE-NER. This observation agrees well with the recent trend which has demonstrated the correlation between the model size and task performance [58, 9]. This indicates that KLUE will be useful for the future investigation into how much gain we can expect by simply increasing the model size further. The second observation is that the monolingual models, which were specifically designed for and trained with a more carefully curated corpus in the target language (Korean), generally outperform the multilingual counterparts, especially when we compare models of similar sizes. We make this observation again across all the tasks, except for KLUE-NER, where the XLM-R LARGE performs similarly to the best performer, KoELECTRA BASE , in terms of character-level F1 score. This observation re-iterates the importance of investing effort in understanding a target language and customizing data, models and learning algorithms for the target language.\n\n5.4 Analysis of Models\nThere were two major decisions we made in preparing the pretraining corpus and preprocessing data. They were 1) whether to pseudonymize PIIs and 2) the tokenzation strategy. In this section, we analyze the impact of our choices, using KLUE-RoBERTa BASE by training it on the MODU corpus only.\nCorpus Pseudonymization It can be expected that noise introduced in the process of pseudonymization may have detrimental effect on the downstream task performance. Our finding, presented in Table 33, however shows that there is some drop in a subset of the tasks, but such drop is quite minimal. This suggests that the minimal level of pseudonymization, just like what we have done, is already a good way to balance the task performance and the risk of leaking private information. Tokenization Strategy We contrast our tokenization scheme, morpheme-based subword tokenization, against the standard byte pair encoding (BPE). First, we investigate the difference in how words are segmented into subword tokens. Following Rust et al. [119], we consider subword fertility, proportion of continued words, and UNK ratio. On the subword fertility, which measures the average number of subwords produced per word, the proposed tokenization scheme ends up slightly higher than BPE does. However, when we look at the proportion of continued words, which measures the number of words that were split into at least two subwords, we observe the opposite trend. This implies that our algorithm maintains the original words as much as it can, and only when it is necessary, it splits each word into potentially more subword pieces. The efficacy of the proposed scheme over BPE is evident from the UNK ratio, as it produces fewer UNK tokens compared to BPE when the vocabulary size was controlled to be 32k for both methods. See Table 35.\nTable 35 : Overview of tokenization metrics. We build each vocabs using MODU corpus and compare them on WIKIPEDIA corpus. We find these qualitative differences between two schemes lead to significant differences in the task performance in the cases of KLUE-NER, KLUE-MRC and WoS. These tasks often involve tagging, detection and even generation at the morpheme level, and we suspect that morphologically consistent tokenization facilitates better prediction overall. On the other hand, the difference in the tokenization strategy does not manifest itself in the performance of classification or word-level tagging, likely as a corresponding NLU system can more readily overcome inconsistencies in subword segmentation when merging subword token representations into that of a larger unit. Overall, we recommend future researchers use the proposed tokenization strategy as a default option.\n\n6 Ethical Considerations\nIn building KLUE and accompanying baseline models, we have incorporated various mechanisms to avoid any harmful and negative consequences from releasing both data and models. These mechanisms are described in detail wherever they were introduced and used, but in this section, we summarize these mechanisms, considerations and our principles behind them.\n\n6.1 Copyright and Accessibility\nMost NLP datasets are built upon the existing text sources. This raises a question on the terms of using such datasets, especially when the underlying source datasets are not well-specified nor carefully investigated. In order to avoid any such doubt on the terms of using KLUE and to accelerate NLP research in Korean, we fully adhere to the copyright act of Korea, which went effective on Dec. 8, 2020. 64 and include only text for which we know we can release under a license that permits both redistribution and re-mix without any restriction on the use.\nSource Corpora Our goal is to secure and maximize the continued availability and usefulness of the benchmark. In other words, we must guarantee the possibility of derive new work and redistribute it freely, which comes together with CC BY-SA. To release KLUE under CC BY-SA, we have built a source corpus set by including only text that is either 1) not protected by copyright or 2) under CC0, CC BY, CC BY-SA or KOGL Type 1 license. In the case of news articles, which are copyrighted, we have signed contracts with the providers, Korean Economics Daily (KED) news media and Acrofan, that allow us to make KLUE-MRC and release them under CC BY-SA.\nTask-Specific (Annotated) Datasets We subsample and annotate the source corpus for each KLUE benchmark task. We release each under CC BY-SA. This allows users of KLUE benchamrk to copy, redistribute, remix, transform and build upon it for both commercial and non-commercial purposes, as long as derivatives are distributed under the same license (CC BY-SA). We expect this to greatly facilitate future NLP research and development.\nPretraining Corpora and Language Models As was discussed earlier 4.1, we cannot guarantee that our pretraining corpus, built using MODU, CC-100-Kor and NEWSCRAWL, does not contain any copyrighted work, although these are all created from publicly available text. Unfortunately without these corpora, it is not possible to find a sufficiently large resource to train large-scale language models for Korean. We thus use them for pretraining but do not publicly release the pretraining corpora in order to avoid any issues in the future, which is in contrast to KLUE. Instead, we openly release pretrained language models to facilitate future research. As the parameters of a language model does not\n[express] human thoughts and emotions, they do not meet the requirement of being copyrighted.\n\n6.2 Toxic Content\nAlthough large-scale, accessible benchmark datasets advance machine learning and its applications to adjacent fields, such as natural language processing, toxic and unwanted contents within these datasets may be amplified via large-scale models we train on them. We have been aware of this issue from the beginning of the project, and here we describe how we have addressed these toxic contents in KLUE.\nTask-Specific Datasets For each task-specific dataset, we apply three stages to minimize the introduction of toxic contents. First, we automatically detect hate speech and gender-biased sentences using toxicity classifiers and remove those even before sending these sentences for annotation (see Section 2.3). Second, we explicitly and clearly instruct annotators to mark any instance that exhibits social biases and/or is toxic (see Section 3), after providing them with clear definitions of bias and hate speech. Finally, we manually examine these marked sentences and exclude them from the final dataset. This three-stage process may not catch all possible such instances, and we plan to use an online forum 65 to receive feedback and complaints from users of KLUE.\nPretrained Language Models We use our pretraining corpora (see Section 4.1) as they are, for three reasons. First, manual inspection is simply not tractable due to the sheer scale. Second, it is challenging to build an automated tool to detect hate speech and biased sentences. This issue is made even more severe for Korean, because there is only one known hate speech dataset of limited size [92]. Lastly, we envision the future in which these pretrained language models are used to build better tools for automatically detecting various toxic contents as well as undesirable social biases. In order for such pretrained models to be aware of these issues, they must have been trained with such toxic contents as well.\n\n6.3 Personally Identifiable Information\nIt has recently been discovered by that a large-scale, pretrained language model memorizes a large amount of personally identifiable information (PII) and that an algorithm can be designed to retrieved those private information. We thus design two different approaches for pseudonymizing task-specific datasets and pretraining corpora, respectively.\nTask-Specific Datasets In the case of task-specific datasets, we rely on manual inspection during annotation to detect PII. We discard any sentences that was reported to contain PII after manual inspection. In the case of DST, which relies on simulated dialogues, we pseudonymize the database entries rather than actual text, using the faker library. 66 retraining Corpora There is a trade-off between removing PII and the performance of a pretrained language model, as we will demonstrate later in this paper. We thus pseudonymize 16 PII types that are detectable purely by regular expressions. See Section 4.1\n\n7 Related Work\nGeneral-Purpose NLU Benchmarks General Language Understanding Evaluation (GLUE) [133] benchmark, a collection of evaluation dataset for English, was the first general-purpose evaluation benchmarks for NLU. It is generalpurpose in that it is not limited to a single task. It consists of 11 downstream tasks, including tasks that measures the capability of capturing semantic textual similarity (QQP, MRPC, STS) [3, 13], measures the capability of inference (MNLI, QNLI, RTE, WNLI) [8, 138] and that evaluates the capability of classifying a single sentence into a predefined set of categories (CoLA, SST) [134, 126]. GLUE exclusively focused on English, and its variants in different languages have been built and released over the past couple of years, including CLUE in Chinese [142], a French version [72], an Indonesian version [137], a version for Indic languages [57], Russian SuperGLUE [125], and Persian GLUE [61]. In all these cases, substantial efforts were carried out to follow the philosophy of the original GLUE, covering a broad spectrum of domains and tasks, while incorporating language-specific characteristics. On the other hand, there have been efforts to build a multilingual version of such benchmark, largely relying on automated methods, such as XGLUE [78] and XTREME [54]. Korean as a language has been included in subsets of these latter benchmarks, but there has not been a serious attempt at building a general-purpose language understanding evaluation suite for Korean, until this paper.\nAbsence of a Standard NLU Benchmark in Korean Until this paper, a number of task-specific benchmarks in Korean have been proposed and released. For example, NSMC is used for sentiment classification, PAWS-X [144] for paraphrase detection, KorNLI and KorSTS [45] for NLI and STS, KorQuAD 1 and 2 [79, 63] for MRC, and BEEP! [92] for hate speech detection. At this point, one may wonder whether it would have been easier and more convenient to simply aggregate these datasets to build KLUE. After all, this has been a popular strategy for constructing monolingual [133] as well as multilingual [78, 54] benchmarks. Unfortunately this approach comes with two major issues that we directly address in this paper.\nFirst, the existing datasets are constructed individually without considering other datasets and their properties. In other words, the aggregate of these individual datasets is unlikely to cover a broad spectrum of domains and writing styles, unlike KLUE for which we carefully curate the source corpora as well as subsets for downstream tasks to have broad coverage of domains and styles. This goes beyond domains and styles, but also the coverage of linguistic phenomena under evaluation. Most of the existing benchmarks, listed above, focus on semantics rather than syntax, and it is difficult to find any widely-available benchmark that captures pragmatics. We address this issue by carefully selecting a set of downstream tasks.\nSecond, these existing datasets are not always publicly available, 67 and some are distributed with a highly restrictive license that prohibits redistribution nor the transformation of the original. These are often the ones published and released by government-affiliated institutes. In some cases, it is necessary to obtain a special permission to access datasets, which is often not easily accessible by non-Korean researchers. We address all these issues with KLUE by releaseing the entire benchmark data under CC BY-SA, both by careful curation of source corpora and by direct agreements with publishers.\nPretrained Language Models (PLMs) The recent trend of large-scale pretrained language models was sparked by the success of earlier models, such as ELMo [104], GPT-2 [110] and BERT [30], on the GLUE and other similar NLU benchmarks. This earlier success has led to a series of advances in large-scale language models, including XLNet [146], ALBERT [71], RoBERTa [82], ELECTRA [22], and Deberta [49], again largely driven by the availability of standardized benchmarks. This advance in language models, not only in terms of the model size but also in learning algorithms, in turn also sparked the interest in building and improving existing language understanding benchmarks. Some of the recently released, challenging benchmarks include SuperGLUE [132] and KILT [105]. The availability of such a standard language understanding benchmark, such as KLUE from this paper, is expected to start such a virtuous cycle for Korean language understanding.\nPretrained Language Models for Korean Inspired by the development in other languages and multilingual models, PLM's for the Korean language have been trained and released by multiple research groups and individuals. SKT released KoBERT, 68 followed by KorBERT 69 from ETRI, HanBERT 70 from TwoBlock AI, KR-BERT [75] from Seoul National University. There are a few pretrained models released by individual researchers, such as KoELECTRA [102] and KcBERT [73].\nUnfortunately, it is unclear how we should compare this stream of pretrained language models in Korean, due to the lack of a standarded benchmark in Korean. Subsets of these models have been compared based on subsets of a few downtream NLP tasks in Korean above, but because these are not standardized, it is not easy to draw solid conclusions from these limited experiments. We expect the proposed KLUE benchmark will serve as a standard way to track the progress of research in language models for Korean.\n\n8 Discussion\nOpen Access We distribute KLUE under CC BY-SA. The license allows everyone to freely copy and redistribute our benchmarks in any medium or format. In addition, one can improve our benchmark to build more challenging datasets after performance saturation. To function as a NLU benchmark, open access is a must. If the original author does not allow derivative development of the benchmark, other researchers cannot improve it, for example by removing toxic content, or building a more challenging dataset to accelerate research for technical improvements. If commercial use is not allowed, researchers working at for-profit organizations would not be able to benefit from nor to (easily) contribute to the benchmark. Redistribution is another crucial factor because it significantly limits research if, for example, sharing the datasets with another researcher is prohibited. Another existing practice that limits research is transferring the responsibility of copyright infringement of related conflicts to researchers. To set a good precedent for open access of data, we allow using our datasets for 1) any purpose, 2) derivative work, and 3) redistribution, as long as the existing copyrights in our benchmark datasets are respected. We also open our pretrained Korean language models and the implementation of pretraining and fine-tuning pipelines. This enhances reproducibility of our work, and allows anyone to fix and improve our data and models. We hope to contribute to the Korean NLP research community as well the wider NLP community.\nFacilitating Korean NLP Research We developed KLUE with the aim of facilitating Korean NLP research, in response to the recent active development efforts of large Korean language models. The entire NLP community has seen BERT [30] and its variants outperforming the previous NLU models for GLUE [133] and SuperGLUE [132], as well as the more recent GPT3 [9] with outstanding performance without fine-tuning (and with in-context learning) in natural language understanding and generation. Motivated by these models, many Korean researchers at various institutions rushed to pretrain large-scale Transformer-based Korean language models. Consequently, a number of nearly identical pretrained language models have been released to open-source communities. However, we could not systematically understand the behaviors and characteristics of these models because of the lack of well-designed general-purpose benchmarks like GLUE for Korean. KLUE will allow us to conduct controlled experiments to understand how and why various Korean LMs perform on certain tasks and thus obtain detailed insights into those models. Furthermore, since KLUE includes many representative NLU tasks that are also conducted in other languages, KLUE will function as a fundamental resource to NLP researchers who aim to conduct multilingual research with Korean and other languages.\nMeasuring Overall Performance of NLU models We do not average all scores gained from each task in KLUE. The performance of all tasks are measured by different evaluation metrics. This is because we carefully choose the metric for each task with considering its own characteristics.\n\nFootnotes:\n1: https://www.kisa.or.kr/public/laws/laws2_View.jsp?cPage=1&mode=view&p_No=282&b_No=282&d_No=3\n2: https://www.law.go.kr/LSW//lsInfoP.do?lsiSeq=213857&chrClsCd=010203&urlMode=engLsInfoR&viewCls=engLsInfoR#0000\n3: See the precedent set by the Supreme Court in Korea: \ub300\ubc95\uc6d0 2011. 9. 2. \uc120\uace0 2008\ub2e442430 \uc804\uc6d0\ud569\uc758\uccb4 \ud310\uacb0 available at https://glaw.scourt.go.kr/wsjo/panre/sjo100.do?contId=2060159&q=2008%EB%8B%A442430.\n4: https://creativecommons.org/licenses/by-sa/4.0/\n5: https://creativecommons.org/publicdomain/zero/1.0/\n6: https://creativecommons.org/licenses/by/4.0/\n7: https://creativecommons.org/licenses/by-sa/4.0/\n8: https://www.kogl.or.kr/info/license.do#05-tab\n9: See https://www.law.go.kr/%EB%B2%95%EB%A0%B9/%EC%A0%80%EC%9E%91%EA%B6%8C%EB%B2%95 for the copyright act which went effective as of Dec 8 2020.\n10: Although Wikitree was found to include some contents that could be considered unethical, socially biased and/or of low quality in general, we include it, as Wikitree is the largest source of license-free news articles. We address these problematic contents via annotation.\n11: http://insideairbnb.com/get-the-data.html\n12: https://movie.naver.com/movie/point/af/list.nhn\n13: See the precedent set by the Supreme Court in Korea: \ub300\ubc95\uc6d0 2011. 9. 2. \uc120\uace0 2008\ub2e442430 \uc804\uc6d0\ud569\uc758\uccb4 \ud310\uacb0 available at https://glaw.scourt.go.kr/wsjo/panre/sjo100.do?contId=2060159&q=2008%EB%8B%A442430.\n14: https://github.com/hyunwoongko/kss\n15: https://huggingface.co/monologg/koelectra-base-v3-gender-bias\n16: https://huggingface.co/monologg/koelectra-base-v3-hate-speech\n17: https://news.naver.com/\n18: https://selectstar.ai/\n19: More information available in http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html\n20: https://papago.naver.com/\n21: This might be replaced to any other similarity measures.\n22: https://selectstar.ai/\n23: https://selectstar.ai/\n24: https://github.com/kmounlp/NER\n25: https://committee.tta.or.kr/data/standard_view.jsp?nowPage=2&pk_num=TTAK.KO-10.0852&commit_ code=PG606\n26: Daum: http://search.daum.net/search?nil_suggest=btn&nil_ch=&rtupcoll=&w=tot&m=&f=&lpp=&q=%C0%CE% B9%B0%B0%CB%BB%F6 / Naver: https://people.search.naver.com/\n27: https://deepnatural.ai/\n28: KMOU utilizes a modified guideline KMOU-NLP-2018-001 based on the TTA scheme, which is available in https://github. com/kmounlp/NER/blob/master/NER%20Guideline%20(ver%201.0).pdf\n29: https://aihub.or.kr/aidata/84\n30: https://namu.wiki\n31: https://github.com/hyunwoongko/kss\n32: https://github.com/monologg/KoELECTRA\n33: https://github.com/kmounlp/NER\n34: Released by NIA, a government-funded institution. Available at https://aihub.or.kr/aidata/84.\n35: https://deepnatural.ai/\n36: https://aiopen.etri.re.kr/data/003.%EC%9D%98%EC%A1%B4%EA%B5%AC%EB%AC%B8%EB%B6%84%EC%84%9D_%EA% B0%80%EC%9D%B4%EB%93%9C%EB%9D%BC%EC%9D%B8.pdf\n37: https://aiopen.etri.re.kr/data/001.%ED%98%95%ED%83%9C%EC%86%8C%EB%B6%84%EC%84%9D_%EA%B0%80%EC% 9D%B4%EB%93%9C%EB%9D%BC%EC%9D%B8.pdf\n38: https://deepnatural.ai/\n39: https://corpus.korean.go.kr/main.do\n40: https://universaldependencies.org\n41: https://www.tagtog.net/\n42: https://selectstar.ai/\n43: We fine-tune pretrained RoBERTa-base model with following hyperparameters: epochs 5, batch size 16, learning rate 3e-5, lr warmup ratio 0.0.\n44: Twitter tagger of KoNLPy [101].\n45: https://www.imdb.com/\n46: https://translate.google.co.kr/\n47: We adopt the evaluation script of https://github.com/jasonwu0731/trade-dst\n48: A user goal is what the worker playing user should follow as shown in Table24.\n49: https://faker.readthedocs.io\n50: Dontcare means a user has no preference and None means a user is yet to specify a valid value for given slot[50,\n51: ].\n51: A transliteration of a Korean word '\ubaa8\ub450' which means 'Everyone'.\n52: https://corpus.korean.go.kr/\n53: http://data.statmt.org/cc-100/\n54: http://dump.thewiki.kr\n55: https://www1.president.go.kr/petitions\n56: https://ko-nlp.github.io/Korpora/en-docs/corpuslist/korean_petitions.html\n57: https://github.com/likejazz/korean-sentence-splitter\n62: https://github.com/PyTorchLightning/pytorch-lightning\n63: See Appendix A for the corresponding table however computed on the development set.\n64: https://www.law.go.kr/%EB%B2%95%EB%A0%B9/%EC%A0%80%EC%9E%91%EA%B6%8C%EB%B2%95\n65: https://github.com/KLUE-benchmark/KLUE/issues\n66: https://github.com/joke2k/faker\n67: Some of these are publicly available in Korea but not internationally.\n68: https://github.com/SKTBrain/KoBERT\n69: https://aiopen.etri.re.kr/service_dataset.php\n70: https://github.com/tbai2019/HanBert-54k-N\n\nReferences:\n\n- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval- 2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252-263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL https://www.aclweb.org/ anthology/S15-2045.- Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 497-511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https://www.aclweb.org/anthology/S16-1081.\n\n- Jens Allwood. An activity based approach to pragmatics. In Harry Bunt and William Black, editors, Abduction, belief and context in dialogue: Studies in computational pragmatics, chapter 2, pages 47-80. John Benjamins, Amsterdam, Netherlands, 2000.\n\n- Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks: Dis- tributional similarity for relation learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895-2905, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1279. URL https://www.aclweb.org/anthology/P19-1279.\n\n- Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6: 587-604, 2018. doi: 10.1162/tacl_a_00041. URL https://www.aclweb.org/anthology/Q18-1041.\n\n- Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language understanding? arXiv preprint arXiv:2104.02145, 2021.\n\n- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://www.aclweb.org/anthology/D15-1075.\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\n- Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u0107. MultiWOZ -a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016-5026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1547. URL https://www.aclweb.org/anthology/D18-1547.\n\n- Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duck- worth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. Taskmaster-1: Toward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4516-4525, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1459. URL https://www.aclweb.org/anthology/D19-1459.\n\n- Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.\n\n- Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/ S17-2001.\n\n- Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent advances and new frontiers. SIGKDD Explor. Newsl., 19(2):25-35, November 2017. ISSN 1931-0145. doi: 10.1145/3166054. 3166058. URL https://doi.org/10.1145/3166054.3166058.\n\n- Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. FairFil: Contrastive neural debiasing method for pretrained text encoders. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=N6JECD-PI5w.\n\n- Nancy A. Chinchor. Overview of MUC-7. In Seventh Message Understanding Conference (MUC-7): Proceedings of a Conference Held in Fairfax, Virginia, April 29 -May 1, 1998, 1998. URL https://www.aclweb.org/ anthology/M98-1001.\n\n- Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179.\n\n- Won Ik Cho, Jong In Kim, Young Ki Moon, and Nam Soo Kim. Discourse component to sentence (DC2S): An efficient human-aided construction of paraphrase and sentence similarity dataset. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 6819-6826, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/ 2020.lrec-1.842.\n\n- Won Ik Cho, Sangwhan Moon, and Youngsook Song. Open Korean corpora: A practical report. In Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS), pages 85-93, Online, November 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.nlposs-1.12.\n\n- Jayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Universal Dependency treebanks in Korean. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https: //www.aclweb.org/anthology/L18-1347.\n\n- Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://www.aclweb.org/anthology/ N19-1300.\n\n- Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2020. URL https://openreview.net/pdf?id=r1xMH1BtvB.\n\n- Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL https://www.aclweb. org/anthology/D17-1070.\n\n- Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://www.aclweb.org/anthology/D18-1269.\n\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.747. URL https://www.aclweb.org/anthology/2020.acl-main.747.\n\n- Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Joaquin Qui\u00f1onero-Candela, Ido Dagan, Bernardo Magnini, and Florence d'Alch\u00e9 Buc, editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.\n\n- Marie-Catherine de Marneffe and Christopher D. Manning. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1-8, Manchester, UK, August 2008. Coling 2008 Organizing Committee. URL https://www.aclweb.org/ anthology/W08-1301.\n\n- Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06), Genoa, Italy, May 2006. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\n\n- George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. The automatic content extraction (ACE) program -tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04), Lisbon, Portu- gal, May 2004. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/ proceedings/lrec2004/pdf/5.pdf.\n\n- William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www. aclweb.org/anthology/I05-5002.\n\n- Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id= Hk95PK9le.\n\n- David M. Eberhard and Charles D. Simons, Gary F. Fenning. Ethnologue: Languages of the World. SIL International, Dallas, Texas, 24 edition, 2021. URL http://www.ethnologue.com.\n\n- Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 207-219, Saarbr\u00fccken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5526. URL https://www.aclweb.org/ anthology/W17-5526.\n\n- Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 37-49, Saarbr\u00fccken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/ v1/W17-5506. URL https://www.aclweb.org/anthology/W17-5506.\n\n- Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 422-428, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-1.53.\n\n- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anthology/P19-1346.\n\n- Daniel Fern\u00e1ndez-Gonz\u00e1lez and Carlos G\u00f3mez-Rodr\u00edguez. Left-to-right dependency parsing with pointer networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 710-716, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1076. URL https://www.aclweb.org/anthology/N19-1076.\n\n- Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 363-370, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219885. URL https://www.aclweb.org/anthology/ P05-1045.\n\n- Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.\n\n- Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650-655, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2103. URL https://www.aclweb.org/anthology/P18-2103.\n\n- Ralph Grishman and Beth Sundheim. Message Understanding Conference-6: A brief history. In COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics, 1996. URL https://www.aclweb. org/anthology/C96-1079.\n\n- Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2017. URL https://www.aclweb.org/anthology/N18-2017.\n\n- Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, and Hyungjoon Soh. KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding. In Findings of the Association for Compu- tational Linguistics: EMNLP 2020, pages 422-430, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.39. URL https://www.aclweb.org/anthology/2020. findings-emnlp.39.\n\n- Ji Yoon Han, Tae Hwan Oh, Lee Jin, and Hansaem Kim. Annotation issues in Universal Dependencies for Korean and Japanese. In Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020), pages 99-108, Barcelona, Spain (Online), December 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.udw-1.12.\n\n- Na-Rae Han, Shijong Ryu, Sook-Hee Chae, Seung-yun Yang, Seunghun Lee, and Martha Palmer. Korean treebank annotations version 2.0. Linguistic Data Consortium (LDC), Philadelphia, 2006.\n\n- Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. FewRel: A large- scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803-4809, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1514. URL https://www.aclweb.org/anthology/D18-1514.\n\n- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=XPZIaotutsD.\n\n- Matthew Henderson, Blaise Thomson, and Jason D. Williams. The second dialog state tracking challenge. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263-272, Philadelphia, PA, U.S.A., June 2014. Association for Computational Linguistics. doi: 10.3115/ v1/W14-4337. URL https://www.aclweb.org/anthology/W14-4337.\n\n- Matthew Henderson, Blaise Thomson, and Jason D Williams. The third dialog state tracking challenge. In 2014 IEEE Spoken Language Technology Workshop (SLT), pages 324-329. IEEE, 2014.\n\n- Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid \u00d3 S\u00e9aghdha, Sebastian Pad\u00f3, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33-38, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/S10-1006.\n\n- Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra K\u00fcbler, and Lawrence Moss. OCNLI: Original Chinese Natural Language Inference. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3512-3526, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.314. URL https://www.aclweb.org/anthology/2020.findings-emnlp.314.\n\n- Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411-4421. PMLR, 13-18 Jul 2020. URL http: //proceedings.mlr.press/v119/hu20b.html.\n\n- Ali Jabbari, Olivier Sauvage, Hamada Zeine, and Hamza Chergui. A French corpus and annotation schema for named entity recognition and relation extraction of financial news. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2293-2299, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-1.279.\n\n- Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/ anthology/P17-1147.\n\n- Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948-4961, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.findings-emnlp.445. URL https://www.aclweb.org/anthology/2020.findings-emnlp.445.\n\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\n- J. F. Kelley. An iterative design methodology for user-friendly natural language office information applications. ACM Trans. Inf. Syst., 2(1):26-41, January 1984. ISSN 1046-8188. doi: 10.1145/357417.357420. URL https://doi.org/10.1145/357417.357420.\n\n- Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https://www.aclweb.org/anthology/ N18-1023.\n\n- Daniel Khashabi, Arman Cohan, Siamak Shakeri, Pedram Hosseini, Pouya Pezeshkpour, Malihe Alikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze Brahman, Sarik Ghazarian, et al. ParsiNLU: a suite of language understanding challenges for persian. arXiv preprint arXiv:2012.06154, 2020.\n\n- Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 2611-2624. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf.\n\n- Youngmin Kim, Seungyoung Lim, Hyunjeong Lee, Soyoon Park, and Myungji Kim. KorQuAD 2.0: Korean QA dataset for web document machine comprehension. Journal of KIISE, 47:577-586, 2020. ISSN 2383-630X. URL https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART99691770&dbt=NART.\n\n- Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirectional LSTM feature representations. Transactions of the Association for Computational Linguistics, 4:313-327, 2016. doi: 10.1162/tacl_a_00101. URL https://www.aclweb.org/anthology/Q16-1023.\n\n- Dan Klein and Christopher D. Manning. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423-430, Sapporo, Japan, July 2003. Association for Computational Linguistics. doi: 10.3115/1075096.1075150. URL https://www.aclweb.org/anthology/ P03-1054.\n\n- Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first sentence: Position bias in question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1109-1121, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.84. URL https://www.aclweb.org/anthology/2020.emnlp-main.84.\n\n- K. Krippendorff. Computing Krippendorff's alpha-reliability. 2011.\n\n- Taku Kudo. MeCab: Yet another part-of-speech and morphological analyzer, 2006. URL https://taku910. github.io/mecab/.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, March 2019. doi: 10.1162/tacl_a_00276. URL https://www.aclweb.org/anthology/Q19-1026.\n\n- John P. Lalor and Hong Yu. Dynamic data selection for curriculum learning via ability estimation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 545-555, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.48. URL https://www. aclweb.org/anthology/2020.findings-emnlp.48.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.\n\n- Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabb\u00e9, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model pre-training for French. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2479-2490, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-1.302.\n\n- Junbum Lee. KcBERT: Korean comments BERT. In Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology, pages 437-440, 2020.\n\n- Kyungjae Lee, Kyoungho Yoon, Sunghyun Park, and Seung-won Hwang. Semi-supervised training data generation for multilingual question answering. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://www.aclweb.org/anthology/L18-1437.\n\n- Sangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, and Hyopil Shin. KR-BERT: A small-scale Korean-specific language model. arXiv preprint arXiv:2008.03979, 2020.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703.\n\n- Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen Rajani, Xifeng Yan, Yingbo Zhou, and Caiming Xiong. CoCo: Controllable counterfactuals for evaluating dialogue state trackers. arXiv preprint arXiv:2010.12850, 2020.\n\n- Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008-6018, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.484. URL https://www.aclweb.org/anthology/2020.emnlp-main.484.\n\n- Seungyoung Lim, Myungji Kim, and Jooyoul Lee. KorQuAD1.0: Korean QA dataset for machine reading comprehension. arXiv preprint arXiv:1909.07005, 2019.\n\n- Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/W04-1013.\n\n- Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. On-the-fly controlled text generation with experts and anti-experts, 2021.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\n- Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993. URL https://www.aclweb. org/anthology/J93-2004.\n\n- Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n\n- Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T\u00e4ckstr\u00f6m, Claudia Bedini, N\u00faria Bertomeu Castell\u00f3, and Jungmee Lee. Universal Dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92-97, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P13-2017.\n\n- Paul McNamee and Hoa Trang Dang. Overview of the TAC 2009 knowledge base population track. In Text Analysis Conference (TAC), volume 17, pages 111-113, 2009.\n\n- Shikib Mehri, Mihail Eric, and Dilek Hakkani-Tur. DialoGLUE: A natural language understanding benchmark for task-oriented dialogue. arXiv preprint arXiv:2009.13570, 2020.\n\n- Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. Syntactic data augmentation increases robustness to inference heuristics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2339-2352, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.212. URL https://www.aclweb.org/anthology/2020.acl-main.212.\n\n- Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compo- sitional questions do not necessitate multi-hop reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4249-4257, Florence, Italy, July 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/P19-1416. URL https://www.aclweb.org/anthology/P19-1416.\n\n- Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. Distant supervision for relation extraction without la- beled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003-1011, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P09-1113.\n\n- Jihyung Moon, Won Ik Cho, and Junbum Lee. BEEP! Korean corpus of online news comments for toxic speech detection. In Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 25-31, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.socialnlp-1.4. URL https://www.aclweb.org/anthology/2020.socialnlp-1.4.\n\n- Sangha Nam, Minho Lee, Donghwan Kim, Kijong Han, Kuntae Kim, Sooji Yoon, Eun-kyung Kim, and Key-Sun Choi. Effective crowdsourcing of multiple tasks for comprehensive knowledge extraction. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 212-219, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/ 2020.lrec-1.27.\n\n- Nikita Nangia and Samuel R. Bowman. Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4566-4575, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1449. URL https://www.aclweb.org/anthology/P19-1449.\n\n- Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953-1967, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154. URL https://www.aclweb.org/ anthology/2020.emnlp-main.154.\n\n- Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated MAchine reading COmprehension dataset. November 2016. URL https://www.microsoft.com/en-us/research/publication/ ms-marco-human-generated-machine-reading-comprehension-dataset/.\n\n- Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Haji\u010d, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 1659-1666, Portoro\u017e, Slovenia, May 2016. European Language Resources Association (ELRA). URL https://www.aclweb.org/anthology/L16-1262.\n\n- National Institute of Korean Languages. NIKL CORPORA 2020 (v.1.0), 2020. URL https://corpus.korean. go.kr.\n\n- Tae Hwan Oh, Ji Yoon Han, Hyonsu Choe, Seokwon Park, Han He, Jinho D. Choi, Na-Rae Han, Jena D. Hwang, and Hansaem Kim. Analysis of the Penn Korean Universal Dependency treebank (PKT-UD): Manual revision to build robust parsing model in Korean. In Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies, pages 122-131, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.iwpt-1.13. URL https://www.aclweb.org/anthology/2020.iwpt-1.13.\n\n- Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL https://www.aclweb.org/anthology/ P17-1178.\n\n- Eunjeong L. Park and Sungzoon Cho. KoNLPy: Korean natural language processing in Python. In Proceedings of the 26th Annual Conference on Human & Cognitive Language Technology, Chuncheon, Korea, October 2014.\n\n- Jangwon Park. KoELECTRA: Pretrained ELECTRA model for Korean. https://github.com/monologg/ KoELECTRA, 2020.\n\n- Kyubyong Park, Joohong Lee, Seongbo Jang, and Dawoon Jung. An empirical study of tokenization strategies for various Korean NLP tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 133-142, Suzhou, China, December 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.aacl-main.17.\n\n- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle- moyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://www.aclweb.org/anthology/N18-1202.\n\n- Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. KILT: a benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252, 2020.\n\n- Slav Petrov, Dipanjan Das, and Ryan McDonald. A universal part-of-speech tagset. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 2089-2096, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/ proceedings/lrec2012/pdf/274_Paper.pdf.\n\n- Jason Phang, Thibault F\u00e9vry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\n\n- Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL https://www.aclweb.org/anthology/S18-2023.\n\n- Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, and Deyi Xiong. RiSAWOZ: A large-scale multi-domain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 930-940, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.67. URL https://www.aclweb.org/anthology/2020.emnlp-main.67.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/anthology/D16-1264.\n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb.org/anthology/P18-2124.\n\n- Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8689-8696, Apr. 2020. doi: 10.1609/aaai.v34i05.6394. URL https://ojs. aaai.org/index.php/AAAI/article/view/6394.\n\n- Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://www.aclweb.org/anthology/D19-1410.\n\n- Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Jos\u00e9 Luis Balc\u00e1zar, Francesco Bonchi, Aristides Gionis, and Mich\u00e8le Sebag, editors, Machine Learning and Knowledge Discovery in Databases, pages 148-163, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-15939-8.\n\n- Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524-1534, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/D11-1141.\n\n- Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. Nist Special Publication Sp, 109:109, 1995.\n\n- Phillip Rust, Jonas Pfeiffer, Ivan Vuli\u0107, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. arXiv preprint arXiv:2012.15613, 2020.\n\n- Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683-1693, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1156. URL https: //www.aclweb.org/anthology/P18-1156.\n\n- Martin Schiersch, Veselina Mironova, Maximilian Schmitt, Philippe Thomas, Aleksandra Gabryszak, and Leonhard Hennig. A German corpus for fine-grained named entity recognition and relation extraction of traffic and industry events. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https://www.aclweb.org/anthology/L18-1703.\n\n- Priyanka Sen and Amir Saffari. What do models learn from question answering datasets? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2429-2438, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.190. URL https://www.aclweb.org/anthology/2020.emnlp-main.190.\n\n- Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.org/anthology/P16-1162.\n\n- Pararth Shah, Dilek Hakkani-T\u00fcr, Gokhan T\u00fcr, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and Larry Heck. Building a conversational agent overnight with dialogue self-play. arXiv preprint arXiv:1801.04871, 2018.\n\n- Tatiana Shavrina, Alena Fenogenova, Emelyanov Anton, Denis Shevelev, Ekaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria Tikhonova, Andrey Chertok, and Andrey Evlampiev. RussianSuperGLUE: A Russian language understanding evaluation benchmark. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4717-4726, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.381. URL https://www.aclweb.org/ anthology/2020.emnlp-main.381.\n\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/D13-1170.\n\n- Benjamin Strauss, Bethany Toma, Alan Ritter, Marie-Catherine de Marneffe, and Wei Xu. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), pages 138-144, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL https://www.aclweb.org/anthology/W16-3919.\n\n- Key sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. KAIST tree bank project for Korean: Present and future development. In In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 7-14, 1994.\n\n- Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Language- independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142-147, 2003. URL https://www.aclweb.org/anthology/W03-0419.\n\n- Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191-200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL <https://www.aclweb.org/anthology/W17-2623>.\n\n- Clara Vania, Ruijie Chen, and Samuel R. Bowman. Asking Crowdworkers to Write Entailment Examples: The Best of Bad options. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 672-686, Suzhou, China, December 2020. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/2020.aacl-main.68.\n\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings. neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.\n\n- Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641, March 2019. doi: 10.1162/tacl_a_00290. URL https://www.aclweb.org/anthology/Q19-1040.\n\n- Tsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 438-449, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/E17-1042.\n\n- Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb. org/anthology/2020.lrec-1.494.\n\n- Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, and Ayu Purwarianti. IndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 843-857, Suzhou, China, December 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.aacl-main.85.\n\n- Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence under- standing through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.org/anthology/N18-1101.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\n\n- Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808-819, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1078. URL https://www.aclweb.org/anthology/ P19-1078.\n\n- Jingjing Xu, Ji Wen, Xu Sun, and Qi Su. A discourse-level named entity recognition and relation extraction dataset for Chinese literature text. arXiv preprint arXiv:1711.07010, 2017.\n\n- Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 4762-4772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.419. URL https://www.aclweb.org/anthology/2020.coling-main.419.\n\n- Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013-2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1237. URL https://www.aclweb.org/anthology/D15-1237.\n\n- Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687-3692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL https://www.aclweb.org/anthology/D19-1382.\n\n- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL <https://www.aclweb.org/anthology/D18-1259>.\n\n- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz- imer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.\n\n- Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 764-777, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1074. URL https://www.aclweb.org/ anthology/P19-1074.\n\n- Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014. doi: 10.1162/tacl_a_00166. URL https://www.aclweb.org/ anthology/Q14-1006.\n\n- Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4927-4940, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL https://www.aclweb. org/anthology/2020.acl-main.444.\n\n- Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridg- ing the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\n- Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/ paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.\n\n- Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL https: //www.aclweb.org/anthology/N19-1131.\n\n- Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-aware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35-45, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1004. URL https://www.aclweb.org/anthology/D17-1004.\n\n- Qi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and Minlie Huang. CrossWOZ: A large-scale Chinese cross- domain task-oriented dialogue dataset. Transactions of the Association for Computational Linguistics, 8:281-295, 2020. doi: 10.1162/tacl_a_00314. URL https://www.aclweb.org/anthology/2020.tacl-1.19. Contribution Sungjoon Park led the project as project manager, initiated the project, made decisions on overall progress of this project, secured financial resources, signed up with ACROFAN for the articles, and organized IRB submission and research paper. Jihyung Moon led the project as project manager, managed overall datasets, models, and ethical concerns, signed up with ACROFAN for the articles, prepared IRB, as well as contributed to NER, STS, NLI, MRC dataset constructions, AIRBNB, POLICY corpora collection, and leaderboard design.\n\n- Sungdong Kim managed overall fine-tuning of language models, served as a person in charge (PIC) of DST, and contributed to the dataset construction of TC, STS, and RE.\n\n- Won Ik Cho managed the overall dataset construction of TC, STS, NLI, RE, MRC, and DST, provided the original corpus of PARAKQC, and served as a PIC of STS.\n\n- Jiyoon Han managed the overall dataset construction of DP and NER, served as a PIC of NLI, contributed to the dataset construction of STS, and took part in preparing IRB.\n\n- Jangwon Park served as a PIC of model pretraining, contributed to the text collection of YNA, collected and pre-processed MODU, CC-100-Kor, NAMUWIKI, NEWSCRAWL, and PETITION, and conducted the fine-tuning of TC. Chisung Song served as a PIC of NER and contributed to the dataset construction of DP and DST.\n\n- Junseong Kim served as a PIC of MRC, conducted the fine-tuning of MRC, and contributed to the text collection of WIKITREE and WIKIPEDIA. Youngsook Song served as a PIC of TC and contributed to the dataset construction of NER and DST.\n\n- Taehwan Oh served as a PIC of DP, contributed to the dataset construction of NER and NLI, and took part in preparing IRB. Joohong Lee served as a PIC of RE, and conducted the fine-tuning of RE.\n\n- Juhyun Oh contributed to the dataset construction of NLI, NER, DP and STS, took part in ethical considerations, IRB preparation and setup for model pretraining.\n\n- Sungwon Lyu contributed to the dataset construction of STS, NLI, RE, and MRC, taking part in the overall model pretraining and task-wise fine-tuning.\n\n- Younghoon Jeong contributed to the text collection of WIKINEWS, modeling of DP, and pre-processing of the pretraining corpus. Inkwon Lee contributed to the text collection, modeling of DP and pre-processing of the pretraining corpus. Sangwoo Seo contributed to the dataset construction of RE, and took part in preparing IRB. Dongjun Lee contributed to the construction of the fine-tuning pipeline and the modeling of STS.\n\n- Hyunwoo Kim contributed to the dataset construction of MRC, and took part in preparing IRB. Myeonghwa Lee contributed to the dataset construction of STS and TC.\n\n- Seongbo Jang contributed to the dataset construction of RE, and took part in preparing IRB. Seungwon Do contributed to the dataset construction of DST and text collection.\n\n- Sunkyoung Kim contributed to the dataset construction of RE and MRC, and modeling of MRC. Kyungtae Lim contributed to the dataset construction of DP. Jongwon Lee contributed to the dataset construction and modeling of DST. Kyumin Park contributed to the dataset construction of DST, and took part in preparing IRB. Jamin Shin contributed to the dataset construction of DST.\n\n- Seonghyun Kim contributed to the dataset construction and modeling of NER. Lucy Park contributed to the dataset construction of MRC and provided the original corpus of NSMC.\n\n- Alice Oh advised the project, sponsored the project via KAIST, provided feedback and suggested better way to improve the quality of our dataset and models, and helped with the final manuscript. Jung-Woo Ha advised the project, sponsored annotation cost and computing cloud credits the project via NAVER, provided license-free news articles from The Korea Economy Daily, and helped with the final manuscript.\n\n- Kyunghyun Cho advised the project, provided critical feedback, suggested better way to improve the quality of our dataset and models, and helped a lot with polishing and rewriting the final manuscript. All participants contributed to this manuscript.\n\n", "annotations": {"Abstract": [{"begin": 48, "end": 1905, "idx": 0}], "Head": [{"begin": 1908, "end": 1922, "n": "1", "idx": 0}, {"begin": 4311, "end": 4322, "n": "1.1", "idx": 1}, {"begin": 8200, "end": 8214, "idx": 2}, {"begin": 8417, "end": 8445, "idx": 3}, {"begin": 10413, "end": 10423, "idx": 4}, {"begin": 15889, "end": 15905, "n": "2", "idx": 5}, {"begin": 16407, "end": 16437, "n": "2.1", "idx": 6}, {"begin": 20367, "end": 20387, "n": "2.2", "idx": 7}, {"begin": 20974, "end": 20995, "idx": 8}, {"begin": 25195, "end": 25219, "n": "2.2.1", "idx": 9}, {"begin": 28068, "end": 28085, "n": "2.3", "idx": 10}, {"begin": 28597, "end": 28612, "idx": 11}, {"begin": 30002, "end": 30013, "idx": 12}, {"begin": 30244, "end": 30263, "n": "2.4", "idx": 13}, {"begin": 31865, "end": 31881, "n": "3", "idx": 14}, {"begin": 32440, "end": 32469, "n": "3.1", "idx": 15}, {"begin": 33480, "end": 33506, "n": "3.1.1", "idx": 16}, {"begin": 36052, "end": 36070, "idx": 17}, {"begin": 38340, "end": 38363, "n": "3.1.2", "idx": 18}, {"begin": 38578, "end": 38596, "n": "3.1.3", "idx": 19}, {"begin": 40031, "end": 40047, "n": "3.1.4", "idx": 20}, {"begin": 40639, "end": 40676, "n": "3.2", "idx": 21}, {"begin": 41588, "end": 41614, "n": "3.2.1", "idx": 22}, {"begin": 50973, "end": 50997, "n": "3.2.2", "idx": 23}, {"begin": 53976, "end": 53992, "n": "3.2.4", "idx": 24}, {"begin": 54511, "end": 54547, "n": "3.3", "idx": 25}, {"begin": 55412, "end": 55438, "n": "3.3.1", "idx": 26}, {"begin": 61803, "end": 61816, "idx": 27}, {"begin": 66659, "end": 66682, "n": "3.3.2", "idx": 28}, {"begin": 66961, "end": 66979, "n": "3.3.3", "idx": 29}, {"begin": 70575, "end": 70591, "n": "3.3.4", "idx": 30}, {"begin": 71619, "end": 71653, "n": "3.4", "idx": 31}, {"begin": 72763, "end": 72789, "n": "3.4.1", "idx": 32}, {"begin": 78396, "end": 78420, "n": "3.4.2", "idx": 33}, {"begin": 78655, "end": 78673, "n": "3.4.3", "idx": 34}, {"begin": 81107, "end": 81123, "n": "3.4.4", "idx": 35}, {"begin": 81724, "end": 81752, "n": "3.5", "idx": 36}, {"begin": 83378, "end": 83401, "n": "3.5.1", "idx": 37}, {"begin": 85492, "end": 85522, "n": "1.", "idx": 38}, {"begin": 86028, "end": 86053, "n": "2.", "idx": 39}, {"begin": 89171, "end": 89189, "n": "3.", "idx": 40}, {"begin": 89859, "end": 89881, "n": "4.", "idx": 41}, {"begin": 91379, "end": 91400, "n": "5.", "idx": 42}, {"begin": 93068, "end": 93092, "n": "3.5.2", "idx": 43}, {"begin": 93866, "end": 93884, "n": "3.5.3", "idx": 44}, {"begin": 95651, "end": 95667, "n": "3.5.4", "idx": 45}, {"begin": 96211, "end": 96238, "n": "3.6", "idx": 46}, {"begin": 98323, "end": 98349, "n": "3.6.1", "idx": 47}, {"begin": 101914, "end": 101938, "n": "3.6.2", "idx": 48}, {"begin": 102706, "end": 102724, "n": "3.6.3", "idx": 49}, {"begin": 107319, "end": 107335, "n": "3.6.4", "idx": 50}, {"begin": 107989, "end": 108028, "n": "3.7", "idx": 51}, {"begin": 110854, "end": 110880, "n": "3.7.1", "idx": 52}, {"begin": 112790, "end": 112810, "n": "1.", "idx": 53}, {"begin": 114966, "end": 114993, "n": "2.", "idx": 54}, {"begin": 115142, "end": 115177, "n": "2.1.", "idx": 55}, {"begin": 115455, "end": 115468, "idx": 56}, {"begin": 116693, "end": 116734, "n": "2.2.", "idx": 57}, {"begin": 118775, "end": 118811, "n": "2.3.", "idx": 58}, {"begin": 120870, "end": 120894, "n": "3.7.2", "idx": 59}, {"begin": 122277, "end": 122291, "n": "3.7.3", "idx": 60}, {"begin": 122764, "end": 122810, "idx": 61}, {"begin": 124548, "end": 124564, "idx": 62}, {"begin": 124923, "end": 124941, "n": "3.7.4", "idx": 63}, {"begin": 127832, "end": 127848, "n": "3.7.5", "idx": 64}, {"begin": 128232, "end": 128265, "n": "3.8", "idx": 65}, {"begin": 129930, "end": 129956, "n": "3.8.1", "idx": 66}, {"begin": 131015, "end": 131038, "n": "1.", "idx": 67}, {"begin": 132884, "end": 132910, "n": "2.", "idx": 68}, {"begin": 133660, "end": 133694, "n": "3.", "idx": 69}, {"begin": 136392, "end": 136408, "n": "3.2.", "idx": 70}, {"begin": 138185, "end": 138208, "n": "4.", "idx": 71}, {"begin": 139641, "end": 139665, "n": "3.8.2", "idx": 72}, {"begin": 140243, "end": 140257, "n": "3.8.3", "idx": 73}, {"begin": 140506, "end": 140524, "n": "3.8.4", "idx": 74}, {"begin": 145120, "end": 145136, "n": "3.8.5", "idx": 75}, {"begin": 145785, "end": 145813, "n": "4", "idx": 76}, {"begin": 146239, "end": 146258, "n": "4.1", "idx": 77}, {"begin": 151820, "end": 151851, "idx": 78}, {"begin": 153053, "end": 153081, "n": "4.2", "idx": 79}, {"begin": 154216, "end": 154252, "n": "5.1.1", "idx": 80}, {"begin": 155062, "end": 155109, "n": "5.1.2", "idx": 81}, {"begin": 155820, "end": 155865, "n": "5.1.3", "idx": 82}, {"begin": 156684, "end": 156706, "n": "5.1.4", "idx": 83}, {"begin": 159060, "end": 159090, "n": "5.2", "idx": 84}, {"begin": 159676, "end": 159698, "n": "5.3", "idx": 85}, {"begin": 161395, "end": 161417, "n": "5.4", "idx": 86}, {"begin": 164126, "end": 164150, "n": "6", "idx": 87}, {"begin": 164507, "end": 164538, "n": "6.1", "idx": 88}, {"begin": 166971, "end": 166988, "n": "6.2", "idx": 89}, {"begin": 168883, "end": 168922, "n": "6.3", "idx": 90}, {"begin": 169886, "end": 169900, "n": "7", "idx": 91}, {"begin": 175383, "end": 175395, "n": "8", "idx": 92}], "ReferenceToBib": [{"begin": 2004, "end": 2008, "target": "#b27", "idx": 0}, {"begin": 2026, "end": 2030, "target": "#b79", "idx": 1}, {"begin": 2031, "end": 2034, "target": "#b20", "idx": 2}, {"begin": 2035, "end": 2038, "target": "#b46", "idx": 3}, {"begin": 2056, "end": 2061, "target": "#b107", "idx": 4}, {"begin": 2079, "end": 2084, "target": "#b108", "idx": 5}, {"begin": 2085, "end": 2088, "target": "#b73", "idx": 6}, {"begin": 2089, "end": 2091, "target": "#b7", "idx": 7}, {"begin": 2234, "end": 2239, "target": "#b130", "idx": 8}, {"begin": 2254, "end": 2259, "target": "#b129", "idx": 9}, {"begin": 2949, "end": 2954, "target": "#b139", "idx": 10}, {"begin": 2988, "end": 2992, "target": "#b69", "idx": 11}, {"begin": 3017, "end": 3022, "target": "#b134", "idx": 12}, {"begin": 3041, "end": 3045, "target": "#b54", "idx": 13}, {"begin": 3081, "end": 3086, "target": "#b122", "idx": 14}, {"begin": 3329, "end": 3333, "target": "#b75", "idx": 15}, {"begin": 3345, "end": 3349, "target": "#b51", "idx": 16}, {"begin": 3680, "end": 3684, "target": "#b31", "idx": 17}, {"begin": 6170, "end": 6174, "target": "#b92", "idx": 18}, {"begin": 8861, "end": 8866, "target": "#b97", "idx": 19}, {"begin": 8933, "end": 8937, "target": "#b83", "idx": 20}, {"begin": 8938, "end": 8941, "target": "#b43", "idx": 21}, {"begin": 9512, "end": 9515, "target": "#b5", "idx": 22}, {"begin": 9733, "end": 9737, "target": "#b89", "idx": 23}, {"begin": 11639, "end": 11642, "target": "#b6", "idx": 24}, {"begin": 11652, "end": 11657, "target": "#b135", "idx": 25}, {"begin": 16272, "end": 16276, "target": "#b38", "idx": 26}, {"begin": 16297, "end": 16300, "target": "#b4", "idx": 27}, {"begin": 17050, "end": 17055, "target": "#b129", "idx": 28}, {"begin": 17067, "end": 17071, "target": "#b51", "idx": 29}, {"begin": 17088, "end": 17092, "target": "#b54", "idx": 30}, {"begin": 18036, "end": 18041, "target": "#b110", "idx": 31}, {"begin": 18081, "end": 18086, "target": "#b109", "idx": 32}, {"begin": 22939, "end": 22943, "target": "#b16", "idx": 33}, {"begin": 26869, "end": 26873, "target": "#b89", "idx": 34}, {"begin": 27109, "end": 27113, "target": "#b16", "idx": 35}, {"begin": 29350, "end": 29354, "target": "#b89", "idx": 36}, {"begin": 30937, "end": 30942, "target": "#b135", "idx": 37}, {"begin": 32737, "end": 32742, "target": "#b148", "idx": 38}, {"begin": 32979, "end": 32984, "target": "#b139", "idx": 39}, {"begin": 32999, "end": 33003, "target": "#b54", "idx": 40}, {"begin": 34479, "end": 34484, "target": "#b139", "idx": 41}, {"begin": 34496, "end": 34501, "target": "#b148", "idx": 42}, {"begin": 34768, "end": 34772, "idx": 43}, {"begin": 37791, "end": 37795, "target": "#b64", "idx": 44}, {"begin": 38744, "end": 38749, "target": "#b148", "idx": 45}, {"begin": 39116, "end": 39120, "target": "#b54", "idx": 46}, {"begin": 39358, "end": 39363, "target": "#b139", "idx": 47}, {"begin": 39590, "end": 39594, "target": "#b54", "idx": 48}, {"begin": 39692, "end": 39697, "target": "#b139", "idx": 49}, {"begin": 40935, "end": 40939, "target": "#b11", "idx": 50}, {"begin": 40948, "end": 40953, "target": "#b130", "idx": 51}, {"begin": 41058, "end": 41063, "target": "#b139", "idx": 52}, {"begin": 41086, "end": 41091, "target": "#b130", "idx": 53}, {"begin": 41092, "end": 41095, "target": "#b54", "idx": 54}, {"begin": 41126, "end": 41131, "target": "#b122", "idx": 55}, {"begin": 41132, "end": 41135, "target": "#b69", "idx": 56}, {"begin": 41433, "end": 41437, "target": "#b11", "idx": 57}, {"begin": 41767, "end": 41771, "target": "#b16", "idx": 58}, {"begin": 42428, "end": 42431, "target": "#b0", "idx": 59}, {"begin": 43793, "end": 43797, "target": "#b77", "idx": 60}, {"begin": 44262, "end": 44265, "target": "#b0", "idx": 61}, {"begin": 44697, "end": 44702, "target": "#b100", "idx": 62}, {"begin": 45240, "end": 45243, "target": "#b2", "idx": 63}, {"begin": 47604, "end": 47607, "target": "#b1", "idx": 64}, {"begin": 47620, "end": 47624, "target": "#b11", "idx": 65}, {"begin": 47736, "end": 47740, "target": "#b64", "idx": 66}, {"begin": 50338, "end": 50342, "target": "#b65", "idx": 67}, {"begin": 51246, "end": 51250, "target": "#b11", "idx": 68}, {"begin": 51745, "end": 51750, "target": "#b130", "idx": 69}, {"begin": 51751, "end": 51755, "target": "#b139", "idx": 70}, {"begin": 51862, "end": 51865, "target": "#b0", "idx": 71}, {"begin": 51866, "end": 51868, "target": "#b1", "idx": 72}, {"begin": 51869, "end": 51872, "target": "#b11", "idx": 73}, {"begin": 52083, "end": 52087, "target": "#b42", "idx": 74}, {"begin": 52167, "end": 52171, "target": "#b11", "idx": 75}, {"begin": 52737, "end": 52741, "target": "#b29", "idx": 76}, {"begin": 52770, "end": 52775, "target": "#b130", "idx": 77}, {"begin": 52785, "end": 52790, "target": "#b149", "idx": 78}, {"begin": 52802, "end": 52807, "target": "#b141", "idx": 79}, {"begin": 53012, "end": 53016, "target": "#b16", "idx": 80}, {"begin": 53511, "end": 53516, "target": "#b141", "idx": 81}, {"begin": 53555, "end": 53560, "target": "#b149", "idx": 82}, {"begin": 53868, "end": 53872, "target": "#b95", "idx": 83}, {"begin": 54503, "end": 54508, "target": "#b112", "idx": 84}, {"begin": 54887, "end": 54891, "target": "#b24", "idx": 85}, {"begin": 55047, "end": 55052, "target": "#b130", "idx": 86}, {"begin": 55067, "end": 55072, "target": "#b129", "idx": 87}, {"begin": 55133, "end": 55137, "target": "#b21", "idx": 88}, {"begin": 55138, "end": 55142, "target": "#b104", "idx": 89}, {"begin": 55143, "end": 55147, "target": "#b112", "idx": 90}, {"begin": 55514, "end": 55517, "target": "#b6", "idx": 91}, {"begin": 55527, "end": 55532, "target": "#b135", "idx": 92}, {"begin": 55921, "end": 55926, "target": "#b135", "idx": 93}, {"begin": 58160, "end": 58164, "target": "#b41", "idx": 94}, {"begin": 58165, "end": 58169, "target": "#b105", "idx": 95}, {"begin": 58397, "end": 58402, "target": "#b128", "idx": 96}, {"begin": 59173, "end": 59177, "target": "#b86", "idx": 97}, {"begin": 59178, "end": 59181, "target": "#b39", "idx": 98}, {"begin": 63780, "end": 63784, "target": "#b41", "idx": 99}, {"begin": 63785, "end": 63789, "target": "#b128", "idx": 100}, {"begin": 64128, "end": 64132, "target": "#b42", "idx": 101}, {"begin": 64177, "end": 64181, "target": "#b22", "idx": 102}, {"begin": 66746, "end": 66749, "target": "#b6", "idx": 103}, {"begin": 66759, "end": 66764, "target": "#b135", "idx": 104}, {"begin": 67017, "end": 67021, "target": "#b24", "idx": 105}, {"begin": 67519, "end": 67522, "target": "#b6", "idx": 106}, {"begin": 67573, "end": 67578, "target": "#b135", "idx": 107}, {"begin": 67777, "end": 67782, "target": "#b145", "idx": 108}, {"begin": 68151, "end": 68155, "target": "#b22", "idx": 109}, {"begin": 68471, "end": 68475, "target": "#b22", "idx": 110}, {"begin": 68812, "end": 68816, "target": "#b42", "idx": 111}, {"begin": 68839, "end": 68843, "target": "#b42", "idx": 112}, {"begin": 69145, "end": 69149, "target": "#b42", "idx": 113}, {"begin": 69616, "end": 69620, "target": "#b41", "idx": 114}, {"begin": 69621, "end": 69625, "target": "#b105", "idx": 115}, {"begin": 69975, "end": 69980, "target": "#b128", "idx": 116}, {"begin": 70165, "end": 70169, "target": "#b50", "idx": 117}, {"begin": 71602, "end": 71606, "target": "#b21", "idx": 118}, {"begin": 71607, "end": 71611, "target": "#b104", "idx": 119}, {"begin": 71612, "end": 71616, "target": "#b112", "idx": 120}, {"begin": 72123, "end": 72128, "target": "#b134", "idx": 121}, {"begin": 72129, "end": 72132, "target": "#b54", "idx": 122}, {"begin": 72133, "end": 72136, "target": "#b75", "idx": 123}, {"begin": 72137, "end": 72140, "target": "#b51", "idx": 124}, {"begin": 74797, "end": 74801, "target": "#b14", "idx": 125}, {"begin": 77966, "end": 77971, "target": "#b126", "idx": 126}, {"begin": 78282, "end": 78286, "target": "#b43", "idx": 127}, {"begin": 78684, "end": 78689, "target": "#b126", "idx": 128}, {"begin": 79022, "end": 79026, "target": "#b40", "idx": 129}, {"begin": 79138, "end": 79142, "target": "#b40", "idx": 130}, {"begin": 79153, "end": 79157, "target": "#b14", "idx": 131}, {"begin": 79294, "end": 79298, "target": "#b37", "idx": 132}, {"begin": 79365, "end": 79370, "target": "#b124", "idx": 133}, {"begin": 79460, "end": 79465, "target": "#b114", "idx": 134}, {"begin": 80299, "end": 80303, "target": "#b17", "idx": 135}, {"begin": 83422, "end": 83426, "target": "#b88", "idx": 136}, {"begin": 84177, "end": 84182, "target": "#b113", "idx": 137}, {"begin": 84197, "end": 84202, "target": "#b150", "idx": 138}, {"begin": 84218, "end": 84222, "target": "#b90", "idx": 139}, {"begin": 84340, "end": 84345, "target": "#b113", "idx": 140}, {"begin": 85954, "end": 85958, "target": "#b89", "idx": 141}, {"begin": 89465, "end": 89469, "target": "#b95", "idx": 142}, {"begin": 90761, "end": 90766, "target": "#b150", "idx": 143}, {"begin": 93056, "end": 93060, "target": "#b64", "idx": 144}, {"begin": 94091, "end": 94095, "target": "#b28", "idx": 145}, {"begin": 94117, "end": 94121, "target": "#b49", "idx": 146}, {"begin": 94265, "end": 94269, "target": "#b88", "idx": 147}, {"begin": 94440, "end": 94445, "target": "#b113", "idx": 148}, {"begin": 94446, "end": 94450, "target": "#b150", "idx": 149}, {"begin": 94451, "end": 94454, "target": "#b45", "idx": 150}, {"begin": 94455, "end": 94459, "target": "#b144", "idx": 151}, {"begin": 94460, "end": 94464, "target": "#b146", "idx": 152}, {"begin": 94501, "end": 94506, "target": "#b150", "idx": 153}, {"begin": 94591, "end": 94595, "target": "#b84", "idx": 154}, {"begin": 94751, "end": 94756, "target": "#b146", "idx": 155}, {"begin": 95088, "end": 95093, "target": "#b138", "idx": 156}, {"begin": 95109, "end": 95114, "target": "#b118", "idx": 157}, {"begin": 95134, "end": 95138, "target": "#b52", "idx": 158}, {"begin": 95151, "end": 95155, "target": "#b90", "idx": 159}, {"begin": 95534, "end": 95539, "target": "#b150", "idx": 160}, {"begin": 96684, "end": 96688, "target": "#b26", "idx": 161}, {"begin": 96689, "end": 96692, "target": "#b25", "idx": 162}, {"begin": 100813, "end": 100817, "target": "#b95", "idx": 163}, {"begin": 102743, "end": 102747, "target": "#b81", "idx": 164}, {"begin": 103197, "end": 103201, "target": "#b83", "idx": 165}, {"begin": 103202, "end": 103205, "target": "#b18", "idx": 166}, {"begin": 103400, "end": 103405, "target": "#b103", "idx": 167}, {"begin": 103536, "end": 103540, "target": "#b25", "idx": 168}, {"begin": 103616, "end": 103620, "target": "#b62", "idx": 169}, {"begin": 103868, "end": 103872, "target": "#b94", "idx": 170}, {"begin": 105326, "end": 105330, "target": "#b83", "idx": 171}, {"begin": 105351, "end": 105356, "target": "#b125", "idx": 172}, {"begin": 105387, "end": 105391, "target": "#b44", "idx": 173}, {"begin": 105433, "end": 105437, "target": "#b18", "idx": 174}, {"begin": 105815, "end": 105819, "target": "#b96", "idx": 175}, {"begin": 105978, "end": 105982, "target": "#b95", "idx": 176}, {"begin": 108296, "end": 108300, "target": "#b19", "idx": 177}, {"begin": 108301, "end": 108304, "target": "#b53", "idx": 178}, {"begin": 108305, "end": 108308, "target": "#b57", "idx": 179}, {"begin": 108309, "end": 108313, "target": "#b109", "idx": 180}, {"begin": 108314, "end": 108318, "target": "#b110", "idx": 181}, {"begin": 108319, "end": 108323, "target": "#b142", "idx": 182}, {"begin": 108324, "end": 108328, "target": "#b147", "idx": 183}, {"begin": 108494, "end": 108499, "target": "#b109", "idx": 184}, {"begin": 108514, "end": 108519, "target": "#b110", "idx": 185}, {"begin": 108565, "end": 108569, "target": "#b27", "idx": 186}, {"begin": 108570, "end": 108573, "target": "#b79", "idx": 187}, {"begin": 108574, "end": 108577, "target": "#b20", "idx": 188}, {"begin": 108578, "end": 108581, "target": "#b68", "idx": 189}, {"begin": 108589, "end": 108593, "target": "#b19", "idx": 190}, {"begin": 108602, "end": 108607, "target": "#b147", "idx": 191}, {"begin": 108621, "end": 108625, "target": "#b57", "idx": 192}, {"begin": 108802, "end": 108806, "target": "#b66", "idx": 193}, {"begin": 108807, "end": 108810, "target": "#b53", "idx": 194}, {"begin": 108811, "end": 108815, "target": "#b142", "idx": 195}, {"begin": 108816, "end": 108819, "target": "#b35", "idx": 196}, {"begin": 108877, "end": 108882, "target": "#b102", "idx": 197}, {"begin": 109005, "end": 109010, "target": "#b134", "idx": 198}, {"begin": 109020, "end": 109025, "target": "#b139", "idx": 199}, {"begin": 109039, "end": 109044, "target": "#b122", "idx": 200}, {"begin": 109249, "end": 109253, "target": "#b76", "idx": 201}, {"begin": 109254, "end": 109256, "idx": 202}, {"begin": 109257, "end": 109260, "target": "#b71", "idx": 203}, {"begin": 113861, "end": 113866, "target": "#b119", "idx": 204}, {"begin": 114959, "end": 114963, "target": "#b63", "idx": 205}, {"begin": 115447, "end": 115452, "target": "#b119", "idx": 206}, {"begin": 117199, "end": 117203, "target": "#b87", "idx": 207}, {"begin": 119158, "end": 119163, "target": "#b110", "idx": 208}, {"begin": 120994, "end": 120998, "target": "#b77", "idx": 209}, {"begin": 122373, "end": 122377, "target": "#b76", "idx": 210}, {"begin": 122386, "end": 122390, "target": "#b60", "idx": 211}, {"begin": 123235, "end": 123239, "target": "#b15", "idx": 212}, {"begin": 123240, "end": 123244, "idx": 213}, {"begin": 125318, "end": 125323, "target": "#b127", "idx": 214}, {"begin": 125564, "end": 125569, "target": "#b117", "idx": 215}, {"begin": 125797, "end": 125802, "target": "#b119", "idx": 216}, {"begin": 126064, "end": 126068, "target": "#b53", "idx": 217}, {"begin": 126314, "end": 126318, "target": "#b57", "idx": 218}, {"begin": 126450, "end": 126455, "target": "#b129", "idx": 219}, {"begin": 126576, "end": 126581, "target": "#b140", "idx": 220}, {"begin": 126582, "end": 126586, "target": "#b127", "idx": 221}, {"begin": 126587, "end": 126590, "target": "#b93", "idx": 222}, {"begin": 126591, "end": 126595, "target": "#b110", "idx": 223}, {"begin": 126596, "end": 126599, "target": "#b66", "idx": 224}, {"begin": 126618, "end": 126623, "target": "#b110", "idx": 225}, {"begin": 126877, "end": 126881, "target": "#b76", "idx": 226}, {"begin": 126882, "end": 126885, "target": "#b60", "idx": 227}, {"begin": 126946, "end": 126951, "target": "#b109", "idx": 228}, {"begin": 127473, "end": 127477, "target": "#b71", "idx": 229}, {"begin": 127530, "end": 127535, "target": "#b109", "idx": 230}, {"begin": 128428, "end": 128432, "target": "#b12", "idx": 231}, {"begin": 128937, "end": 128941, "target": "#b82", "idx": 232}, {"begin": 129042, "end": 129046, "target": "#b85", "idx": 233}, {"begin": 130049, "end": 130053, "target": "#b56", "idx": 234}, {"begin": 130327, "end": 130331, "target": "#b9", "idx": 235}, {"begin": 130458, "end": 130462, "target": "#b9", "idx": 236}, {"begin": 131748, "end": 131752, "target": "#b47", "idx": 237}, {"begin": 131753, "end": 131756, "target": "#b48", "idx": 238}, {"begin": 132551, "end": 132555, "target": "#b8", "idx": 239}, {"begin": 132876, "end": 132881, "target": "#b111", "idx": 240}, {"begin": 133429, "end": 133433, "target": "#b8", "idx": 241}, {"begin": 134315, "end": 134319, "target": "#b8", "idx": 242}, {"begin": 134320, "end": 134324, "target": "#b151", "idx": 243}, {"begin": 135732, "end": 135736, "target": "#b74", "idx": 244}, {"begin": 138178, "end": 138182, "target": "#b34", "idx": 245}, {"begin": 138264, "end": 138268, "target": "#b9", "idx": 246}, {"begin": 140374, "end": 140378, "target": "#b74", "idx": 247}, {"begin": 140544, "end": 140548, "target": "#b56", "idx": 248}, {"begin": 141165, "end": 141170, "target": "#b132", "idx": 249}, {"begin": 141171, "end": 141174, "target": "#b32", "idx": 250}, {"begin": 141175, "end": 141178, "target": "#b33", "idx": 251}, {"begin": 141458, "end": 141462, "target": "#b8", "idx": 252}, {"begin": 141825, "end": 141829, "target": "#b34", "idx": 253}, {"begin": 142048, "end": 142053, "target": "#b151", "idx": 254}, {"begin": 142054, "end": 142058, "target": "#b106", "idx": 255}, {"begin": 142073, "end": 142077, "target": "#b9", "idx": 256}, {"begin": 142514, "end": 142518, "target": "#b9", "idx": 257}, {"begin": 143555, "end": 143560, "target": "#b121", "idx": 258}, {"begin": 143561, "end": 143565, "target": "#b111", "idx": 259}, {"begin": 143740, "end": 143744, "target": "#b74", "idx": 260}, {"begin": 146201, "end": 146205, "target": "#b27", "idx": 261}, {"begin": 146218, "end": 146222, "target": "#b79", "idx": 262}, {"begin": 146964, "end": 146968, "target": "#b95", "idx": 263}, {"begin": 147237, "end": 147242, "target": "#b133", "idx": 264}, {"begin": 147276, "end": 147280, "target": "#b23", "idx": 265}, {"begin": 148355, "end": 148360, "target": "#b115", "idx": 266}, {"begin": 148867, "end": 148871, "target": "#b10", "idx": 267}, {"begin": 149213, "end": 149217, "target": "#b59", "idx": 268}, {"begin": 149396, "end": 149400, "target": "#b78", "idx": 269}, {"begin": 149619, "end": 149623, "target": "#b13", "idx": 270}, {"begin": 150415, "end": 150420, "target": "#b120", "idx": 271}, {"begin": 150503, "end": 150507, "target": "#b65", "idx": 272}, {"begin": 151534, "end": 151538, "target": "#b27", "idx": 273}, {"begin": 151925, "end": 151929, "target": "#b27", "idx": 274}, {"begin": 151942, "end": 151946, "target": "#b79", "idx": 275}, {"begin": 152492, "end": 152496, "target": "#b27", "idx": 276}, {"begin": 152497, "end": 152500, "target": "#b79", "idx": 277}, {"begin": 152600, "end": 152604, "target": "#b79", "idx": 278}, {"begin": 153259, "end": 153263, "target": "#b27", "idx": 279}, {"begin": 153327, "end": 153331, "target": "#b27", "idx": 280}, {"begin": 153453, "end": 153457, "target": "#b23", "idx": 281}, {"begin": 153470, "end": 153474, "target": "#b79", "idx": 282}, {"begin": 153552, "end": 153556, "target": "#b72", "idx": 283}, {"begin": 153728, "end": 153733, "target": "#b99", "idx": 284}, {"begin": 153863, "end": 153867, "target": "#b20", "idx": 285}, {"begin": 153896, "end": 153901, "target": "#b99", "idx": 286}, {"begin": 153945, "end": 153949, "target": "#b95", "idx": 287}, {"begin": 154206, "end": 154210, "target": "#b27", "idx": 288}, {"begin": 154993, "end": 154996, "target": "#b3", "idx": 289}, {"begin": 156098, "end": 156103, "target": "#b137", "idx": 290}, {"begin": 156271, "end": 156275, "target": "#b15", "idx": 291}, {"begin": 156585, "end": 156589, "target": "#b8", "idx": 292}, {"begin": 158251, "end": 158255, "target": "#b36", "idx": 293}, {"begin": 158796, "end": 158800, "target": "#b30", "idx": 294}, {"begin": 158841, "end": 158845, "target": "#b61", "idx": 295}, {"begin": 159148, "end": 159153, "target": "#b136", "idx": 296}, {"begin": 159203, "end": 159207, "target": "#b80", "idx": 297}, {"begin": 160590, "end": 160594, "target": "#b55", "idx": 298}, {"begin": 160595, "end": 160597, "target": "#b7", "idx": 299}, {"begin": 162443, "end": 162448, "target": "#b116", "idx": 300}, {"begin": 168556, "end": 168560, "target": "#b89", "idx": 301}, {"begin": 169981, "end": 169986, "target": "#b130", "idx": 302}, {"begin": 170311, "end": 170314, "target": "#b1", "idx": 303}, {"begin": 170315, "end": 170318, "target": "#b11", "idx": 304}, {"begin": 170381, "end": 170384, "target": "#b6", "idx": 305}, {"begin": 170385, "end": 170389, "target": "#b135", "idx": 306}, {"begin": 170505, "end": 170510, "target": "#b131", "idx": 307}, {"begin": 170511, "end": 170515, "target": "#b123", "idx": 308}, {"begin": 170680, "end": 170685, "target": "#b139", "idx": 309}, {"begin": 170704, "end": 170708, "target": "#b69", "idx": 310}, {"begin": 170732, "end": 170737, "target": "#b134", "idx": 311}, {"begin": 170769, "end": 170773, "target": "#b54", "idx": 312}, {"begin": 170793, "end": 170798, "target": "#b122", "idx": 313}, {"begin": 170817, "end": 170821, "target": "#b58", "idx": 314}, {"begin": 171176, "end": 171180, "target": "#b75", "idx": 315}, {"begin": 171192, "end": 171196, "target": "#b51", "idx": 316}, {"begin": 171624, "end": 171629, "target": "#b141", "idx": 317}, {"begin": 171674, "end": 171678, "target": "#b42", "idx": 318}, {"begin": 171712, "end": 171716, "target": "#b76", "idx": 319}, {"begin": 171717, "end": 171720, "target": "#b60", "idx": 320}, {"begin": 171740, "end": 171744, "target": "#b89", "idx": 321}, {"begin": 171979, "end": 171984, "target": "#b130", "idx": 322}, {"begin": 172009, "end": 172013, "target": "#b75", "idx": 323}, {"begin": 172014, "end": 172017, "target": "#b51", "idx": 324}, {"begin": 173621, "end": 173626, "target": "#b101", "idx": 325}, {"begin": 173634, "end": 173639, "target": "#b107", "idx": 326}, {"begin": 173649, "end": 173653, "target": "#b27", "idx": 327}, {"begin": 173802, "end": 173807, "target": "#b143", "idx": 328}, {"begin": 173816, "end": 173820, "target": "#b68", "idx": 329}, {"begin": 173830, "end": 173834, "target": "#b79", "idx": 330}, {"begin": 173844, "end": 173848, "target": "#b20", "idx": 331}, {"begin": 173862, "end": 173866, "target": "#b46", "idx": 332}, {"begin": 174215, "end": 174220, "target": "#b129", "idx": 333}, {"begin": 174230, "end": 174235, "target": "#b102", "idx": 334}, {"begin": 174726, "end": 174730, "target": "#b72", "idx": 335}, {"begin": 174851, "end": 174856, "target": "#b99", "idx": 336}, {"begin": 174868, "end": 174872, "target": "#b70", "idx": 337}, {"begin": 177167, "end": 177171, "target": "#b27", "idx": 338}, {"begin": 177236, "end": 177241, "target": "#b130", "idx": 339}, {"begin": 177256, "end": 177261, "target": "#b129", "idx": 340}, {"begin": 177295, "end": 177298, "target": "#b7", "idx": 341}, {"begin": 181859, "end": 181864, "target": "#b98", "idx": 342}], "ReferenceToFootnote": [{"begin": 10237, "end": 10238, "target": "#foot_0", "idx": 0}, {"begin": 10346, "end": 10347, "target": "#foot_1", "idx": 1}, {"begin": 17491, "end": 17492, "target": "#foot_3", "idx": 2}, {"begin": 19052, "end": 19053, "target": "#foot_4", "idx": 3}, {"begin": 19061, "end": 19062, "target": "#foot_5", "idx": 4}, {"begin": 19073, "end": 19074, "target": "#foot_6", "idx": 5}, {"begin": 19123, "end": 19124, "target": "#foot_7", "idx": 6}, {"begin": 19212, "end": 19213, "target": "#foot_8", "idx": 7}, {"begin": 20237, "end": 20239, "target": "#foot_9", "idx": 8}, {"begin": 23371, "end": 23373, "target": "#foot_10", "idx": 9}, {"begin": 23680, "end": 23682, "target": "#foot_11", "idx": 10}, {"begin": 27340, "end": 27342, "target": "#foot_12", "idx": 11}, {"begin": 28462, "end": 28464, "target": "#foot_13", "idx": 12}, {"begin": 29379, "end": 29381, "target": "#foot_14", "idx": 13}, {"begin": 29410, "end": 29412, "target": "#foot_15", "idx": 14}, {"begin": 34084, "end": 34086, "target": "#foot_16", "idx": 15}, {"begin": 35105, "end": 35107, "target": "#foot_17", "idx": 16}, {"begin": 38913, "end": 38915, "target": "#foot_18", "idx": 17}, {"begin": 43374, "end": 43376, "target": "#foot_19", "idx": 18}, {"begin": 43903, "end": 43905, "target": "#foot_20", "idx": 19}, {"begin": 46922, "end": 46924, "target": "#foot_21", "idx": 20}, {"begin": 56063, "end": 56065, "target": "#foot_22", "idx": 21}, {"begin": 74144, "end": 74146, "target": "#foot_23", "idx": 22}, {"begin": 74784, "end": 74786, "target": "#foot_24", "idx": 23}, {"begin": 76562, "end": 76564, "target": "#foot_25", "idx": 24}, {"begin": 76732, "end": 76734, "target": "#foot_26", "idx": 25}, {"begin": 79886, "end": 79888, "target": "#foot_27", "idx": 26}, {"begin": 84675, "end": 84677, "target": "#foot_28", "idx": 27}, {"begin": 84755, "end": 84757, "target": "#foot_29", "idx": 28}, {"begin": 85770, "end": 85772, "target": "#foot_30", "idx": 29}, {"begin": 89303, "end": 89305, "target": "#foot_31", "idx": 30}, {"begin": 89532, "end": 89534, "target": "#foot_32", "idx": 31}, {"begin": 90358, "end": 90360, "target": "#foot_33", "idx": 32}, {"begin": 91442, "end": 91444, "target": "#foot_34", "idx": 33}, {"begin": 97403, "end": 97405, "target": "#foot_35", "idx": 34}, {"begin": 99276, "end": 99278, "target": "#foot_36", "idx": 35}, {"begin": 100475, "end": 100477, "target": "#foot_37", "idx": 36}, {"begin": 100810, "end": 100812, "target": "#foot_38", "idx": 37}, {"begin": 103277, "end": 103279, "target": "#foot_39", "idx": 38}, {"begin": 112485, "end": 112487, "target": "#foot_40", "idx": 39}, {"begin": 123056, "end": 123058, "target": "#foot_42", "idx": 40}, {"begin": 124330, "end": 124332, "target": "#foot_43", "idx": 41}, {"begin": 125648, "end": 125650, "target": "#foot_44", "idx": 42}, {"begin": 127504, "end": 127506, "target": "#foot_45", "idx": 43}, {"begin": 129693, "end": 129695, "target": "#foot_46", "idx": 44}, {"begin": 131398, "end": 131400, "target": "#foot_47", "idx": 45}, {"begin": 133606, "end": 133608, "target": "#foot_48", "idx": 46}, {"begin": 135586, "end": 135588, "target": "#foot_49", "idx": 47}, {"begin": 146954, "end": 146956, "target": "#foot_51", "idx": 48}, {"begin": 147058, "end": 147060, "target": "#foot_52", "idx": 49}, {"begin": 147166, "end": 147168, "target": "#foot_53", "idx": 50}, {"begin": 147499, "end": 147501, "target": "#foot_54", "idx": 51}, {"begin": 147825, "end": 147827, "target": "#foot_55", "idx": 52}, {"begin": 147870, "end": 147872, "target": "#foot_56", "idx": 53}, {"begin": 148101, "end": 148103, "target": "#foot_57", "idx": 54}, {"begin": 159177, "end": 159179, "target": "#foot_58", "idx": 55}, {"begin": 159828, "end": 159830, "target": "#foot_59", "idx": 56}, {"begin": 164944, "end": 164946, "target": "#foot_60", "idx": 57}, {"begin": 168104, "end": 168106, "target": "#foot_61", "idx": 58}, {"begin": 169624, "end": 169626, "target": "#foot_62", "idx": 59}, {"begin": 172927, "end": 172929, "target": "#foot_63", "idx": 60}, {"begin": 174652, "end": 174654, "target": "#foot_64", "idx": 61}, {"begin": 174675, "end": 174677, "target": "#foot_65", "idx": 62}, {"begin": 174697, "end": 174699, "target": "#foot_66", "idx": 63}], "SectionFootnote": [{"begin": 178582, "end": 183111, "idx": 0}], "ReferenceString": [{"begin": 183128, "end": 183678, "id": "b0", "idx": 0}, {"begin": 183680, "end": 184147, "id": "b1", "idx": 1}, {"begin": 184151, "end": 184398, "id": "b2", "idx": 2}, {"begin": 184402, "end": 184805, "id": "b3", "idx": 3}, {"begin": 184809, "end": 185107, "id": "b4", "idx": 4}, {"begin": 185111, "end": 185257, "id": "b5", "idx": 5}, {"begin": 185261, "end": 185662, "id": "b6", "idx": 6}, {"begin": 185666, "end": 186460, "id": "b7", "idx": 7}, {"begin": 186464, "end": 186944, "id": "b8", "idx": 8}, {"begin": 186948, "end": 187525, "id": "b9", "idx": 9}, {"begin": 187529, "end": 187783, "id": "b10", "idx": 10}, {"begin": 187787, "end": 188211, "id": "b11", "idx": 11}, {"begin": 188215, "end": 188479, "id": "b12", "idx": 12}, {"begin": 188483, "end": 188740, "id": "b13", "idx": 13}, {"begin": 188744, "end": 188966, "id": "b14", "idx": 14}, {"begin": 188970, "end": 189446, "id": "b15", "idx": 15}, {"begin": 189450, "end": 189870, "id": "b16", "idx": 16}, {"begin": 189874, "end": 190169, "id": "b17", "idx": 17}, {"begin": 190173, "end": 190511, "id": "b18", "idx": 18}, {"begin": 190515, "end": 191042, "id": "b19", "idx": 19}, {"begin": 191046, "end": 191303, "id": "b20", "idx": 20}, {"begin": 191307, "end": 191743, "id": "b21", "idx": 21}, {"begin": 191747, "end": 192186, "id": "b22", "idx": 22}, {"begin": 192190, "end": 192684, "id": "b23", "idx": 23}, {"begin": 192688, "end": 193105, "id": "b24", "idx": 24}, {"begin": 193109, "end": 193433, "id": "b25", "idx": 25}, {"begin": 193437, "end": 193815, "id": "b26", "idx": 26}, {"begin": 193819, "end": 194316, "id": "b27", "idx": 27}, {"begin": 194320, "end": 194747, "id": "b28", "idx": 28}, {"begin": 194751, "end": 194982, "id": "b29", "idx": 29}, {"begin": 194986, "end": 195193, "id": "b30", "idx": 30}, {"begin": 195197, "end": 195373, "id": "b31", "idx": 31}, {"begin": 195377, "end": 195815, "id": "b32", "idx": 32}, {"begin": 195819, "end": 196196, "id": "b33", "idx": 33}, {"begin": 196200, "end": 196689, "id": "b34", "idx": 34}, {"begin": 196693, "end": 197071, "id": "b35", "idx": 35}, {"begin": 197075, "end": 197532, "id": "b36", "idx": 36}, {"begin": 197536, "end": 197955, "id": "b37", "idx": 37}, {"begin": 197959, "end": 198148, "id": "b38", "idx": 38}, {"begin": 198152, "end": 198557, "id": "b39", "idx": 39}, {"begin": 198561, "end": 198790, "id": "b40", "idx": 40}, {"begin": 198794, "end": 199287, "id": "b41", "idx": 41}, {"begin": 199291, "end": 199707, "id": "b42", "idx": 42}, {"begin": 199711, "end": 200058, "id": "b43", "idx": 43}, {"begin": 200062, "end": 200245, "id": "b44", "idx": 44}, {"begin": 200249, "end": 200705, "id": "b45", "idx": 45}, {"begin": 200709, "end": 200941, "id": "b46", "idx": 46}, {"begin": 200945, "end": 201324, "id": "b47", "idx": 47}, {"begin": 201328, "end": 201510, "id": "b48", "idx": 48}, {"begin": 201514, "end": 201969, "id": "b49", "idx": 49}, {"begin": 201973, "end": 202362, "id": "b50", "idx": 50}, {"begin": 202366, "end": 202816, "id": "b51", "idx": 51}, {"begin": 202820, "end": 203230, "id": "b52", "idx": 52}, {"begin": 203234, "end": 203666, "id": "b53", "idx": 53}, {"begin": 203670, "end": 204179, "id": "b54", "idx": 54}, {"begin": 204183, "end": 204404, "id": "b55", "idx": 55}, {"begin": 204408, "end": 204657, "id": "b56", "idx": 56}, {"begin": 204661, "end": 205175, "id": "b57", "idx": 57}, {"begin": 205179, "end": 205456, "id": "b58", "idx": 58}, {"begin": 205460, "end": 205930, "id": "b59", "idx": 59}, {"begin": 205934, "end": 206220, "id": "b60", "idx": 60}, {"begin": 206224, "end": 206505, "id": "b61", "idx": 61}, {"begin": 206509, "end": 206833, "id": "b62", "idx": 62}, {"begin": 206837, "end": 207245, "id": "b63", "idx": 63}, {"begin": 207249, "end": 207315, "id": "b64", "idx": 64}, {"begin": 207319, "end": 207436, "id": "b65", "idx": 65}, {"begin": 207440, "end": 207949, "id": "b66", "idx": 66}, {"begin": 207953, "end": 208308, "id": "b67", "idx": 67}, {"begin": 208312, "end": 208594, "id": "b68", "idx": 68}, {"begin": 208598, "end": 209057, "id": "b69", "idx": 69}, {"begin": 209061, "end": 209212, "id": "b70", "idx": 70}, {"begin": 209216, "end": 209586, "id": "b71", "idx": 71}, {"begin": 209590, "end": 209749, "id": "b72", "idx": 72}, {"begin": 209753, "end": 210261, "id": "b73", "idx": 73}, {"begin": 210265, "end": 210499, "id": "b74", "idx": 74}, {"begin": 210503, "end": 211181, "id": "b75", "idx": 75}, {"begin": 211185, "end": 211334, "id": "b76", "idx": 76}, {"begin": 211338, "end": 211576, "id": "b77", "idx": 77}, {"begin": 211580, "end": 211761, "id": "b78", "idx": 78}, {"begin": 211765, "end": 211996, "id": "b79", "idx": 79}, {"begin": 212000, "end": 212183, "id": "b80", "idx": 80}, {"begin": 212187, "end": 212415, "id": "b81", "idx": 81}, {"begin": 212419, "end": 212603, "id": "b82", "idx": 82}, {"begin": 212607, "end": 213126, "id": "b83", "idx": 83}, {"begin": 213130, "end": 213287, "id": "b84", "idx": 84}, {"begin": 213291, "end": 213461, "id": "b85", "idx": 85}, {"begin": 213465, "end": 213876, "id": "b86", "idx": 86}, {"begin": 213880, "end": 214297, "id": "b87", "idx": 87}, {"begin": 214301, "end": 214730, "id": "b88", "idx": 88}, {"begin": 214734, "end": 215117, "id": "b89", "idx": 89}, {"begin": 215121, "end": 215544, "id": "b90", "idx": 90}, {"begin": 215548, "end": 215923, "id": "b91", "idx": 91}, {"begin": 215927, "end": 216360, "id": "b92", "idx": 92}, {"begin": 216364, "end": 216665, "id": "b93", "idx": 93}, {"begin": 216669, "end": 217175, "id": "b94", "idx": 94}, {"begin": 217179, "end": 217285, "id": "b95", "idx": 95}, {"begin": 217289, "end": 217851, "id": "b96", "idx": 96}, {"begin": 217855, "end": 218278, "id": "b97", "idx": 97}, {"begin": 218282, "end": 218489, "id": "b98", "idx": 98}, {"begin": 218493, "end": 218600, "id": "b99", "idx": 99}, {"begin": 218604, "end": 219072, "id": "b100", "idx": 100}, {"begin": 219076, "end": 219569, "id": "b101", "idx": 101}, {"begin": 219573, "end": 219832, "id": "b102", "idx": 102}, {"begin": 219836, "end": 220181, "id": "b103", "idx": 103}, {"begin": 220185, "end": 220361, "id": "b104", "idx": 104}, {"begin": 220365, "end": 220765, "id": "b105", "idx": 105}, {"begin": 220769, "end": 221228, "id": "b106", "idx": 106}, {"begin": 221232, "end": 221372, "id": "b107", "idx": 107}, {"begin": 221376, "end": 221686, "id": "b108", "idx": 108}, {"begin": 221690, "end": 222072, "id": "b109", "idx": 109}, {"begin": 222076, "end": 222463, "id": "b110", "idx": 110}, {"begin": 222467, "end": 222821, "id": "b111", "idx": 111}, {"begin": 222825, "end": 223268, "id": "b112", "idx": 112}, {"begin": 223272, "end": 223616, "id": "b113", "idx": 113}, {"begin": 223620, "end": 223964, "id": "b114", "idx": 114}, {"begin": 223968, "end": 224127, "id": "b115", "idx": 115}, {"begin": 224131, "end": 224340, "id": "b116", "idx": 116}, {"begin": 224344, "end": 224792, "id": "b117", "idx": 117}, {"begin": 224796, "end": 225254, "id": "b118", "idx": 118}, {"begin": 225258, "end": 225621, "id": "b119", "idx": 119}, {"begin": 225625, "end": 226015, "id": "b120", "idx": 120}, {"begin": 226019, "end": 226227, "id": "b121", "idx": 121}, {"begin": 226231, "end": 226764, "id": "b122", "idx": 122}, {"begin": 226768, "end": 227198, "id": "b123", "idx": 123}, {"begin": 227202, "end": 227548, "id": "b124", "idx": 124}, {"begin": 227552, "end": 227783, "id": "b125", "idx": 125}, {"begin": 227787, "end": 228079, "id": "b126", "idx": 126}, {"begin": 228083, "end": 228471, "id": "b127", "idx": 127}, {"begin": 228475, "end": 228932, "id": "b128", "idx": 128}, {"begin": 228936, "end": 229424, "id": "b129", "idx": 129}, {"begin": 229428, "end": 229714, "id": "b130", "idx": 130}, {"begin": 229718, "end": 229971, "id": "b131", "idx": 131}, {"begin": 229975, "end": 230439, "id": "b132", "idx": 132}, {"begin": 230443, "end": 230879, "id": "b133", "idx": 133}, {"begin": 230883, "end": 231487, "id": "b134", "idx": 134}, {"begin": 231491, "end": 231964, "id": "b135", "idx": 135}, {"begin": 231968, "end": 232637, "id": "b136", "idx": 136}, {"begin": 232641, "end": 233072, "id": "b137", "idx": 137}, {"begin": 233076, "end": 233258, "id": "b138", "idx": 138}, {"begin": 233262, "end": 234003, "id": "b139", "idx": 139}, {"begin": 234007, "end": 234373, "id": "b140", "idx": 140}, {"begin": 234377, "end": 234855, "id": "b141", "idx": 141}, {"begin": 234859, "end": 235320, "id": "b142", "idx": 142}, {"begin": 235324, "end": 235775, "id": "b143", "idx": 143}, {"begin": 235779, "end": 236214, "id": "b144", "idx": 144}, {"begin": 236218, "end": 236549, "id": "b145", "idx": 145}, {"begin": 236553, "end": 236898, "id": "b146", "idx": 146}, {"begin": 236902, "end": 237118, "id": "b147", "idx": 147}, {"begin": 237122, "end": 237488, "id": "b148", "idx": 148}, {"begin": 237492, "end": 237937, "id": "b149", "idx": 149}, {"begin": 237941, "end": 238346, "id": "b150", "idx": 150}, {"begin": 238350, "end": 239196, "id": "b151", "idx": 151}, {"begin": 239200, "end": 239367, "id": "b152", "idx": 152}, {"begin": 239371, "end": 239526, "id": "b153", "idx": 153}, {"begin": 239530, "end": 239700, "id": "b154", "idx": 154}, {"begin": 239704, "end": 240010, "id": "b155", "idx": 155}, {"begin": 240014, "end": 240247, "id": "b156", "idx": 156}, {"begin": 240251, "end": 240444, "id": "b157", "idx": 157}, {"begin": 240448, "end": 240608, "id": "b158", "idx": 158}, {"begin": 240612, "end": 240761, "id": "b159", "idx": 159}, {"begin": 240765, "end": 241186, "id": "b160", "idx": 160}, {"begin": 241190, "end": 241350, "id": "b161", "idx": 161}, {"begin": 241354, "end": 241525, "id": "b162", "idx": 162}, {"begin": 241529, "end": 241902, "id": "b163", "idx": 163}, {"begin": 241906, "end": 242079, "id": "b164", "idx": 164}, {"begin": 242083, "end": 242490, "id": "b165", "idx": 165}, {"begin": 242494, "end": 242744, "id": "b166", "idx": 166}], "ReferenceToTable": [{"begin": 6849, "end": 6850, "target": "#tab_1", "idx": 0}, {"begin": 14838, "end": 14840, "target": "#tab_36", "idx": 1}, {"begin": 15604, "end": 15605, "target": "#tab_1", "idx": 2}, {"begin": 19422, "end": 19423, "target": "#tab_2", "idx": 3}, {"begin": 20363, "end": 20364, "target": "#tab_2", "idx": 4}, {"begin": 35526, "end": 35527, "target": "#tab_3", "idx": 5}, {"begin": 50779, "end": 50780, "target": "#tab_6", "idx": 6}, {"begin": 61212, "end": 61213, "target": "#tab_7", "idx": 7}, {"begin": 61926, "end": 61927, "target": "#tab_8", "idx": 8}, {"begin": 64707, "end": 64708, "target": "#tab_9", "idx": 9}, {"begin": 77554, "end": 77555, "target": "#tab_11", "idx": 10}, {"begin": 77606, "end": 77607, "target": "#tab_12", "idx": 11}, {"begin": 83225, "end": 83227, "target": "#tab_14", "idx": 12}, {"begin": 99624, "end": 99626, "target": "#tab_17", "idx": 13}, {"begin": 101865, "end": 101875, "target": "#tab_19", "idx": 14}, {"begin": 102701, "end": 102703, "target": "#tab_19", "idx": 15}, {"begin": 116327, "end": 116329, "target": "#tab_20", "idx": 16}, {"begin": 117807, "end": 117809, "target": "#tab_21", "idx": 17}, {"begin": 120357, "end": 120359, "target": "#tab_22", "idx": 18}, {"begin": 120817, "end": 120826, "target": "#tab_24", "idx": 19}, {"begin": 123029, "end": 123031, "target": "#tab_25", "idx": 20}, {"begin": 123535, "end": 123537, "target": "#tab_25", "idx": 21}, {"begin": 124854, "end": 124856, "target": "#tab_26", "idx": 22}, {"begin": 128616, "end": 128618, "target": "#tab_27", "idx": 23}, {"begin": 131246, "end": 131248, "target": "#tab_28", "idx": 24}, {"begin": 133615, "end": 133617, "target": "#tab_30", "idx": 25}, {"begin": 134172, "end": 134174, "target": "#tab_29", "idx": 26}, {"begin": 140426, "end": 140428, "target": "#tab_32", "idx": 27}, {"begin": 146913, "end": 146915, "target": "#tab_33", "idx": 28}, {"begin": 150171, "end": 150173, "target": "#tab_34", "idx": 29}, {"begin": 150913, "end": 150915, "target": "#tab_4", "idx": 30}, {"begin": 151319, "end": 151321, "target": "#tab_4", "idx": 31}, {"begin": 151329, "end": 151331, "target": "#tab_4", "idx": 32}, {"begin": 151992, "end": 151994, "target": "#tab_35", "idx": 33}, {"begin": 152403, "end": 152405, "target": "#tab_35", "idx": 34}, {"begin": 159824, "end": 159826, "target": "#tab_36", "idx": 35}, {"begin": 161907, "end": 161909, "target": "#tab_37", "idx": 36}, {"begin": 163231, "end": 163233, "target": "#tab_7", "idx": 37}, {"begin": 163241, "end": 163243, "target": "#tab_7", "idx": 38}], "Footnote": [{"begin": 178593, "end": 178688, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 178689, "end": 178802, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 178803, "end": 178994, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 178995, "end": 179045, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 179046, "end": 179099, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 179100, "end": 179147, "id": "foot_5", "n": "6", "idx": 5}, {"begin": 179148, "end": 179198, "id": "foot_6", "n": "7", "idx": 6}, {"begin": 179199, "end": 179247, "id": "foot_7", "n": "8", "idx": 7}, {"begin": 179248, "end": 179393, "id": "foot_8", "n": "9", "idx": 8}, {"begin": 179394, "end": 179670, "id": "foot_9", "n": "10", "idx": 9}, {"begin": 179671, "end": 179716, "id": "foot_10", "n": "11", "idx": 10}, {"begin": 179717, "end": 179768, "id": "foot_11", "n": "12", "idx": 11}, {"begin": 179769, "end": 179961, "id": "foot_12", "n": "13", "idx": 12}, {"begin": 179962, "end": 180000, "id": "foot_13", "n": "14", "idx": 13}, {"begin": 180001, "end": 180066, "id": "foot_14", "n": "15", "idx": 14}, {"begin": 180067, "end": 180132, "id": "foot_15", "n": "16", "idx": 15}, {"begin": 180133, "end": 180160, "id": "foot_16", "n": "17", "idx": 16}, {"begin": 180161, "end": 180187, "id": "foot_17", "n": "18", "idx": 17}, {"begin": 180188, "end": 180286, "id": "foot_18", "n": "19", "idx": 18}, {"begin": 180287, "end": 180316, "id": "foot_19", "n": "20", "idx": 19}, {"begin": 180317, "end": 180377, "id": "foot_20", "n": "21", "idx": 20}, {"begin": 180378, "end": 180404, "id": "foot_21", "n": "22", "idx": 21}, {"begin": 180405, "end": 180431, "id": "foot_22", "n": "23", "idx": 22}, {"begin": 180432, "end": 180466, "id": "foot_23", "n": "24", "idx": 23}, {"begin": 180467, "end": 180573, "id": "foot_24", "n": "25", "idx": 24}, {"begin": 180574, "end": 180734, "id": "foot_25", "n": "26", "idx": 25}, {"begin": 180735, "end": 180762, "id": "foot_26", "n": "27", "idx": 26}, {"begin": 180763, "end": 180944, "id": "foot_27", "n": "28", "idx": 27}, {"begin": 180945, "end": 180978, "id": "foot_28", "n": "29", "idx": 28}, {"begin": 180979, "end": 181000, "id": "foot_29", "n": "30", "idx": 29}, {"begin": 181001, "end": 181039, "id": "foot_30", "n": "31", "idx": 30}, {"begin": 181040, "end": 181081, "id": "foot_31", "n": "32", "idx": 31}, {"begin": 181082, "end": 181116, "id": "foot_32", "n": "33", "idx": 32}, {"begin": 181117, "end": 181214, "id": "foot_33", "n": "34", "idx": 33}, {"begin": 181215, "end": 181242, "id": "foot_34", "n": "35", "idx": 34}, {"begin": 181243, "end": 181387, "id": "foot_35", "n": "36", "idx": 35}, {"begin": 181388, "end": 181523, "id": "foot_36", "n": "37", "idx": 36}, {"begin": 181524, "end": 181551, "id": "foot_37", "n": "38", "idx": 37}, {"begin": 181552, "end": 181591, "id": "foot_38", "n": "39", "idx": 38}, {"begin": 181592, "end": 181629, "id": "foot_39", "n": "40", "idx": 39}, {"begin": 181630, "end": 181657, "id": "foot_40", "n": "41", "idx": 40}, {"begin": 181658, "end": 181684, "id": "foot_41", "n": "42", "idx": 41}, {"begin": 181685, "end": 181829, "id": "foot_42", "n": "43", "idx": 42}, {"begin": 181830, "end": 181865, "id": "foot_43", "n": "44", "idx": 43}, {"begin": 181866, "end": 181891, "id": "foot_44", "n": "45", "idx": 44}, {"begin": 181892, "end": 181927, "id": "foot_45", "n": "46", "idx": 45}, {"begin": 181928, "end": 182006, "id": "foot_46", "n": "47", "idx": 46}, {"begin": 182007, "end": 182089, "id": "foot_47", "n": "48", "idx": 47}, {"begin": 182090, "end": 182122, "id": "foot_48", "n": "49", "idx": 48}, {"begin": 182123, "end": 182239, "id": "foot_49", "n": "50", "idx": 49}, {"begin": 182240, "end": 182246, "id": "foot_50", "n": "51", "idx": 50}, {"begin": 182247, "end": 182314, "id": "foot_51", "n": "51", "idx": 51}, {"begin": 182315, "end": 182347, "id": "foot_52", "n": "52", "idx": 52}, {"begin": 182348, "end": 182382, "id": "foot_53", "n": "53", "idx": 53}, {"begin": 182383, "end": 182409, "id": "foot_54", "n": "54", "idx": 54}, {"begin": 182410, "end": 182452, "id": "foot_55", "n": "55", "idx": 55}, {"begin": 182453, "end": 182530, "id": "foot_56", "n": "56", "idx": 56}, {"begin": 182531, "end": 182587, "id": "foot_57", "n": "57", "idx": 57}, {"begin": 182588, "end": 182645, "id": "foot_58", "n": "62", "idx": 58}, {"begin": 182646, "end": 182733, "id": "foot_59", "n": "63", "idx": 59}, {"begin": 182734, "end": 182815, "id": "foot_60", "n": "64", "idx": 60}, {"begin": 182816, "end": 182865, "id": "foot_61", "n": "65", "idx": 61}, {"begin": 182866, "end": 182901, "id": "foot_62", "n": "66", "idx": 62}, {"begin": 182902, "end": 182976, "id": "foot_63", "n": "67", "idx": 63}, {"begin": 182977, "end": 183015, "id": "foot_64", "n": "68", "idx": 64}, {"begin": 183016, "end": 183065, "id": "foot_65", "n": "69", "idx": 65}, {"begin": 183066, "end": 183111, "id": "foot_66", "n": "70", "idx": 66}], "Paragraph": [{"begin": 58, "end": 1905, "idx": 0}, {"begin": 1923, "end": 2565, "idx": 1}, {"begin": 2566, "end": 3542, "idx": 2}, {"begin": 3543, "end": 4198, "idx": 3}, {"begin": 4199, "end": 4309, "idx": 4}, {"begin": 4323, "end": 4901, "idx": 5}, {"begin": 4902, "end": 4975, "idx": 6}, {"begin": 4976, "end": 6653, "idx": 7}, {"begin": 6654, "end": 6914, "idx": 8}, {"begin": 6915, "end": 6991, "idx": 9}, {"begin": 6992, "end": 7081, "idx": 10}, {"begin": 7082, "end": 7177, "idx": 11}, {"begin": 7178, "end": 7245, "idx": 12}, {"begin": 7246, "end": 7338, "idx": 13}, {"begin": 7339, "end": 7412, "idx": 14}, {"begin": 7413, "end": 7512, "idx": 15}, {"begin": 7513, "end": 7590, "idx": 16}, {"begin": 7591, "end": 8018, "idx": 17}, {"begin": 8019, "end": 8059, "idx": 18}, {"begin": 8060, "end": 8071, "idx": 19}, {"begin": 8072, "end": 8082, "idx": 20}, {"begin": 8083, "end": 8093, "idx": 21}, {"begin": 8094, "end": 8107, "idx": 22}, {"begin": 8108, "end": 8117, "idx": 23}, {"begin": 8118, "end": 8134, "idx": 24}, {"begin": 8135, "end": 8165, "idx": 25}, {"begin": 8166, "end": 8198, "idx": 26}, {"begin": 8215, "end": 8415, "idx": 27}, {"begin": 8446, "end": 8574, "idx": 28}, {"begin": 8575, "end": 9041, "idx": 29}, {"begin": 9042, "end": 9414, "idx": 30}, {"begin": 9415, "end": 10411, "idx": 31}, {"begin": 10424, "end": 10675, "idx": 32}, {"begin": 10676, "end": 11158, "idx": 33}, {"begin": 11159, "end": 11579, "idx": 34}, {"begin": 11580, "end": 11789, "idx": 35}, {"begin": 11790, "end": 12201, "idx": 36}, {"begin": 12202, "end": 12796, "idx": 37}, {"begin": 12797, "end": 14254, "idx": 38}, {"begin": 14255, "end": 15580, "idx": 39}, {"begin": 15581, "end": 15887, "idx": 40}, {"begin": 15906, "end": 16405, "idx": 41}, {"begin": 16438, "end": 17016, "idx": 42}, {"begin": 17017, "end": 17574, "idx": 43}, {"begin": 17575, "end": 17745, "idx": 44}, {"begin": 17746, "end": 18087, "idx": 45}, {"begin": 18088, "end": 18848, "idx": 46}, {"begin": 18849, "end": 19424, "idx": 47}, {"begin": 19425, "end": 20365, "idx": 48}, {"begin": 20388, "end": 20603, "idx": 49}, {"begin": 20604, "end": 20649, "idx": 50}, {"begin": 20650, "end": 20972, "idx": 51}, {"begin": 20996, "end": 21331, "idx": 52}, {"begin": 21332, "end": 21613, "idx": 53}, {"begin": 21614, "end": 22340, "idx": 54}, {"begin": 22341, "end": 22799, "idx": 55}, {"begin": 22800, "end": 23132, "idx": 56}, {"begin": 23133, "end": 23585, "idx": 57}, {"begin": 23586, "end": 23929, "idx": 58}, {"begin": 23930, "end": 24440, "idx": 59}, {"begin": 24441, "end": 25193, "idx": 60}, {"begin": 25220, "end": 25408, "idx": 61}, {"begin": 25409, "end": 26453, "idx": 62}, {"begin": 26454, "end": 26809, "idx": 63}, {"begin": 26810, "end": 27016, "idx": 64}, {"begin": 27017, "end": 27176, "idx": 65}, {"begin": 27177, "end": 27443, "idx": 66}, {"begin": 27444, "end": 27862, "idx": 67}, {"begin": 27863, "end": 28066, "idx": 68}, {"begin": 28086, "end": 28595, "idx": 69}, {"begin": 28613, "end": 29120, "idx": 70}, {"begin": 29121, "end": 30000, "idx": 71}, {"begin": 30014, "end": 30242, "idx": 72}, {"begin": 30264, "end": 30543, "idx": 73}, {"begin": 30544, "end": 30663, "idx": 74}, {"begin": 30664, "end": 31863, "idx": 75}, {"begin": 31882, "end": 32438, "idx": 76}, {"begin": 32470, "end": 32743, "idx": 77}, {"begin": 32744, "end": 33192, "idx": 78}, {"begin": 33193, "end": 33478, "idx": 79}, {"begin": 33507, "end": 33829, "idx": 80}, {"begin": 33830, "end": 34432, "idx": 81}, {"begin": 34433, "end": 34773, "idx": 82}, {"begin": 34774, "end": 35085, "idx": 83}, {"begin": 35086, "end": 35528, "idx": 84}, {"begin": 35529, "end": 35841, "idx": 85}, {"begin": 35842, "end": 36050, "idx": 86}, {"begin": 36071, "end": 36817, "idx": 87}, {"begin": 36818, "end": 37229, "idx": 88}, {"begin": 37230, "end": 37588, "idx": 89}, {"begin": 37589, "end": 37796, "idx": 90}, {"begin": 37797, "end": 38338, "idx": 91}, {"begin": 38364, "end": 38576, "idx": 92}, {"begin": 38597, "end": 39485, "idx": 93}, {"begin": 39486, "end": 40029, "idx": 94}, {"begin": 40048, "end": 40637, "idx": 95}, {"begin": 40677, "end": 41136, "idx": 96}, {"begin": 41137, "end": 41586, "idx": 97}, {"begin": 41615, "end": 41839, "idx": 98}, {"begin": 41840, "end": 42432, "idx": 99}, {"begin": 42433, "end": 42669, "idx": 100}, {"begin": 42670, "end": 43228, "idx": 101}, {"begin": 43229, "end": 44814, "idx": 102}, {"begin": 44815, "end": 45468, "idx": 103}, {"begin": 45469, "end": 45549, "idx": 104}, {"begin": 45550, "end": 45625, "idx": 105}, {"begin": 45626, "end": 45774, "idx": 106}, {"begin": 45775, "end": 45902, "idx": 107}, {"begin": 45903, "end": 46045, "idx": 108}, {"begin": 46046, "end": 46161, "idx": 109}, {"begin": 46162, "end": 46866, "idx": 110}, {"begin": 46867, "end": 47516, "idx": 111}, {"begin": 47517, "end": 47944, "idx": 112}, {"begin": 47945, "end": 48899, "idx": 113}, {"begin": 48900, "end": 49512, "idx": 114}, {"begin": 49513, "end": 50545, "idx": 115}, {"begin": 50546, "end": 50971, "idx": 116}, {"begin": 50998, "end": 52061, "idx": 117}, {"begin": 52062, "end": 52573, "idx": 118}, {"begin": 52574, "end": 52975, "idx": 119}, {"begin": 52976, "end": 53974, "idx": 120}, {"begin": 53993, "end": 54509, "idx": 121}, {"begin": 54548, "end": 54892, "idx": 122}, {"begin": 54893, "end": 55148, "idx": 123}, {"begin": 55149, "end": 55410, "idx": 124}, {"begin": 55439, "end": 56098, "idx": 125}, {"begin": 56099, "end": 56548, "idx": 126}, {"begin": 56549, "end": 57170, "idx": 127}, {"begin": 57171, "end": 57465, "idx": 128}, {"begin": 57466, "end": 57694, "idx": 129}, {"begin": 57695, "end": 58170, "idx": 130}, {"begin": 58171, "end": 58822, "idx": 131}, {"begin": 58823, "end": 59341, "idx": 132}, {"begin": 59342, "end": 59922, "idx": 133}, {"begin": 59923, "end": 60436, "idx": 134}, {"begin": 60437, "end": 60899, "idx": 135}, {"begin": 60900, "end": 61168, "idx": 136}, {"begin": 61169, "end": 61801, "idx": 137}, {"begin": 61817, "end": 62184, "idx": 138}, {"begin": 62185, "end": 62422, "idx": 139}, {"begin": 62423, "end": 62926, "idx": 140}, {"begin": 62927, "end": 63574, "idx": 141}, {"begin": 63575, "end": 63999, "idx": 142}, {"begin": 64040, "end": 64709, "idx": 143}, {"begin": 64710, "end": 65396, "idx": 144}, {"begin": 65397, "end": 66135, "idx": 145}, {"begin": 66136, "end": 66657, "idx": 146}, {"begin": 66683, "end": 66959, "idx": 147}, {"begin": 66980, "end": 67326, "idx": 148}, {"begin": 67327, "end": 67433, "idx": 149}, {"begin": 67434, "end": 67913, "idx": 150}, {"begin": 67914, "end": 68745, "idx": 151}, {"begin": 68746, "end": 69497, "idx": 152}, {"begin": 69498, "end": 69893, "idx": 153}, {"begin": 69894, "end": 70573, "idx": 154}, {"begin": 70592, "end": 70973, "idx": 155}, {"begin": 70974, "end": 71426, "idx": 156}, {"begin": 71427, "end": 71617, "idx": 157}, {"begin": 71654, "end": 71918, "idx": 158}, {"begin": 71919, "end": 72379, "idx": 159}, {"begin": 72380, "end": 72761, "idx": 160}, {"begin": 72790, "end": 73318, "idx": 161}, {"begin": 73319, "end": 73978, "idx": 162}, {"begin": 73979, "end": 74427, "idx": 163}, {"begin": 74428, "end": 74635, "idx": 164}, {"begin": 74636, "end": 75216, "idx": 165}, {"begin": 75217, "end": 76092, "idx": 166}, {"begin": 76093, "end": 76628, "idx": 167}, {"begin": 76629, "end": 77064, "idx": 168}, {"begin": 77065, "end": 77422, "idx": 169}, {"begin": 77423, "end": 77747, "idx": 170}, {"begin": 77748, "end": 78394, "idx": 171}, {"begin": 78421, "end": 78653, "idx": 172}, {"begin": 78674, "end": 79299, "idx": 173}, {"begin": 79300, "end": 79549, "idx": 174}, {"begin": 79550, "end": 80184, "idx": 175}, {"begin": 80185, "end": 81105, "idx": 176}, {"begin": 81124, "end": 81722, "idx": 177}, {"begin": 81753, "end": 82214, "idx": 178}, {"begin": 82215, "end": 82555, "idx": 179}, {"begin": 82556, "end": 83376, "idx": 180}, {"begin": 83402, "end": 83730, "idx": 181}, {"begin": 83731, "end": 84474, "idx": 182}, {"begin": 84475, "end": 85247, "idx": 183}, {"begin": 85248, "end": 85490, "idx": 184}, {"begin": 85523, "end": 86026, "idx": 185}, {"begin": 86054, "end": 86132, "idx": 186}, {"begin": 86133, "end": 86230, "idx": 187}, {"begin": 86231, "end": 86337, "idx": 188}, {"begin": 86338, "end": 86407, "idx": 189}, {"begin": 86408, "end": 86667, "idx": 190}, {"begin": 86668, "end": 86760, "idx": 191}, {"begin": 86761, "end": 86951, "idx": 192}, {"begin": 86952, "end": 87013, "idx": 193}, {"begin": 87014, "end": 87072, "idx": 194}, {"begin": 87073, "end": 87137, "idx": 195}, {"begin": 87138, "end": 87202, "idx": 196}, {"begin": 87203, "end": 87256, "idx": 197}, {"begin": 87257, "end": 87327, "idx": 198}, {"begin": 87328, "end": 87463, "idx": 199}, {"begin": 87464, "end": 87562, "idx": 200}, {"begin": 87563, "end": 87611, "idx": 201}, {"begin": 87612, "end": 87661, "idx": 202}, {"begin": 87662, "end": 87721, "idx": 203}, {"begin": 87722, "end": 88014, "idx": 204}, {"begin": 88015, "end": 88170, "idx": 205}, {"begin": 88171, "end": 88713, "idx": 206}, {"begin": 88714, "end": 88882, "idx": 207}, {"begin": 88883, "end": 88999, "idx": 208}, {"begin": 89000, "end": 89169, "idx": 209}, {"begin": 89190, "end": 89857, "idx": 210}, {"begin": 89882, "end": 90084, "idx": 211}, {"begin": 90085, "end": 90578, "idx": 212}, {"begin": 90579, "end": 90767, "idx": 213}, {"begin": 90768, "end": 91377, "idx": 214}, {"begin": 91401, "end": 92126, "idx": 215}, {"begin": 92127, "end": 93061, "idx": 216}, {"begin": 93093, "end": 93864, "idx": 217}, {"begin": 93885, "end": 94974, "idx": 218}, {"begin": 94975, "end": 95649, "idx": 219}, {"begin": 95668, "end": 96209, "idx": 220}, {"begin": 96239, "end": 96573, "idx": 221}, {"begin": 96574, "end": 97007, "idx": 222}, {"begin": 97008, "end": 97471, "idx": 223}, {"begin": 97472, "end": 97920, "idx": 224}, {"begin": 97921, "end": 98037, "idx": 225}, {"begin": 98038, "end": 98321, "idx": 226}, {"begin": 98350, "end": 98928, "idx": 227}, {"begin": 98929, "end": 99549, "idx": 228}, {"begin": 99550, "end": 100339, "idx": 229}, {"begin": 100340, "end": 100565, "idx": 230}, {"begin": 100566, "end": 101329, "idx": 231}, {"begin": 101330, "end": 101536, "idx": 232}, {"begin": 101537, "end": 101912, "idx": 233}, {"begin": 101939, "end": 102704, "idx": 234}, {"begin": 102725, "end": 103206, "idx": 235}, {"begin": 103207, "end": 104012, "idx": 236}, {"begin": 104013, "end": 104979, "idx": 237}, {"begin": 104980, "end": 105820, "idx": 238}, {"begin": 105821, "end": 107317, "idx": 239}, {"begin": 107336, "end": 107987, "idx": 240}, {"begin": 108029, "end": 108227, "idx": 241}, {"begin": 108228, "end": 108883, "idx": 242}, {"begin": 108884, "end": 109389, "idx": 243}, {"begin": 109390, "end": 109678, "idx": 244}, {"begin": 109679, "end": 109971, "idx": 245}, {"begin": 109972, "end": 110180, "idx": 246}, {"begin": 110181, "end": 110470, "idx": 247}, {"begin": 110471, "end": 110852, "idx": 248}, {"begin": 110881, "end": 111562, "idx": 249}, {"begin": 111563, "end": 112048, "idx": 250}, {"begin": 112049, "end": 112788, "idx": 251}, {"begin": 112811, "end": 113031, "idx": 252}, {"begin": 113032, "end": 113050, "idx": 253}, {"begin": 113051, "end": 113362, "idx": 254}, {"begin": 113363, "end": 113612, "idx": 255}, {"begin": 113613, "end": 113867, "idx": 256}, {"begin": 113868, "end": 114139, "idx": 257}, {"begin": 114140, "end": 114539, "idx": 258}, {"begin": 114540, "end": 114557, "idx": 259}, {"begin": 114558, "end": 114797, "idx": 260}, {"begin": 114798, "end": 114964, "idx": 261}, {"begin": 114994, "end": 115140, "idx": 262}, {"begin": 115178, "end": 115453, "idx": 263}, {"begin": 115469, "end": 115720, "idx": 264}, {"begin": 115721, "end": 115811, "idx": 265}, {"begin": 115812, "end": 116098, "idx": 266}, {"begin": 116099, "end": 116218, "idx": 267}, {"begin": 116219, "end": 116691, "idx": 268}, {"begin": 116735, "end": 117372, "idx": 269}, {"begin": 117373, "end": 117462, "idx": 270}, {"begin": 117463, "end": 117551, "idx": 271}, {"begin": 117552, "end": 117686, "idx": 272}, {"begin": 117687, "end": 117693, "idx": 273}, {"begin": 117741, "end": 117785, "idx": 274}, {"begin": 117786, "end": 118773, "idx": 275}, {"begin": 118812, "end": 119308, "idx": 276}, {"begin": 119309, "end": 119454, "idx": 277}, {"begin": 119455, "end": 119691, "idx": 278}, {"begin": 119692, "end": 120034, "idx": 279}, {"begin": 120035, "end": 120868, "idx": 280}, {"begin": 120895, "end": 122275, "idx": 281}, {"begin": 122292, "end": 122762, "idx": 282}, {"begin": 122811, "end": 123155, "idx": 283}, {"begin": 123156, "end": 123681, "idx": 284}, {"begin": 123682, "end": 124546, "idx": 285}, {"begin": 124565, "end": 124921, "idx": 286}, {"begin": 124942, "end": 125150, "idx": 287}, {"begin": 125151, "end": 125906, "idx": 288}, {"begin": 125907, "end": 126513, "idx": 289}, {"begin": 126514, "end": 126718, "idx": 290}, {"begin": 126719, "end": 127830, "idx": 291}, {"begin": 127849, "end": 128230, "idx": 292}, {"begin": 128266, "end": 128789, "idx": 293}, {"begin": 128790, "end": 129208, "idx": 294}, {"begin": 129209, "end": 129780, "idx": 295}, {"begin": 129781, "end": 129928, "idx": 296}, {"begin": 129957, "end": 130552, "idx": 297}, {"begin": 130553, "end": 131013, "idx": 298}, {"begin": 131039, "end": 131249, "idx": 299}, {"begin": 131250, "end": 131757, "idx": 300}, {"begin": 131758, "end": 132882, "idx": 301}, {"begin": 132911, "end": 133658, "idx": 302}, {"begin": 133695, "end": 133816, "idx": 303}, {"begin": 133817, "end": 134175, "idx": 304}, {"begin": 134176, "end": 134767, "idx": 305}, {"begin": 134768, "end": 136390, "idx": 306}, {"begin": 136409, "end": 137511, "idx": 307}, {"begin": 137512, "end": 138183, "idx": 308}, {"begin": 138209, "end": 139042, "idx": 309}, {"begin": 139043, "end": 139639, "idx": 310}, {"begin": 139666, "end": 140241, "idx": 311}, {"begin": 140258, "end": 140504, "idx": 312}, {"begin": 140525, "end": 141113, "idx": 313}, {"begin": 141114, "end": 142059, "idx": 314}, {"begin": 142060, "end": 143169, "idx": 315}, {"begin": 143170, "end": 143644, "idx": 316}, {"begin": 143645, "end": 144274, "idx": 317}, {"begin": 144275, "end": 145118, "idx": 318}, {"begin": 145137, "end": 145783, "idx": 319}, {"begin": 145814, "end": 146237, "idx": 320}, {"begin": 146259, "end": 146939, "idx": 321}, {"begin": 146940, "end": 147143, "idx": 322}, {"begin": 147144, "end": 147326, "idx": 323}, {"begin": 147327, "end": 147633, "idx": 324}, {"begin": 147634, "end": 148361, "idx": 325}, {"begin": 148362, "end": 149624, "idx": 326}, {"begin": 149625, "end": 150174, "idx": 327}, {"begin": 150175, "end": 150916, "idx": 328}, {"begin": 150917, "end": 151322, "idx": 329}, {"begin": 151323, "end": 151818, "idx": 330}, {"begin": 151852, "end": 152738, "idx": 331}, {"begin": 153082, "end": 153250, "idx": 332}, {"begin": 153251, "end": 153541, "idx": 333}, {"begin": 153542, "end": 153950, "idx": 334}, {"begin": 153951, "end": 154214, "idx": 335}, {"begin": 154253, "end": 154587, "idx": 336}, {"begin": 154588, "end": 155060, "idx": 337}, {"begin": 155110, "end": 155334, "idx": 338}, {"begin": 155335, "end": 155818, "idx": 339}, {"begin": 155866, "end": 156682, "idx": 340}, {"begin": 156707, "end": 157947, "idx": 341}, {"begin": 157948, "end": 159058, "idx": 342}, {"begin": 159091, "end": 159674, "idx": 343}, {"begin": 159699, "end": 160245, "idx": 344}, {"begin": 160246, "end": 161393, "idx": 345}, {"begin": 161418, "end": 161710, "idx": 346}, {"begin": 161711, "end": 163234, "idx": 347}, {"begin": 163235, "end": 164124, "idx": 348}, {"begin": 164151, "end": 164505, "idx": 349}, {"begin": 164539, "end": 165097, "idx": 350}, {"begin": 165098, "end": 165746, "idx": 351}, {"begin": 165747, "end": 166178, "idx": 352}, {"begin": 166179, "end": 166875, "idx": 353}, {"begin": 166876, "end": 166969, "idx": 354}, {"begin": 166989, "end": 167392, "idx": 355}, {"begin": 167393, "end": 168161, "idx": 356}, {"begin": 168162, "end": 168881, "idx": 357}, {"begin": 168923, "end": 169272, "idx": 358}, {"begin": 169273, "end": 169884, "idx": 359}, {"begin": 169901, "end": 171416, "idx": 360}, {"begin": 171417, "end": 172125, "idx": 361}, {"begin": 172126, "end": 172859, "idx": 362}, {"begin": 172860, "end": 173468, "idx": 363}, {"begin": 173469, "end": 174414, "idx": 364}, {"begin": 174415, "end": 174873, "idx": 365}, {"begin": 174874, "end": 175381, "idx": 366}, {"begin": 175396, "end": 176940, "idx": 367}, {"begin": 176941, "end": 178298, "idx": 368}, {"begin": 178299, "end": 178580, "idx": 369}], "SectionHeader": [{"begin": 0, "end": 1905, "idx": 0}], "SectionReference": [{"begin": 183113, "end": 242746, "idx": 0}], "Sentence": [{"begin": 58, "end": 129, "idx": 0}, {"begin": 130, "end": 422, "idx": 1}, {"begin": 423, "end": 579, "idx": 2}, {"begin": 580, "end": 658, "idx": 3}, {"begin": 659, "end": 808, "idx": 4}, {"begin": 809, "end": 981, "idx": 5}, {"begin": 982, "end": 1160, "idx": 6}, {"begin": 1161, "end": 1289, "idx": 7}, {"begin": 1290, "end": 1501, "idx": 8}, {"begin": 1502, "end": 1679, "idx": 9}, {"begin": 1680, "end": 1855, "idx": 10}, {"begin": 1856, "end": 1905, "idx": 11}, {"begin": 1923, "end": 2228, "idx": 12}, {"begin": 2229, "end": 2400, "idx": 13}, {"begin": 2401, "end": 2565, "idx": 14}, {"begin": 2566, "end": 2765, "idx": 15}, {"begin": 2766, "end": 2818, "idx": 16}, {"begin": 2819, "end": 3087, "idx": 17}, {"begin": 3088, "end": 3295, "idx": 18}, {"begin": 3296, "end": 3350, "idx": 19}, {"begin": 3351, "end": 3542, "idx": 20}, {"begin": 3543, "end": 3729, "idx": 21}, {"begin": 3730, "end": 4006, "idx": 22}, {"begin": 4007, "end": 4198, "idx": 23}, {"begin": 4199, "end": 4309, "idx": 24}, {"begin": 4323, "end": 4576, "idx": 25}, {"begin": 4577, "end": 4703, "idx": 26}, {"begin": 4704, "end": 4901, "idx": 27}, {"begin": 4902, "end": 4975, "idx": 28}, {"begin": 4976, "end": 5251, "idx": 29}, {"begin": 5252, "end": 5444, "idx": 30}, {"begin": 5445, "end": 5594, "idx": 31}, {"begin": 5595, "end": 5829, "idx": 32}, {"begin": 5830, "end": 5997, "idx": 33}, {"begin": 5998, "end": 6175, "idx": 34}, {"begin": 6176, "end": 6426, "idx": 35}, {"begin": 6427, "end": 6576, "idx": 36}, {"begin": 6577, "end": 6653, "idx": 37}, {"begin": 6654, "end": 6838, "idx": 38}, {"begin": 6839, "end": 6914, "idx": 39}, {"begin": 6915, "end": 6991, "idx": 40}, {"begin": 6992, "end": 7081, "idx": 41}, {"begin": 7082, "end": 7177, "idx": 42}, {"begin": 7178, "end": 7245, "idx": 43}, {"begin": 7246, "end": 7338, "idx": 44}, {"begin": 7339, "end": 7412, "idx": 45}, {"begin": 7413, "end": 7512, "idx": 46}, {"begin": 7513, "end": 7590, "idx": 47}, {"begin": 7591, "end": 7736, "idx": 48}, {"begin": 7737, "end": 7850, "idx": 49}, {"begin": 7851, "end": 8018, "idx": 50}, {"begin": 8019, "end": 8059, "idx": 51}, {"begin": 8060, "end": 8071, "idx": 52}, {"begin": 8072, "end": 8082, "idx": 53}, {"begin": 8083, "end": 8093, "idx": 54}, {"begin": 8094, "end": 8107, "idx": 55}, {"begin": 8108, "end": 8117, "idx": 56}, {"begin": 8118, "end": 8134, "idx": 57}, {"begin": 8135, "end": 8165, "idx": 58}, {"begin": 8166, "end": 8198, "idx": 59}, {"begin": 8215, "end": 8336, "idx": 60}, {"begin": 8337, "end": 8415, "idx": 61}, {"begin": 8446, "end": 8506, "idx": 62}, {"begin": 8507, "end": 8574, "idx": 63}, {"begin": 8575, "end": 8942, "idx": 64}, {"begin": 8943, "end": 9041, "idx": 65}, {"begin": 9042, "end": 9312, "idx": 66}, {"begin": 9313, "end": 9414, "idx": 67}, {"begin": 9415, "end": 9673, "idx": 68}, {"begin": 9674, "end": 9738, "idx": 69}, {"begin": 9739, "end": 10013, "idx": 70}, {"begin": 10014, "end": 10109, "idx": 71}, {"begin": 10110, "end": 10347, "idx": 72}, {"begin": 10348, "end": 10411, "idx": 73}, {"begin": 10424, "end": 10576, "idx": 74}, {"begin": 10577, "end": 10675, "idx": 75}, {"begin": 10676, "end": 10816, "idx": 76}, {"begin": 10817, "end": 11052, "idx": 77}, {"begin": 11053, "end": 11158, "idx": 78}, {"begin": 11159, "end": 11309, "idx": 79}, {"begin": 11310, "end": 11371, "idx": 80}, {"begin": 11372, "end": 11468, "idx": 81}, {"begin": 11469, "end": 11579, "idx": 82}, {"begin": 11580, "end": 11789, "idx": 83}, {"begin": 11790, "end": 11992, "idx": 84}, {"begin": 11993, "end": 12201, "idx": 85}, {"begin": 12202, "end": 12385, "idx": 86}, {"begin": 12386, "end": 12416, "idx": 87}, {"begin": 12417, "end": 12633, "idx": 88}, {"begin": 12634, "end": 12796, "idx": 89}, {"begin": 12797, "end": 12971, "idx": 90}, {"begin": 12972, "end": 13172, "idx": 91}, {"begin": 13173, "end": 13256, "idx": 92}, {"begin": 13257, "end": 13511, "idx": 93}, {"begin": 13512, "end": 13615, "idx": 94}, {"begin": 13616, "end": 13769, "idx": 95}, {"begin": 13770, "end": 13920, "idx": 96}, {"begin": 13921, "end": 14073, "idx": 97}, {"begin": 14074, "end": 14162, "idx": 98}, {"begin": 14163, "end": 14254, "idx": 99}, {"begin": 14255, "end": 14417, "idx": 100}, {"begin": 14418, "end": 14603, "idx": 101}, {"begin": 14604, "end": 14801, "idx": 102}, {"begin": 14802, "end": 14891, "idx": 103}, {"begin": 14892, "end": 14972, "idx": 104}, {"begin": 14973, "end": 15189, "idx": 105}, {"begin": 15190, "end": 15327, "idx": 106}, {"begin": 15328, "end": 15580, "idx": 107}, {"begin": 15581, "end": 15748, "idx": 108}, {"begin": 15749, "end": 15887, "idx": 109}, {"begin": 15906, "end": 16039, "idx": 110}, {"begin": 16040, "end": 16203, "idx": 111}, {"begin": 16204, "end": 16301, "idx": 112}, {"begin": 16302, "end": 16405, "idx": 113}, {"begin": 16438, "end": 16578, "idx": 114}, {"begin": 16579, "end": 16616, "idx": 115}, {"begin": 16617, "end": 16787, "idx": 116}, {"begin": 16788, "end": 16838, "idx": 117}, {"begin": 16839, "end": 17016, "idx": 118}, {"begin": 17017, "end": 17250, "idx": 119}, {"begin": 17251, "end": 17426, "idx": 120}, {"begin": 17427, "end": 17492, "idx": 121}, {"begin": 17493, "end": 17574, "idx": 122}, {"begin": 17575, "end": 17745, "idx": 123}, {"begin": 17746, "end": 17982, "idx": 124}, {"begin": 17983, "end": 18087, "idx": 125}, {"begin": 18088, "end": 18260, "idx": 126}, {"begin": 18261, "end": 18441, "idx": 127}, {"begin": 18442, "end": 18750, "idx": 128}, {"begin": 18751, "end": 18848, "idx": 129}, {"begin": 18849, "end": 19290, "idx": 130}, {"begin": 19291, "end": 19396, "idx": 131}, {"begin": 19397, "end": 19424, "idx": 132}, {"begin": 19425, "end": 19570, "idx": 133}, {"begin": 19571, "end": 19946, "idx": 134}, {"begin": 19947, "end": 20033, "idx": 135}, {"begin": 20034, "end": 20239, "idx": 136}, {"begin": 20240, "end": 20331, "idx": 137}, {"begin": 20332, "end": 20365, "idx": 138}, {"begin": 20388, "end": 20490, "idx": 139}, {"begin": 20491, "end": 20603, "idx": 140}, {"begin": 20604, "end": 20649, "idx": 141}, {"begin": 20650, "end": 20765, "idx": 142}, {"begin": 20766, "end": 20864, "idx": 143}, {"begin": 20865, "end": 20972, "idx": 144}, {"begin": 20996, "end": 21201, "idx": 145}, {"begin": 21202, "end": 21263, "idx": 146}, {"begin": 21264, "end": 21331, "idx": 147}, {"begin": 21332, "end": 21484, "idx": 148}, {"begin": 21485, "end": 21613, "idx": 149}, {"begin": 21614, "end": 21767, "idx": 150}, {"begin": 21768, "end": 22074, "idx": 151}, {"begin": 22075, "end": 22166, "idx": 152}, {"begin": 22167, "end": 22219, "idx": 153}, {"begin": 22220, "end": 22304, "idx": 154}, {"begin": 22305, "end": 22340, "idx": 155}, {"begin": 22341, "end": 22487, "idx": 156}, {"begin": 22488, "end": 22570, "idx": 157}, {"begin": 22571, "end": 22747, "idx": 158}, {"begin": 22748, "end": 22799, "idx": 159}, {"begin": 22800, "end": 22944, "idx": 160}, {"begin": 22945, "end": 23095, "idx": 161}, {"begin": 23096, "end": 23132, "idx": 162}, {"begin": 23133, "end": 23251, "idx": 163}, {"begin": 23252, "end": 23373, "idx": 164}, {"begin": 23374, "end": 23488, "idx": 165}, {"begin": 23489, "end": 23554, "idx": 166}, {"begin": 23555, "end": 23585, "idx": 167}, {"begin": 23586, "end": 23682, "idx": 168}, {"begin": 23683, "end": 23723, "idx": 169}, {"begin": 23724, "end": 23803, "idx": 170}, {"begin": 23804, "end": 23839, "idx": 171}, {"begin": 23840, "end": 23900, "idx": 172}, {"begin": 23901, "end": 23929, "idx": 173}, {"begin": 23930, "end": 24021, "idx": 174}, {"begin": 24022, "end": 24124, "idx": 175}, {"begin": 24125, "end": 24302, "idx": 176}, {"begin": 24303, "end": 24374, "idx": 177}, {"begin": 24375, "end": 24440, "idx": 178}, {"begin": 24441, "end": 24623, "idx": 179}, {"begin": 24624, "end": 24771, "idx": 180}, {"begin": 24772, "end": 24960, "idx": 181}, {"begin": 24961, "end": 25048, "idx": 182}, {"begin": 25049, "end": 25193, "idx": 183}, {"begin": 25220, "end": 25310, "idx": 184}, {"begin": 25311, "end": 25408, "idx": 185}, {"begin": 25409, "end": 25717, "idx": 186}, {"begin": 25718, "end": 25980, "idx": 187}, {"begin": 25981, "end": 26119, "idx": 188}, {"begin": 26120, "end": 26266, "idx": 189}, {"begin": 26267, "end": 26400, "idx": 190}, {"begin": 26401, "end": 26453, "idx": 191}, {"begin": 26454, "end": 26596, "idx": 192}, {"begin": 26597, "end": 26684, "idx": 193}, {"begin": 26685, "end": 26809, "idx": 194}, {"begin": 26810, "end": 26948, "idx": 195}, {"begin": 26949, "end": 27016, "idx": 196}, {"begin": 27017, "end": 27114, "idx": 197}, {"begin": 27115, "end": 27176, "idx": 198}, {"begin": 27177, "end": 27342, "idx": 199}, {"begin": 27343, "end": 27443, "idx": 200}, {"begin": 27444, "end": 27574, "idx": 201}, {"begin": 27575, "end": 27715, "idx": 202}, {"begin": 27716, "end": 27862, "idx": 203}, {"begin": 27863, "end": 27937, "idx": 204}, {"begin": 27938, "end": 28066, "idx": 205}, {"begin": 28086, "end": 28270, "idx": 206}, {"begin": 28271, "end": 28464, "idx": 207}, {"begin": 28465, "end": 28595, "idx": 208}, {"begin": 28613, "end": 28685, "idx": 209}, {"begin": 28686, "end": 28878, "idx": 210}, {"begin": 28879, "end": 28957, "idx": 211}, {"begin": 28958, "end": 29120, "idx": 212}, {"begin": 29121, "end": 29312, "idx": 213}, {"begin": 29313, "end": 29412, "idx": 214}, {"begin": 29413, "end": 29520, "idx": 215}, {"begin": 29521, "end": 29626, "idx": 216}, {"begin": 29627, "end": 29682, "idx": 217}, {"begin": 29683, "end": 29817, "idx": 218}, {"begin": 29818, "end": 30000, "idx": 219}, {"begin": 30014, "end": 30109, "idx": 220}, {"begin": 30110, "end": 30242, "idx": 221}, {"begin": 30264, "end": 30359, "idx": 222}, {"begin": 30360, "end": 30458, "idx": 223}, {"begin": 30459, "end": 30543, "idx": 224}, {"begin": 30544, "end": 30663, "idx": 225}, {"begin": 30664, "end": 30773, "idx": 226}, {"begin": 30774, "end": 30885, "idx": 227}, {"begin": 30886, "end": 30985, "idx": 228}, {"begin": 30986, "end": 31048, "idx": 229}, {"begin": 31049, "end": 31176, "idx": 230}, {"begin": 31177, "end": 31276, "idx": 231}, {"begin": 31277, "end": 31343, "idx": 232}, {"begin": 31344, "end": 31477, "idx": 233}, {"begin": 31478, "end": 31622, "idx": 234}, {"begin": 31623, "end": 31659, "idx": 235}, {"begin": 31660, "end": 31735, "idx": 236}, {"begin": 31736, "end": 31863, "idx": 237}, {"begin": 31882, "end": 32034, "idx": 238}, {"begin": 32035, "end": 32113, "idx": 239}, {"begin": 32114, "end": 32279, "idx": 240}, {"begin": 32280, "end": 32378, "idx": 241}, {"begin": 32379, "end": 32438, "idx": 242}, {"begin": 32470, "end": 32579, "idx": 243}, {"begin": 32580, "end": 32743, "idx": 244}, {"begin": 32744, "end": 32894, "idx": 245}, {"begin": 32895, "end": 33056, "idx": 246}, {"begin": 33057, "end": 33192, "idx": 247}, {"begin": 33193, "end": 33350, "idx": 248}, {"begin": 33351, "end": 33478, "idx": 249}, {"begin": 33507, "end": 33555, "idx": 250}, {"begin": 33556, "end": 33829, "idx": 251}, {"begin": 33830, "end": 33966, "idx": 252}, {"begin": 33967, "end": 34086, "idx": 253}, {"begin": 34087, "end": 34216, "idx": 254}, {"begin": 34217, "end": 34367, "idx": 255}, {"begin": 34368, "end": 34432, "idx": 256}, {"begin": 34433, "end": 34574, "idx": 257}, {"begin": 34575, "end": 34674, "idx": 258}, {"begin": 34675, "end": 34773, "idx": 259}, {"begin": 34774, "end": 34963, "idx": 260}, {"begin": 34964, "end": 35085, "idx": 261}, {"begin": 35086, "end": 35179, "idx": 262}, {"begin": 35180, "end": 35259, "idx": 263}, {"begin": 35260, "end": 35355, "idx": 264}, {"begin": 35356, "end": 35434, "idx": 265}, {"begin": 35435, "end": 35528, "idx": 266}, {"begin": 35529, "end": 35665, "idx": 267}, {"begin": 35666, "end": 35730, "idx": 268}, {"begin": 35731, "end": 35841, "idx": 269}, {"begin": 35842, "end": 35986, "idx": 270}, {"begin": 35987, "end": 36050, "idx": 271}, {"begin": 36071, "end": 36157, "idx": 272}, {"begin": 36158, "end": 36294, "idx": 273}, {"begin": 36295, "end": 36357, "idx": 274}, {"begin": 36358, "end": 36446, "idx": 275}, {"begin": 36447, "end": 36600, "idx": 276}, {"begin": 36601, "end": 36647, "idx": 277}, {"begin": 36648, "end": 36770, "idx": 278}, {"begin": 36771, "end": 36817, "idx": 279}, {"begin": 36818, "end": 36884, "idx": 280}, {"begin": 36885, "end": 36958, "idx": 281}, {"begin": 36959, "end": 37033, "idx": 282}, {"begin": 37034, "end": 37129, "idx": 283}, {"begin": 37130, "end": 37229, "idx": 284}, {"begin": 37230, "end": 37298, "idx": 285}, {"begin": 37299, "end": 37405, "idx": 286}, {"begin": 37406, "end": 37479, "idx": 287}, {"begin": 37480, "end": 37588, "idx": 288}, {"begin": 37589, "end": 37690, "idx": 289}, {"begin": 37691, "end": 37796, "idx": 290}, {"begin": 37797, "end": 37948, "idx": 291}, {"begin": 37949, "end": 38000, "idx": 292}, {"begin": 38001, "end": 38131, "idx": 293}, {"begin": 38132, "end": 38322, "idx": 294}, {"begin": 38323, "end": 38338, "idx": 295}, {"begin": 38364, "end": 38413, "idx": 296}, {"begin": 38414, "end": 38518, "idx": 297}, {"begin": 38519, "end": 38576, "idx": 298}, {"begin": 38597, "end": 38735, "idx": 299}, {"begin": 38736, "end": 39009, "idx": 300}, {"begin": 39010, "end": 39105, "idx": 301}, {"begin": 39106, "end": 39341, "idx": 302}, {"begin": 39342, "end": 39485, "idx": 303}, {"begin": 39486, "end": 39686, "idx": 304}, {"begin": 39687, "end": 39811, "idx": 305}, {"begin": 39812, "end": 40029, "idx": 306}, {"begin": 40048, "end": 40115, "idx": 307}, {"begin": 40116, "end": 40222, "idx": 308}, {"begin": 40223, "end": 40329, "idx": 309}, {"begin": 40330, "end": 40391, "idx": 310}, {"begin": 40392, "end": 40476, "idx": 311}, {"begin": 40477, "end": 40554, "idx": 312}, {"begin": 40555, "end": 40637, "idx": 313}, {"begin": 40677, "end": 40782, "idx": 314}, {"begin": 40783, "end": 40925, "idx": 315}, {"begin": 40926, "end": 41136, "idx": 316}, {"begin": 41137, "end": 41322, "idx": 317}, {"begin": 41323, "end": 41438, "idx": 318}, {"begin": 41439, "end": 41586, "idx": 319}, {"begin": 41615, "end": 41796, "idx": 320}, {"begin": 41797, "end": 41839, "idx": 321}, {"begin": 41840, "end": 41959, "idx": 322}, {"begin": 41960, "end": 42093, "idx": 323}, {"begin": 42094, "end": 42216, "idx": 324}, {"begin": 42217, "end": 42390, "idx": 325}, {"begin": 42391, "end": 42432, "idx": 326}, {"begin": 42433, "end": 42530, "idx": 327}, {"begin": 42531, "end": 42669, "idx": 328}, {"begin": 42670, "end": 42753, "idx": 329}, {"begin": 42754, "end": 42851, "idx": 330}, {"begin": 42852, "end": 43024, "idx": 331}, {"begin": 43025, "end": 43126, "idx": 332}, {"begin": 43127, "end": 43228, "idx": 333}, {"begin": 43229, "end": 43328, "idx": 334}, {"begin": 43329, "end": 43561, "idx": 335}, {"begin": 43562, "end": 43605, "idx": 336}, {"begin": 43606, "end": 43745, "idx": 337}, {"begin": 43746, "end": 43905, "idx": 338}, {"begin": 43906, "end": 44054, "idx": 339}, {"begin": 44055, "end": 44171, "idx": 340}, {"begin": 44172, "end": 44266, "idx": 341}, {"begin": 44267, "end": 44375, "idx": 342}, {"begin": 44376, "end": 44453, "idx": 343}, {"begin": 44454, "end": 44550, "idx": 344}, {"begin": 44551, "end": 44703, "idx": 345}, {"begin": 44704, "end": 44814, "idx": 346}, {"begin": 44815, "end": 44926, "idx": 347}, {"begin": 44927, "end": 44983, "idx": 348}, {"begin": 44984, "end": 45083, "idx": 349}, {"begin": 45084, "end": 45174, "idx": 350}, {"begin": 45175, "end": 45244, "idx": 351}, {"begin": 45245, "end": 45320, "idx": 352}, {"begin": 45321, "end": 45415, "idx": 353}, {"begin": 45416, "end": 45468, "idx": 354}, {"begin": 45469, "end": 45549, "idx": 355}, {"begin": 45550, "end": 45592, "idx": 356}, {"begin": 45593, "end": 45625, "idx": 357}, {"begin": 45626, "end": 45668, "idx": 358}, {"begin": 45669, "end": 45774, "idx": 359}, {"begin": 45775, "end": 45813, "idx": 360}, {"begin": 45814, "end": 45902, "idx": 361}, {"begin": 45903, "end": 45941, "idx": 362}, {"begin": 45942, "end": 46006, "idx": 363}, {"begin": 46007, "end": 46045, "idx": 364}, {"begin": 46046, "end": 46084, "idx": 365}, {"begin": 46085, "end": 46161, "idx": 366}, {"begin": 46162, "end": 46226, "idx": 367}, {"begin": 46227, "end": 46324, "idx": 368}, {"begin": 46325, "end": 46456, "idx": 369}, {"begin": 46457, "end": 46492, "idx": 370}, {"begin": 46493, "end": 46545, "idx": 371}, {"begin": 46546, "end": 46687, "idx": 372}, {"begin": 46688, "end": 46817, "idx": 373}, {"begin": 46818, "end": 46866, "idx": 374}, {"begin": 46867, "end": 47007, "idx": 375}, {"begin": 47008, "end": 47060, "idx": 376}, {"begin": 47061, "end": 47201, "idx": 377}, {"begin": 47202, "end": 47283, "idx": 378}, {"begin": 47284, "end": 47454, "idx": 379}, {"begin": 47455, "end": 47516, "idx": 380}, {"begin": 47517, "end": 47625, "idx": 381}, {"begin": 47626, "end": 47766, "idx": 382}, {"begin": 47767, "end": 47877, "idx": 383}, {"begin": 47878, "end": 47944, "idx": 384}, {"begin": 47945, "end": 47986, "idx": 385}, {"begin": 47987, "end": 48074, "idx": 386}, {"begin": 48075, "end": 48170, "idx": 387}, {"begin": 48171, "end": 48278, "idx": 388}, {"begin": 48279, "end": 48343, "idx": 389}, {"begin": 48344, "end": 48394, "idx": 390}, {"begin": 48395, "end": 48536, "idx": 391}, {"begin": 48537, "end": 48601, "idx": 392}, {"begin": 48602, "end": 48654, "idx": 393}, {"begin": 48655, "end": 48807, "idx": 394}, {"begin": 48808, "end": 48857, "idx": 395}, {"begin": 48858, "end": 48899, "idx": 396}, {"begin": 48900, "end": 49041, "idx": 397}, {"begin": 49042, "end": 49133, "idx": 398}, {"begin": 49134, "end": 49258, "idx": 399}, {"begin": 49259, "end": 49351, "idx": 400}, {"begin": 49352, "end": 49406, "idx": 401}, {"begin": 49407, "end": 49512, "idx": 402}, {"begin": 49513, "end": 49596, "idx": 403}, {"begin": 49597, "end": 49695, "idx": 404}, {"begin": 49696, "end": 49814, "idx": 405}, {"begin": 49815, "end": 49966, "idx": 406}, {"begin": 49967, "end": 50071, "idx": 407}, {"begin": 50072, "end": 50263, "idx": 408}, {"begin": 50264, "end": 50343, "idx": 409}, {"begin": 50344, "end": 50459, "idx": 410}, {"begin": 50460, "end": 50545, "idx": 411}, {"begin": 50546, "end": 50676, "idx": 412}, {"begin": 50677, "end": 50722, "idx": 413}, {"begin": 50723, "end": 50781, "idx": 414}, {"begin": 50782, "end": 50876, "idx": 415}, {"begin": 50877, "end": 50971, "idx": 416}, {"begin": 50998, "end": 51104, "idx": 417}, {"begin": 51105, "end": 51251, "idx": 418}, {"begin": 51252, "end": 51381, "idx": 419}, {"begin": 51382, "end": 51463, "idx": 420}, {"begin": 51464, "end": 51527, "idx": 421}, {"begin": 51528, "end": 51675, "idx": 422}, {"begin": 51676, "end": 51756, "idx": 423}, {"begin": 51757, "end": 51873, "idx": 424}, {"begin": 51874, "end": 52061, "idx": 425}, {"begin": 52062, "end": 52141, "idx": 426}, {"begin": 52142, "end": 52233, "idx": 427}, {"begin": 52234, "end": 52375, "idx": 428}, {"begin": 52376, "end": 52447, "idx": 429}, {"begin": 52448, "end": 52573, "idx": 430}, {"begin": 52574, "end": 52808, "idx": 431}, {"begin": 52809, "end": 52975, "idx": 432}, {"begin": 52976, "end": 53183, "idx": 433}, {"begin": 53184, "end": 53503, "idx": 434}, {"begin": 53504, "end": 53571, "idx": 435}, {"begin": 53572, "end": 53737, "idx": 436}, {"begin": 53738, "end": 53974, "idx": 437}, {"begin": 53993, "end": 54137, "idx": 438}, {"begin": 54138, "end": 54254, "idx": 439}, {"begin": 54255, "end": 54404, "idx": 440}, {"begin": 54405, "end": 54509, "idx": 441}, {"begin": 54548, "end": 54696, "idx": 442}, {"begin": 54697, "end": 54823, "idx": 443}, {"begin": 54824, "end": 54892, "idx": 444}, {"begin": 54893, "end": 54976, "idx": 445}, {"begin": 54977, "end": 55148, "idx": 446}, {"begin": 55149, "end": 55341, "idx": 447}, {"begin": 55342, "end": 55410, "idx": 448}, {"begin": 55439, "end": 55533, "idx": 449}, {"begin": 55534, "end": 55592, "idx": 450}, {"begin": 55593, "end": 55737, "idx": 451}, {"begin": 55738, "end": 55869, "idx": 452}, {"begin": 55870, "end": 55974, "idx": 453}, {"begin": 55975, "end": 56098, "idx": 454}, {"begin": 56099, "end": 56244, "idx": 455}, {"begin": 56245, "end": 56313, "idx": 456}, {"begin": 56314, "end": 56451, "idx": 457}, {"begin": 56452, "end": 56548, "idx": 458}, {"begin": 56549, "end": 56630, "idx": 459}, {"begin": 56631, "end": 56679, "idx": 460}, {"begin": 56680, "end": 56816, "idx": 461}, {"begin": 56817, "end": 57012, "idx": 462}, {"begin": 57013, "end": 57062, "idx": 463}, {"begin": 57063, "end": 57082, "idx": 464}, {"begin": 57083, "end": 57170, "idx": 465}, {"begin": 57171, "end": 57315, "idx": 466}, {"begin": 57316, "end": 57413, "idx": 467}, {"begin": 57414, "end": 57465, "idx": 468}, {"begin": 57466, "end": 57694, "idx": 469}, {"begin": 57695, "end": 57790, "idx": 470}, {"begin": 57791, "end": 57880, "idx": 471}, {"begin": 57881, "end": 58062, "idx": 472}, {"begin": 58063, "end": 58170, "idx": 473}, {"begin": 58171, "end": 58265, "idx": 474}, {"begin": 58266, "end": 58403, "idx": 475}, {"begin": 58404, "end": 58481, "idx": 476}, {"begin": 58482, "end": 58578, "idx": 477}, {"begin": 58579, "end": 58822, "idx": 478}, {"begin": 58823, "end": 58917, "idx": 479}, {"begin": 58918, "end": 59084, "idx": 480}, {"begin": 59085, "end": 59182, "idx": 481}, {"begin": 59183, "end": 59341, "idx": 482}, {"begin": 59342, "end": 59388, "idx": 483}, {"begin": 59389, "end": 59516, "idx": 484}, {"begin": 59517, "end": 59658, "idx": 485}, {"begin": 59659, "end": 59814, "idx": 486}, {"begin": 59815, "end": 59922, "idx": 487}, {"begin": 59923, "end": 60057, "idx": 488}, {"begin": 60058, "end": 60183, "idx": 489}, {"begin": 60184, "end": 60317, "idx": 490}, {"begin": 60318, "end": 60436, "idx": 491}, {"begin": 60437, "end": 60576, "idx": 492}, {"begin": 60577, "end": 60739, "idx": 493}, {"begin": 60740, "end": 60830, "idx": 494}, {"begin": 60831, "end": 60899, "idx": 495}, {"begin": 60900, "end": 60952, "idx": 496}, {"begin": 60953, "end": 61101, "idx": 497}, {"begin": 61102, "end": 61168, "idx": 498}, {"begin": 61169, "end": 61214, "idx": 499}, {"begin": 61215, "end": 61302, "idx": 500}, {"begin": 61303, "end": 61392, "idx": 501}, {"begin": 61393, "end": 61538, "idx": 502}, {"begin": 61539, "end": 61695, "idx": 503}, {"begin": 61696, "end": 61801, "idx": 504}, {"begin": 61817, "end": 61919, "idx": 505}, {"begin": 61920, "end": 61970, "idx": 506}, {"begin": 61971, "end": 62090, "idx": 507}, {"begin": 62091, "end": 62184, "idx": 508}, {"begin": 62185, "end": 62353, "idx": 509}, {"begin": 62354, "end": 62422, "idx": 510}, {"begin": 62423, "end": 62549, "idx": 511}, {"begin": 62550, "end": 62692, "idx": 512}, {"begin": 62693, "end": 62926, "idx": 513}, {"begin": 62927, "end": 63106, "idx": 514}, {"begin": 63107, "end": 63268, "idx": 515}, {"begin": 63269, "end": 63350, "idx": 516}, {"begin": 63351, "end": 63461, "idx": 517}, {"begin": 63462, "end": 63574, "idx": 518}, {"begin": 63575, "end": 63652, "idx": 519}, {"begin": 63653, "end": 63790, "idx": 520}, {"begin": 63791, "end": 63944, "idx": 521}, {"begin": 63945, "end": 63999, "idx": 522}, {"begin": 64040, "end": 64231, "idx": 523}, {"begin": 64232, "end": 64369, "idx": 524}, {"begin": 64370, "end": 64468, "idx": 525}, {"begin": 64469, "end": 64539, "idx": 526}, {"begin": 64540, "end": 64675, "idx": 527}, {"begin": 64676, "end": 64709, "idx": 528}, {"begin": 64710, "end": 64820, "idx": 529}, {"begin": 64821, "end": 64941, "idx": 530}, {"begin": 64942, "end": 64984, "idx": 531}, {"begin": 64985, "end": 65067, "idx": 532}, {"begin": 65068, "end": 65171, "idx": 533}, {"begin": 65172, "end": 65278, "idx": 534}, {"begin": 65279, "end": 65339, "idx": 535}, {"begin": 65340, "end": 65396, "idx": 536}, {"begin": 65397, "end": 65493, "idx": 537}, {"begin": 65494, "end": 65698, "idx": 538}, {"begin": 65699, "end": 65920, "idx": 539}, {"begin": 65921, "end": 66020, "idx": 540}, {"begin": 66021, "end": 66135, "idx": 541}, {"begin": 66136, "end": 66242, "idx": 542}, {"begin": 66243, "end": 66402, "idx": 543}, {"begin": 66403, "end": 66480, "idx": 544}, {"begin": 66481, "end": 66657, "idx": 545}, {"begin": 66683, "end": 66765, "idx": 546}, {"begin": 66766, "end": 66839, "idx": 547}, {"begin": 66840, "end": 66959, "idx": 548}, {"begin": 66980, "end": 67111, "idx": 549}, {"begin": 67112, "end": 67252, "idx": 550}, {"begin": 67253, "end": 67326, "idx": 551}, {"begin": 67327, "end": 67433, "idx": 552}, {"begin": 67434, "end": 67579, "idx": 553}, {"begin": 67580, "end": 67669, "idx": 554}, {"begin": 67670, "end": 67783, "idx": 555}, {"begin": 67784, "end": 67913, "idx": 556}, {"begin": 67914, "end": 68135, "idx": 557}, {"begin": 68136, "end": 68320, "idx": 558}, {"begin": 68321, "end": 68455, "idx": 559}, {"begin": 68456, "end": 68613, "idx": 560}, {"begin": 68614, "end": 68745, "idx": 561}, {"begin": 68746, "end": 68831, "idx": 562}, {"begin": 68832, "end": 69124, "idx": 563}, {"begin": 69125, "end": 69364, "idx": 564}, {"begin": 69365, "end": 69497, "idx": 565}, {"begin": 69498, "end": 69626, "idx": 566}, {"begin": 69627, "end": 69770, "idx": 567}, {"begin": 69771, "end": 69893, "idx": 568}, {"begin": 69894, "end": 69961, "idx": 569}, {"begin": 69962, "end": 70158, "idx": 570}, {"begin": 70159, "end": 70348, "idx": 571}, {"begin": 70349, "end": 70573, "idx": 572}, {"begin": 70592, "end": 70695, "idx": 573}, {"begin": 70696, "end": 70840, "idx": 574}, {"begin": 70841, "end": 70973, "idx": 575}, {"begin": 70974, "end": 71149, "idx": 576}, {"begin": 71150, "end": 71276, "idx": 577}, {"begin": 71277, "end": 71426, "idx": 578}, {"begin": 71427, "end": 71617, "idx": 579}, {"begin": 71654, "end": 71787, "idx": 580}, {"begin": 71788, "end": 71918, "idx": 581}, {"begin": 71919, "end": 72141, "idx": 582}, {"begin": 72142, "end": 72281, "idx": 583}, {"begin": 72282, "end": 72379, "idx": 584}, {"begin": 72380, "end": 72490, "idx": 585}, {"begin": 72491, "end": 72590, "idx": 586}, {"begin": 72591, "end": 72761, "idx": 587}, {"begin": 72790, "end": 72914, "idx": 588}, {"begin": 72915, "end": 73052, "idx": 589}, {"begin": 73053, "end": 73108, "idx": 590}, {"begin": 73109, "end": 73242, "idx": 591}, {"begin": 73243, "end": 73318, "idx": 592}, {"begin": 73319, "end": 73428, "idx": 593}, {"begin": 73429, "end": 73558, "idx": 594}, {"begin": 73559, "end": 73672, "idx": 595}, {"begin": 73673, "end": 73813, "idx": 596}, {"begin": 73814, "end": 73912, "idx": 597}, {"begin": 73913, "end": 73978, "idx": 598}, {"begin": 73979, "end": 74056, "idx": 599}, {"begin": 74057, "end": 74206, "idx": 600}, {"begin": 74207, "end": 74319, "idx": 601}, {"begin": 74320, "end": 74427, "idx": 602}, {"begin": 74428, "end": 74584, "idx": 603}, {"begin": 74585, "end": 74635, "idx": 604}, {"begin": 74636, "end": 74802, "idx": 605}, {"begin": 74803, "end": 74938, "idx": 606}, {"begin": 74939, "end": 75104, "idx": 607}, {"begin": 75105, "end": 75216, "idx": 608}, {"begin": 75217, "end": 75376, "idx": 609}, {"begin": 75377, "end": 75494, "idx": 610}, {"begin": 75495, "end": 75739, "idx": 611}, {"begin": 75740, "end": 75832, "idx": 612}, {"begin": 75833, "end": 75982, "idx": 613}, {"begin": 75983, "end": 76092, "idx": 614}, {"begin": 76093, "end": 76307, "idx": 615}, {"begin": 76308, "end": 76414, "idx": 616}, {"begin": 76415, "end": 76564, "idx": 617}, {"begin": 76565, "end": 76628, "idx": 618}, {"begin": 76629, "end": 76773, "idx": 619}, {"begin": 76774, "end": 76842, "idx": 620}, {"begin": 76843, "end": 76925, "idx": 621}, {"begin": 76926, "end": 76993, "idx": 622}, {"begin": 76994, "end": 77064, "idx": 623}, {"begin": 77065, "end": 77159, "idx": 624}, {"begin": 77160, "end": 77325, "idx": 625}, {"begin": 77326, "end": 77422, "idx": 626}, {"begin": 77423, "end": 77557, "idx": 627}, {"begin": 77558, "end": 77608, "idx": 628}, {"begin": 77609, "end": 77747, "idx": 629}, {"begin": 77748, "end": 77839, "idx": 630}, {"begin": 77840, "end": 77972, "idx": 631}, {"begin": 77973, "end": 78088, "idx": 632}, {"begin": 78089, "end": 78287, "idx": 633}, {"begin": 78288, "end": 78346, "idx": 634}, {"begin": 78347, "end": 78394, "idx": 635}, {"begin": 78421, "end": 78547, "idx": 636}, {"begin": 78548, "end": 78653, "idx": 637}, {"begin": 78674, "end": 78778, "idx": 638}, {"begin": 78779, "end": 78919, "idx": 639}, {"begin": 78920, "end": 79100, "idx": 640}, {"begin": 79101, "end": 79299, "idx": 641}, {"begin": 79300, "end": 79383, "idx": 642}, {"begin": 79384, "end": 79466, "idx": 643}, {"begin": 79467, "end": 79549, "idx": 644}, {"begin": 79550, "end": 79795, "idx": 645}, {"begin": 79796, "end": 79888, "idx": 646}, {"begin": 79889, "end": 80011, "idx": 647}, {"begin": 80012, "end": 80184, "idx": 648}, {"begin": 80185, "end": 80274, "idx": 649}, {"begin": 80275, "end": 80449, "idx": 650}, {"begin": 80450, "end": 80568, "idx": 651}, {"begin": 80569, "end": 80638, "idx": 652}, {"begin": 80639, "end": 80799, "idx": 653}, {"begin": 80800, "end": 80959, "idx": 654}, {"begin": 80960, "end": 81105, "idx": 655}, {"begin": 81124, "end": 81239, "idx": 656}, {"begin": 81240, "end": 81337, "idx": 657}, {"begin": 81338, "end": 81445, "idx": 658}, {"begin": 81446, "end": 81722, "idx": 659}, {"begin": 81753, "end": 81839, "idx": 660}, {"begin": 81840, "end": 81953, "idx": 661}, {"begin": 81954, "end": 82115, "idx": 662}, {"begin": 82116, "end": 82214, "idx": 663}, {"begin": 82215, "end": 82325, "idx": 664}, {"begin": 82326, "end": 82439, "idx": 665}, {"begin": 82440, "end": 82555, "idx": 666}, {"begin": 82556, "end": 82613, "idx": 667}, {"begin": 82614, "end": 82732, "idx": 668}, {"begin": 82733, "end": 82913, "idx": 669}, {"begin": 82914, "end": 82969, "idx": 670}, {"begin": 82970, "end": 83036, "idx": 671}, {"begin": 83037, "end": 83163, "idx": 672}, {"begin": 83164, "end": 83228, "idx": 673}, {"begin": 83229, "end": 83376, "idx": 674}, {"begin": 83402, "end": 83593, "idx": 675}, {"begin": 83594, "end": 83730, "idx": 676}, {"begin": 83731, "end": 83857, "idx": 677}, {"begin": 83858, "end": 84080, "idx": 678}, {"begin": 84081, "end": 84183, "idx": 679}, {"begin": 84184, "end": 84325, "idx": 680}, {"begin": 84326, "end": 84474, "idx": 681}, {"begin": 84475, "end": 84603, "idx": 682}, {"begin": 84604, "end": 84804, "idx": 683}, {"begin": 84805, "end": 85004, "idx": 684}, {"begin": 85005, "end": 85125, "idx": 685}, {"begin": 85126, "end": 85247, "idx": 686}, {"begin": 85248, "end": 85438, "idx": 687}, {"begin": 85439, "end": 85490, "idx": 688}, {"begin": 85523, "end": 85659, "idx": 689}, {"begin": 85660, "end": 85799, "idx": 690}, {"begin": 85800, "end": 85959, "idx": 691}, {"begin": 85960, "end": 86026, "idx": 692}, {"begin": 86054, "end": 86132, "idx": 693}, {"begin": 86133, "end": 86230, "idx": 694}, {"begin": 86231, "end": 86337, "idx": 695}, {"begin": 86338, "end": 86407, "idx": 696}, {"begin": 86408, "end": 86667, "idx": 697}, {"begin": 86668, "end": 86760, "idx": 698}, {"begin": 86761, "end": 86951, "idx": 699}, {"begin": 86952, "end": 87013, "idx": 700}, {"begin": 87014, "end": 87072, "idx": 701}, {"begin": 87073, "end": 87137, "idx": 702}, {"begin": 87138, "end": 87202, "idx": 703}, {"begin": 87203, "end": 87256, "idx": 704}, {"begin": 87257, "end": 87327, "idx": 705}, {"begin": 87328, "end": 87463, "idx": 706}, {"begin": 87464, "end": 87562, "idx": 707}, {"begin": 87563, "end": 87611, "idx": 708}, {"begin": 87612, "end": 87661, "idx": 709}, {"begin": 87662, "end": 87721, "idx": 710}, {"begin": 87722, "end": 88014, "idx": 711}, {"begin": 88015, "end": 88170, "idx": 712}, {"begin": 88171, "end": 88313, "idx": 713}, {"begin": 88314, "end": 88419, "idx": 714}, {"begin": 88420, "end": 88594, "idx": 715}, {"begin": 88595, "end": 88713, "idx": 716}, {"begin": 88714, "end": 88782, "idx": 717}, {"begin": 88783, "end": 88882, "idx": 718}, {"begin": 88883, "end": 88929, "idx": 719}, {"begin": 88930, "end": 88944, "idx": 720}, {"begin": 88945, "end": 88999, "idx": 721}, {"begin": 89000, "end": 89080, "idx": 722}, {"begin": 89081, "end": 89169, "idx": 723}, {"begin": 89190, "end": 89256, "idx": 724}, {"begin": 89257, "end": 89407, "idx": 725}, {"begin": 89408, "end": 89534, "idx": 726}, {"begin": 89535, "end": 89671, "idx": 727}, {"begin": 89672, "end": 89758, "idx": 728}, {"begin": 89759, "end": 89857, "idx": 729}, {"begin": 89882, "end": 89990, "idx": 730}, {"begin": 89991, "end": 90084, "idx": 731}, {"begin": 90085, "end": 90247, "idx": 732}, {"begin": 90248, "end": 90291, "idx": 733}, {"begin": 90292, "end": 90578, "idx": 734}, {"begin": 90579, "end": 90767, "idx": 735}, {"begin": 90768, "end": 90893, "idx": 736}, {"begin": 90894, "end": 91029, "idx": 737}, {"begin": 91030, "end": 91146, "idx": 738}, {"begin": 91147, "end": 91208, "idx": 739}, {"begin": 91209, "end": 91300, "idx": 740}, {"begin": 91301, "end": 91377, "idx": 741}, {"begin": 91401, "end": 91549, "idx": 742}, {"begin": 91550, "end": 91631, "idx": 743}, {"begin": 91632, "end": 91789, "idx": 744}, {"begin": 91790, "end": 91934, "idx": 745}, {"begin": 91935, "end": 92044, "idx": 746}, {"begin": 92045, "end": 92126, "idx": 747}, {"begin": 92127, "end": 92257, "idx": 748}, {"begin": 92258, "end": 92356, "idx": 749}, {"begin": 92357, "end": 92414, "idx": 750}, {"begin": 92415, "end": 92515, "idx": 751}, {"begin": 92516, "end": 92629, "idx": 752}, {"begin": 92630, "end": 92724, "idx": 753}, {"begin": 92725, "end": 92770, "idx": 754}, {"begin": 92771, "end": 92885, "idx": 755}, {"begin": 92886, "end": 92972, "idx": 756}, {"begin": 92973, "end": 93061, "idx": 757}, {"begin": 93093, "end": 93245, "idx": 758}, {"begin": 93246, "end": 93316, "idx": 759}, {"begin": 93317, "end": 93389, "idx": 760}, {"begin": 93390, "end": 93484, "idx": 761}, {"begin": 93485, "end": 93618, "idx": 762}, {"begin": 93619, "end": 93754, "idx": 763}, {"begin": 93755, "end": 93864, "idx": 764}, {"begin": 93885, "end": 94072, "idx": 765}, {"begin": 94073, "end": 94251, "idx": 766}, {"begin": 94252, "end": 94379, "idx": 767}, {"begin": 94380, "end": 94465, "idx": 768}, {"begin": 94466, "end": 94653, "idx": 769}, {"begin": 94654, "end": 94740, "idx": 770}, {"begin": 94741, "end": 94878, "idx": 771}, {"begin": 94879, "end": 94974, "idx": 772}, {"begin": 94975, "end": 95139, "idx": 773}, {"begin": 95140, "end": 95262, "idx": 774}, {"begin": 95263, "end": 95400, "idx": 775}, {"begin": 95401, "end": 95540, "idx": 776}, {"begin": 95541, "end": 95649, "idx": 777}, {"begin": 95668, "end": 95742, "idx": 778}, {"begin": 95743, "end": 95902, "idx": 779}, {"begin": 95903, "end": 96052, "idx": 780}, {"begin": 96053, "end": 96209, "idx": 781}, {"begin": 96239, "end": 96334, "idx": 782}, {"begin": 96335, "end": 96461, "idx": 783}, {"begin": 96462, "end": 96573, "idx": 784}, {"begin": 96574, "end": 96693, "idx": 785}, {"begin": 96694, "end": 96898, "idx": 786}, {"begin": 96899, "end": 97007, "idx": 787}, {"begin": 97008, "end": 97075, "idx": 788}, {"begin": 97076, "end": 97216, "idx": 789}, {"begin": 97217, "end": 97294, "idx": 790}, {"begin": 97295, "end": 97343, "idx": 791}, {"begin": 97344, "end": 97471, "idx": 792}, {"begin": 97472, "end": 97625, "idx": 793}, {"begin": 97626, "end": 97734, "idx": 794}, {"begin": 97735, "end": 97920, "idx": 795}, {"begin": 97921, "end": 98037, "idx": 796}, {"begin": 98038, "end": 98107, "idx": 797}, {"begin": 98108, "end": 98321, "idx": 798}, {"begin": 98350, "end": 98489, "idx": 799}, {"begin": 98490, "end": 98535, "idx": 800}, {"begin": 98536, "end": 98619, "idx": 801}, {"begin": 98620, "end": 98775, "idx": 802}, {"begin": 98776, "end": 98928, "idx": 803}, {"begin": 98929, "end": 99085, "idx": 804}, {"begin": 99086, "end": 99225, "idx": 805}, {"begin": 99226, "end": 99278, "idx": 806}, {"begin": 99279, "end": 99344, "idx": 807}, {"begin": 99345, "end": 99426, "idx": 808}, {"begin": 99427, "end": 99549, "idx": 809}, {"begin": 99550, "end": 99605, "idx": 810}, {"begin": 99606, "end": 99696, "idx": 811}, {"begin": 99697, "end": 99982, "idx": 812}, {"begin": 99983, "end": 100198, "idx": 813}, {"begin": 100199, "end": 100339, "idx": 814}, {"begin": 100340, "end": 100565, "idx": 815}, {"begin": 100566, "end": 100688, "idx": 816}, {"begin": 100689, "end": 100874, "idx": 817}, {"begin": 100875, "end": 100932, "idx": 818}, {"begin": 100933, "end": 101062, "idx": 819}, {"begin": 101063, "end": 101144, "idx": 820}, {"begin": 101145, "end": 101255, "idx": 821}, {"begin": 101256, "end": 101329, "idx": 822}, {"begin": 101330, "end": 101369, "idx": 823}, {"begin": 101370, "end": 101461, "idx": 824}, {"begin": 101462, "end": 101536, "idx": 825}, {"begin": 101537, "end": 101670, "idx": 826}, {"begin": 101671, "end": 101790, "idx": 827}, {"begin": 101791, "end": 101857, "idx": 828}, {"begin": 101858, "end": 101912, "idx": 829}, {"begin": 101939, "end": 102086, "idx": 830}, {"begin": 102087, "end": 102254, "idx": 831}, {"begin": 102255, "end": 102398, "idx": 832}, {"begin": 102399, "end": 102451, "idx": 833}, {"begin": 102452, "end": 102653, "idx": 834}, {"begin": 102654, "end": 102704, "idx": 835}, {"begin": 102725, "end": 102807, "idx": 836}, {"begin": 102808, "end": 102945, "idx": 837}, {"begin": 102946, "end": 103036, "idx": 838}, {"begin": 103037, "end": 103134, "idx": 839}, {"begin": 103135, "end": 103206, "idx": 840}, {"begin": 103207, "end": 103279, "idx": 841}, {"begin": 103280, "end": 103378, "idx": 842}, {"begin": 103379, "end": 103506, "idx": 843}, {"begin": 103507, "end": 103621, "idx": 844}, {"begin": 103622, "end": 103777, "idx": 845}, {"begin": 103778, "end": 103825, "idx": 846}, {"begin": 103826, "end": 103949, "idx": 847}, {"begin": 103950, "end": 104012, "idx": 848}, {"begin": 104013, "end": 104114, "idx": 849}, {"begin": 104115, "end": 104306, "idx": 850}, {"begin": 104307, "end": 104428, "idx": 851}, {"begin": 104429, "end": 104814, "idx": 852}, {"begin": 104815, "end": 104979, "idx": 853}, {"begin": 104980, "end": 105071, "idx": 854}, {"begin": 105072, "end": 105258, "idx": 855}, {"begin": 105259, "end": 105438, "idx": 856}, {"begin": 105439, "end": 105547, "idx": 857}, {"begin": 105548, "end": 105690, "idx": 858}, {"begin": 105691, "end": 105820, "idx": 859}, {"begin": 105821, "end": 106045, "idx": 860}, {"begin": 106046, "end": 106171, "idx": 861}, {"begin": 106172, "end": 106336, "idx": 862}, {"begin": 106337, "end": 106513, "idx": 863}, {"begin": 106514, "end": 106633, "idx": 864}, {"begin": 106634, "end": 106851, "idx": 865}, {"begin": 106852, "end": 106924, "idx": 866}, {"begin": 106925, "end": 107120, "idx": 867}, {"begin": 107121, "end": 107251, "idx": 868}, {"begin": 107252, "end": 107317, "idx": 869}, {"begin": 107336, "end": 107438, "idx": 870}, {"begin": 107439, "end": 107521, "idx": 871}, {"begin": 107522, "end": 107684, "idx": 872}, {"begin": 107685, "end": 107871, "idx": 873}, {"begin": 107872, "end": 107987, "idx": 874}, {"begin": 108029, "end": 108227, "idx": 875}, {"begin": 108228, "end": 108329, "idx": 876}, {"begin": 108330, "end": 108483, "idx": 877}, {"begin": 108484, "end": 108582, "idx": 878}, {"begin": 108583, "end": 108707, "idx": 879}, {"begin": 108708, "end": 108883, "idx": 880}, {"begin": 108884, "end": 109045, "idx": 881}, {"begin": 109046, "end": 109261, "idx": 882}, {"begin": 109262, "end": 109389, "idx": 883}, {"begin": 109390, "end": 109583, "idx": 884}, {"begin": 109584, "end": 109678, "idx": 885}, {"begin": 109679, "end": 109874, "idx": 886}, {"begin": 109875, "end": 109971, "idx": 887}, {"begin": 109972, "end": 110077, "idx": 888}, {"begin": 110078, "end": 110180, "idx": 889}, {"begin": 110181, "end": 110282, "idx": 890}, {"begin": 110283, "end": 110379, "idx": 891}, {"begin": 110380, "end": 110470, "idx": 892}, {"begin": 110471, "end": 110559, "idx": 893}, {"begin": 110560, "end": 110682, "idx": 894}, {"begin": 110683, "end": 110790, "idx": 895}, {"begin": 110791, "end": 110852, "idx": 896}, {"begin": 110881, "end": 111011, "idx": 897}, {"begin": 111012, "end": 111101, "idx": 898}, {"begin": 111102, "end": 111210, "idx": 899}, {"begin": 111211, "end": 111268, "idx": 900}, {"begin": 111269, "end": 111479, "idx": 901}, {"begin": 111480, "end": 111562, "idx": 902}, {"begin": 111563, "end": 111608, "idx": 903}, {"begin": 111609, "end": 111743, "idx": 904}, {"begin": 111744, "end": 111807, "idx": 905}, {"begin": 111808, "end": 111935, "idx": 906}, {"begin": 111936, "end": 112048, "idx": 907}, {"begin": 112049, "end": 112138, "idx": 908}, {"begin": 112139, "end": 112206, "idx": 909}, {"begin": 112207, "end": 112317, "idx": 910}, {"begin": 112318, "end": 112451, "idx": 911}, {"begin": 112452, "end": 112507, "idx": 912}, {"begin": 112508, "end": 112652, "idx": 913}, {"begin": 112653, "end": 112788, "idx": 914}, {"begin": 112811, "end": 112872, "idx": 915}, {"begin": 112873, "end": 113031, "idx": 916}, {"begin": 113032, "end": 113050, "idx": 917}, {"begin": 113051, "end": 113173, "idx": 918}, {"begin": 113174, "end": 113255, "idx": 919}, {"begin": 113256, "end": 113362, "idx": 920}, {"begin": 113363, "end": 113467, "idx": 921}, {"begin": 113468, "end": 113525, "idx": 922}, {"begin": 113526, "end": 113612, "idx": 923}, {"begin": 113613, "end": 113764, "idx": 924}, {"begin": 113765, "end": 113867, "idx": 925}, {"begin": 113868, "end": 113972, "idx": 926}, {"begin": 113973, "end": 114076, "idx": 927}, {"begin": 114077, "end": 114139, "idx": 928}, {"begin": 114140, "end": 114265, "idx": 929}, {"begin": 114266, "end": 114381, "idx": 930}, {"begin": 114382, "end": 114539, "idx": 931}, {"begin": 114540, "end": 114557, "idx": 932}, {"begin": 114558, "end": 114679, "idx": 933}, {"begin": 114680, "end": 114780, "idx": 934}, {"begin": 114781, "end": 114797, "idx": 935}, {"begin": 114798, "end": 114964, "idx": 936}, {"begin": 114994, "end": 115046, "idx": 937}, {"begin": 115047, "end": 115140, "idx": 938}, {"begin": 115178, "end": 115300, "idx": 939}, {"begin": 115301, "end": 115453, "idx": 940}, {"begin": 115469, "end": 115656, "idx": 941}, {"begin": 115657, "end": 115720, "idx": 942}, {"begin": 115721, "end": 115811, "idx": 943}, {"begin": 115812, "end": 115905, "idx": 944}, {"begin": 115906, "end": 115987, "idx": 945}, {"begin": 115988, "end": 116098, "idx": 946}, {"begin": 116099, "end": 116218, "idx": 947}, {"begin": 116219, "end": 116308, "idx": 948}, {"begin": 116309, "end": 116499, "idx": 949}, {"begin": 116500, "end": 116596, "idx": 950}, {"begin": 116597, "end": 116691, "idx": 951}, {"begin": 116735, "end": 116815, "idx": 952}, {"begin": 116816, "end": 116953, "idx": 953}, {"begin": 116954, "end": 117097, "idx": 954}, {"begin": 117098, "end": 117181, "idx": 955}, {"begin": 117182, "end": 117372, "idx": 956}, {"begin": 117373, "end": 117462, "idx": 957}, {"begin": 117463, "end": 117551, "idx": 958}, {"begin": 117552, "end": 117609, "idx": 959}, {"begin": 117610, "end": 117686, "idx": 960}, {"begin": 117687, "end": 117693, "idx": 961}, {"begin": 117741, "end": 117785, "idx": 962}, {"begin": 117786, "end": 117844, "idx": 963}, {"begin": 117845, "end": 118122, "idx": 964}, {"begin": 118123, "end": 118279, "idx": 965}, {"begin": 118280, "end": 118351, "idx": 966}, {"begin": 118352, "end": 118540, "idx": 967}, {"begin": 118541, "end": 118682, "idx": 968}, {"begin": 118683, "end": 118773, "idx": 969}, {"begin": 118812, "end": 118889, "idx": 970}, {"begin": 118890, "end": 118932, "idx": 971}, {"begin": 118933, "end": 119015, "idx": 972}, {"begin": 119016, "end": 119176, "idx": 973}, {"begin": 119177, "end": 119308, "idx": 974}, {"begin": 119309, "end": 119419, "idx": 975}, {"begin": 119420, "end": 119454, "idx": 976}, {"begin": 119455, "end": 119561, "idx": 977}, {"begin": 119562, "end": 119616, "idx": 978}, {"begin": 119617, "end": 119691, "idx": 979}, {"begin": 119692, "end": 119810, "idx": 980}, {"begin": 119811, "end": 119871, "idx": 981}, {"begin": 119872, "end": 120034, "idx": 982}, {"begin": 120035, "end": 120183, "idx": 983}, {"begin": 120184, "end": 120260, "idx": 984}, {"begin": 120261, "end": 120350, "idx": 985}, {"begin": 120351, "end": 120399, "idx": 986}, {"begin": 120400, "end": 120473, "idx": 987}, {"begin": 120474, "end": 120587, "idx": 988}, {"begin": 120588, "end": 120691, "idx": 989}, {"begin": 120692, "end": 120815, "idx": 990}, {"begin": 120816, "end": 120868, "idx": 991}, {"begin": 120895, "end": 121084, "idx": 992}, {"begin": 121085, "end": 121208, "idx": 993}, {"begin": 121209, "end": 121307, "idx": 994}, {"begin": 121308, "end": 121406, "idx": 995}, {"begin": 121407, "end": 121543, "idx": 996}, {"begin": 121544, "end": 121670, "idx": 997}, {"begin": 121671, "end": 121783, "idx": 998}, {"begin": 121784, "end": 121885, "idx": 999}, {"begin": 121886, "end": 122053, "idx": 1000}, {"begin": 122054, "end": 122186, "idx": 1001}, {"begin": 122187, "end": 122275, "idx": 1002}, {"begin": 122292, "end": 122360, "idx": 1003}, {"begin": 122361, "end": 122487, "idx": 1004}, {"begin": 122488, "end": 122625, "idx": 1005}, {"begin": 122626, "end": 122689, "idx": 1006}, {"begin": 122690, "end": 122762, "idx": 1007}, {"begin": 122811, "end": 122904, "idx": 1008}, {"begin": 122905, "end": 123032, "idx": 1009}, {"begin": 123033, "end": 123155, "idx": 1010}, {"begin": 123156, "end": 123321, "idx": 1011}, {"begin": 123322, "end": 123438, "idx": 1012}, {"begin": 123439, "end": 123528, "idx": 1013}, {"begin": 123529, "end": 123606, "idx": 1014}, {"begin": 123607, "end": 123681, "idx": 1015}, {"begin": 123682, "end": 123850, "idx": 1016}, {"begin": 123851, "end": 124001, "idx": 1017}, {"begin": 124002, "end": 124161, "idx": 1018}, {"begin": 124162, "end": 124332, "idx": 1019}, {"begin": 124333, "end": 124426, "idx": 1020}, {"begin": 124427, "end": 124511, "idx": 1021}, {"begin": 124512, "end": 124546, "idx": 1022}, {"begin": 124565, "end": 124689, "idx": 1023}, {"begin": 124690, "end": 124779, "idx": 1024}, {"begin": 124780, "end": 124847, "idx": 1025}, {"begin": 124848, "end": 124921, "idx": 1026}, {"begin": 124942, "end": 125150, "idx": 1027}, {"begin": 125151, "end": 125300, "idx": 1028}, {"begin": 125301, "end": 125431, "idx": 1029}, {"begin": 125432, "end": 125551, "idx": 1030}, {"begin": 125552, "end": 125650, "idx": 1031}, {"begin": 125651, "end": 125780, "idx": 1032}, {"begin": 125781, "end": 125906, "idx": 1033}, {"begin": 125907, "end": 125974, "idx": 1034}, {"begin": 125975, "end": 126050, "idx": 1035}, {"begin": 126051, "end": 126133, "idx": 1036}, {"begin": 126134, "end": 126297, "idx": 1037}, {"begin": 126298, "end": 126435, "idx": 1038}, {"begin": 126436, "end": 126513, "idx": 1039}, {"begin": 126514, "end": 126600, "idx": 1040}, {"begin": 126601, "end": 126718, "idx": 1041}, {"begin": 126719, "end": 126822, "idx": 1042}, {"begin": 126823, "end": 126952, "idx": 1043}, {"begin": 126953, "end": 127098, "idx": 1044}, {"begin": 127099, "end": 127231, "idx": 1045}, {"begin": 127232, "end": 127314, "idx": 1046}, {"begin": 127315, "end": 127465, "idx": 1047}, {"begin": 127466, "end": 127548, "idx": 1048}, {"begin": 127549, "end": 127689, "idx": 1049}, {"begin": 127690, "end": 127830, "idx": 1050}, {"begin": 127849, "end": 127915, "idx": 1051}, {"begin": 127916, "end": 128100, "idx": 1052}, {"begin": 128101, "end": 128230, "idx": 1053}, {"begin": 128266, "end": 128433, "idx": 1054}, {"begin": 128434, "end": 128591, "idx": 1055}, {"begin": 128592, "end": 128703, "idx": 1056}, {"begin": 128704, "end": 128747, "idx": 1057}, {"begin": 128748, "end": 128789, "idx": 1058}, {"begin": 128790, "end": 128914, "idx": 1059}, {"begin": 128915, "end": 129142, "idx": 1060}, {"begin": 129143, "end": 129208, "idx": 1061}, {"begin": 129209, "end": 129421, "idx": 1062}, {"begin": 129422, "end": 129495, "idx": 1063}, {"begin": 129496, "end": 129732, "idx": 1064}, {"begin": 129733, "end": 129780, "idx": 1065}, {"begin": 129781, "end": 129928, "idx": 1066}, {"begin": 129957, "end": 130118, "idx": 1067}, {"begin": 130119, "end": 130263, "idx": 1068}, {"begin": 130264, "end": 130332, "idx": 1069}, {"begin": 130333, "end": 130463, "idx": 1070}, {"begin": 130464, "end": 130552, "idx": 1071}, {"begin": 130553, "end": 130755, "idx": 1072}, {"begin": 130756, "end": 130904, "idx": 1073}, {"begin": 130905, "end": 131013, "idx": 1074}, {"begin": 131039, "end": 131124, "idx": 1075}, {"begin": 131125, "end": 131249, "idx": 1076}, {"begin": 131250, "end": 131327, "idx": 1077}, {"begin": 131328, "end": 131450, "idx": 1078}, {"begin": 131451, "end": 131592, "idx": 1079}, {"begin": 131593, "end": 131757, "idx": 1080}, {"begin": 131758, "end": 131930, "idx": 1081}, {"begin": 131931, "end": 132157, "idx": 1082}, {"begin": 132158, "end": 132182, "idx": 1083}, {"begin": 132183, "end": 132308, "idx": 1084}, {"begin": 132309, "end": 132383, "idx": 1085}, {"begin": 132384, "end": 132433, "idx": 1086}, {"begin": 132434, "end": 132731, "idx": 1087}, {"begin": 132732, "end": 132882, "idx": 1088}, {"begin": 132911, "end": 133056, "idx": 1089}, {"begin": 133057, "end": 133207, "idx": 1090}, {"begin": 133208, "end": 133264, "idx": 1091}, {"begin": 133265, "end": 133434, "idx": 1092}, {"begin": 133435, "end": 133608, "idx": 1093}, {"begin": 133609, "end": 133658, "idx": 1094}, {"begin": 133695, "end": 133816, "idx": 1095}, {"begin": 133817, "end": 133821, "idx": 1096}, {"begin": 133822, "end": 133883, "idx": 1097}, {"begin": 133884, "end": 133999, "idx": 1098}, {"begin": 134000, "end": 134079, "idx": 1099}, {"begin": 134080, "end": 134145, "idx": 1100}, {"begin": 134146, "end": 134175, "idx": 1101}, {"begin": 134176, "end": 134325, "idx": 1102}, {"begin": 134326, "end": 134467, "idx": 1103}, {"begin": 134468, "end": 134545, "idx": 1104}, {"begin": 134546, "end": 134633, "idx": 1105}, {"begin": 134634, "end": 134767, "idx": 1106}, {"begin": 134768, "end": 134889, "idx": 1107}, {"begin": 134890, "end": 134988, "idx": 1108}, {"begin": 134989, "end": 135077, "idx": 1109}, {"begin": 135078, "end": 135265, "idx": 1110}, {"begin": 135266, "end": 135426, "idx": 1111}, {"begin": 135427, "end": 135522, "idx": 1112}, {"begin": 135523, "end": 135588, "idx": 1113}, {"begin": 135589, "end": 135737, "idx": 1114}, {"begin": 135738, "end": 135903, "idx": 1115}, {"begin": 135904, "end": 136029, "idx": 1116}, {"begin": 136030, "end": 136210, "idx": 1117}, {"begin": 136211, "end": 136390, "idx": 1118}, {"begin": 136409, "end": 136584, "idx": 1119}, {"begin": 136585, "end": 136690, "idx": 1120}, {"begin": 136691, "end": 136905, "idx": 1121}, {"begin": 136906, "end": 137048, "idx": 1122}, {"begin": 137049, "end": 137128, "idx": 1123}, {"begin": 137129, "end": 137249, "idx": 1124}, {"begin": 137250, "end": 137320, "idx": 1125}, {"begin": 137321, "end": 137390, "idx": 1126}, {"begin": 137391, "end": 137511, "idx": 1127}, {"begin": 137512, "end": 137700, "idx": 1128}, {"begin": 137701, "end": 137810, "idx": 1129}, {"begin": 137811, "end": 138006, "idx": 1130}, {"begin": 138007, "end": 138183, "idx": 1131}, {"begin": 138209, "end": 138350, "idx": 1132}, {"begin": 138351, "end": 138409, "idx": 1133}, {"begin": 138410, "end": 138571, "idx": 1134}, {"begin": 138572, "end": 138674, "idx": 1135}, {"begin": 138675, "end": 138902, "idx": 1136}, {"begin": 138903, "end": 139042, "idx": 1137}, {"begin": 139043, "end": 139143, "idx": 1138}, {"begin": 139144, "end": 139359, "idx": 1139}, {"begin": 139360, "end": 139539, "idx": 1140}, {"begin": 139540, "end": 139639, "idx": 1141}, {"begin": 139666, "end": 139756, "idx": 1142}, {"begin": 139757, "end": 139941, "idx": 1143}, {"begin": 139942, "end": 140008, "idx": 1144}, {"begin": 140009, "end": 140157, "idx": 1145}, {"begin": 140158, "end": 140241, "idx": 1146}, {"begin": 140258, "end": 140429, "idx": 1147}, {"begin": 140430, "end": 140504, "idx": 1148}, {"begin": 140525, "end": 140592, "idx": 1149}, {"begin": 140593, "end": 140711, "idx": 1150}, {"begin": 140712, "end": 140777, "idx": 1151}, {"begin": 140778, "end": 140908, "idx": 1152}, {"begin": 140909, "end": 141040, "idx": 1153}, {"begin": 141041, "end": 141113, "idx": 1154}, {"begin": 141114, "end": 141358, "idx": 1155}, {"begin": 141359, "end": 141420, "idx": 1156}, {"begin": 141421, "end": 141653, "idx": 1157}, {"begin": 141654, "end": 141830, "idx": 1158}, {"begin": 141831, "end": 142059, "idx": 1159}, {"begin": 142060, "end": 142359, "idx": 1160}, {"begin": 142360, "end": 142473, "idx": 1161}, {"begin": 142474, "end": 142761, "idx": 1162}, {"begin": 142762, "end": 142942, "idx": 1163}, {"begin": 142943, "end": 143169, "idx": 1164}, {"begin": 143170, "end": 143278, "idx": 1165}, {"begin": 143279, "end": 143436, "idx": 1166}, {"begin": 143437, "end": 143566, "idx": 1167}, {"begin": 143567, "end": 143644, "idx": 1168}, {"begin": 143645, "end": 143721, "idx": 1169}, {"begin": 143722, "end": 143856, "idx": 1170}, {"begin": 143857, "end": 144004, "idx": 1171}, {"begin": 144005, "end": 144161, "idx": 1172}, {"begin": 144162, "end": 144274, "idx": 1173}, {"begin": 144275, "end": 144387, "idx": 1174}, {"begin": 144388, "end": 144488, "idx": 1175}, {"begin": 144489, "end": 144672, "idx": 1176}, {"begin": 144673, "end": 144761, "idx": 1177}, {"begin": 144762, "end": 144937, "idx": 1178}, {"begin": 144938, "end": 145118, "idx": 1179}, {"begin": 145137, "end": 145316, "idx": 1180}, {"begin": 145317, "end": 145397, "idx": 1181}, {"begin": 145398, "end": 145633, "idx": 1182}, {"begin": 145634, "end": 145783, "idx": 1183}, {"begin": 145814, "end": 145932, "idx": 1184}, {"begin": 145933, "end": 146131, "idx": 1185}, {"begin": 146132, "end": 146237, "idx": 1186}, {"begin": 146259, "end": 146340, "idx": 1187}, {"begin": 146341, "end": 146485, "idx": 1188}, {"begin": 146486, "end": 146522, "idx": 1189}, {"begin": 146523, "end": 146649, "idx": 1190}, {"begin": 146650, "end": 146811, "idx": 1191}, {"begin": 146812, "end": 146902, "idx": 1192}, {"begin": 146903, "end": 146939, "idx": 1193}, {"begin": 146940, "end": 147060, "idx": 1194}, {"begin": 147061, "end": 147143, "idx": 1195}, {"begin": 147144, "end": 147243, "idx": 1196}, {"begin": 147244, "end": 147281, "idx": 1197}, {"begin": 147282, "end": 147326, "idx": 1198}, {"begin": 147327, "end": 147435, "idx": 1199}, {"begin": 147436, "end": 147501, "idx": 1200}, {"begin": 147502, "end": 147633, "idx": 1201}, {"begin": 147634, "end": 147768, "idx": 1202}, {"begin": 147769, "end": 147872, "idx": 1203}, {"begin": 147873, "end": 147967, "idx": 1204}, {"begin": 147968, "end": 148103, "idx": 1205}, {"begin": 148104, "end": 148245, "idx": 1206}, {"begin": 148246, "end": 148361, "idx": 1207}, {"begin": 148362, "end": 148527, "idx": 1208}, {"begin": 148528, "end": 148639, "idx": 1209}, {"begin": 148640, "end": 148670, "idx": 1210}, {"begin": 148671, "end": 148754, "idx": 1211}, {"begin": 148755, "end": 148872, "idx": 1212}, {"begin": 148873, "end": 148953, "idx": 1213}, {"begin": 148954, "end": 149034, "idx": 1214}, {"begin": 149035, "end": 149218, "idx": 1215}, {"begin": 149219, "end": 149401, "idx": 1216}, {"begin": 149402, "end": 149624, "idx": 1217}, {"begin": 149625, "end": 149693, "idx": 1218}, {"begin": 149694, "end": 149830, "idx": 1219}, {"begin": 149831, "end": 149956, "idx": 1220}, {"begin": 149957, "end": 150075, "idx": 1221}, {"begin": 150076, "end": 150137, "idx": 1222}, {"begin": 150138, "end": 150174, "idx": 1223}, {"begin": 150175, "end": 150269, "idx": 1224}, {"begin": 150270, "end": 150449, "idx": 1225}, {"begin": 150450, "end": 150624, "idx": 1226}, {"begin": 150625, "end": 150663, "idx": 1227}, {"begin": 150664, "end": 150840, "idx": 1228}, {"begin": 150841, "end": 150880, "idx": 1229}, {"begin": 150881, "end": 150916, "idx": 1230}, {"begin": 150917, "end": 151070, "idx": 1231}, {"begin": 151071, "end": 151166, "idx": 1232}, {"begin": 151167, "end": 151322, "idx": 1233}, {"begin": 151323, "end": 151373, "idx": 1234}, {"begin": 151374, "end": 151471, "idx": 1235}, {"begin": 151472, "end": 151513, "idx": 1236}, {"begin": 151514, "end": 151646, "idx": 1237}, {"begin": 151647, "end": 151719, "idx": 1238}, {"begin": 151720, "end": 151818, "idx": 1239}, {"begin": 151852, "end": 151885, "idx": 1240}, {"begin": 151886, "end": 151985, "idx": 1241}, {"begin": 151986, "end": 152032, "idx": 1242}, {"begin": 152033, "end": 152194, "idx": 1243}, {"begin": 152195, "end": 152306, "idx": 1244}, {"begin": 152307, "end": 152357, "idx": 1245}, {"begin": 152358, "end": 152501, "idx": 1246}, {"begin": 152502, "end": 152634, "idx": 1247}, {"begin": 152635, "end": 152677, "idx": 1248}, {"begin": 152678, "end": 152738, "idx": 1249}, {"begin": 153082, "end": 153250, "idx": 1250}, {"begin": 153251, "end": 153332, "idx": 1251}, {"begin": 153333, "end": 153444, "idx": 1252}, {"begin": 153445, "end": 153541, "idx": 1253}, {"begin": 153542, "end": 153627, "idx": 1254}, {"begin": 153628, "end": 153715, "idx": 1255}, {"begin": 153716, "end": 153868, "idx": 1256}, {"begin": 153869, "end": 153950, "idx": 1257}, {"begin": 153951, "end": 154160, "idx": 1258}, {"begin": 154161, "end": 154214, "idx": 1259}, {"begin": 154253, "end": 154390, "idx": 1260}, {"begin": 154391, "end": 154587, "idx": 1261}, {"begin": 154588, "end": 154733, "idx": 1262}, {"begin": 154734, "end": 154836, "idx": 1263}, {"begin": 154837, "end": 154997, "idx": 1264}, {"begin": 154998, "end": 155060, "idx": 1265}, {"begin": 155110, "end": 155234, "idx": 1266}, {"begin": 155235, "end": 155334, "idx": 1267}, {"begin": 155335, "end": 155417, "idx": 1268}, {"begin": 155418, "end": 155549, "idx": 1269}, {"begin": 155550, "end": 155672, "idx": 1270}, {"begin": 155673, "end": 155818, "idx": 1271}, {"begin": 155866, "end": 156026, "idx": 1272}, {"begin": 156027, "end": 156206, "idx": 1273}, {"begin": 156207, "end": 156313, "idx": 1274}, {"begin": 156314, "end": 156422, "idx": 1275}, {"begin": 156423, "end": 156590, "idx": 1276}, {"begin": 156591, "end": 156682, "idx": 1277}, {"begin": 156707, "end": 156788, "idx": 1278}, {"begin": 156789, "end": 157020, "idx": 1279}, {"begin": 157021, "end": 157049, "idx": 1280}, {"begin": 157050, "end": 157218, "idx": 1281}, {"begin": 157219, "end": 157286, "idx": 1282}, {"begin": 157287, "end": 157427, "idx": 1283}, {"begin": 157428, "end": 157543, "idx": 1284}, {"begin": 157544, "end": 157656, "idx": 1285}, {"begin": 157657, "end": 157775, "idx": 1286}, {"begin": 157776, "end": 157892, "idx": 1287}, {"begin": 157893, "end": 157947, "idx": 1288}, {"begin": 157948, "end": 157995, "idx": 1289}, {"begin": 157996, "end": 158155, "idx": 1290}, {"begin": 158156, "end": 158309, "idx": 1291}, {"begin": 158310, "end": 158424, "idx": 1292}, {"begin": 158425, "end": 158646, "idx": 1293}, {"begin": 158647, "end": 158743, "idx": 1294}, {"begin": 158744, "end": 158889, "idx": 1295}, {"begin": 158890, "end": 158988, "idx": 1296}, {"begin": 158989, "end": 159058, "idx": 1297}, {"begin": 159091, "end": 159179, "idx": 1298}, {"begin": 159180, "end": 159384, "idx": 1299}, {"begin": 159385, "end": 159471, "idx": 1300}, {"begin": 159472, "end": 159568, "idx": 1301}, {"begin": 159569, "end": 159674, "idx": 1302}, {"begin": 159699, "end": 159830, "idx": 1303}, {"begin": 159831, "end": 160009, "idx": 1304}, {"begin": 160010, "end": 160077, "idx": 1305}, {"begin": 160078, "end": 160245, "idx": 1306}, {"begin": 160246, "end": 160277, "idx": 1307}, {"begin": 160278, "end": 160455, "idx": 1308}, {"begin": 160456, "end": 160598, "idx": 1309}, {"begin": 160599, "end": 160745, "idx": 1310}, {"begin": 160746, "end": 161023, "idx": 1311}, {"begin": 161024, "end": 161216, "idx": 1312}, {"begin": 161217, "end": 161393, "idx": 1313}, {"begin": 161418, "end": 161516, "idx": 1314}, {"begin": 161517, "end": 161591, "idx": 1315}, {"begin": 161592, "end": 161710, "idx": 1316}, {"begin": 161711, "end": 161874, "idx": 1317}, {"begin": 161875, "end": 162006, "idx": 1318}, {"begin": 162007, "end": 162192, "idx": 1319}, {"begin": 162193, "end": 162335, "idx": 1320}, {"begin": 162336, "end": 162420, "idx": 1321}, {"begin": 162421, "end": 162526, "idx": 1322}, {"begin": 162527, "end": 162689, "idx": 1323}, {"begin": 162690, "end": 162859, "idx": 1324}, {"begin": 162860, "end": 163028, "idx": 1325}, {"begin": 163029, "end": 163220, "idx": 1326}, {"begin": 163221, "end": 163234, "idx": 1327}, {"begin": 163235, "end": 163279, "idx": 1328}, {"begin": 163280, "end": 163356, "idx": 1329}, {"begin": 163357, "end": 163514, "idx": 1330}, {"begin": 163515, "end": 163701, "idx": 1331}, {"begin": 163702, "end": 164023, "idx": 1332}, {"begin": 164024, "end": 164124, "idx": 1333}, {"begin": 164151, "end": 164325, "idx": 1334}, {"begin": 164326, "end": 164505, "idx": 1335}, {"begin": 164539, "end": 164598, "idx": 1336}, {"begin": 164599, "end": 164756, "idx": 1337}, {"begin": 164757, "end": 164946, "idx": 1338}, {"begin": 164947, "end": 165097, "idx": 1339}, {"begin": 165098, "end": 165207, "idx": 1340}, {"begin": 165208, "end": 165340, "idx": 1341}, {"begin": 165341, "end": 165531, "idx": 1342}, {"begin": 165532, "end": 165746, "idx": 1343}, {"begin": 165747, "end": 165855, "idx": 1344}, {"begin": 165856, "end": 165887, "idx": 1345}, {"begin": 165888, "end": 166104, "idx": 1346}, {"begin": 166105, "end": 166178, "idx": 1347}, {"begin": 166179, "end": 166441, "idx": 1348}, {"begin": 166442, "end": 166584, "idx": 1349}, {"begin": 166585, "end": 166743, "idx": 1350}, {"begin": 166744, "end": 166828, "idx": 1351}, {"begin": 166829, "end": 166875, "idx": 1352}, {"begin": 166876, "end": 166969, "idx": 1353}, {"begin": 166989, "end": 167251, "idx": 1354}, {"begin": 167252, "end": 167392, "idx": 1355}, {"begin": 167393, "end": 167517, "idx": 1356}, {"begin": 167518, "end": 167702, "idx": 1357}, {"begin": 167703, "end": 167907, "idx": 1358}, {"begin": 167908, "end": 168000, "idx": 1359}, {"begin": 168001, "end": 168161, "idx": 1360}, {"begin": 168162, "end": 168269, "idx": 1361}, {"begin": 168270, "end": 168342, "idx": 1362}, {"begin": 168343, "end": 168439, "idx": 1363}, {"begin": 168440, "end": 168561, "idx": 1364}, {"begin": 168562, "end": 168754, "idx": 1365}, {"begin": 168755, "end": 168881, "idx": 1366}, {"begin": 168923, "end": 169151, "idx": 1367}, {"begin": 169152, "end": 169272, "idx": 1368}, {"begin": 169273, "end": 169396, "idx": 1369}, {"begin": 169397, "end": 169479, "idx": 1370}, {"begin": 169480, "end": 169626, "idx": 1371}, {"begin": 169627, "end": 169783, "idx": 1372}, {"begin": 169784, "end": 169868, "idx": 1373}, {"begin": 169869, "end": 169884, "idx": 1374}, {"begin": 169901, "end": 170106, "idx": 1375}, {"begin": 170107, "end": 170171, "idx": 1376}, {"begin": 170172, "end": 170516, "idx": 1377}, {"begin": 170517, "end": 170822, "idx": 1378}, {"begin": 170823, "end": 171029, "idx": 1379}, {"begin": 171030, "end": 171197, "idx": 1380}, {"begin": 171198, "end": 171416, "idx": 1381}, {"begin": 171417, "end": 171560, "idx": 1382}, {"begin": 171561, "end": 171771, "idx": 1383}, {"begin": 171772, "end": 171905, "idx": 1384}, {"begin": 171906, "end": 172029, "idx": 1385}, {"begin": 172030, "end": 172125, "idx": 1386}, {"begin": 172126, "end": 172240, "idx": 1387}, {"begin": 172241, "end": 172515, "idx": 1388}, {"begin": 172516, "end": 172616, "idx": 1389}, {"begin": 172617, "end": 172787, "idx": 1390}, {"begin": 172788, "end": 172859, "idx": 1391}, {"begin": 172860, "end": 173058, "idx": 1392}, {"begin": 173059, "end": 173143, "idx": 1393}, {"begin": 173144, "end": 173289, "idx": 1394}, {"begin": 173290, "end": 173468, "idx": 1395}, {"begin": 173469, "end": 173700, "idx": 1396}, {"begin": 173701, "end": 173936, "idx": 1397}, {"begin": 173937, "end": 174142, "idx": 1398}, {"begin": 174143, "end": 174236, "idx": 1399}, {"begin": 174237, "end": 174414, "idx": 1400}, {"begin": 174415, "end": 174630, "idx": 1401}, {"begin": 174631, "end": 174762, "idx": 1402}, {"begin": 174763, "end": 174873, "idx": 1403}, {"begin": 174874, "end": 175030, "idx": 1404}, {"begin": 175031, "end": 175249, "idx": 1405}, {"begin": 175250, "end": 175381, "idx": 1406}, {"begin": 175396, "end": 175442, "idx": 1407}, {"begin": 175443, "end": 175542, "idx": 1408}, {"begin": 175543, "end": 175650, "idx": 1409}, {"begin": 175651, "end": 175705, "idx": 1410}, {"begin": 175706, "end": 175950, "idx": 1411}, {"begin": 175951, "end": 176111, "idx": 1412}, {"begin": 176112, "end": 176270, "idx": 1413}, {"begin": 176271, "end": 176415, "idx": 1414}, {"begin": 176416, "end": 176631, "idx": 1415}, {"begin": 176632, "end": 176747, "idx": 1416}, {"begin": 176748, "end": 176848, "idx": 1417}, {"begin": 176849, "end": 176940, "idx": 1418}, {"begin": 176941, "end": 177127, "idx": 1419}, {"begin": 177128, "end": 177428, "idx": 1420}, {"begin": 177429, "end": 177576, "idx": 1421}, {"begin": 177577, "end": 177693, "idx": 1422}, {"begin": 177694, "end": 177877, "idx": 1423}, {"begin": 177878, "end": 178053, "idx": 1424}, {"begin": 178054, "end": 178298, "idx": 1425}, {"begin": 178299, "end": 178402, "idx": 1426}, {"begin": 178403, "end": 178477, "idx": 1427}, {"begin": 178478, "end": 178580, "idx": 1428}], "ReferenceToFigure": [{"begin": 49049, "end": 49050, "target": "#fig_0", "idx": 0}, {"begin": 49812, "end": 49813, "target": "#fig_0", "idx": 1}, {"begin": 77836, "end": 77837, "target": "#fig_2", "idx": 2}, {"begin": 92364, "end": 92365, "target": "#fig_2", "idx": 3}, {"begin": 96919, "end": 96920, "target": "#fig_5", "idx": 4}, {"begin": 97928, "end": 97929, "idx": 5}, {"begin": 98105, "end": 98106, "idx": 6}, {"begin": 137693, "end": 137694, "target": "#fig_2", "idx": 7}, {"begin": 156199, "end": 156200, "target": "#fig_5", "idx": 8}, {"begin": 157032, "end": 157033, "target": "#fig_9", "idx": 9}, {"begin": 159000, "end": 159002, "target": "#fig_10", "idx": 10}], "Div": [{"begin": 58, "end": 1905, "idx": 0}, {"begin": 1908, "end": 4309, "idx": 1}, {"begin": 4311, "end": 8198, "idx": 2}, {"begin": 8200, "end": 8415, "idx": 3}, {"begin": 8417, "end": 10411, "idx": 4}, {"begin": 10413, "end": 15887, "idx": 5}, {"begin": 15889, "end": 16405, "idx": 6}, {"begin": 16407, "end": 20365, "idx": 7}, {"begin": 20367, "end": 20972, "idx": 8}, {"begin": 20974, "end": 25193, "idx": 9}, {"begin": 25195, "end": 28066, "idx": 10}, {"begin": 28068, "end": 28595, "idx": 11}, {"begin": 28597, "end": 30000, "idx": 12}, {"begin": 30002, "end": 30242, "idx": 13}, {"begin": 30244, "end": 31863, "idx": 14}, {"begin": 31865, "end": 32438, "idx": 15}, {"begin": 32440, "end": 33478, "idx": 16}, {"begin": 33480, "end": 36050, "idx": 17}, {"begin": 36052, "end": 38338, "idx": 18}, {"begin": 38340, "end": 38576, "idx": 19}, {"begin": 38578, "end": 40029, "idx": 20}, {"begin": 40031, "end": 40637, "idx": 21}, {"begin": 40639, "end": 41586, "idx": 22}, {"begin": 41588, "end": 50971, "idx": 23}, {"begin": 50973, "end": 53974, "idx": 24}, {"begin": 53976, "end": 54509, "idx": 25}, {"begin": 54511, "end": 55410, "idx": 26}, {"begin": 55412, "end": 61801, "idx": 27}, {"begin": 61803, "end": 66657, "idx": 28}, {"begin": 66659, "end": 66959, "idx": 29}, {"begin": 66961, "end": 70573, "idx": 30}, {"begin": 70575, "end": 71617, "idx": 31}, {"begin": 71619, "end": 72761, "idx": 32}, {"begin": 72763, "end": 78394, "idx": 33}, {"begin": 78396, "end": 78653, "idx": 34}, {"begin": 78655, "end": 81105, "idx": 35}, {"begin": 81107, "end": 81722, "idx": 36}, {"begin": 81724, "end": 83376, "idx": 37}, {"begin": 83378, "end": 85490, "idx": 38}, {"begin": 85492, "end": 86026, "idx": 39}, {"begin": 86028, "end": 89169, "idx": 40}, {"begin": 89171, "end": 89857, "idx": 41}, {"begin": 89859, "end": 91377, "idx": 42}, {"begin": 91379, "end": 93066, "idx": 43}, {"begin": 93068, "end": 93864, "idx": 44}, {"begin": 93866, "end": 95649, "idx": 45}, {"begin": 95651, "end": 96209, "idx": 46}, {"begin": 96211, "end": 98321, "idx": 47}, {"begin": 98323, "end": 101912, "idx": 48}, {"begin": 101914, "end": 102704, "idx": 49}, {"begin": 102706, "end": 107317, "idx": 50}, {"begin": 107319, "end": 107987, "idx": 51}, {"begin": 107989, "end": 110852, "idx": 52}, {"begin": 110854, "end": 112788, "idx": 53}, {"begin": 112790, "end": 114964, "idx": 54}, {"begin": 114966, "end": 115140, "idx": 55}, {"begin": 115142, "end": 115453, "idx": 56}, {"begin": 115455, "end": 116691, "idx": 57}, {"begin": 116693, "end": 118773, "idx": 58}, {"begin": 118775, "end": 120868, "idx": 59}, {"begin": 120870, "end": 122275, "idx": 60}, {"begin": 122277, "end": 122762, "idx": 61}, {"begin": 122764, "end": 124546, "idx": 62}, {"begin": 124548, "end": 124921, "idx": 63}, {"begin": 124923, "end": 127830, "idx": 64}, {"begin": 127832, "end": 128230, "idx": 65}, {"begin": 128232, "end": 129928, "idx": 66}, {"begin": 129930, "end": 131013, "idx": 67}, {"begin": 131015, "end": 132882, "idx": 68}, {"begin": 132884, "end": 133658, "idx": 69}, {"begin": 133660, "end": 136390, "idx": 70}, {"begin": 136392, "end": 138183, "idx": 71}, {"begin": 138185, "end": 139639, "idx": 72}, {"begin": 139641, "end": 140241, "idx": 73}, {"begin": 140243, "end": 140504, "idx": 74}, {"begin": 140506, "end": 145118, "idx": 75}, {"begin": 145120, "end": 145783, "idx": 76}, {"begin": 145785, "end": 146237, "idx": 77}, {"begin": 146239, "end": 151818, "idx": 78}, {"begin": 151820, "end": 153051, "idx": 79}, {"begin": 153053, "end": 154214, "idx": 80}, {"begin": 154216, "end": 155060, "idx": 81}, {"begin": 155062, "end": 155818, "idx": 82}, {"begin": 155820, "end": 156682, "idx": 83}, {"begin": 156684, "end": 159058, "idx": 84}, {"begin": 159060, "end": 159674, "idx": 85}, {"begin": 159676, "end": 161393, "idx": 86}, {"begin": 161395, "end": 164124, "idx": 87}, {"begin": 164126, "end": 164505, "idx": 88}, {"begin": 164507, "end": 166969, "idx": 89}, {"begin": 166971, "end": 168881, "idx": 90}, {"begin": 168883, "end": 169884, "idx": 91}, {"begin": 169886, "end": 175381, "idx": 92}, {"begin": 175383, "end": 178580, "idx": 93}], "SectionMain": [{"begin": 1905, "end": 178580, "idx": 0}]}}