{"text": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine\n\nAbstract:\nWe publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.\n\n\n1 Introduction\nOne of the driving forces behind the recent success of deep learning in challenging tasks, such as object recognition (Krizhevsky et al., 2012), speech recognition (Xiong et al., 2016) and machine translation (Bahdanau et al., 2014), has been the increasing availability of large-scale annotated data.\nThis observation has also led to the interest in building a large-scale annotated dataset for question-answering. In 2015, Bordes et al. (2015) released a large-scale dataset of 100k open-world question-answer pairs constructed from Freebase, and Hermann et al. (2015) released two datasets, each consisting of closed-world question-answer pairs automatically generated from news articles. The latter was followed by Hill et al. (2015), Rajpurkar et al. (2016) and Onishi et al. (2016), each of which has released a set of large-scale closedworld question-answer pairs focused on a specific aspect of question-answering.\nLet us first take a step back, and ask what a full end-to-end pipeline for question-answering would look like. A general question-answering system would be able to answer a question about any domain, based on the world knowledge. This system would consist of three stages. A given question is read and reformulated in the first stage, followed by information retrieval via a search engine. An answer is then synthesized based on the query and a set of retrieved documents.\nWe notice a gap between the existing closedworld question-answering data sets and this conceptual picture of a general question-answering system. The general question-answering system must deal with a noisy set of retrieved documents, which likely consist of many irrelevant documents as well as semantically and syntactically illformed documents. On the other hand, most of the existing closed-world question-answering datasets were constructed in a way that the context provided for each question is guaranteed relevant and well-written. This guarantee comes from the fact that each question-answer-context tuple was generated starting from the context from which the question and answer were extracted.\nIn this paper, we build a new closed-world question-answering dataset that narrows this gap.\nUnlike most of the existing work, we start by building a set of question-answer pairs from Jeopardy!. We augment each question-answer pair, which does not have any context attached to it, by querying Google with the question. This process enables us to retrieve a realistic set of relevant/irrelevant documents, or more specifically their snippets. We filter out those questions whose answers could not be found within the retrieved snippets and those with less than forty web pages returned by Google. We end up with 140k+ question-answer pairs, and in total 6.9M snippets. 1 e evaluate this new dataset, to which we refer as SearchQA, with a variant of recently proposed attention sum reader (Kadlec et al., 2016) and with human volunteers. The evaluation shows that the proposed SearchQA is a challenging task both for humans and machines but there is still a significant gap between them. This suggests that the new dataset would be a valuable resource for further research and advance our ability to build a better automated question-answering system.\n\n2 SearchQA\nCollection A major goal of the new dataset is to build and provide to the public a machine comprehension dataset that better reflects a noisy information retrieval system. In order to achieve this goal, we need to introduce a natural, realistic noise to the context of each question-answer pair. We use a production-level search engine -Google-for this purpose.\nWe crawled the entire set of question-answer pairs from J! Archive 2 which has archived all the question-answer pairs from the popular television show Jeopardy!. We used the question from each pair to query Google in order to retrieve a set of relevant web page snippets. The relevancy in this case was fully determined by an unknown, but inproduction, algorithm underlying Google's search engine, making it much closer to a realistic scenario of question-answering.\nCleaning Because we do not have any control over the internals of Google search engine, we extensively cleaned up the entire set of questionanswer-context tuples. First, we removed any snippet returned that included the air-date of the Jeopardy! episode, the exact copy of the question, or a term \"Jeopardy!\", \"quiz\" or \"trivia\", to ensure that the answer could not be found trivially by a process of word/phrase matching. Furthermore, we manually checked any URL, from which these removed snippets were taken, that occurs more than 50 times and removed any that explicitly contains Jeopardy! question-answer pairs.\nAmong the remaining question-answer-context tuples, we removed any tuple whose context did not include the answer. This was done mainly for computational efficiency in building a questionanswering system using the proposed dataset. We kept only those tuples whose answers were three or less words long.\nBasic Statistics After all these processes, we have ended up with 140,461 question-answer pairs. Each pair is coupled with a set of 49.6\u00b12.10 snippets on average. Each snippet is 37.3\u00b111.7 tokens long on average. Answers are on average 1.47\u00b10.58 tokens long. There are 1,257,327 unique tokens.\n\nMeta-Data\nWe collected for each questionanswer-context tuple additional metadata from Jeopardy! and returned by Google. More specifically, from Jeopardy! we have the category, dollar value, show number and air date for each question. From Google, we have the URL, title and a set of related links (often none) for each snippet. Although we do not use them in this paper, these items are included in the public release of SearchQA and may be used in the future. An example of one question-answer pair with just one snippet is presented in Fig. 1.\nTraining, Validation and Test Sets In order to maximize its reusability and reproducibility, we provide a predefined split of the dataset into training, validation and test sets. One of the most important aspects in question-answering is whether a question-answering machine would generalize to unseen questions from the future. We thus ensure that these three sets consist of questionanswer pairs from non-overlapping years, and that the validation and test question-answer pairs are from years later than the training set's pairs. The training, validation and test sets consist of 99,820, 13,393 and 27,248 examples, respectively. Among these, examples with unigram answers are respectively 55,648, 8,672 and 17,056.\n\n3 Related Work\nOpen-World Question-Answering An openworld question-answering dataset consists of a set of question-answer pairs and the knowledge database. It does not come with an explicit link between each question-answer pair and any specific entry in the knowledge database. A representative example of such a dataset is SimpleQA by (Bordes et al., 2015). SimpleQA consists of 100k questionanswer pairs, and uses Freebase as a knowledge database. The major limitation of this dataset is that all the questions are simple in that all of them are in the form of (subject, relationship, ?).\n\nClosed-World Question-Answering\nAlthough we use open-world snippets, the final SearchQA is a closed-world question-answering dataset since each question can be answered entirely based on the associated snippets. One family of such datasets includes Children's Book dataset (Hill et al., 2015), CNN and DailyMail (Hermann et al., 2015). Each question-answer-context tuple in these datasets was constructed by first selecting the context article and then creating a questionanswer pair, where the question is a sentence with a missing word and the answer is the missing word. This family differs from SearchQA in two aspects. First, in SearchQA we start from a question-answer pair, and, second, our question is not necessarily of a fill-in-a-word type.\nAnother family is an extension of the former family of datasets.\nThis family includes SQuAD (Rajpurkar et al., 2016) and NEWSQA (Trischler et al., 2016). Unlike the first family, answers in this family are often multiword phrases, and they do not necessarily appear as they are in the corresponding context. In contrast, in SearchQA we ensure that all multi-word phrase answers appear in their corresponding context. Answers, often as well as questions, are thus often crowd-sourced in this family of datasets. Nonetheless, each tuple in these datasets was however also constructed starting from a corresponding context article, making them less realistic than the proposed SearchQA.\n\nAnswer\nUnigram MS MARCO (Nguyen et al., 2016) -the most recently released dataset to our knowledge-is perhaps most similar to the proposed SearchQA.  Nguyen et al. (2016) selected a subset of actual user-generated queries to Microsoft Bing that correspond to questions. These questions are augmented with a manually selected subset of snippets returned by Bing. The question is then answered by a human. Two major differences between MS MARCO and SearchQA are the choice of questions and search engine. We believe the comparison between MS MARCO and the proposed SearchQA would be valuable for expanding our understanding on how the choice of search engines as well as types of questions impact question-answering systems in the future.\n\n4 Experiments and Results\nAs a part of our release of SearchQA, we provide a set of baseline performances against which other researchers may compare their future approaches. Unlike most of the previous datasets, SearchQA augments each question-answer pair with a noisy, real context retrieved from the largest search engine in the world. This implies that the human performance is not necessarily the upper-bound but we nevertheless provide it as a guideline.\n\n4.1 Human Evaluation\nWe designed a web interface that displays a query and retrieved snippets and lets a user select an answer by clicking words on the screen. A user is given up to 40 minutes to answer as many questions as possible. We randomly select questionanswer-context pairs from the test set.\nWe recruited thirteen volunteers from the master's program in the Center for Data Science at NYU. They were uniform-randomly split into two groups. The first group was presented with questions that have single-word (unigram) answers only, and the other group with questions that have either single-word or multi-word (n-gram) answers. On average, each participant answers 47.23 questions with the standard deviation of 30.42.\nWe report the average and standard deviation of the accuracy achieved by the volunteers in Table 1. We notice the significant gap between the accuracies by the first and second groups, suggesting that the difficulty of question-answering grows as the length of the answer increases. Also, according to the F1 scores, we observe a large gap between the ASR and humans. This suggests the potential for the proposed SearchQA as a benchmark for advancing question-answering research. Overall, we found the performance of human volunteers much lower than expected and suspect the following underlying reasons. First, snippets are noisy, as they are often excerpts not full sentences. Second, human volunteers may have become exhausted over the trial. We leave more detailed analysis of the performance of human subjects on the proposed SearchQA for the future.\n\n4.2 Machine Baselines\nTF-IDF Max An interesting property of the proposed SearchQA is that the context of each question-answer pair was retrieved by Google with the question as a query. This implies that the information about the question itself may be implicitly embedded in the snippets. We therefore test a naive strategy (TF-IDF Max) of selecting the word with the highest TF-IDF score in the context as an answer. Note that this can only be used for the questions with a unigram answer.\nAttention Sum Reader Attention sum reader (ASR, Kadlec et al., 2016) is a variant of a pointer network (Vinyals et al., 2015) that was specifically constructed to solve a cloze-style questionanswering task. ASR consists of two encoding recurrent networks. The first network encodes a given context c, which is the concatenation of all the snippets in the case of SearchQA, into a set of hidden vectors {h c j }, and the second network encodes a question q into a single vector h q . The dot product between each hidden vector from the context and the question vector is exponentiated to form word scores \u03b2 j = exp(h q h c j ). ASR then pulls these word scores by summing the scores of the same word, resulting in a set of unique word scores \u03b2 i = j\u2208D i \u03b2 j , where D i indicates where the word i appears in the context. These uniqueword scores are normalized, and we obtain an answer distribution p(i|c, q) = \u03b2 i / i \u03b2 i . The ASR is trained to maximize this (log-)probability of the correct answer word in the context. Table 2 : The accuracies on the validation and test sets using the non-trainable baseline (TF-IDF Max) and the trainable baseline (ASR). We report top-1/5 accuracies for unigram answers, and otherwise, F1 scores. This vanilla ASR only works with a unigram answer and is not suitable for an n-gram answer. We avoid this issue by introducing another recurrent network which encodes the previous answer words (\u00e2 1 , . . . , \u00e2l\u22121 ) into a vector h a . This vector is added to the question vectors, i.e., h q \u2190 h q +h a .\nDuring training, we use the correct previou answer words, while we let the model, called n-gram ASR, predict one answer at a time until it predicts answer . This special token, appended to the context, indicates the end of the answer.\nWe try both the vanilla and n-gram ASR's on the unigram-answer-only subset and on the whole set, respectively. We use recurrent networks with 100 gated recurrent units (GRU, Cho et al., 2014) for both unigram and n-gram models, respectively. We use Adam (Kingma and Ba, 2014) and dropout (Srivastava et al., 2014) for training.\nResult We report the results in Table 2. We see that the attention sum reader is below human evaluation, albeit by a rather small margin. Also, TF-IDF Max scores are not on par when compared to ASR which is perhaps not surprising. Given the unstructured nature of SearchQA, we believe improvements on the benchmarks presented are crucial for developing a real-world Q&A system.\n\n5 Conclusion\nWe constructed a new dataset for questionanswering research, called SearchQA. It was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. We conducted human evaluation as well as machine evaluation. Using the latest technique, ASR, we show that there is a meaningful gap between humans and machines, which suggests the potential of SearchQA as a benchmark task for question-answering research. We release SearchQA publicly, including our own implementation of ASR and n-gram ASR in PyTorch. 3\n\nFootnotes:\n1: The dataset can be found at https://github.com/ nyu-dl/SearchQA.\n2: http://j-archive.com\n\nReferences:\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .- Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075 .\n\n- Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014).\n\n- Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems. pages 1693- 1701.\n\n- Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representa- tions. arXiv preprint arXiv:1511.02301 .\n\n- Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. arXiv preprint arXiv:1603.01547 .\n\n- Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classification with deep con- volutional neural networks. In Advances in neural information processing systems. pages 1097-1105.\n\n- Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 . 3 http://pytorch.org/ Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gim- pel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. arXiv preprint arXiv:1608.05457 .\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 .\n\n- Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Re- search 15(1):1929-1958.\n\n- Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2016. NewsQA: A machine compre- hension dataset. arXiv preprint arXiv:1611.09830 .\n\n- Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural In- formation Processing Systems. pages 2692-2700.\n\n- Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. 2016. Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256 .\n\n", "annotations": {"Abstract": [{"begin": 73, "end": 1227, "idx": 0}], "Head": [{"begin": 1230, "end": 1244, "n": "1", "idx": 0}, {"begin": 4498, "end": 4508, "n": "2", "idx": 1}, {"begin": 6552, "end": 6561, "idx": 2}, {"begin": 7818, "end": 7832, "n": "3", "idx": 3}, {"begin": 8411, "end": 8442, "idx": 4}, {"begin": 9848, "end": 9854, "idx": 5}, {"begin": 10586, "end": 10611, "n": "4", "idx": 6}, {"begin": 11048, "end": 11068, "n": "4.1", "idx": 7}, {"begin": 12632, "end": 12653, "n": "4.2", "idx": 8}, {"begin": 15602, "end": 15614, "n": "5", "idx": 9}], "ReferenceToBib": [{"begin": 1363, "end": 1388, "target": "#b7", "idx": 0}, {"begin": 1409, "end": 1429, "target": "#b13", "idx": 1}, {"begin": 1454, "end": 1477, "target": "#b0", "idx": 2}, {"begin": 1670, "end": 1690, "target": "#b1", "idx": 3}, {"begin": 1780, "end": 1815, "idx": 4}, {"begin": 1964, "end": 1982, "target": "#b4", "idx": 5}, {"begin": 1984, "end": 2007, "target": "#b9", "idx": 6}, {"begin": 2012, "end": 2032, "target": "#b8", "idx": 7}, {"begin": 4134, "end": 4155, "target": "#b5", "idx": 8}, {"begin": 8155, "end": 8176, "target": "#b1", "idx": 9}, {"begin": 8684, "end": 8703, "target": "#b4", "idx": 10}, {"begin": 8705, "end": 8745, "idx": 11}, {"begin": 9255, "end": 9279, "target": "#b9", "idx": 12}, {"begin": 9291, "end": 9315, "target": "#b11", "idx": 13}, {"begin": 9872, "end": 9893, "target": "#b8", "idx": 14}, {"begin": 9998, "end": 10018, "target": "#b8", "idx": 15}, {"begin": 13171, "end": 13191, "target": "#b5", "idx": 16}, {"begin": 13226, "end": 13248, "target": "#b12", "idx": 17}, {"begin": 15069, "end": 15086, "target": "#b2", "idx": 18}, {"begin": 15149, "end": 15170, "target": "#b6", "idx": 19}, {"begin": 15183, "end": 15208, "target": "#b10", "idx": 20}], "ReferenceToFootnote": [{"begin": 4015, "end": 4016, "target": "#foot_0", "idx": 0}, {"begin": 4938, "end": 4939, "target": "#foot_1", "idx": 1}], "SectionFootnote": [{"begin": 16267, "end": 16369, "idx": 0}], "ReferenceString": [{"begin": 16386, "end": 16549, "id": "b0", "idx": 0}, {"begin": 16551, "end": 16715, "id": "b1", "idx": 1}, {"begin": 16719, "end": 17006, "id": "b2", "idx": 2}, {"begin": 17010, "end": 17254, "id": "b3", "idx": 3}, {"begin": 17258, "end": 17448, "id": "b4", "idx": 4}, {"begin": 17452, "end": 17615, "id": "b5", "idx": 5}, {"begin": 17619, "end": 17731, "id": "b6", "idx": 6}, {"begin": 17735, "end": 17936, "id": "b7", "idx": 7}, {"begin": 17940, "end": 18342, "id": "b8", "idx": 8}, {"begin": 18346, "end": 18513, "id": "b9", "idx": 9}, {"begin": 18517, "end": 18742, "id": "b10", "idx": 10}, {"begin": 18746, "end": 18943, "id": "b11", "idx": 11}, {"begin": 18947, "end": 19095, "id": "b12", "idx": 12}, {"begin": 19099, "end": 19314, "id": "b13", "idx": 13}], "ReferenceToTable": [{"begin": 11872, "end": 11873, "target": "#tab_0", "idx": 0}, {"begin": 14149, "end": 14150, "idx": 1}, {"begin": 15261, "end": 15262, "idx": 2}], "Footnote": [{"begin": 16278, "end": 16345, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 16346, "end": 16369, "id": "foot_1", "n": "2", "idx": 1}], "Paragraph": [{"begin": 83, "end": 1227, "idx": 0}, {"begin": 1245, "end": 1546, "idx": 1}, {"begin": 1547, "end": 2167, "idx": 2}, {"begin": 2168, "end": 2640, "idx": 3}, {"begin": 2641, "end": 3346, "idx": 4}, {"begin": 3347, "end": 3439, "idx": 5}, {"begin": 3440, "end": 4496, "idx": 6}, {"begin": 4509, "end": 4870, "idx": 7}, {"begin": 4871, "end": 5337, "idx": 8}, {"begin": 5338, "end": 5953, "idx": 9}, {"begin": 5954, "end": 6256, "idx": 10}, {"begin": 6257, "end": 6550, "idx": 11}, {"begin": 6562, "end": 7097, "idx": 12}, {"begin": 7098, "end": 7816, "idx": 13}, {"begin": 7833, "end": 8409, "idx": 14}, {"begin": 8443, "end": 9162, "idx": 15}, {"begin": 9163, "end": 9227, "idx": 16}, {"begin": 9228, "end": 9846, "idx": 17}, {"begin": 9855, "end": 10584, "idx": 18}, {"begin": 10612, "end": 11046, "idx": 19}, {"begin": 11069, "end": 11348, "idx": 20}, {"begin": 11349, "end": 11774, "idx": 21}, {"begin": 11775, "end": 12630, "idx": 22}, {"begin": 12654, "end": 13122, "idx": 23}, {"begin": 13123, "end": 14659, "idx": 24}, {"begin": 14660, "end": 14894, "idx": 25}, {"begin": 14895, "end": 15222, "idx": 26}, {"begin": 15223, "end": 15600, "idx": 27}, {"begin": 15615, "end": 16265, "idx": 28}], "SectionHeader": [{"begin": 0, "end": 1227, "idx": 0}], "SectionReference": [{"begin": 16371, "end": 19316, "idx": 0}], "Sentence": [{"begin": 83, "end": 196, "idx": 0}, {"begin": 197, "end": 369, "idx": 1}, {"begin": 370, "end": 582, "idx": 2}, {"begin": 583, "end": 729, "idx": 3}, {"begin": 730, "end": 904, "idx": 4}, {"begin": 905, "end": 1048, "idx": 5}, {"begin": 1049, "end": 1131, "idx": 6}, {"begin": 1132, "end": 1227, "idx": 7}, {"begin": 1245, "end": 1546, "idx": 8}, {"begin": 1547, "end": 1660, "idx": 9}, {"begin": 1661, "end": 1936, "idx": 10}, {"begin": 1937, "end": 2167, "idx": 11}, {"begin": 2168, "end": 2278, "idx": 12}, {"begin": 2279, "end": 2397, "idx": 13}, {"begin": 2398, "end": 2440, "idx": 14}, {"begin": 2441, "end": 2557, "idx": 15}, {"begin": 2558, "end": 2640, "idx": 16}, {"begin": 2641, "end": 2786, "idx": 17}, {"begin": 2787, "end": 2988, "idx": 18}, {"begin": 2989, "end": 3180, "idx": 19}, {"begin": 3181, "end": 3346, "idx": 20}, {"begin": 3347, "end": 3439, "idx": 21}, {"begin": 3440, "end": 3541, "idx": 22}, {"begin": 3542, "end": 3665, "idx": 23}, {"begin": 3666, "end": 3788, "idx": 24}, {"begin": 3789, "end": 3942, "idx": 25}, {"begin": 3943, "end": 4016, "idx": 26}, {"begin": 4017, "end": 4182, "idx": 27}, {"begin": 4183, "end": 4332, "idx": 28}, {"begin": 4333, "end": 4496, "idx": 29}, {"begin": 4509, "end": 4680, "idx": 30}, {"begin": 4681, "end": 4804, "idx": 31}, {"begin": 4805, "end": 4870, "idx": 32}, {"begin": 4871, "end": 5032, "idx": 33}, {"begin": 5033, "end": 5142, "idx": 34}, {"begin": 5143, "end": 5337, "idx": 35}, {"begin": 5338, "end": 5500, "idx": 36}, {"begin": 5501, "end": 5583, "idx": 37}, {"begin": 5584, "end": 5647, "idx": 38}, {"begin": 5648, "end": 5760, "idx": 39}, {"begin": 5761, "end": 5930, "idx": 40}, {"begin": 5931, "end": 5953, "idx": 41}, {"begin": 5954, "end": 6068, "idx": 42}, {"begin": 6069, "end": 6185, "idx": 43}, {"begin": 6186, "end": 6256, "idx": 44}, {"begin": 6257, "end": 6353, "idx": 45}, {"begin": 6354, "end": 6398, "idx": 46}, {"begin": 6399, "end": 6419, "idx": 47}, {"begin": 6420, "end": 6469, "idx": 48}, {"begin": 6470, "end": 6502, "idx": 49}, {"begin": 6503, "end": 6515, "idx": 50}, {"begin": 6516, "end": 6550, "idx": 51}, {"begin": 6562, "end": 6671, "idx": 52}, {"begin": 6672, "end": 6705, "idx": 53}, {"begin": 6706, "end": 6785, "idx": 54}, {"begin": 6786, "end": 6879, "idx": 55}, {"begin": 6880, "end": 7012, "idx": 56}, {"begin": 7013, "end": 7097, "idx": 57}, {"begin": 7098, "end": 7276, "idx": 58}, {"begin": 7277, "end": 7426, "idx": 59}, {"begin": 7427, "end": 7630, "idx": 60}, {"begin": 7631, "end": 7730, "idx": 61}, {"begin": 7731, "end": 7816, "idx": 62}, {"begin": 7833, "end": 7973, "idx": 63}, {"begin": 7974, "end": 8096, "idx": 64}, {"begin": 8097, "end": 8177, "idx": 65}, {"begin": 8178, "end": 8268, "idx": 66}, {"begin": 8269, "end": 8409, "idx": 67}, {"begin": 8443, "end": 8622, "idx": 68}, {"begin": 8623, "end": 8746, "idx": 69}, {"begin": 8747, "end": 8984, "idx": 70}, {"begin": 8985, "end": 9034, "idx": 71}, {"begin": 9035, "end": 9162, "idx": 72}, {"begin": 9163, "end": 9227, "idx": 73}, {"begin": 9228, "end": 9316, "idx": 74}, {"begin": 9317, "end": 9470, "idx": 75}, {"begin": 9471, "end": 9579, "idx": 76}, {"begin": 9580, "end": 9673, "idx": 77}, {"begin": 9674, "end": 9846, "idx": 78}, {"begin": 9855, "end": 9996, "idx": 79}, {"begin": 9997, "end": 10117, "idx": 80}, {"begin": 10118, "end": 10209, "idx": 81}, {"begin": 10210, "end": 10251, "idx": 82}, {"begin": 10252, "end": 10350, "idx": 83}, {"begin": 10351, "end": 10584, "idx": 84}, {"begin": 10612, "end": 10760, "idx": 85}, {"begin": 10761, "end": 10924, "idx": 86}, {"begin": 10925, "end": 11046, "idx": 87}, {"begin": 11069, "end": 11207, "idx": 88}, {"begin": 11208, "end": 11281, "idx": 89}, {"begin": 11282, "end": 11348, "idx": 90}, {"begin": 11349, "end": 11446, "idx": 91}, {"begin": 11447, "end": 11496, "idx": 92}, {"begin": 11497, "end": 11683, "idx": 93}, {"begin": 11684, "end": 11774, "idx": 94}, {"begin": 11775, "end": 11874, "idx": 95}, {"begin": 11875, "end": 12057, "idx": 96}, {"begin": 12058, "end": 12142, "idx": 97}, {"begin": 12143, "end": 12254, "idx": 98}, {"begin": 12255, "end": 12379, "idx": 99}, {"begin": 12380, "end": 12453, "idx": 100}, {"begin": 12454, "end": 12520, "idx": 101}, {"begin": 12521, "end": 12630, "idx": 102}, {"begin": 12654, "end": 12816, "idx": 103}, {"begin": 12817, "end": 12920, "idx": 104}, {"begin": 12921, "end": 13049, "idx": 105}, {"begin": 13050, "end": 13122, "idx": 106}, {"begin": 13123, "end": 13329, "idx": 107}, {"begin": 13330, "end": 13378, "idx": 108}, {"begin": 13379, "end": 13605, "idx": 109}, {"begin": 13606, "end": 13749, "idx": 110}, {"begin": 13750, "end": 13942, "idx": 111}, {"begin": 13943, "end": 14045, "idx": 112}, {"begin": 14046, "end": 14142, "idx": 113}, {"begin": 14143, "end": 14279, "idx": 114}, {"begin": 14280, "end": 14355, "idx": 115}, {"begin": 14356, "end": 14447, "idx": 116}, {"begin": 14448, "end": 14561, "idx": 117}, {"begin": 14562, "end": 14590, "idx": 118}, {"begin": 14591, "end": 14659, "idx": 119}, {"begin": 14660, "end": 14816, "idx": 120}, {"begin": 14817, "end": 14894, "idx": 121}, {"begin": 14895, "end": 15005, "idx": 122}, {"begin": 15006, "end": 15136, "idx": 123}, {"begin": 15137, "end": 15222, "idx": 124}, {"begin": 15223, "end": 15263, "idx": 125}, {"begin": 15264, "end": 15360, "idx": 126}, {"begin": 15361, "end": 15453, "idx": 127}, {"begin": 15454, "end": 15600, "idx": 128}, {"begin": 15615, "end": 15692, "idx": 129}, {"begin": 15693, "end": 15755, "idx": 130}, {"begin": 15756, "end": 15910, "idx": 131}, {"begin": 15911, "end": 15971, "idx": 132}, {"begin": 15972, "end": 16166, "idx": 133}, {"begin": 16167, "end": 16265, "idx": 134}], "ReferenceToFigure": [{"begin": 7095, "end": 7096, "target": "#fig_0", "idx": 0}], "Div": [{"begin": 83, "end": 1227, "idx": 0}, {"begin": 1230, "end": 4496, "idx": 1}, {"begin": 4498, "end": 6550, "idx": 2}, {"begin": 6552, "end": 7816, "idx": 3}, {"begin": 7818, "end": 8409, "idx": 4}, {"begin": 8411, "end": 9846, "idx": 5}, {"begin": 9848, "end": 10584, "idx": 6}, {"begin": 10586, "end": 11046, "idx": 7}, {"begin": 11048, "end": 12630, "idx": 8}, {"begin": 12632, "end": 15600, "idx": 9}, {"begin": 15602, "end": 16265, "idx": 10}], "SectionMain": [{"begin": 1227, "end": 16265, "idx": 0}]}}