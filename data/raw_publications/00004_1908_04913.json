{"text": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age\n\nAbstract:\nExisting public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups. The dataset will be released via {https://github.com/joojs/fairface}.\n\n\n1. Introduction\nTo date, numerous large scale face image datasets [22, 31, 13, 69, 37, 24, 41, 68, 15, 27, 46, 8, 39] have been proposed and fostered research and development for automated face detection [35, 21], alignment [66, 44], recognition [57, 49], generation [67, 5, 26, 58], modification [3, 32, 19], and attribute classification [31, 37]. These systems have been successfully translated into many areas including security, medicine, education, and social sciences.\nDespite the sheer amount of available data, existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. A recent study shows that most existing large scale face databases are biased towards \"lighter skin\" faces (around 80%), e.g., White, compared to \"darker\" faces, e.g., Black [39]. This means the model may not apply to some subpopulations and its results may not be compared across different groups without calibration. Biased data will pro-duce biased models trained from it. This will raise ethical concerns about fairness of automated systems, which has emerged as a critical topic of study in the recent machine learning and AI literature [17, 11].\nFor example, several commercial computer vision systems (Microsoft, IBM, Face++) have been criticized due to their asymmetric accuracy across sub-demographics in recent studies [7, 42]. These studies found that the commercial face gender classification systems all perform better on male and on light faces. This can be caused by the biases in their training data. Various unwanted biases in image datasets can easily occur due to biased selection, capture, and negative sets [60]. Most public large scale face datasets have been collected from popular online media -newspapers, Wikipedia, or webs search-and these platforms are more frequently used by or showing White people.\nTo mitigate the race bias in the existing face datasets, we propose a novel face dataset with an emphasis of balanced race composition. Our dataset contains 108,501 facial images collected primarily from the YFCC-100M Flickr dataset [59], which can be freely shared for a research purpose, and also includes examples from other sources such as Twitter and online newspaper outlets. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Our dataset is well-balanced on these 7 groups (See Figure 3 and 2)\nOur paper make three main contributions. First, we emprically show that existing face attribute datasets and models learned from them do not generalize well to unseen data in which more non-White faces are present. Second, we show that our new dataset perform better on the novel data, not only on average, but also across racial groups, i.e. more consistent. Third, to the best of our knowledge, our dataset is the first large scale face attribute dataset in the wild which includes Latino and Middle Eastern and differentiates East Asian and South East Asian. Computer vision has been rapidly transferred into other fields such as economics or social sciences, where researchers want to analyze different demographics using image data. The inclusion of major racial groups, which have been missing in existing datasets, therefore significantly enlarges the appli-\n\nIndian\n\n\nLatino\nBalanced? W ME E SE PPB [7] Gov. Official Profiles 1K **Skin color prediction MORPH [45] Public Data 55K merged no PubFig [31] Celebrity 13K Model generated predictions no IMDB-WIKI [46] IMDB, WIKI 500K no FotW [13] Flickr 25K yes CACD [10] celebrity 160K no DiF [39] Flickr 1M **Skin color prediction \u2020CelebA [37] CelebFace [54, 55] LFW [23] 200K no LFW+ [16] LFW [23 cability of computer vision methods to these fields.\n\n2. Related Work\n\n\n2.1. Face Attribute Recognition\nFace attribute recognition is a task to classify various human attributes such as gender, race, age, emotions, expressions or other facial traits from facial appearance [31, 25, 74, 37]. While there have been many techniques developed for the task, we mainly review datasets which are the main concern of this paper.\nTable 1 summarizes the statistics of existing large scale face attribute datasets and our new dataset. The is not an exhaustive list but we focus on public and in-the-wild datasets on gender, race, and age. As stated earlier, most of these datasets were constructed from online sources, which are typically dominated by the White race.\nFace attribute recognition has been applied as a subcomponent to other computer vision systems. For example, Kumar et al. [31] used facial attributes such as gender, race, hair style, expressions, and accessories as features for face verification, as the attributes characterize individual traits. Attributes are also widely used for person re-identification in images or videos, combining features from human face and body appearance [33, 34, 53], especially effective when faces are not fully visible or too small. These systems have applications in security such as authentication for elec-tronic devices (e.g., unlocking smartphones) or monitoring surveillance CCTVs [14].\nIt is imperative to ensure that these systems perform evenly well on difference gender and race groups. Failing to do so can be detrimental to the reputations of individual service providers and the public trust about the machine learning and computer vision research community. Most notable incidents regarding the racial bias include Google Photos recognizing African American faces as Gorilla and Nikon's digital cameras prompting a message asking \"did someone blink?\" to Asian users [73]. These incidents, regardless of whether the models were trained improperly or how much they actually affected the users, often result in the termination of the service or features (e.g., dropping sensitive output categories). For the reason, most commercial service providers have stopped providing a race classifier.\nFace attribute recognition is also widely used for demographic survey performed in marketing or social science research, aimed at understanding human social behaviors and their relations to demographic backgrounds of individuals. Using off-the-shelf tools [2, 4] and commercial services, social scientists, who traditionally didn't use images, begun to use images of people to infer their demographic attributes and analyze their behaviors in many studies. Notable examples are demographic analyses of social media users using their photographs [9, 43, 64, 65, 62]. The cost of unfair classification is huge as it can over-or under-estimate specific sub-populations in their analysis, which may have policy implications.\n\n2.2. Fair Classification and Dataset Bias\nResearchers in AI and machine learning have increasingly paid attention to algorithmic fairness and dataset and model biases [71, 11, 76, 72]. There exist many different definitions of fairness used in the literature [61]. In this paper, we focus on balanced accuracy-whether the attribute classification accuracy is independent of race and gender. More generally, research in fairness is concerned with a model's ability to produce fair outcomes (e.g., loan approval) independent of protected or sensitive attributes such as race or gender.\nStudies in algorithmic fairness have focused on either 1) discovering (auditing) existing bias in datasets or systems [50, 7, 30], 2) making a better dataset [39, 1], or 3) designing a better algorithm or model [12, 1, 47, 71, 70]. Our paper falls into the first two categories.\nIn computer vision, it has been shown that popular large scale image datasets such as Imagenet are biased in terms of the origin of images (45% were from the U.S.) [56] or the underlying association between scene and race [52]. Can we make a perfectly balanced dataset? It is \"infeasible to balance across all possible co-occurrences\" of attributes [20]. This is possible in a lab-controlled setting, but not in a dataset \"in-the-wild\".\nTherefore, the contribution of our paper is to mitigate, not entirely solve, the current limitation and biases of existing databases by collecting more diverse face images from non-White race groups. We empirically show this significantly improves the generalization performance to novel image datasets whose racial compositions are not dominated by the White race. Furthermore, as shown in Table 1, our dataset is the first large scale in-the-wild face image dataset which includes Southeast Asian and Middle Eastern races. While their faces share similarity with East Asian and White groups, we argue that not having these major race groups in datasets is a strong form of discrimination.\n\n3. Dataset Construction\n\n\n3.1. Race Taxonomy\nIn this study, we define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino.Race and ethnicity are different categorizations of humans. Race is defined based on physical trait and ethnicity is based on cultural similarities [48]. For example, Asian immigrants in Latin America can be of Latino ethnicity. In practice, these two terms are often used interchangeably. Race is not a discrete concept and needs to be clearly defined before data collection.\nWe first adopted a commonly accepted race classification from the U.S. Census Bureau (White, Black, Asian, Hawaiian and Pacific Islanders, Native Americans, and Latino). Latino is often treated as an ethnicity, but we consider Latino a race, which can be judged from the facial appearance. We then further divided subgroups such as Middle Eastern, East Asian, Southeast Asian, and Indian, as they look clearly distinct. During the data collection, we found very few examples for Hawaiian and Pacific Islanders and Native Americans and discarded these categories. All the experiments conducted in this paper were therefore based on 7 race classification.\nA few recent studies [7, 39] use skin color as a proxy to racial or ethnicity grouping. While skin color can be easily computed without subjective annotations, it has limitations. First, skin color is heavily affected by illumination and light conditions. The Pilot Parliaments Benchmark (PPB) dataset [7] only used profile photographs of government officials taken in well controlled lighting, which makes it non-inthe-wild. Second, within-group variations of skin color are huge. Even same individuals can show different skin colors over time. Third, most importantly, race is a multidimensional concept whereas skin color (i.e., brightness) is one dimensional. Figure 1 shows the distributions of the skin color of multiple race groups, measured by Individual Typology Angle (ITA) [63]. As clearly shown here, the skin color provides no information to differentiate many groups\n\n3.2. Image Collection and Annotation\nMany existing face datasets have relied on photographs of public figures such as politicians or celebrities [31, 23, 24, 46, 37]. Despite the easiness of collecting images and ground truth attributes, the selection of these populations may be biased. For examples, politicians may be older and actors may be more attractive than typical faces. Their images are usually taken by professional photographers in limited situations, leading to the quality bias. Some datasets were collected via web search using keywords such as \"Asian boy\" [75]. These queries may return only stereotypical faces or prioritize celebrities in those categories rather than diverse individuals among general public.\nOur goal is to minimize the selection bias introduced by such filtering and maximize the diversity and coverage of the dataaset. We started from a huge public image dataset, Yahoo YFCC100M dataset [59], and detected faces from the images without any preselection. A recent work also used the same dataset to construct a huge unfiltered face dataset (Diversity in Face, DiF) [39]. Our dataset is smaller but more balanced on race (See Figure 2). Specifically, we incrementally increased the dataset size. We first detected and annotated 7,125 faces randomly sampled from the entire YFCC100M dataset ignoring the locations of images. After obtaining annotations on this initial set, we estimated demographic compositions of each country. Based on this statistic, we adaptively adjusted the number of images for each country sampled from the dataset such that the dataset is not dominated by the White race. Consequently, we excluded the U.S. and European countries in the later stage of data collection after we sampled enough White faces from those countries. The minimum size of a detected face was set to 50 by 50 pixels. This is a relatively smaller size compared to other datasets, but we find the attributes are still recognizable and these examples can actually make the classifiers more robust against noisy data. We only used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses, which allow derivative work and commercial usages.\nWe used Amazon Mechanical Turk to verify the race, gender and age group for each face. We assigned three workers for each image. If two or three workers agreed on their judgements, we took the values as ground-truth. If all three workers produced different responses, we republished the image to another 3 workers and subsequently discarded the image if the new annotators did not agree. These annotations at this stage were still noisy. We further refined the annotations by training a model from the initial ground truth annotations and applying back to the dataset. We then manually re-verified the annotations for images whose annotations differ from model predictions.\n\n4. Experiments\n\n\n4.1. Measuring Bias in Datasets\nWe first measure how skewed each dataset is in terms of its race composition. For the datasets with race annotations, we use the reported statistics. For the other datasets, we annotated the race labels for 3,000 random samples drawn from each dataset. See Figure 2 for the result. As expected, most existing face attribute datasets, especially the ones focusing on celebrities or politicians, are biased toward the White race. Unlike race, we find that most datasets are relatively more balanced on gender ranging from 40%-60% male ratio.\n\n4.2. Model and Cross-Dataset Performance\nTo compare model performance of different datasets, we used an identical model architecture, ResNet-34 [18], to be trained from each dataset. We used ADAM optimization [29] with a learning rate of 0.0001. Given an image, we detected faces using the dlib 1 's CNN-based face detector [28] and ran the attribute classifier on each face. The experiment was done in pyTorch.\nThroughout the evaluations, we compare our dataset with three other datasets: UTKFace [75], LFWA+, and CelebA [37]. Both UTKFace and LFWA+ have race annotations, and thus, are suitable for comparison with our dataset. CelebA does not have race annotations, so we only use it for gender classification. See Table 1 for more detailed dataset characteristics.\nUsing models trained from these datasets, we first performed cross-dataset classifications, by alternating training sets and test sets. Note that FairFace is the only dataset with 6 races. To make it compatible with other datasets, we merged our fine racial groups when tested on other datasets. CelebA does not have race annotations but was included for gender classification.\nTables 3 and 4 show the classification results for race, gender, and age on the datasets across subpopulations. As expected, each model tends to perform better on the same dataset on which it was trained. However, the accuracy of 1 dlib.net our model was highest on some variables on the LFWA+ dataset and also very close to the leader in other cases. This is partly because LFWA+ is the most biased dataset and ours is the most diverse, and thus more generalizable dataset.\n\n4.3. Generalization Performance\n\n\n4.3.1 Datasets\nTo test the generalization performance of the models, we consider three novel datasets. Note that these datasets were collected from completely different sources than our data from Flickr and not used in training. Since we want to measure the effectiveness of the model on diverse races, we chose the test datasets that contain people in different locations as follows.\nGeo-tagged Tweets. First we consider images uploaded by Twitter users whose locations are identified by geotags (longitude and latitude), provided by [51]. From this set, we chose four countries (France, Iraq, Philippines, and Venezuela) and randomly sampled 5,000 faces.\nMedia Photographs. Next, we also use photographs posted by 500 online professional media outlets. Specifically, we use a public dataset of tweet IDs [36] posted by 4,000 known media accounts, e.g., @nytimes. Note that although we use Twitter to access the photographs, these tweets are simply external links to pages in the main newspaper sites. Therefore this data is considered as media photographs and different from general tweet images mostly uploaded by ordinary users. We randomly sampled 8,000 faces from the set.\nProtest Dataset. Lastly, we also use a public image dataset collected for a recent protest activity study [64]. The authors collected the majority of data from Google Image search by using keywords such as \"Venezuela protest\" or \"football game\" (for hard negatives). The dataset exhibits a wide range of diverse race and gender groups engaging in different activities in various countries. We randomly sampled 8,000 faces from the set.\nThese faces were annotated for gender, race, and age by Amazon Mechanical Turk workers.\n\n4.3.2 Result\nTable 6 shows the classification accuracy of different models. Because our dataset is larger than LFWA+ and UTK-Face, we report the three variants of the FairFace model by limiting the size of a training set (9k, 18k, and Full) for fair comparisons.\nImproved Accuracy. As clearly shown in the result, the model trained by FairFace outperforms all the other models for race, gender, and age, on the novel datasets, which have never been used in training and also come from different data sources. The models trained with fewer training images (9k and 18k) still outperform other datasets including CelebA which is larger than FairFace. This suggests that the dataset size is not the only reason for the performance improvement.\nBalanced Accuracy. Our model also produces more consistent results -for race, gender, age classificationacross different race groups compared to other datasets. We measure the model consistency by standard deviations of classification accuracy measured on different subpopulations, as shown in Table 5. More formally, one can consider conditional use accuracy equality [6] or equalized odds [17] as the measure of fair classification. For gender classification:P ( Y = i|Y = i, A = j) = P ( Y = i|Y = i, A = k), i \u2208 {male, female}, \u2200j, k \u2208 D, (1)\nwhere Y is the predicted gender, Y is the true gender, A refers to the demographic group, and D is the set of different demographic groups being considered (i.e. race). When we consider different gender groups for A, this needs to be modified to measure accuracy equality [6] :P ( Y = Y |A = j) = P ( Y = Y |A = k), \u2200j, k \u2208 D. (2)\nWe therefore define the maximum accuracy disparity of a classifier as follows:( Y ) = max \u2200j,k\u2208D log P ( Y = Y |A = j) P ( Y = Y |A = k) . ()\nTable 2 shows the gender classification accuracy of different models measured on the external validation datasets for each race and gender group. The FairFace model achieves the lowest maximum accuracy disparity. The LFWA+ model yields the highest disparity, strongly biased toward the male cateogory. The CelebA model tends to exhibit a bias toward the female category as the dataset contains more female images than male.\nThe FairFace model achieves less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White for gender classification (Table 6). All the other models show a strong bias toward the male class, yielding much lower accuracy on the female group, and perform more inaccurately on the non-White group. The gender performance gap was the biggest in LFWA+ (32%), which is the smallest among the datasets used in the experiment. Recent work has also reported asymmetric gender biases in commercial computer vision services [7], and our result further suggests the cause is likely to due to the unbalanced representation in training data.\nData Coverage and Diversity. We further investigate dataset characteristics to measure the data diversity in our dataset. We first visualize randomly sampled faces in 2D space using t-SNE [38] as shown in Figure 4. We used the facial embedding based on ResNet-34 from dlib, which was trained from the FaceScrub dataset [40], the VGG-Face dataset [41] and other online sources, which are likely dominated by the White faces. The faces in FairFace are well spread in the space, and the race groups are loosely separated from each other. This is in part because the embedding was trained from biased datasets, but it also suggests that the dataset contains many non-typical examples. LFWA+ was derived from LFW, which was developed for face recognition, and therefore contains multiple images of the same individuals, i.e., clusters. UTKFace also tends to focus more on local clusters compared to FairFace.\nTo explicitly measure the diversity of faces in these datasets, we examine the distributions of pairwise distance between faces (Figure 5). On the random subsets, we first obtained the same 128-dimensional facial embedding from  dlib and measured pair-wise distance. Figure 5 shows the CDF functions for 3 datasets. As conjectured, UTKFace had more faces that are tightly clustered together and very similar to each other, compared to our dataset. Surprisingly, the faces in LFWA+ were shown very diverse and far from each other, even though the majority of the examples contained a white face. We believe this is mostly due to the fact that the face embedding was also trained on a very similar white-oriented dataset which will be effective in separating white faces, not because the appearance of their faces is actually diverse. (See Figure 3)\n\n5. Conclusion\nThis paper proposes a novel face image dataset balanced on race, gender and age. Compared to existing large-scale in-the-wild datasets, our dataset achieves much better generalization classification performance for gender, race, and age on novel image datasets collected from Twitter, international online newspapers, and web search, which contain more non-White faces than typical face datasets. We show that the model trained from our dataset produces balanced accuracy across race, whereas other datasets often lead to asymmetric accuracy on different race groups.\nThis dataset was derived from the Yahoo YFCC100m dataset [59] for the images with Creative Common Licenses by Attribution and Share Alike, which permit both academic and commercial usage. Our dataset can be used for training a new model and verifying balanced accuracy of existing classifiers.\nAlgorithmic fairness is an important aspect to consider in designing and developing AI systems, especially because these systems are being translated into many areas in our society and affecting our decision making. Large scale image datasets have contributed to the recent success in computer vision by improving model accuracy; yet the public and media have doubts about its transparency. The novel dataset proposed in this paper will help us discover and mitigate race and gender bias present in computer vision systems such that such systems can be more easily accepted in society.\n\n6. Acknowledgement\nThis work was supported by the National Science Foundation SMA-1831848, Hellman Fellowship, and UCLA Faculty Career Development Award.\n\nFootnotes:\n\nReferences:\n\n- M. Alvi, A. Zisserman, and C. Nellaaker. Turning a blind eye: Explicit removal of biases and variation from deep neu- ral network embeddings. In The European Conference on Computer Vision (ECCV) Workshops, September 2018. 3- B. Amos, B. Ludwiczuk, M. Satyanarayanan, et al. Open- face: A general-purpose face recognition library with mobile applications. CMU School of Computer Science, 6, 2016. 2\n\n- G. Antipov, M. Baccouche, and J.-L. Dugelay. Face aging with conditional generative adversarial networks. In 2017 IEEE International Conference on Image Processing (ICIP), pages 2089-2093. IEEE, 2017. 1\n\n- T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency. Openface 2.0: Facial behavior analysis toolkit. In 2018 13th IEEE International Conference on Automatic Face & Ges- ture Recognition (FG 2018), pages 59-66. IEEE, 2018. 2\n\n- J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. Cvae-gan: fine- grained image generation through asymmetric training. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 2745-2754, 2017. 1\n\n- R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, page 0049124118782533. 6\n\n- J. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, pages 77-91, 2018. 1, 2, 3, 6\n\n- Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE International Conference on Auto- matic Face & Gesture Recognition (FG 2018), pages 67-74. IEEE, 2018. 1\n\n- A. Chakraborty, J. Messias, F. Benevenuto, S. Ghosh, N. Ganguly, and K. P. Gummadi. Who makes trends? under- standing demographic biases in crowdsourced recommenda- tions. In Eleventh International AAAI Conference on Web and Social Media, 2017. 2\n\n- B.-C. Chen, C.-S. Chen, and W. H. Hsu. Face recognition and retrieval using cross-age reference coding with cross- age celebrity dataset. IEEE Transactions on Multimedia, 17(6):804-815, 2015. 2\n\n- S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining, pages 797-806. ACM, 2017. 1, 3\n\n- A. Das, A. Dantcheva, and F. Bremond. Mitigating bias in gender, age and ethnicity classification: a multi-task con- volution neural network approach. In The European Con- ference on Computer Vision (ECCV) Workshops, September 2018. 3\n\n- S. Escalera, M. Torres Torres, B. Martinez, X. Bar\u00f3, H. Jair Escalante, I. Guyon, G. Tzimiropoulos, C. Corneou, M. Oliu, M. Ali Bagheri, et al. Chalearn looking at people and faces of the world: Face analysis workshop and chal- lenge 2016. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition Workshops, pages 1-8, 2016. 1, 2\n\n- M. Grgic, K. Delac, and S. Grgic. Scface-surveillance cameras face database. Multimedia tools and applications, 51(3):863-879, 2011. 2\n\n- Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In European Conference on Computer Vision, pages 87-102. Springer, 2016. 1\n\n- H. Han, A. K. Jain, F. Wang, S. Shan, and X. Chen. Hetero- geneous face attribute estimation: A deep multi-task learn- ing approach. IEEE transactions on pattern analysis and machine intelligence, 40(11):2597-2609, 2018. 2\n\n- M. Hardt, E. Price, N. Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315-3323, 2016. 1, 6\n\n- K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016. 5\n\n- Z. He, W. Zuo, M. Kan, S. Shan, and X. Chen. Arbitrary facial attribute editing: Only change what you want. arXiv preprint arXiv:1711.10678, 1(3), 2017. 1\n\n- L. A. Hendricks, K. Burns, K. Saenko, T. Darrell, and A. Rohrbach. Women also snowboard: Overcoming bias in captioning models. In European Conference on Computer Vision, pages 793-811. Springer, 2018. 3\n\n- P. Hu and D. Ramanan. Finding tiny faces. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 951-959, 2017. 1\n\n- G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. La- beled faces in the wild: A database forstudying face recog- nition in unconstrained environments. In Workshop on faces in'Real-Life'Images: detection, alignment, and recognition, 2008. 1\n\n- G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Re- port 07-49, University of Massachusetts, Amherst, October 2007. 2, 4\n\n- J. Joo, F. F. Steen, and S.-C. Zhu. Automated facial trait judg- ment and election outcome prediction: Social dimensions of face. In Proceedings of the IEEE international conference on computer vision, pages 3712-3720, 2015. 1, 4\n\n- J. Joo, S. Wang, and S.-C. Zhu. Human attribute recognition by rich appearance dictionary. In Proceedings of the IEEE International Conference on Computer Vision, pages 721- 728, 2013. 2\n\n- T. Karras, S. Laine, and T. Aila. A style-based genera- tor architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2018. 1\n\n- I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1 million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4873- 4882, 2016. 1\n\n- D. E. King. Max-margin object detection. arXiv preprint arXiv:1502.00046, 2015. 5\n\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\n- S. Kiritchenko and S. M. Mohammad. Examining gender and race bias in two hundred sentiment analysis systems. arXiv preprint arXiv:1805.04508, 2018. 3\n\n- N. Kumar, A. Berg, P. N. Belhumeur, and S. Nayar. Describ- able visual attributes for face verification and image search. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 33(10):1962-1977, 2011. 1, 2, 4\n\n- G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. De- noyer, et al. Fader networks: Manipulating images by slid- ing attributes. In Advances in Neural Information Processing Systems, pages 5967-5976, 2017. 1\n\n- R. Layne, T. M. Hospedales, S. Gong, and Q. Mary. Person re-identification by attributes. In Bmvc, volume 2, page 8, 2012. 2\n\n- A. Li, L. Liu, K. Wang, S. Liu, and S. Yan. Clothing at- tributes assisted person reidentification. IEEE Transactions on Circuits and Systems for Video Technology, 25(5):869- 878, 2015. 2\n\n- H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua. A convolu- tional neural network cascade for face detection. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 5325-5334, 2015. 1\n\n- J. Littman, L. Wrubel, D. Kerchner, and Y. Bromberg Gaber. News Outlet Tweet Ids, 2017. 5\n\n- Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Con- ference on Computer Vision 2015. 1, 2, 4, 5\n\n- L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605, 2008. 6\n\n- M. Merler, N. Ratha, R. S. Feris, and J. R. Smith. Diversity in faces. arXiv preprint arXiv:1901.10436, 2019. 1, 2, 3, 4\n\n- H.-W. Ng and S. Winkler. A data-driven approach to clean- ing large face datasets. In 2014 IEEE International Con- ference on Image Processing (ICIP), pages 343-347. IEEE, 2014. 6\n\n- O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. Deep face recognition. In bmvc, volume 1, page 6, 2015. 1, 6\n\n- I. D. Raji and J. Buolamwini. Actionable auditing: Inves- tigating the impact of publicly naming biased performance results of commercial ai products. In AAAI/ACM Conf. on AI Ethics and Society, volume 1, 2019. 1\n\n- J. Reis, H. Kwak, J. An, J. Messias, and F. Benevenuto. De- mographics of news sharing in the us twittersphere. In Pro- ceedings of the 28th ACM Conference on Hypertext and So- cial Media, pages 195-204. ACM, 2017. 2\n\n- S. Ren, X. Cao, Y. Wei, and J. Sun. Face alignment at 3000 fps via regressing local binary features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1685-1692, 2014. 1\n\n- K. Ricanek and T. Tesafaye. Morph: A longitudinal image database of normal adult age-progression. In 7th Interna- tional Conference on Automatic Face and Gesture Recogni- tion (FGR06), pages 341-345. IEEE, 2006. 2\n\n- R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and apparent age from a single image without fa- cial landmarks. International Journal of Computer Vision (IJCV), July 2016. 1, 2, 4\n\n- H. J. Ryu, H. Adam, and M. Mitchell. Inclusivefacenet: Im- proving face attribute detection with race and gender diver- sity. arXiv preprint arXiv:1712.00193, 2017. 3\n\n- R. T. Schaefer. Encyclopedia of race, ethnicity, and society, volume 1. Sage, 2008. 3\n\n- F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815-823, 2015. 1\n\n- S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, and D. Sculley. No classification without representation: Assess- ing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017. 3\n\n- Z. C. Steinert-Threlkeld. Twitter as data. Cambridge Uni- versity Press, 2018. 5\n\n- P. Stock and M. Cisse. Convnets and imagenet beyond ac- curacy: Understanding mistakes and uncovering biases. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 498-512, 2018. 3\n\n- C. Su, F. Yang, S. Zhang, Q. Tian, L. S. Davis, and W. Gao. Multi-task learning with low rank attribute embedding for multi-camera person re-identification. IEEE transactions on pattern analysis and machine intelligence, 40(5):1167-1181, 2018. 2\n\n- Y. Sun, X. Wang, and X. Tang. Hybrid deep learning for face verification. In Proceedings of the IEEE international conference on computer vision, pages 1489-1496, 2013. 2\n\n- Y. Sun, X. Wang, and X. Tang. Deep learning face repre- sentation from predicting 10,000 classes. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 1891-1898, 2014. 2\n\n- H. Suresh, J. J. Gong, and J. V. Guttag. Learning tasks for multitask learning: Heterogenous patient populations in the icu. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 802-810. ACM, 2018. 3\n\n- Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verifi- cation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701-1708, 2014. 1\n\n- C. Thomas and A. Kovashka. Persuasive faces: Generating faces in advertisements. arXiv preprint arXiv:1807.09882, 2018. 1\n\n- B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64-73. 1, 4, 7\n\n- A. Torralba and A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 1521-1528. IEEE Com- puter Society, 2011. 1\n\n- S. Verma and J. Rubin. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fair- ness (FairWare), pages 1-7. IEEE, 2018. 3\n\n- Y. Wang, Y. Feng, Z. Hong, R. Berger, and J. Luo. How polarized have we become? a multimodal classification of trump followers and clinton followers. In International Conference on Social Informatics, pages 440-456. Springer, 2017. 2\n\n- M. Wilkes, C. Y. Wright, J. L. du Plessis, and A. Reeder. Fitzpatrick skin type, individual typology angle, and melanin index in an african population: steps toward universally ap- plicable skin photosensitivity assessments. JAMA dermatol- ogy, 151(8):902-903, 2015. 4\n\n- D. Won, Z. C. Steinert-Threlkeld, and J. Joo. Protest activity detection and perceived violence estimation from social me- dia images. In Proceedings of the 25th ACM international conference on Multimedia, pages 786-794. ACM, 2017. 2, 5\n\n- N. Xi, D. Ma, M. Liou, Z. C. Steinert-Threlkeld, J. Anas- tasopoulos, and J. Joo. Understanding the political ideol- ogy of legislators from social media images. arXiv preprint arXiv:1907.09594, 2019. 2\n\n- X. Xiong and F. De la Torre. Supervised descent method and its applications to face alignment. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 532-539, 2013. 1\n\n- X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Con- ditional image generation from visual attributes. In European Conference on Computer Vision, pages 776-791. Springer, 2016. 1\n\n- S. Yang, P. Luo, C.-C. Loy, and X. Tang. Wider face: A face detection benchmark. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 5525-5533, 2016. 1\n\n- D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen- tation from scratch. arXiv preprint arXiv:1411.7923, 2014. 1\n\n- M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi. Fairness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pages 962-970, 2017. 3\n\n- R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325-333, 2013. 3\n\n- B. H. Zhang, B. Lemoine, and M. Mitchell. Mitigating un- wanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335-340. ACM, 2018. 3\n\n- M. Zhang. Google photos tags two african-americans as go- rillas through facial recognition software, Jul 2015. 2\n\n- Z. Zhang, P. Luo, C.-C. Loy, and X. Tang. Learning social relation traits from face images. In Proceedings of the IEEE International Conference on Computer Vision, pages 3631- 3639, 2015. 2\n\n- Z. Zhang, Y. Song, and H. Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 5810-5818, 2017. 2, 4, 5\n\n- J. Zou and L. Schiebinger. Ai can be sexist and racistits time to make it fair, 2018. 3\n\n", "annotations": {"Abstract": [{"begin": 69, "end": 1149, "idx": 0}], "Head": [{"begin": 1152, "end": 1167, "n": "1.", "idx": 0}, {"begin": 4460, "end": 4466, "idx": 1}, {"begin": 4469, "end": 4475, "idx": 2}, {"begin": 4899, "end": 4914, "n": "2.", "idx": 3}, {"begin": 4917, "end": 4948, "n": "2.1.", "idx": 4}, {"begin": 7811, "end": 7852, "n": "2.2.", "idx": 5}, {"begin": 9803, "end": 9826, "n": "3.", "idx": 6}, {"begin": 9829, "end": 9847, "n": "3.1.", "idx": 7}, {"begin": 11876, "end": 11912, "n": "3.2.", "idx": 8}, {"begin": 14735, "end": 14749, "n": "4.", "idx": 9}, {"begin": 14752, "end": 14783, "n": "4.1.", "idx": 10}, {"begin": 15325, "end": 15365, "n": "4.2.", "idx": 11}, {"begin": 16948, "end": 16979, "n": "4.3.", "idx": 12}, {"begin": 16982, "end": 16996, "n": "4.3.1", "idx": 13}, {"begin": 18686, "end": 18698, "n": "4.3.2", "idx": 14}, {"begin": 23266, "end": 23279, "n": "5.", "idx": 15}, {"begin": 24729, "end": 24747, "n": "6.", "idx": 16}], "ReferenceToBib": [{"begin": 1218, "end": 1222, "target": "#b21", "idx": 0}, {"begin": 1223, "end": 1226, "target": "#b30", "idx": 1}, {"begin": 1227, "end": 1230, "target": "#b12", "idx": 2}, {"begin": 1231, "end": 1234, "target": "#b68", "idx": 3}, {"begin": 1235, "end": 1238, "target": "#b36", "idx": 4}, {"begin": 1239, "end": 1242, "target": "#b23", "idx": 5}, {"begin": 1243, "end": 1246, "target": "#b40", "idx": 6}, {"begin": 1247, "end": 1250, "target": "#b67", "idx": 7}, {"begin": 1251, "end": 1254, "target": "#b14", "idx": 8}, {"begin": 1255, "end": 1258, "target": "#b26", "idx": 9}, {"begin": 1259, "end": 1262, "target": "#b45", "idx": 10}, {"begin": 1263, "end": 1265, "target": "#b7", "idx": 11}, {"begin": 1266, "end": 1269, "target": "#b38", "idx": 12}, {"begin": 1356, "end": 1360, "target": "#b34", "idx": 13}, {"begin": 1361, "end": 1364, "target": "#b20", "idx": 14}, {"begin": 1376, "end": 1380, "target": "#b65", "idx": 15}, {"begin": 1381, "end": 1384, "target": "#b43", "idx": 16}, {"begin": 1398, "end": 1402, "target": "#b56", "idx": 17}, {"begin": 1403, "end": 1406, "target": "#b48", "idx": 18}, {"begin": 1419, "end": 1423, "target": "#b66", "idx": 19}, {"begin": 1424, "end": 1426, "target": "#b4", "idx": 20}, {"begin": 1427, "end": 1430, "target": "#b25", "idx": 21}, {"begin": 1431, "end": 1434, "target": "#b57", "idx": 22}, {"begin": 1449, "end": 1452, "target": "#b2", "idx": 23}, {"begin": 1453, "end": 1456, "target": "#b31", "idx": 24}, {"begin": 1457, "end": 1460, "target": "#b18", "idx": 25}, {"begin": 1491, "end": 1495, "target": "#b30", "idx": 26}, {"begin": 1496, "end": 1499, "target": "#b36", "idx": 27}, {"begin": 1986, "end": 1990, "target": "#b38", "idx": 28}, {"begin": 2354, "end": 2358, "target": "#b16", "idx": 29}, {"begin": 2359, "end": 2362, "target": "#b10", "idx": 30}, {"begin": 2541, "end": 2544, "target": "#b6", "idx": 31}, {"begin": 2545, "end": 2548, "target": "#b41", "idx": 32}, {"begin": 2840, "end": 2844, "target": "#b59", "idx": 33}, {"begin": 3275, "end": 3279, "target": "#b58", "idx": 34}, {"begin": 4500, "end": 4503, "target": "#b6", "idx": 35}, {"begin": 4560, "end": 4564, "target": "#b44", "idx": 36}, {"begin": 4598, "end": 4602, "target": "#b30", "idx": 37}, {"begin": 4658, "end": 4662, "target": "#b45", "idx": 38}, {"begin": 4687, "end": 4691, "target": "#b12", "idx": 39}, {"begin": 4712, "end": 4716, "target": "#b9", "idx": 40}, {"begin": 4739, "end": 4743, "target": "#b38", "idx": 41}, {"begin": 4786, "end": 4790, "target": "#b36", "idx": 42}, {"begin": 4801, "end": 4805, "target": "#b53", "idx": 43}, {"begin": 4806, "end": 4809, "target": "#b54", "idx": 44}, {"begin": 4814, "end": 4818, "target": "#b22", "idx": 45}, {"begin": 4832, "end": 4836, "target": "#b15", "idx": 46}, {"begin": 4841, "end": 4844, "target": "#b22", "idx": 47}, {"begin": 5118, "end": 5122, "target": "#b30", "idx": 48}, {"begin": 5123, "end": 5126, "target": "#b24", "idx": 49}, {"begin": 5127, "end": 5130, "target": "#b73", "idx": 50}, {"begin": 5131, "end": 5134, "target": "#b36", "idx": 51}, {"begin": 5724, "end": 5728, "target": "#b30", "idx": 52}, {"begin": 6037, "end": 6041, "target": "#b32", "idx": 53}, {"begin": 6042, "end": 6045, "target": "#b33", "idx": 54}, {"begin": 6046, "end": 6049, "target": "#b52", "idx": 55}, {"begin": 6273, "end": 6277, "target": "#b13", "idx": 56}, {"begin": 6766, "end": 6770, "target": "#b72", "idx": 57}, {"begin": 7345, "end": 7348, "target": "#b1", "idx": 58}, {"begin": 7349, "end": 7351, "target": "#b3", "idx": 59}, {"begin": 7634, "end": 7637, "target": "#b8", "idx": 60}, {"begin": 7638, "end": 7641, "target": "#b42", "idx": 61}, {"begin": 7642, "end": 7645, "target": "#b63", "idx": 62}, {"begin": 7646, "end": 7649, "target": "#b64", "idx": 63}, {"begin": 7650, "end": 7653, "target": "#b61", "idx": 64}, {"begin": 7978, "end": 7982, "target": "#b70", "idx": 65}, {"begin": 7983, "end": 7986, "target": "#b10", "idx": 66}, {"begin": 7987, "end": 7990, "target": "#b75", "idx": 67}, {"begin": 7991, "end": 7994, "target": "#b71", "idx": 68}, {"begin": 8070, "end": 8074, "target": "#b60", "idx": 69}, {"begin": 8513, "end": 8517, "target": "#b49", "idx": 70}, {"begin": 8518, "end": 8520, "target": "#b6", "idx": 71}, {"begin": 8521, "end": 8524, "target": "#b29", "idx": 72}, {"begin": 8553, "end": 8557, "target": "#b38", "idx": 73}, {"begin": 8558, "end": 8560, "target": "#b0", "idx": 74}, {"begin": 8606, "end": 8610, "target": "#b11", "idx": 75}, {"begin": 8611, "end": 8613, "target": "#b0", "idx": 76}, {"begin": 8614, "end": 8617, "target": "#b46", "idx": 77}, {"begin": 8618, "end": 8621, "target": "#b70", "idx": 78}, {"begin": 8622, "end": 8625, "target": "#b69", "idx": 79}, {"begin": 8838, "end": 8842, "target": "#b55", "idx": 80}, {"begin": 8896, "end": 8900, "target": "#b51", "idx": 81}, {"begin": 9023, "end": 9027, "target": "#b19", "idx": 82}, {"begin": 10111, "end": 10115, "target": "#b47", "idx": 83}, {"begin": 11015, "end": 11018, "target": "#b6", "idx": 84}, {"begin": 11019, "end": 11022, "target": "#b38", "idx": 85}, {"begin": 11296, "end": 11299, "target": "#b6", "idx": 86}, {"begin": 11778, "end": 11782, "target": "#b62", "idx": 87}, {"begin": 12021, "end": 12025, "target": "#b30", "idx": 88}, {"begin": 12026, "end": 12029, "target": "#b22", "idx": 89}, {"begin": 12030, "end": 12033, "target": "#b23", "idx": 90}, {"begin": 12034, "end": 12037, "target": "#b45", "idx": 91}, {"begin": 12038, "end": 12041, "target": "#b36", "idx": 92}, {"begin": 12449, "end": 12453, "target": "#b74", "idx": 93}, {"begin": 12802, "end": 12806, "target": "#b58", "idx": 94}, {"begin": 12979, "end": 12983, "target": "#b38", "idx": 95}, {"begin": 15469, "end": 15473, "target": "#b17", "idx": 96}, {"begin": 15534, "end": 15538, "target": "#b28", "idx": 97}, {"begin": 15649, "end": 15653, "target": "#b27", "idx": 98}, {"begin": 15823, "end": 15827, "target": "#b74", "idx": 99}, {"begin": 15847, "end": 15851, "target": "#b36", "idx": 100}, {"begin": 17517, "end": 17521, "target": "#b50", "idx": 101}, {"begin": 17788, "end": 17792, "target": "#b35", "idx": 102}, {"begin": 18267, "end": 18271, "target": "#b63", "idx": 103}, {"begin": 19795, "end": 19798, "target": "#b5", "idx": 104}, {"begin": 19817, "end": 19821, "target": "#b16", "idx": 105}, {"begin": 20245, "end": 20248, "target": "#b5", "idx": 106}, {"begin": 21398, "end": 21401, "target": "#b6", "idx": 107}, {"begin": 21701, "end": 21705, "target": "#b37", "idx": 108}, {"begin": 21832, "end": 21836, "target": "#b39", "idx": 109}, {"begin": 21859, "end": 21863, "target": "#b40", "idx": 110}, {"begin": 23905, "end": 23909, "target": "#b58", "idx": 111}], "SectionFootnote": [{"begin": 24884, "end": 24894, "idx": 0}], "ReferenceString": [{"begin": 24911, "end": 25134, "id": "b0", "idx": 0}, {"begin": 25136, "end": 25308, "id": "b1", "idx": 1}, {"begin": 25312, "end": 25514, "id": "b2", "idx": 2}, {"begin": 25518, "end": 25745, "id": "b3", "idx": 3}, {"begin": 25749, "end": 25961, "id": "b4", "idx": 4}, {"begin": 25965, "end": 26148, "id": "b5", "idx": 5}, {"begin": 26152, "end": 26357, "id": "b6", "idx": 6}, {"begin": 26361, "end": 26603, "id": "b7", "idx": 7}, {"begin": 26607, "end": 26853, "id": "b8", "idx": 8}, {"begin": 26857, "end": 27050, "id": "b9", "idx": 9}, {"begin": 27054, "end": 27306, "id": "b10", "idx": 10}, {"begin": 27310, "end": 27544, "id": "b11", "idx": 11}, {"begin": 27548, "end": 27903, "id": "b12", "idx": 12}, {"begin": 27907, "end": 28041, "id": "b13", "idx": 13}, {"begin": 28045, "end": 28234, "id": "b14", "idx": 14}, {"begin": 28238, "end": 28460, "id": "b15", "idx": 15}, {"begin": 28464, "end": 28631, "id": "b16", "idx": 16}, {"begin": 28635, "end": 28826, "id": "b17", "idx": 17}, {"begin": 28830, "end": 28984, "id": "b18", "idx": 18}, {"begin": 28988, "end": 29190, "id": "b19", "idx": 19}, {"begin": 29194, "end": 29342, "id": "b20", "idx": 20}, {"begin": 29346, "end": 29592, "id": "b21", "idx": 21}, {"begin": 29596, "end": 29833, "id": "b22", "idx": 22}, {"begin": 29837, "end": 30066, "id": "b23", "idx": 23}, {"begin": 30070, "end": 30256, "id": "b24", "idx": 24}, {"begin": 30260, "end": 30410, "id": "b25", "idx": 25}, {"begin": 30414, "end": 30655, "id": "b26", "idx": 26}, {"begin": 30659, "end": 30740, "id": "b27", "idx": 27}, {"begin": 30744, "end": 30851, "id": "b28", "idx": 28}, {"begin": 30855, "end": 31004, "id": "b29", "idx": 29}, {"begin": 31008, "end": 31227, "id": "b30", "idx": 30}, {"begin": 31231, "end": 31439, "id": "b31", "idx": 31}, {"begin": 31443, "end": 31567, "id": "b32", "idx": 32}, {"begin": 31571, "end": 31758, "id": "b33", "idx": 33}, {"begin": 31762, "end": 31978, "id": "b34", "idx": 34}, {"begin": 31982, "end": 32071, "id": "b35", "idx": 35}, {"begin": 32075, "end": 32236, "id": "b36", "idx": 36}, {"begin": 32240, "end": 32364, "id": "b37", "idx": 37}, {"begin": 32368, "end": 32488, "id": "b38", "idx": 38}, {"begin": 32492, "end": 32671, "id": "b39", "idx": 39}, {"begin": 32675, "end": 32782, "id": "b40", "idx": 40}, {"begin": 32786, "end": 32998, "id": "b41", "idx": 41}, {"begin": 33002, "end": 33218, "id": "b42", "idx": 42}, {"begin": 33222, "end": 33431, "id": "b43", "idx": 43}, {"begin": 33435, "end": 33648, "id": "b44", "idx": 44}, {"begin": 33652, "end": 33847, "id": "b45", "idx": 45}, {"begin": 33851, "end": 34017, "id": "b46", "idx": 46}, {"begin": 34021, "end": 34106, "id": "b47", "idx": 47}, {"begin": 34110, "end": 34325, "id": "b48", "idx": 48}, {"begin": 34329, "end": 34559, "id": "b49", "idx": 49}, {"begin": 34563, "end": 34643, "id": "b50", "idx": 50}, {"begin": 34647, "end": 34850, "id": "b51", "idx": 51}, {"begin": 34854, "end": 35099, "id": "b52", "idx": 52}, {"begin": 35103, "end": 35273, "id": "b53", "idx": 53}, {"begin": 35277, "end": 35483, "id": "b54", "idx": 54}, {"begin": 35487, "end": 35740, "id": "b55", "idx": 55}, {"begin": 35744, "end": 35973, "id": "b56", "idx": 56}, {"begin": 35977, "end": 36098, "id": "b57", "idx": 57}, {"begin": 36102, "end": 36290, "id": "b58", "idx": 58}, {"begin": 36294, "end": 36489, "id": "b59", "idx": 59}, {"begin": 36493, "end": 36647, "id": "b60", "idx": 60}, {"begin": 36651, "end": 36884, "id": "b61", "idx": 61}, {"begin": 36888, "end": 37156, "id": "b62", "idx": 62}, {"begin": 37160, "end": 37396, "id": "b63", "idx": 63}, {"begin": 37400, "end": 37602, "id": "b64", "idx": 64}, {"begin": 37606, "end": 37807, "id": "b65", "idx": 65}, {"begin": 37811, "end": 37996, "id": "b66", "idx": 66}, {"begin": 38000, "end": 38189, "id": "b67", "idx": 67}, {"begin": 38193, "end": 38315, "id": "b68", "idx": 68}, {"begin": 38319, "end": 38502, "id": "b69", "idx": 69}, {"begin": 38506, "end": 38663, "id": "b70", "idx": 70}, {"begin": 38667, "end": 38867, "id": "b71", "idx": 71}, {"begin": 38871, "end": 38984, "id": "b72", "idx": 72}, {"begin": 38988, "end": 39177, "id": "b73", "idx": 73}, {"begin": 39181, "end": 39392, "id": "b74", "idx": 74}, {"begin": 39396, "end": 39483, "id": "b75", "idx": 75}], "ReferenceToTable": [{"begin": 5272, "end": 5273, "target": "#tab_0", "idx": 0}, {"begin": 9508, "end": 9509, "target": "#tab_0", "idx": 1}, {"begin": 16049, "end": 16050, "target": "#tab_0", "idx": 2}, {"begin": 16479, "end": 16486, "target": "#tab_4", "idx": 3}, {"begin": 18705, "end": 18706, "target": "#tab_6", "idx": 4}, {"begin": 19726, "end": 19727, "target": "#tab_5", "idx": 5}, {"begin": 20452, "end": 20453, "target": "#tab_2", "idx": 6}, {"begin": 21009, "end": 21010, "target": "#tab_6", "idx": 7}], "Paragraph": [{"begin": 79, "end": 1149, "idx": 0}, {"begin": 1168, "end": 1626, "idx": 1}, {"begin": 1627, "end": 2363, "idx": 2}, {"begin": 2364, "end": 3041, "idx": 3}, {"begin": 3042, "end": 3592, "idx": 4}, {"begin": 3593, "end": 4458, "idx": 5}, {"begin": 4476, "end": 4897, "idx": 6}, {"begin": 4949, "end": 5265, "idx": 7}, {"begin": 5266, "end": 5601, "idx": 8}, {"begin": 5602, "end": 6278, "idx": 9}, {"begin": 6279, "end": 7088, "idx": 10}, {"begin": 7089, "end": 7809, "idx": 11}, {"begin": 7853, "end": 8394, "idx": 12}, {"begin": 8395, "end": 8673, "idx": 13}, {"begin": 8674, "end": 9110, "idx": 14}, {"begin": 9111, "end": 9801, "idx": 15}, {"begin": 9848, "end": 10339, "idx": 16}, {"begin": 10340, "end": 10993, "idx": 17}, {"begin": 10994, "end": 11874, "idx": 18}, {"begin": 11913, "end": 12604, "idx": 19}, {"begin": 12605, "end": 14059, "idx": 20}, {"begin": 14060, "end": 14733, "idx": 21}, {"begin": 14784, "end": 15323, "idx": 22}, {"begin": 15366, "end": 15736, "idx": 23}, {"begin": 15737, "end": 16093, "idx": 24}, {"begin": 16094, "end": 16471, "idx": 25}, {"begin": 16472, "end": 16946, "idx": 26}, {"begin": 16997, "end": 17366, "idx": 27}, {"begin": 17367, "end": 17638, "idx": 28}, {"begin": 17639, "end": 18160, "idx": 29}, {"begin": 18161, "end": 18596, "idx": 30}, {"begin": 18597, "end": 18684, "idx": 31}, {"begin": 18699, "end": 18948, "idx": 32}, {"begin": 18949, "end": 19425, "idx": 33}, {"begin": 19426, "end": 19887, "idx": 34}, {"begin": 19973, "end": 20250, "idx": 35}, {"begin": 20304, "end": 20382, "idx": 36}, {"begin": 20446, "end": 20869, "idx": 37}, {"begin": 20870, "end": 21512, "idx": 38}, {"begin": 21513, "end": 22416, "idx": 39}, {"begin": 22417, "end": 23264, "idx": 40}, {"begin": 23280, "end": 23847, "idx": 41}, {"begin": 23848, "end": 24141, "idx": 42}, {"begin": 24142, "end": 24727, "idx": 43}, {"begin": 24748, "end": 24882, "idx": 44}], "SectionHeader": [{"begin": 0, "end": 1149, "idx": 0}], "SectionReference": [{"begin": 24896, "end": 39485, "idx": 0}], "Sentence": [{"begin": 79, "end": 219, "idx": 0}, {"begin": 220, "end": 406, "idx": 1}, {"begin": 407, "end": 581, "idx": 2}, {"begin": 582, "end": 682, "idx": 3}, {"begin": 683, "end": 785, "idx": 4}, {"begin": 786, "end": 919, "idx": 5}, {"begin": 920, "end": 1079, "idx": 6}, {"begin": 1080, "end": 1149, "idx": 7}, {"begin": 1168, "end": 1500, "idx": 8}, {"begin": 1501, "end": 1626, "idx": 9}, {"begin": 1627, "end": 1811, "idx": 10}, {"begin": 1812, "end": 1991, "idx": 11}, {"begin": 1992, "end": 2130, "idx": 12}, {"begin": 2131, "end": 2187, "idx": 13}, {"begin": 2188, "end": 2363, "idx": 14}, {"begin": 2364, "end": 2549, "idx": 15}, {"begin": 2550, "end": 2671, "idx": 16}, {"begin": 2672, "end": 2728, "idx": 17}, {"begin": 2729, "end": 2845, "idx": 18}, {"begin": 2846, "end": 3041, "idx": 19}, {"begin": 3042, "end": 3177, "idx": 20}, {"begin": 3178, "end": 3423, "idx": 21}, {"begin": 3424, "end": 3524, "idx": 22}, {"begin": 3525, "end": 3592, "idx": 23}, {"begin": 3593, "end": 3633, "idx": 24}, {"begin": 3634, "end": 3807, "idx": 25}, {"begin": 3808, "end": 3952, "idx": 26}, {"begin": 3953, "end": 4154, "idx": 27}, {"begin": 4155, "end": 4330, "idx": 28}, {"begin": 4331, "end": 4458, "idx": 29}, {"begin": 4476, "end": 4485, "idx": 30}, {"begin": 4486, "end": 4897, "idx": 31}, {"begin": 4949, "end": 5135, "idx": 32}, {"begin": 5136, "end": 5265, "idx": 33}, {"begin": 5266, "end": 5368, "idx": 34}, {"begin": 5369, "end": 5472, "idx": 35}, {"begin": 5473, "end": 5601, "idx": 36}, {"begin": 5602, "end": 5697, "idx": 37}, {"begin": 5698, "end": 5899, "idx": 38}, {"begin": 5900, "end": 6118, "idx": 39}, {"begin": 6119, "end": 6278, "idx": 40}, {"begin": 6279, "end": 6382, "idx": 41}, {"begin": 6383, "end": 6557, "idx": 42}, {"begin": 6558, "end": 6771, "idx": 43}, {"begin": 6772, "end": 6996, "idx": 44}, {"begin": 6997, "end": 7088, "idx": 45}, {"begin": 7089, "end": 7318, "idx": 46}, {"begin": 7319, "end": 7545, "idx": 47}, {"begin": 7546, "end": 7654, "idx": 48}, {"begin": 7655, "end": 7809, "idx": 49}, {"begin": 7853, "end": 7995, "idx": 50}, {"begin": 7996, "end": 8075, "idx": 51}, {"begin": 8076, "end": 8201, "idx": 52}, {"begin": 8202, "end": 8394, "idx": 53}, {"begin": 8395, "end": 8626, "idx": 54}, {"begin": 8627, "end": 8673, "idx": 55}, {"begin": 8674, "end": 8901, "idx": 56}, {"begin": 8902, "end": 8943, "idx": 57}, {"begin": 8944, "end": 9028, "idx": 58}, {"begin": 9029, "end": 9110, "idx": 59}, {"begin": 9111, "end": 9310, "idx": 60}, {"begin": 9311, "end": 9476, "idx": 61}, {"begin": 9477, "end": 9635, "idx": 62}, {"begin": 9636, "end": 9801, "idx": 63}, {"begin": 9848, "end": 10022, "idx": 64}, {"begin": 10023, "end": 10116, "idx": 65}, {"begin": 10117, "end": 10191, "idx": 66}, {"begin": 10192, "end": 10252, "idx": 67}, {"begin": 10253, "end": 10339, "idx": 68}, {"begin": 10340, "end": 10509, "idx": 69}, {"begin": 10510, "end": 10629, "idx": 70}, {"begin": 10630, "end": 10759, "idx": 71}, {"begin": 10760, "end": 10902, "idx": 72}, {"begin": 10903, "end": 10993, "idx": 73}, {"begin": 10994, "end": 11081, "idx": 74}, {"begin": 11082, "end": 11173, "idx": 75}, {"begin": 11174, "end": 11249, "idx": 76}, {"begin": 11250, "end": 11419, "idx": 77}, {"begin": 11420, "end": 11475, "idx": 78}, {"begin": 11476, "end": 11539, "idx": 79}, {"begin": 11540, "end": 11657, "idx": 80}, {"begin": 11658, "end": 11783, "idx": 81}, {"begin": 11784, "end": 11874, "idx": 82}, {"begin": 11913, "end": 12042, "idx": 83}, {"begin": 12043, "end": 12163, "idx": 84}, {"begin": 12164, "end": 12256, "idx": 85}, {"begin": 12257, "end": 12369, "idx": 86}, {"begin": 12370, "end": 12454, "idx": 87}, {"begin": 12455, "end": 12604, "idx": 88}, {"begin": 12605, "end": 12733, "idx": 89}, {"begin": 12734, "end": 12868, "idx": 90}, {"begin": 12869, "end": 12984, "idx": 91}, {"begin": 12985, "end": 13049, "idx": 92}, {"begin": 13050, "end": 13108, "idx": 93}, {"begin": 13109, "end": 13236, "idx": 94}, {"begin": 13237, "end": 13340, "idx": 95}, {"begin": 13341, "end": 13509, "idx": 96}, {"begin": 13510, "end": 13663, "idx": 97}, {"begin": 13664, "end": 13727, "idx": 98}, {"begin": 13728, "end": 13924, "idx": 99}, {"begin": 13925, "end": 14059, "idx": 100}, {"begin": 14060, "end": 14146, "idx": 101}, {"begin": 14147, "end": 14188, "idx": 102}, {"begin": 14189, "end": 14276, "idx": 103}, {"begin": 14277, "end": 14447, "idx": 104}, {"begin": 14448, "end": 14497, "idx": 105}, {"begin": 14498, "end": 14628, "idx": 106}, {"begin": 14629, "end": 14733, "idx": 107}, {"begin": 14784, "end": 14861, "idx": 108}, {"begin": 14862, "end": 14933, "idx": 109}, {"begin": 14934, "end": 15036, "idx": 110}, {"begin": 15037, "end": 15065, "idx": 111}, {"begin": 15066, "end": 15211, "idx": 112}, {"begin": 15212, "end": 15323, "idx": 113}, {"begin": 15366, "end": 15507, "idx": 114}, {"begin": 15508, "end": 15570, "idx": 115}, {"begin": 15571, "end": 15700, "idx": 116}, {"begin": 15701, "end": 15736, "idx": 117}, {"begin": 15737, "end": 15852, "idx": 118}, {"begin": 15853, "end": 15954, "idx": 119}, {"begin": 15955, "end": 16038, "idx": 120}, {"begin": 16039, "end": 16093, "idx": 121}, {"begin": 16094, "end": 16229, "idx": 122}, {"begin": 16230, "end": 16282, "idx": 123}, {"begin": 16283, "end": 16389, "idx": 124}, {"begin": 16390, "end": 16471, "idx": 125}, {"begin": 16472, "end": 16583, "idx": 126}, {"begin": 16584, "end": 16676, "idx": 127}, {"begin": 16677, "end": 16712, "idx": 128}, {"begin": 16713, "end": 16823, "idx": 129}, {"begin": 16824, "end": 16946, "idx": 130}, {"begin": 16997, "end": 17084, "idx": 131}, {"begin": 17085, "end": 17210, "idx": 132}, {"begin": 17211, "end": 17366, "idx": 133}, {"begin": 17367, "end": 17385, "idx": 134}, {"begin": 17386, "end": 17522, "idx": 135}, {"begin": 17523, "end": 17638, "idx": 136}, {"begin": 17639, "end": 17657, "idx": 137}, {"begin": 17658, "end": 17736, "idx": 138}, {"begin": 17737, "end": 17846, "idx": 139}, {"begin": 17847, "end": 17984, "idx": 140}, {"begin": 17985, "end": 18114, "idx": 141}, {"begin": 18115, "end": 18160, "idx": 142}, {"begin": 18161, "end": 18177, "idx": 143}, {"begin": 18178, "end": 18272, "idx": 144}, {"begin": 18273, "end": 18427, "idx": 145}, {"begin": 18428, "end": 18550, "idx": 146}, {"begin": 18551, "end": 18596, "idx": 147}, {"begin": 18597, "end": 18684, "idx": 148}, {"begin": 18699, "end": 18761, "idx": 149}, {"begin": 18762, "end": 18948, "idx": 150}, {"begin": 18949, "end": 18967, "idx": 151}, {"begin": 18968, "end": 19194, "idx": 152}, {"begin": 19195, "end": 19333, "idx": 153}, {"begin": 19334, "end": 19425, "idx": 154}, {"begin": 19426, "end": 19444, "idx": 155}, {"begin": 19445, "end": 19586, "idx": 156}, {"begin": 19587, "end": 19728, "idx": 157}, {"begin": 19729, "end": 19860, "idx": 158}, {"begin": 19861, "end": 19887, "idx": 159}, {"begin": 19973, "end": 20134, "idx": 160}, {"begin": 20135, "end": 20141, "idx": 161}, {"begin": 20142, "end": 20250, "idx": 162}, {"begin": 20304, "end": 20382, "idx": 163}, {"begin": 20446, "end": 20591, "idx": 164}, {"begin": 20592, "end": 20658, "idx": 165}, {"begin": 20659, "end": 20747, "idx": 166}, {"begin": 20748, "end": 20869, "idx": 167}, {"begin": 20870, "end": 21012, "idx": 168}, {"begin": 21013, "end": 21179, "idx": 169}, {"begin": 21180, "end": 21303, "idx": 170}, {"begin": 21304, "end": 21512, "idx": 171}, {"begin": 21513, "end": 21541, "idx": 172}, {"begin": 21542, "end": 21634, "idx": 173}, {"begin": 21635, "end": 21936, "idx": 174}, {"begin": 21937, "end": 22047, "idx": 175}, {"begin": 22048, "end": 22193, "idx": 176}, {"begin": 22194, "end": 22343, "idx": 177}, {"begin": 22344, "end": 22416, "idx": 178}, {"begin": 22417, "end": 22556, "idx": 179}, {"begin": 22557, "end": 22683, "idx": 180}, {"begin": 22684, "end": 22732, "idx": 181}, {"begin": 22733, "end": 22864, "idx": 182}, {"begin": 22865, "end": 23011, "idx": 183}, {"begin": 23012, "end": 23249, "idx": 184}, {"begin": 23250, "end": 23264, "idx": 185}, {"begin": 23280, "end": 23360, "idx": 186}, {"begin": 23361, "end": 23676, "idx": 187}, {"begin": 23677, "end": 23847, "idx": 188}, {"begin": 23848, "end": 24035, "idx": 189}, {"begin": 24036, "end": 24141, "idx": 190}, {"begin": 24142, "end": 24357, "idx": 191}, {"begin": 24358, "end": 24532, "idx": 192}, {"begin": 24533, "end": 24727, "idx": 193}, {"begin": 24748, "end": 24882, "idx": 194}], "ReferenceToFigure": [{"begin": 3584, "end": 3585, "target": "#fig_3", "idx": 0}, {"begin": 11665, "end": 11666, "target": "#fig_0", "idx": 1}, {"begin": 13046, "end": 13047, "target": "#fig_1", "idx": 2}, {"begin": 15048, "end": 15049, "target": "#fig_1", "idx": 3}, {"begin": 21725, "end": 21726, "target": "#fig_4", "idx": 4}, {"begin": 22553, "end": 22554, "target": "#fig_5", "idx": 5}, {"begin": 22691, "end": 22692, "target": "#fig_5", "idx": 6}, {"begin": 23262, "end": 23263, "target": "#fig_3", "idx": 7}], "Div": [{"begin": 79, "end": 1149, "idx": 0}, {"begin": 1152, "end": 4458, "idx": 1}, {"begin": 4460, "end": 4467, "idx": 2}, {"begin": 4469, "end": 4897, "idx": 3}, {"begin": 4899, "end": 4915, "idx": 4}, {"begin": 4917, "end": 7809, "idx": 5}, {"begin": 7811, "end": 9801, "idx": 6}, {"begin": 9803, "end": 9827, "idx": 7}, {"begin": 9829, "end": 11874, "idx": 8}, {"begin": 11876, "end": 14733, "idx": 9}, {"begin": 14735, "end": 14750, "idx": 10}, {"begin": 14752, "end": 15323, "idx": 11}, {"begin": 15325, "end": 16946, "idx": 12}, {"begin": 16948, "end": 16980, "idx": 13}, {"begin": 16982, "end": 18684, "idx": 14}, {"begin": 18686, "end": 23264, "idx": 15}, {"begin": 23266, "end": 24727, "idx": 16}, {"begin": 24729, "end": 24882, "idx": 17}], "SectionMain": [{"begin": 1149, "end": 24882, "idx": 0}]}}