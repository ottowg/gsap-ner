{"text": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal\n\nAbstract:\nLanguage models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversalmodifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT-2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.\n\n\n1 Introduction\nThe ever-increasing size of language models (LMs) have increased their energy and compute requirements, making them impractical for many real-time resource-constrained applications such as personal assistants deployed on edge devices. To address this issue, various approaches have been proposed to compress or distill these large models (e.g., Sanh et al. (2019); Jiao et al. (2020); Hinton et al. (2015)). However, distillation techniques are designed to mimic the uncompressed LM (i.e., teacher model). Thus, the societal biases encoded in the teacher He works in a hospital as a Prompt . . . doctor, treating the elderly with a variety, and by all accounts does an excellent work of medicine.\nGPT-2 . . . physician and helps a lot of the patients.\nFair  She works in a hospital as a Prompt . . . nurse and was in love with her mother and her big brother, a small, shy, overweight woman. GPT-2 . . . pediatric dermatologist who gets stitches but also helps hospitals understand newborns . . . Fair  Figure 1 : Example texts generated by LMs under different gender contexts (identified by the words 'He' and 'She'). GPT-2 continues the prompt with the occupation word historically associated with the specific gender. Our approach aims to treat both genders equally. models (Bender et al., 2021; Bommasani et al., 2021; Sheng et al., 2021) will propagate to the distilled models. In fact, our experiments show that distilled models are adjudged to be more unfair than their teacher model counterparts. In this work, we devise techniques to train models that mitigate societal biases during knowledge distillation.\nOne way to demonstrate this manifestation of societal biases is by looking at text generated by LMs, as illustrated in Fig. 1. As such, the output text focuses on different characteristics of the person, solely based on which gender is mentioned in the context. To this end, we focus on reducing the disparity between groups during the language generation, considering the fairness definition for openended text generations as proposed in Dhamala et al. (2021) and Sheng et al. (2019). We propose an approach that uses counterfactual role-reversed sentences during knowledge distillation. In other words, our approach uses counterfactual texts that are generated by substituting mentions of one demographic group with the other. We employ an automated way to generate these counterfactuals, requiring only a paired list of words from each demographic group.\nTypical knowledge distillation training loss has two components: (a) the LM training loss such as crossentropy to learn information from the training data, and (b) a loss that enforces similarity between outcomes of teacher and student models 1. The counterfactual knowledge is used to correct these loss components in the following ways: (a) augmenting the training set itself, which alters the training loss to learn from more equitable data; and (b) modifying the teacher's output toward more equitability so that the student learns from a more equitable output distribution.\nWe first demonstrate our method using English GPT2-small (Radford et al., 2019) as the teacher and a 6-layer GPT-2 (called DistilGPT-2) as the student model. We focus on binary gender disparities (male vs. female) and use the gender polarity metric for profession prompts from the BOLD dataset (Dhamala et al., 2021) as the primary fairness definition. We show that our approach lowers the gender disparity in the generated text. Next, we demonstrate the applicability of our approach for finetuning English GPT2-small, i.e., using the same architecture for teacher and student models in the distillation framework. Finally, we evaluated the resultant model's gender fairness on downstream tasks such as Contextual Embedding Association Tests (CEAT) (Caliskan et al., 2017) and finetuning on Bios-Bias classification task (De-Arteaga et al., 2019). We find that reduced disparity in openended text generation does not necessarily lead to fairness on other tasks.\n\n2 Related Work\nLarge LMs embody societal biases that could result in harms such as misinformation, stereotype propagation, and disparate resource allocation (Bender et al., 2021; Sheng et al., 2021). Multiple studies have shown that LMs are biased in producing outputs with negative connotations such as toxicity (Gehman et al., 2020; Zhou et al., 2021; Xu et al., 2021) and negative regard (Sheng et al., 2020 (Sheng et al., , 2021) ) towards minority populations. Others have shown that LMs encode prevalent gender biases, such as one gender being more associated with a particular class of professions. Such biases can be revealed via contextual embedding tests (Guo and Caliskan, 2021), stereotype tests (Sap et al., 2020; Nangia et al., 2020), and evaluation of generated texts (Dhamala et al., 2021; Sheng et al., 2019). Few works have also shown that LM can be biased towards ideologies, e.g., Islam (Brown et al., 2020).\nApproaches to mitigate bias in LMs can be broadly summarized as: (a) training or finetuning on a balanced dataset (Solaiman and Dennison, 2021; Dinan et al., 2020)), (b) attaching prefix at inference or training time (Sheng et al., 2020), and (c) using a bias or attribute classifier (e.g., toxicity classifier) to control fairness in text generation (Dathathri et al., 2020; Liang et al., 2021; Liu et al., 2021; Krause et al., 2021). While all these debiasing approaches can be used to mitigate bias in an LM after it is distilled, no prior work aims to directly debias and distill in a single step. Furthermore, the majority of existing approaches focus on reducing toxic text generation (Solaiman and Dennison, 2021; Dathathri et al., 2020; Liang et al., 2021; Liu et al., 2021; Krause et al., 2021). Different from existing works, we present an approach for fair knowledge distillation that aims to mitigate gender bias in text generated from the distilled models.\nOur approach is inspired by the counterfactual notion of fairness (Kusner et al., 2017) and introduces two modifications to the standard distillation: (a) counterfactual data augmentation, and (b) using modified teacher probabilities. Counterfactual fairness and related notions have been previously used for bias mitigation in hate speech detection (Mostafazadeh Davani et al., 2021), word embeddings (Hall Maudslay et al., 2019; Lu et al., 2020; Zhao et al., 2018b), and coreference resolution (Zhao et al., 2018a) tasks. Ours is the first work that uses counterfactual knowledge to achieve equitability in text generation during distillation. Our method is also applicable when the student model or architecture is the same as the teacher model, and we have demonstrated it via experiments.\n\n3 Notion of Language Model Fairness\nWe focus on mitigating gender bias in open-ended language generation from an LM. The bias is mea-sured by assessing the tendency of the LM to associate a specific set of professions to a specific gender, e.g., healthcare professions to female and engineering professions to male. As discussed in Sheng et al. (2021), such societal biases may cause a negative representational impact by propagating stereotypes, misrepresentations, or denigrations of social groups. We consider only binary gender in this paper as LMs often do not encode sufficient representation of non-binary gender context, restricting a meaningful analysis (Dev et al., 2021). We use a related counterfactual notion of fairness, commonly studied in the NLP fairness literature, to motivate our fair distillation approach in Sec. 4. The counterfactual notion of fairness (Kusner et al., 2017) adjudges a model fair if it generates similar predictions before and after swapping the sensitive features in the input.\n\n4 Fair Knowledge Distillation via\nCounterfactual Role Reversal This loss consists of two terms: (a) the crossentropy (CE) between the predicted next token probability and the observed token, and (b) the KLdivergence between the output probabilities from the teacher (P teacher ) and the student (P \u03b8 ) models.\nThe KL-divergence term provides a stronger training signal to the student, leading to more accurate and faster learning (Hinton et al., 2015).\nKnowledge distillation (Eq. ( 1)) will also transfer societal biases while transferring information from the teacher model. To address this problem, we propose to infuse the bias mitigation strategy with knowledge distillation to obtain a less biased and compact model. Our bias mitigating strategy is based on the intuition that given a sequence such as 'She works as a' and its counterfactual 'He works as a', a fair LM should generate similar texts. We materialize this intuition by encouraging student LM to learn similar distribution of probabilities for a sequence of tokens and its counterfactual.\nTo this end, we propose two modifications to the base distillation strategy: (a) Using counterfactual role reversal to modify token probabilities of the teacher model; and (b) Using counterfactual role reversed data for model distillation. We study these two modifications independently and in various combinations 2.\n\n4.1 Counterfactual Role Reversal\nGiven a sequence of tokens referring to a particular demographic group, we want to generate a counterfactual sequence of tokens referring to another related demographic. For example, suppose the original text, referring to the female group was 'She is a mother of two kids and works as a software engineer,' we want to generate a counterfactual referring to the male group 'He is a father of two kids and works as a software engineer.' Inspired by existing works on counterfactual data augmentation for binary gender (Lu et al., 2020; Hall Maudslay et al., 2019), we use word-swapping operations on the sequence of tokens to generate counterfactual sequences. Specifically, we use a curated dictionary of gender words with male female mapping, for instance, father \u2192 mother, she \u2192he, him\u2192her, etc. We generate a counterfactual sequence of tokens from the original sequence by substituting the gendered word in the original sequence with a matching gendered word referring to the opposite gender from this dictionary 3. See Appendix B for the curated dictionary sources and other implementation details.\n\n4.2 Modifying Teacher Probabilities\nNext, we discuss how to use counterfactual sequences to modify knowledge distillation loss. In an open-ended language generation task, the LM produces a natural continuation of text given some context or a prompt (x <t ). To this end, autoregressive LMs such as GPT-2 predict the probability distribution of the next token given the context To mitigate this unchecked transference of gender disparity, we modify the teacher probability of each token by using the next token probabilities from both the original and the counterfactual context (or both genders) during student model training.\nWe combine them to boost the probability of more likely tokens with both genders while the probability of less likely tokens with one or both genders being suppressed or relatively unaffected (See Fig. 2 for a visual illustration). We experiment with different functions to combine these distributions. Let z t = log P (x t |x <t ) and z s = log P (x s |x <s ) are the log-probability distributions (or logits) for the original and the corresponding counterfactual context, respectively 4 . The new unnormalized logits (z t ) are obtained with max, mean, expMean, or swap operation and illustrated in Table 1. We normalize z t so that it is a valid log distribution.\nIntuitively, the max operation would preserve the most likely tokens among either context. The mean is similar to taking the product of the two 4 Due to sub-word tokens, the index of corresponding tokens in the original and counterfactual text may be different. We use index variable s to denote the corresponding token in the counterfactual sentence, indexed at t in the original sentence.\n\nFunction\nOperationmax z t = max{zt, z s } mean z t = z t +z s 2 expMean z t = log e z t +e z s 2 swap z t = z s\nTable 1 : Operations used to modify token probabilities.\ndistributions, thereby increasing the likelihood of words that were more likely in both cases and lowering the likelihood of any other words. One may also consider any weighted combination of z and z . Infact, the swap operation is an extreme case of a weighted combination with the weight of original logits (i.e., z t ) being 0. Finally, expMean is the average of two distributions. Our approach is reminiscent of post-processing approaches that modify the next step probabilities during inference. However, we adapt it here for gender fair-knowledge distillation and use this procedure during training.\n\n4.3 Counterfactual Data Augmentation\nUsing modified probabilities to update the student model rectifies the probability for the tokens generated after the gendered word. However, it only provides a weak signal by changing the log probabilities, and the training data may contain biases, which the student model can learn via cross-entropy loss (See Eq. ( 1)). To this end, we also augment counterfactual data to the training set. Counterfactual data augmentation has been successfully used for gender bias mitigation in various downstream tasks such as static word embedding training (Hall Maudslay et al., 2019) and co-reference resolution (Lu et al., 2020). However, it has not been explored in knowledge distillation or fair\nLM training for open-ended language generation. Therefore, we also experiment with counterfactual data augmentation combined with the proposed next-token logit update strategy.\nWe refer to our approaches as Equitable Role Alteration (ERA). Primarily, the logit modification approach reduces bias in the teacher model's predicated probabilities, thus affecting only the KL divergence component. By contrast, counterfactual data augmentation involves adding new samples to the training set, affecting both loss components.\n\n5 Experiments\n\n\n5.1 Training Setup\nWe use GPT2-small, a 12 layer transformer-based LM comprising of \u223c124M parameters, as the teacher model and a six-layer version of GPT-2 as the student model. We use OpenWebText corpus, which is an open-source reproduction of WebText corpus that was used to train GPT-2 in Radford et al. (2019). Due to limitations in computational budget, we use 10% of the corpus for training. We used the knowledge distillation procedure presented in Sanh et al. ( 2019), but without the cosine loss between representations during knowledge transfer because adopting knowledge distillation for fair learning requires correcting the 'biased knowledge' from the teacher, but it is hard to amend biased contextual representations. This approach can also be used for fair finetuning of an LM by using the same teacher and the student model. In that case, one may initialize with the pretrained teacher's weights. For fair finetuning experiments, we use GPT2-small as both the teacher and the student. Details on training, text generation, and hyperparameters are provided in Appendix D.\n\n5.2 Evaluation of Open-ended Generation\nFairness. We assess gender fairness in English text generation by evaluating the bias of an LM to associate a gender with gender-neutral professions during open-ended text generation. For this, we use the profession prompts and gender polarity metrics from BOLD (Dhamala et al., 2021). These prompts are 10,195 sentence beginnings extracted from the Wikipedia articles and refer to 18 different profession categories such as engineering, healthcare, arts & entertainment, etc. Some examples of BOLD profession prompts are 'An animator is an artist who' and 'A flight nurse is a registered.'\nTexts generated from the LMs with these prompts as contexts are evaluated for gender polarity.\nThe gender polarity score measures if the text is neutral, female-polar having words such as she, woman, etc., or male-polar having words such as he, boy, etc. It is computed by taking the maximum of the normalized projection of each word vector in the LM generated text onto she \u2212 he.\nThe word vectors are computed on the debiased Word2Vec embeddings (Bolukbasi et al., 2016) 5. We use a threshold of 0.25 on the polarity score to label the text as male or female polar. For each profession group, we compute the equitability ratio as min{ m f , f m }, where m and f are the numbers of text generations labeled as male and female polar, respectively. The equitability ratio \u2208 [0, 1] with 1 indicating equitable treatment. We report average and min equitability scores across all professions to summarize the disparity 6.\nPerplexity/Fluency. For real-world applications, an LM should demonstrate high-quality generations along with fair generations. To this end, we report the perplexity of the wikitext-2 test set (Merity et al., 2017) as predicted by the trained LM. Similar to Liu et al. (2021), we evaluate the fluency of the completed prompts from BOLD. The fluency is measured as the perplexity of generated text predicted by the GPT2-large model. Lower perplexity and fluency scores are better.\n\n5.3 Baselines and Other Methods\nFirst, we test the utility of our approach in knowledge distillation compared to teacher and distilled models trained without fairness constraints. We use pre-trained GPT2-small (unfair teacher model) and DistilGPT-2 from the HuggingFace (HF) model repository 7. Since training hyperparameters and dataset used by DistilGPT-2 (HF) is different from ours, we also train a DistilGPT-2 using our setup.\nNext, we compare our approach with two genderbias mitigation approaches by applying them to the distilled version of GPT-2 and GPT2-small from the HF repository. We finetune the distilled models with the counterfactual and original sequences using only cross-entropy loss, which is similar to CDA (Lu et al., 2020) and DAPT (Gururangan et al., 2020). We also compare with the biasmitigation approach of Sheng et al. (2020), which searches for adversarial prompts that increase the likelihood of specifically curated fair texts.\n\n5.4 Results on Open-ended Text Generation\nTable 2 summarizes results for gender disparity mitigation in open-ended generation for DistilGPT-2 and GPT2-small. We observe that compared to the teacher GPT2-small model, which has more parameters, the distilled versions (DistilGPT-2) are more biased which is indicated by lower equitability scores. Due to using only 10% sequences for training, our implementation of DistilGPT-2 has higher perplexity than the HF's version.\nFair Knowledge Distillation with DistilGPT-2. Rows 4-7 in Table 2 show results of using only modified teacher logits based on counterfactuals (Sec. 4.2) with various operations. Overall, these modifications improve over the baseline Distil-GPT-2 model in terms of equitability ratios with only a slight increase in perplexity. Models trained with expMean, max, and swap scored similar or higher equitability than the teacher model. The mean operation was the least effective at improving fairness. The approach that uses only counterfactual data augmentation (row 8 in Table 2)\nshowed more than 1.5\u00d7 improvement in equitability while keeping perplexity almost equal to the baseline model (40.93 vs. 40.88). By contrast, the two-step process of creating a distilled model and then finetuning with counterfactual data (using only cross-entropy loss) resulted in a worse perplexity of 41.63 but better equitability. Our approach combining logit modification and data augmentation (rows 9-10, Table 2) provides better equitability among all the models. Compared to the two-step finetuning approach (i.e., distillation then bias-mitigation), it has better equitability with similar perplexity. The adversarial prompt-based approach of Sheng et al. ( 2020) performs much worse in terms of fairness. One of the reasons for this could be that the adversarial prompts are created to perform well on a small curated dataset which may not generalize. We omitted the perplexity values for this approach as it is not consistent with our evaluation process.\nWhen combining logit modification and data augmentation, we experimented with modifying logits of both counterfactual and original text, and only of the original text. We found that the results with both approaches are similar and report results of modifying both texts in Table 2. The models obtained by combining the counterfactual data augmentation and logit update produce text with very little disparity and achieve the best fairness. Even though the fluency metrics are low, the perplexity for these models is higher. We noticed a high variance in fluency for some of the models. Upon further investigation, we found that the fluency can be very large for one of the profession groups, resulting in a large overall variance during macro averaging. We remark that fluency is at best a noisy measure as it uses an LM to evaluate the outputs; perplexity should be considered a more reliable measure of LM quality. For further evaluations and discussion, we use models trained with the max operation, as the results with the max operation for logit modification, with and without counterfactual augmentation, were most consistent.\nFair Finetuning with GPT-2. We also experiment with finetuning GPT2-small to train genderfair models. The approach is similar to finetuning with counterfactual augmented data but employs knowledge distillation loss instead. Table 2 (rows 13-16) summarizes the results for training fair GPT2-small models. Unlike results with distilled models, all the approaches are fairly competitive. We remark that finetuning and our best approach have similar fairness performance, but our approach has better perplexity owing to improved learning due to the additional KL-divergence term.\nHowever, models trained using only data augmentation or logit modification resulted in less equitability. The student model has two loss componentscross-entropy and KL divergence loss. When employing only one of the techniques, the student model may receive training signals from unfair teacher logits in the former case and training data in the latter case, learning less equitable models. We also note that only logit modification with max operation led to worse results in terms of quality and fairness compared to the baseline GPT-2 model. This could be due to the cross-entropy loss being the dominant training signal, and original training sequences may have spurious gender correlations. The adversarial-prompt approach of Sheng et al. (2020) has lower fluency than other models. On further inspection of generated texts, we noticed that the LM sometimes generates degenerate phrases related to the adversarial prompt instead of the actual prompt about the profession, leading to poor quality generations. Additionally, we did a human evaluation to assess the quality of generated text (See Appendix A). We find the quality of texts generated from our less biased GPT2-small (ERA)\nto be similar to GPT2-small.\n\n6 Gender Fairness on Other Tasks\nIt is often expected that different fairness measures designed for different but related tasks would be correlated. However, recently Goldfarb-Tarrant et al. ( 2021) found that fairness measures for static word embeddings and downstream tasks do not correlate. To this end, we study if our fair text generation models improve fairness on other tasks.\n\n6.1 Bias in Contextual Embeddings\nWe evaluate if fairness in open-ended generation by LMs obtained via the proposed method also transfers to the LM's embeddings using the CEAT metric (Guo and Caliskan, 2021). The WEAT metric measures the effect size of social bias in a static embedding by computing the relative associations of two sets of target words (e.g., career, office; and home, family) with two sets of attribute words (e.g., girl, woman; and boy, man). CEAT extends WEAT to contextual embedding by computing a distribution of effect sizes, each sample obtained by computing WEAT effect size on contextual embedding computed with a different context. CEAT summarizes the combined magnitude of bias by pooling effect sizes with a random-effects model. We use three CEAT tests that measure gender bias: 1) CEAT test 6 with attributes male/female names and targets career/family, 2) CEAT 7 with attributes male/female terms and target math/arts, and 3) CEAT 8 with attributes male/female terms and targets science/arts. See Appendix D for details.\nResults. According to the combined effect sizes metric (known as Cohen's d), d > 0.5 and d > 0.8 are medium and large effect sizes, respectively. However, the absolute effect size is often used as the magnitude of bias (Goldfarb-Tarrant et al., 2021) 8. As shown in Table 3, baseline models have a larger effect size in tests 6 (male/female names and career/family) and 7 (math/arts and male/female terms). In test 8 (male/female terms and science/arts), there was not a strong bias in the embeddings of baseline models. Overall, we observe that the demonstrated fairness in LMs for open-ended language generation in Sec. 5 is not always reflected in the embeddings. For example, the model trained using modified logits based on max operation has a smaller absolute effect size for\n\n7 Discussion and Limitations\nMitigating disparity across races. We conducted preliminary experiments to test if the proposed approach can be extended to different race groups. Similar to Dhamala et al. ( 2021), we consider race bias manifested via people's names and race-specific tokens across four races common in the US: African, European or White, Hispanic & Latino, and Asian. We construct a many-to-many mapping that maps words referring to a given race to words referring to the other races for the counterfactual generation. The rest of the method remains the same as Sec. 4. For fairness evaluation, we use race prompts from BOLD and regard classifier from Sheng et al. (2019), which evaluates whether the person in the text is portrayed as being 'highly thought of.' Results show that the LMs obtained with the proposed approach were less biased in treating different races similarly, indicating that the proposed approach can be extended to other nonbinary groups. However, the improvements were not as significant as gender bias mitigation, leaving plenty of scope for improvement left for future work. We describe the results and experiments in more detail in Appendix C.\nCounterfactual data generation. Dictionarybased word-swapping is a simple and effective method for counterfactual generation (Lu, 2020; Zhao et al., 2018a). However, blind word swapping can also result in factually and/or grammatically incorrect texts. To quantify these errors, we manually evaluated 500 randomly sampled coun-terfactual texts for gender category. We found that 22 (4.4%) of these sentences were incorrect (See Appendix B.4). In this paper, we demonstrate that despite counterfactual data generation not being perfect, it can effectively reduce the gender biases in the model. We expect our bias mitigation approach to benefit from further research in counterfactual data generation, especially for reducing race disparity.\n\n8 Conclusion\nWe proposed techniques to use counterfactual information during knowledge distillation to mitigate gender bias in LMs. In experiments, we show that this approach improves fairness in text generation, but it does not simultaneously enhance fairness on LM embedding and downstream classification task. LMs have become the Swiss army knife of NLP because modeling next word probabilities can learn versatile models that are effective on many tasks. It was surprising that reducing gender disparity in text generation had little effect on other downstream tasks. This finding underscores the importance of evaluating LM fairness along multiple metrics and tasks.\n\n9 Broader Impact and Ethics Statement\nAs language models become prominent, it is imperative to understand and mitigate various harms that they may provoke (Solaiman et al., 2019; Bommasani et al., 2021). Moreover, to make language processing resource-efficient, more focus should be on achieving good performance with smaller models. Finally, we propose approaches to create less biased LMs. However, similar to how gifts were used as weapons in Le Guin's Gifts (Le Guin, 2006), our approach can be repurposed to cause even more disparate treatment. For example, one may remove the mention of a specific race or gender completely from the training set to create a dystopian LM that does not acknowledge that group or entity's existence or the inaccuracy of counterfactual generation may cause LM to learn from fictional and non-grammatical texts. Nevertheless, we hope that our work will inspire more good than harm.\n\nSupplementary: Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal A Human Evaluation of Generated Text\nWe evaluate the quality of text generated from GPT2-small, fair-GPT2-small (ERA), and Sheng et al. ( 2020) (adversarial prompt method with GPT2-small). We randomly sampled 300 prompts and their corresponding text generations from all three models. We then asked annotators to annotate for two tasks. The first task was to rank the generation quality among three sentences generated with the same prompt. The labels for the ranking task were: 1 -Worst, 2 -Medium, and 3 -Best. The second task was to rate the generation quality on a scale from 1-6 -1 being very poor, 2 being poor, 3 being fair, 4 being average, 5 being good, and 6 being excellent. Unlike the ranking task, the ratings are independent of generations from other models for the same prompt. When rating the quality, we asked the annotators to focus on the following properties of the text.\n\u2022 Is it gibberish and nonsensical?\n\u2022 Does the generation fit the prompt?\n\u2022 Is the text grammatically correct?\n\u2022 Is the text consistent and coherent? Is the generation meaningful?\n\u2022 Could the text have been written extracted from news, books, etc.?\n\u2022 Could the text have been written by a Human? We also provided some example annotations, as shown in Table 4.\nThe four annotators participating in these tasks are volunteers proficient in English, originating from various countries but presently or in the past studied/worked in the US, and familiar with language models. The annotators were informed of the research problem. We followed our institution's review process and approval guidelines for these annotation tasks. For each sentence, we collected three annotations. We only keep the ones where at least two annotators agree out of all annotations.\nThe mean and standard deviation of rankings for generations from GPT2-small, fair GPT2-small, and Sheng et al. (2020) were 2.55 \u00b1 0.55, 2.34 \u00b1 0.64, and 1.12 \u00b1 0.41, respectively. Text generated from GPT2-small is ranked highest most of the time. However, the fairer GPT2-small obtained with our method is a close second. The average ratings for generations from GPT2-small, fair GPT2-small (ERA), and Sheng et al. (2020) were respectively, 3.01 \u00b1 1.04, 2.707 \u00b1 1.07, and 1.12 \u00b1 0.41. Consistent with the ranking results, GPT2-small received the highest rating, followed closely by the generations from fairer GPT2-small obtained with our method. Both ranking and rating results indicate that our approach retains most of the performance while reducing gender disparity in the generated text. We find that Sheng et al. (2020) resulted in low-quality generations. As also discussed in the main paper, this could be because the adversarial prompts are designed to increase the likelihood of specially curated fair text and may not work for diverse prompt datasets like BOLD, which contains diverse sentences beginning from various Wikipedia articles. Moreover, we also noticed that the adversarial prompts could lead to generation unrelated to the actual prompt and generate text referring to phrases in the adversarial prompt instead. We provide some example text generations from these approaches in Table 5.\n\nB Counterfactual Role-Reversal Data Generation\nCounterfactual sequences were generated for \u223c 78% and \u223c 65% of the training sequences for gender and race domain experiments, respectively. We limit sequence lengths to 1024 for training. We generate one counterfactual sequence for every sequence in the training set that has words matching with our lists and referring to the demographic groups. The word lists are described next.\n\nB.1 Gender Word Lists\nTo generate counterfactual texts for gender disparity experiments, we create mappings between maleto-female words and vice versa using word lists from Zhao et al. (2018a) 9. We consider some additional words to mappings derived from the above lists, shown in Table 6.\n\nB.2 Race Word Lists\nWe focus on four US-specific races: Asian-American, Hispanic & Latino-American, European-American, and African-American. To create counterfactual text for mitigating racial disparity, we use word sets from different categories. Table 7 shows the word sets we have used. We process and use these word sets as follows.\n\u2022 For words in the country and race category, we append ' American' and '-American' and their equivalent lower case versions and consider these as the actual word sets. Similarly, we consider both capital and lower case variations of the country and race terms.\n\u2022 For words in the color category of Table 7, we use both capital/lower cases and singular/plural versions.\n\u2022 We use two indicators of Latin race 'latino' and 'latina' and swap them with words from Asian-, African-& European-American countries word sets but not vice versa.\n\u2022 We created the list of first names from Tzioumis (2018). They provide prominent first names and the percentage of times this name belonged to a particular race. We use names that are 100% of the time assigned to a particular race and that are in the top-100 names for each race. We use the capital case version of the first names.\n\u2022 We collected the list of common last names from Comenetz (2016) and used the capital case version. Other works have also used names as the indicator of race (Mishra et al., 2020; Caliskan et al., 2017).\nWe replace the word from a specific row and column with words from other columns in the same row randomly to create a counterfactual text. For example, the original text, 'With each new location, Vazquez and Maritza must maintain the quality their fans have come to associate with the brand.' is converted to 'With each new location, Banks and Maritza must maintain the quality their fans have come to associate with the brand.'. Similarly, in the case of gender, the text 'Your father was a drummer in a rock band?' is converted to 'Your mother was a drummer in a rock band?'.\n\nB.3 Note about Sub-word Embeddings\nWe use counterfactual text in two ways, as described in Sec. 4. Due to sub-word embeddings, the length of the counterfactual sequence may not be the same as the original. This is particularly problematic for modifying probability distribution as we have to know the exact location of the corresponding token in the counterfactual and origi-Generations with GPT2 ERA 1 In their study, geographers use four ices as habitats. The icy crust of Antarctica is seen as an arid backdrop for millions of years. But the same frozen crust, making up just over one third of the continent, was striking new shades of blue on Sept. 24, 2010, when a glacier erupted into Greenland's Lhotse Basin of glacial melt. Journal reference: Geophysical Research Letters, doi:10 2 Biotechnology firms can contribute to future ills and possibilities of human development, this paper suggests. Although the link between the mass production of cellulose, corn, and protein on species-to-species conversion studies and the future of farmers utilizing these crops is well-established, and has been shown to be useful for food-factory improvement, a plethora of gene-fixing (gen-catalogical) techniques could be added to the food production process as a way to understand other 3 A clinical pharmaceutical scientist is a licensed, practicing uthymologist who provides the therapeutic data for clinical trials. Because of their clinical studies and publication that are published, such experts are called \"Nutronans\" because they provide practical and practical information to the public about the treatment of many patients. Recently, scientists published the results of a double-blind trial. The first of the first trials to be published with the Food and Drug Administration (FDA 4 A choreographer is one who creates ides, styles a verse and dances a magic formula on the disco stage. So far, I've been unable to find someone whose chops I have known, despite my unflattering reputation for being a professional choreographer. But one thing I've been more familiar with than a career that started with the punk songs of Riot and Dead Kids has changed my career. As many of you know, in 1990, Comixan Echols, then 5 Military logistics was already practiced in ersed in Persia (some time after the first Persian Oceanic, this practice may have been described as the business of the king). That. He may have been telemarking, as we had started to accept that in Persia.\n\u2022 From the mid 1800s through the late 1800s. \u2022 In between his late forties and after his death in 1874. The chronological discrepancy between Persia and Iran continued through the 6 Recorded social dances of the late 16th vernacular style are often followed by particular theatrical content as outlined by the Scotiablogists of 1289-1500. It is the music and the whole ensemble around which the music has been written. Take for instance the carnatonic pavilion-kiln's on the page as given by Sermon Charles that Nennius prophesied, with the success of Zebes and Maelto, that when the whole orchestra Generations with GPT2 1 There are now school counselors in Shandong County Jail. These are going to help kids transition to a safer place.\" If you want to stay informed about our events, let us know by commenting below or by on our Facebook page or following us on Twitter @Daydreamscout Photo: iStockphoto.com/XiaoXiho 2 The public relations officer or chief communications officer said, The military had no plans to write any violence into Afghanistan's speech. The next day, the term \"andoregional incitement\" was deployed in the speech and incident statement which was based on statements made by Rahmatullah Khan. http://www.newstimes.co.nz/national/1984338/displayingsmoke-and-walls-in-republicans-speech 3 Many nurse educators have a clinical specialty in providing a safe home for social service providers, providing consultations and providing outpatient services for breastfeeding moms. There are many other professional services provided by nurse educators that can be critical to the success of a program such as Attic with mother and child care provider services. We cannot emphasize enough how important breastfeeding and the health of mothers and their babies is as a provider of social services. It's also important to remember that these practices and 4 Breakdancing is typically set to songs which are considered better by a female audience, the more options the audience has to choose between. Other examples are Subverted Sadness, Sadness Goes Home, if sung by The Bugles in The Turner Hour, or hope & change if a male audience chooses to reject it. 5 For biologists, knowledge must be usefully with the interpretation of natural conditions and with the knowledge necessary to define their factors and forms. The objective of any study is to show some evolution of organisms that were designed and or selected, or allowed to evolve, or which have exhibited a novel ability by its natural context and the conditions of the environment. The ability to learn needs to be clearly quantified as a function of one or more physical, chemical, or biological factors and, depending Generations with Sheng et al. (2020) 1 Scenography is the seamless synthesis of vernacular, visual and rhythmic characteristics,\" said Jon Forbes, the development manager at Widtat-MacMulling GmbH. Slating is the next stage for Widtat. Upholstered by the amount of data it can cache on its servers, Widtat launched with a working set of domains at the end of October and has expanded further over the course 2 The movement director may create, or research More Exploring concepts Explore the new direction under the lead of Takahiro Sasaki, an engineering genius. The lead teams of the past three years have worked on a range of graphical APIs that can provide a visual approach to hardware Soiling temperature maps (sometimes called -HotCatter), which reveal temperatures associated with various components Through testing of application applications to monitor, 2988, 373, 257, 34269, 287, 257, 3881, 4097, 30} then converted to {7120, 2802, 373, 257, 34269, 287, 257, 3881, 4097, 30} ('Your mother was a drummer in a rock band?').\nAlso, depending on where and how the word occurs, it can be tokenized differently.\n\nB.4 On Limitations and Correctness of Counterfactual Sentences\nFor counterfactual data generation, we use a dictionary-based word-swapping approach. Such a naive approach has some obvious limitations as it does not guarantee the grammatical and factual correctness of the generated sentences. However, we hypothesize that while this approach can potentially generate incorrect data for some examples, overall, it is still a simple yet effective method to generate counterfactual data. In order to verify our hypothesis, we randomly sampled 500 sentences from the generated counterfactual data for gender category and analyzed these for correctness. Out of these 500 sentences, we found 22 (4.4%) incorrect sentences. Most of the errors are related to incorrect pronoun references, such as a male name being used with 'she' as a reference. One such example is 'Onelki Garcia had another interesting outing as she only allowed 1 hit, but did walk three and lasted just 2.2 innings.'\nWe emphasize that the main focus of the paper is not to generate better counterfactual data but to show that counterfactual data can be used to mitigate bias effectively during knowledge distillation. We expect our proposed approach to further benefit from advances in counterfactual data generation.\n\nC Mitigating Racial Disparity\nCounterfactual Data Generation. While not the main focus of this study, we also conducted experiments to mitigate race bias, manifested towards the names of people from various races and certain race-related phrases/words. Since we consider more than two races and there is no one-toone mapping between names, we cannot use the same one-to-one substitution rule for counterfactual data generation as earlier in this case. Hence, we construct a many-to-many mapping that maps multiple words in a given race to multiple words in the remaining races. For each word in the sequence of tokens referring to one race, we substitute it with a randomly chosen word from the corresponding words-set from another race. Additional details and dictionaries used for counterfactual sentence generation are in Appendix B.\nRacial Fairness Measure. We use race prompts from the BOLD Dataset to measure racial disparity and consider four races -Asian American, European American or Whites, African American or Blacks, and Hispanics & Latin Americans. We use the regard classifier to measure regard for each race.\nThe regard classifier has three categories -positive, negative, and neutral regard. Intuitively, the regard classifier measures if sentences cause group A to be more highly thought of than group B. If this is the case, then the language model perpetuates bias towards group B (Sheng et al., 2019). To this end, we measure the ratio of positive and negatively regarded sentences for each racial group. A fair LM should have the same ratio for all the races. We report the variance across groups for each model to capture this intuition, and lower variance would imply more fair treatment. We also report the fraction of generated sentences labeled as having positive, negative, and neutral regard.\nResult. Table 8 shows the result of mitigating racial disparity in text generation with our proposed approach that exploits counterfactual data. We generated counterfactual data for this purpose by replacing mentions of one racial group with the other (see Appendix B for details). The base-line pre-trained models from Hugging-Face have consistently higher regard ratios than the baseline model we trained, indicating that they generated more positive regard than our models. However, these have more variance across groups, indicating more disparate treatment in terms of regard.\nWe note that our counterfactual mitigation approach using both logit modification and augmentation is promising for reducing different regard to different races, but the improvement is not substantial. This could be due to our simple counterfactual generation implementation since we randomly replace race-related words. We replace first and last names independently, which could create mismatched names. There has been some work on improving counterfactual sequence generation and studying its effects, such as Maudslay et al. ( 2019).\nThe authors show that techniques such as name pairing based on frequency can improve the effectiveness of counterfactual data. Another issue could be that we have focused on races in the American context, but the text sequences referring another context (such as Indian or Asian contexts) can be mistakenly used to create counterfactuals. A better approach should identify and filter such texts. Finally, even though names have been used as indicators of race in our work and previous work, this may be a relatively poor indicator of race. Especially to identify races in the American context only compared to gendered words identifying gender roles leading to suboptimal results. We leave these explorations for future work.\n\nD Training and Evaluation Details D.1 Language Model Training\nWe started with the knowledge distillation setup of Sanh et al. (2019) 10 and tailored it to our requirements. We did not use the cosine loss between the representation. We assigned equal weights of 0.5 to LM loss and KL divergence term with a temperature of 2.0. We only use 10% of the OpenWebText sequences. All the models are trained using HuggingFace (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) for three epochs with a learning rate of 10 \u22123 , AdamW optimizer, and a batch size of 1600. We use DeepSpeed (Rasley et al., 2020) for distributed training using 8 V100 GPUs. One epoch took between 5-8 hours.\nWe used DistilGPT-2, which had six layers, an embedding size of 768, and 12 attention heads as the student model. We initialize student models with weights from the even-numbered layers of the teacher model, i.e., pretrained GPT2-small. When using GPT2-small as the student, we initialize with the pretrained GPT2-small.\nFor finetuning with counterfactual text baseline, we use the same training hyper-parameters as above but set the weight of KL divergence term to 0, and LM loss weight is set to 1. For DistilGPT-2, we initialize with DistilGPT-2 (HF) parameters instead of GPT2-small. This is because we will first distill the model and then finetune for fairness in an actual fair-finetuning setup. However, we remark that this model is slightly advantaged compared to our approach in terms of performance (perplexity). Unlike our ERA models, which only use 10% of text sequences from OpenWebText, it was distilled using all the data. For GPT2-small experiments, we initialize with the parameters of pretrained GPT2-small.\nFor adversarial prompts baseline of Sheng et al.\n(2020) and GPT2-small, we use the adversarial prompt for man/woman condition from their paper (Appendix A, Table 5 in their paper). We use their official implementation for DistilGPT-2 experiments to find the adversarial prompt with bias mitigation setting. We set disassociation and association loss to 1 and use \"The man\" and \"The woman\" as the demographics. The adversarial prompt found was \" genomes genomes Parables Nutrition Nutrition Mathematics\".\n\nD.2 Language Model Evaluation\nText Generation. We use top-p sampling (Holtzman et al., 2020) with p = 0.9 and consider the top 10 sequences for all text generation experiments. We limit the max length of the generated sequence to 100.\nPerplexity & Fluency. Perplexity is measured as the exponentiated average negative log-likelihood of a sequence. Given a token sequence, X = {x 0 , x 1 , . . . , x m }, the perplexity of X, ppl(X) is,ppl(X) = exp \u2212 1 m m t=1 log P (x t |x <t )\nGPT-2 is a fixed-length model with a max length of 1024. For this reason, we compute perplexity in chunks of length 1024 and stride of 512. We define fluency as the perplexity measured by GPT2-large with stride size 0.\n\nD.3 Bios-Bias Training and Evaluation\nWe finetune language models on Bios-Bias task for 20 epochs with a batch size of 256, 10 \u22123 learning rate, and AdamW optimizer. Similar to De-Arteaga et al. (2019), we use a 65-10-25 split of the dataset for training, validation, and testing. We use the validation set to pick the best model for evaluation. We do not update the pretrained language model weights during finetuning and use a weighted combination of all the embeddings. These weights are computed using attention. More specifically, we employ a learnable vector to do a dot-product with resulting embeddings (last-layer output or output before the decoder layer). The dot product result is normalized using softmax to compute the weight vector. The weighted combination of the embeddings is passed through a linear classifier to predict the label.\n\nD.4 CEAT Details\nWe use CEAT Tests 6, 7, and 8. The set of target and attribute words that were considered for each test are shown in Table 9. Since we are evaluating contextual embeddings, we will have multiple embeddings for each word based on the context of the word. Therefore, CEAT samples one of the embeddings of the word to compute ES and refers to it as ES i . A random-effects model is used to combine results of multiple such sampling. Eventually, the combined effect size (CES) is computed as:CES = v i ES i v i ,\nWhere v i is the inverse of the sum of in-sample variance and between-sample invariance.\nDifferent contextual embeddings for a word are derived using the random occurrence of that particular word from Reddit. We use the official implementation of CEAT 11 with N=10000, which is the default in their implementation.\n\nFootnotes:\n1: The teacher model refers to the original LM, and the student model refers to the LM being trained. The latter usually has fewer parameters.\n2: Our approach may use the same student model as the teacher, as we demonstrate in Sec. 5.\n3: We found 96% of the generated data on manual analysis to be correct (See Appendix B.4 for details).\n5: https://github.com/tolga-b/debiaswe\n6: We note that this evaluation is not perfect. Gonen and Goldberg (2019) show that debiased word embedding still reserves some gender information for neutral words.\n7: https://huggingface.co/models\n8: P-values are not reported as it does not indicate the magnitude of the bias, and all models were most certainly biased.\n9: Specifically, we use word lists available at https: //github.com/uclanlp/corefBias/blob/ master/WinoBias/wino/extra_gendered_ words.txt, and https://github.com/uclanlp/ corefBias/blob/master/WinoBias/wino/ generalized_swaps.txt\n10: https://github.com/huggingface/ transformers/tree/master/examples/ research_projects/distillation\n11: https://github.com/weiguowilliam/CEAT\n\nReferences:\n\n- Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and Machine Learning. fairml- book.org.- Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the Dan- gers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA. Association for Computing Machinery.\n\n- Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of \"bias\" in NLP. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454-5476, Online. Association for Computational Linguistics.\n\n- Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis- tics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 1004-1015, Online. Association for Computa- tional Linguistics.\n\n- Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016. Man is to computer programmer as woman is to home- maker? debiasing word embeddings. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 4349-4357.\n\n- Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. ArXiv preprint, abs/2108.07258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Lan- guage models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Sci- ence, 356(6334):183-186.\n\n- Joshua Comenetz. 2016. Frequently occurring sur- names in the 2010 census. United States Census Bu- reau.\n\n- Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\n- Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Choulde- chova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in Bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Account- ability, and Transparency, FAT* '19, page 120-128, New York, NY, USA. Association for Computing Ma- chinery.\n\n- Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar- jun Subramonian, Jeff Phillips, and Kai-Wei Chang. 2021. Harms of gender exclusivity and challenges in non-binary representation in language technologies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1968- 1994, Online and Punta Cana, Dominican Republic. As- sociation for Computational Linguistics.\n\n- Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fair- ness, Accountability, and Transparency, FAccT '21, page 862-872, New York, NY, USA. Association for Computing Machinery.\n\n- Emily Dinan, Angela Fan, Adina Williams, Jack Ur- banek, Douwe Kiela, and Jason Weston. 2020. Queens are powerful too: Mitigating gender bias in dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 8173-8188, Online. Association for Computational Linguistics.\n\n- Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxic- ityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356- 3369, Online. Association for Computational Linguis- tics.\n\n- Seraphina Goldfarb-Tarrant, Rebecca Marchant, Ri- cardo Mu\u00f1oz S\u00e1nchez, Mugdha Pandya, and Adam Lopez. 2021. Intrinsic bias metrics do not correlate with application bias. In Proceedings of the 59th An- nual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pages 1926-1940, Online. Association for Com- putational Linguistics.\n\n- Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender bi- ases in word embeddings but do not remove them. In Proceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 609-614, Minneapolis, Minnesota. Association for Computational Linguistics. Wei Guo and Aylin Caliskan. 2021. Detecting emer- gent intersectional biases: Contextualized word embed- dings contain a distribution of human-like biases. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES '21, page 122-133, New York, NY, USA. Association for Computing Machin- ery. Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.\n\n- Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. 2019. It's all in the name: Mitigat- ing gender bias with name-based counterfactual data substitution. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5267- 5275, Hong Kong, China. Association for Computa- tional Linguistics.\n\n- Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. ArXiv preprint, abs/1503.02531.\n\n- Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language un- derstanding. In Findings of the Association for Compu- tational Linguistics: EMNLP 2020, pages 4163-4174, Online. Association for Computational Linguistics.\n\n- Ben Krause, Akhilesh Deepak Gotmare, Bryan Mc- Cann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Gen- erative discriminator guided sequence generation. In Findings of the Association for Computational Linguis- tics: EMNLP 2021, pages 4929-4952, Punta Cana, Do- minican Republic. Association for Computational Lin- guistics.\n\n- Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual fairness. In Ad- vances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4066-4076.\n\n- Ursula K Le Guin. 2006. Gifts. Wadsworth Publishing.\n\n- Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards understand- ing and mitigating social biases in language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 6565-6576. PMLR.\n\n- Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time con- trolled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Associ- ation for Computational Linguistics and the 11th Inter- national Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pages 6691-6706, Online. Association for Computational Linguistics.\n\n- Daming Lu. 2020. Masked reasoner at SemEval-2020 task 4: Fine-tuning RoBERTa for commonsense rea- soning. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 411-414, Barcelona (on- line). International Committee for Computational Lin- guistics.\n\n- Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman- charla, and Anupam Datta. 2020. Gender bias in neural natural language processing. In Logic, Language, and Security, pages 189-202. Springer. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Represen- tations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Shubhanshu Mishra, Sijun He, and Luca Belli. 2020. Assessing demographic bias in named entity recogni- tion. ArXiv preprint, abs/2008.03415.\n\n- Aida Mostafazadeh Davani, Ali Omrani, Brendan Kennedy, Mohammad Atari, Xiang Ren, and Morteza Dehghani. 2021. Improving counterfactual generation for fair hate speech detection. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 92-101, Online. Association for Computational Linguistics.\n\n- Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis- tics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 5356-5371, Online. Association for Computa- tional Linguistics.\n\n- Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953-1967, Online. Association for Computational Linguistics.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An im- perative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024-8035.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.\n\n- Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System opti- mizations enable training deep learning models with over 100 billion parameters. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23- 27, 2020, pages 3505-3506. ACM.\n\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv preprint, abs/1910.01108. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf- sky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 5477-5490, Online. Association for Computa- tional Linguistics.\n\n- Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2020. Towards Controllable Biases in Language Generation. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2020, pages 3239-3254, Online. Association for Computa- tional Linguistics.\n\n- Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal biases in language gen- eration: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4275-4293, Online. Associa- tion for Computational Linguistics.\n\n- Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP), pages 3407-3412, Hong Kong, China. Association for Computational Linguistics.\n\n- Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of lan- guage models. ArXiv preprint, abs/1908.09203. Irene Solaiman and Christy Dennison. 2021. Pro- cess for adapting language models to society (palms) with values-targeted datasets. ArXiv preprint, abs/2106.10328. Konstantinos Tzioumis. 2018. Data for: Demographic aspects of first names.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguis- tics.\n\n- Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gu- rurangan, Maarten Sap, and Dan Klein. 2021. Detox- ifying language models risks marginalizing minority voices. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, pages 2390-2397, Online. Association for Computa- tional Linguistics.\n\n- Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2018a. Gender bias in coreference resolution: Evaluation and debiasing meth- ods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15-20, New Orleans, Louisiana. Association for Computational Linguistics. Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai- Wei Chang. 2018b. Learning gender-neutral word em- beddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847-4853, Brussels, Belgium. Association for Computational Linguistics.\n\n- Xuhui Zhou, Maarten Sap, Swabha Swayamdipta, Yejin Choi, and Noah Smith. 2021. Challenges in auto- mated debiasing for toxic language detection. In Pro- ceedings of the 16th Conference of the European Chap- ter of the Association for Computational Linguistics: Main Volume, pages 3143-3155, Online. Association for Computational Linguistics.\n\n- Ran Zmigrod, Sabrina J. Mielke, Hanna Wallach, and Ryan Cotterell. 2019. Counterfactual data augmen- tation for mitigating gender stereotypes in languages with rich morphology. In Proceedings of the 57th An- nual Meeting of the Association for Computational Lin- guistics, pages 1651-1661, Florence, Italy. Association for Computational Linguistics.\n\n", "annotations": {"Abstract": [{"begin": 86, "end": 1273, "idx": 0}], "Head": [{"begin": 1276, "end": 1290, "n": "1", "idx": 0}, {"begin": 5308, "end": 5322, "n": "2", "idx": 1}, {"begin": 8002, "end": 8037, "n": "3", "idx": 2}, {"begin": 9022, "end": 9055, "n": "4", "idx": 3}, {"begin": 10399, "end": 10431, "n": "4.1", "idx": 4}, {"begin": 11536, "end": 11571, "n": "4.2", "idx": 5}, {"begin": 13222, "end": 13230, "idx": 6}, {"begin": 13998, "end": 14034, "n": "4.3", "idx": 7}, {"begin": 15248, "end": 15261, "n": "5", "idx": 8}, {"begin": 15264, "end": 15282, "n": "5.1", "idx": 9}, {"begin": 16353, "end": 16392, "n": "5.2", "idx": 10}, {"begin": 18382, "end": 18413, "n": "5.3", "idx": 11}, {"begin": 19343, "end": 19384, "n": "5.4", "idx": 12}, {"begin": 24285, "end": 24317, "n": "6", "idx": 13}, {"begin": 24670, "end": 24703, "n": "6.1", "idx": 14}, {"begin": 26507, "end": 26535, "n": "7", "idx": 15}, {"begin": 28434, "end": 28446, "n": "8", "idx": 16}, {"begin": 29107, "end": 29144, "n": "9", "idx": 17}, {"begin": 30025, "end": 30161, "idx": 18}, {"begin": 33282, "end": 33328, "idx": 19}, {"begin": 33712, "end": 33733, "idx": 20}, {"begin": 34003, "end": 34022, "idx": 21}, {"begin": 35993, "end": 36027, "idx": 22}, {"begin": 42277, "end": 42339, "idx": 23}, {"begin": 43560, "end": 43589, "idx": 24}, {"begin": 47228, "end": 47289, "idx": 25}, {"begin": 49440, "end": 49469, "idx": 26}, {"begin": 50139, "end": 50176, "idx": 27}, {"begin": 50991, "end": 51007, "idx": 28}], "ReferenceToBib": [{"begin": 1636, "end": 1655, "target": "#b31", "idx": 0}, {"begin": 1656, "end": 1674, "target": "#b17", "idx": 1}, {"begin": 1676, "end": 1696, "target": "#b16", "idx": 2}, {"begin": 2567, "end": 2588, "target": "#b1", "idx": 3}, {"begin": 2589, "end": 2612, "target": "#b5", "idx": 4}, {"begin": 2613, "end": 2632, "target": "#b33", "idx": 5}, {"begin": 3346, "end": 3367, "target": "#b10", "idx": 6}, {"begin": 3372, "end": 3391, "target": "#b34", "idx": 7}, {"begin": 4401, "end": 4423, "target": "#b29", "idx": 8}, {"begin": 4638, "end": 4660, "target": "#b10", "idx": 9}, {"begin": 5094, "end": 5117, "target": "#b5", "idx": 10}, {"begin": 5166, "end": 5191, "target": "#b8", "idx": 11}, {"begin": 5465, "end": 5486, "target": "#b1", "idx": 12}, {"begin": 5487, "end": 5506, "target": "#b33", "idx": 13}, {"begin": 5621, "end": 5642, "target": "#b12", "idx": 14}, {"begin": 5643, "end": 5661, "target": "#b39", "idx": 15}, {"begin": 5662, "end": 5678, "target": "#b37", "idx": 16}, {"begin": 5699, "end": 5718, "target": "#b32", "idx": 17}, {"begin": 5719, "end": 5743, "target": "#b33", "idx": 18}, {"begin": 5973, "end": 5997, "target": "#b14", "idx": 19}, {"begin": 6016, "end": 6034, "idx": 20}, {"begin": 6035, "end": 6055, "target": "#b27", "idx": 21}, {"begin": 6091, "end": 6113, "target": "#b10", "idx": 22}, {"begin": 6114, "end": 6133, "target": "#b34", "idx": 23}, {"begin": 6209, "end": 6235, "idx": 24}, {"begin": 6351, "end": 6380, "idx": 25}, {"begin": 6381, "end": 6400, "target": "#b11", "idx": 26}, {"begin": 6454, "end": 6474, "target": "#b32", "idx": 27}, {"begin": 6588, "end": 6612, "target": "#b7", "idx": 28}, {"begin": 6613, "end": 6632, "target": "#b21", "idx": 29}, {"begin": 6633, "end": 6650, "target": "#b22", "idx": 30}, {"begin": 6651, "end": 6671, "target": "#b18", "idx": 31}, {"begin": 6928, "end": 6957, "idx": 32}, {"begin": 6958, "end": 6981, "target": "#b7", "idx": 33}, {"begin": 6982, "end": 7001, "target": "#b21", "idx": 34}, {"begin": 7002, "end": 7019, "target": "#b22", "idx": 35}, {"begin": 7020, "end": 7040, "target": "#b18", "idx": 36}, {"begin": 7273, "end": 7294, "target": "#b19", "idx": 37}, {"begin": 7557, "end": 7591, "target": "#b25", "idx": 38}, {"begin": 7609, "end": 7637, "target": "#b15", "idx": 39}, {"begin": 7638, "end": 7654, "target": "#b24", "idx": 40}, {"begin": 7655, "end": 7674, "target": "#b38", "idx": 41}, {"begin": 7703, "end": 7723, "target": "#b38", "idx": 42}, {"begin": 8334, "end": 8353, "target": "#b33", "idx": 43}, {"begin": 8665, "end": 8683, "target": "#b9", "idx": 44}, {"begin": 8878, "end": 8899, "target": "#b19", "idx": 45}, {"begin": 9452, "end": 9473, "target": "#b16", "idx": 46}, {"begin": 10949, "end": 10966, "target": "#b24", "idx": 47}, {"begin": 10967, "end": 10994, "target": "#b15", "idx": 48}, {"begin": 14582, "end": 14610, "target": "#b15", "idx": 49}, {"begin": 14639, "end": 14656, "target": "#b24", "idx": 50}, {"begin": 15556, "end": 15577, "target": "#b29", "idx": 51}, {"begin": 16655, "end": 16677, "target": "#b10", "idx": 52}, {"begin": 17431, "end": 17455, "target": "#b4", "idx": 53}, {"begin": 18094, "end": 18115, "target": "#b24", "idx": 54}, {"begin": 18159, "end": 18176, "target": "#b22", "idx": 55}, {"begin": 19111, "end": 19128, "target": "#b24", "idx": 56}, {"begin": 19138, "end": 19163, "target": "#b12", "idx": 57}, {"begin": 19217, "end": 19236, "target": "#b32", "idx": 58}, {"begin": 20501, "end": 20518, "idx": 59}, {"begin": 23797, "end": 23816, "target": "#b32", "idx": 60}, {"begin": 27173, "end": 27192, "target": "#b34", "idx": 61}, {"begin": 27817, "end": 27827, "target": "#b23", "idx": 62}, {"begin": 27828, "end": 27847, "target": "#b38", "idx": 63}, {"begin": 29262, "end": 29285, "target": "#b35", "idx": 64}, {"begin": 29286, "end": 29309, "target": "#b5", "idx": 65}, {"begin": 29573, "end": 29584, "target": "#b20", "idx": 66}, {"begin": 31970, "end": 31989, "target": "#b32", "idx": 67}, {"begin": 32274, "end": 32293, "target": "#b32", "idx": 68}, {"begin": 32678, "end": 32697, "target": "#b32", "idx": 69}, {"begin": 33885, "end": 33904, "target": "#b38", "idx": 70}, {"begin": 35259, "end": 35274, "target": "#b6", "idx": 71}, {"begin": 35368, "end": 35389, "idx": 72}, {"begin": 35390, "end": 35412, "target": "#b5", "idx": 73}, {"begin": 41176, "end": 41195, "target": "#b32", "idx": 74}, {"begin": 42021, "end": 42140, "idx": 75}, {"begin": 44961, "end": 44981, "target": "#b34", "idx": 76}, {"begin": 47342, "end": 47360, "target": "#b31", "idx": 77}, {"begin": 47645, "end": 47664, "target": "#b36", "idx": 78}, {"begin": 47677, "end": 47698, "target": "#b28", "idx": 79}, {"begin": 47808, "end": 47829, "target": "#b30", "idx": 80}, {"begin": 50316, "end": 50340, "target": "#b8", "idx": 81}, {"begin": 52269, "end": 52294, "target": "#b14", "idx": 82}], "ReferenceToFootnote": [{"begin": 4008, "end": 4009, "target": "#foot_0", "idx": 0}, {"begin": 10395, "end": 10396, "target": "#foot_1", "idx": 1}, {"begin": 11448, "end": 11449, "target": "#foot_2", "idx": 2}, {"begin": 17456, "end": 17457, "target": "#foot_3", "idx": 3}, {"begin": 17898, "end": 17899, "target": "#foot_4", "idx": 4}, {"begin": 18674, "end": 18675, "target": "#foot_5", "idx": 5}, {"begin": 25975, "end": 25976, "target": "#foot_6", "idx": 6}, {"begin": 33905, "end": 33906, "target": "#foot_7", "idx": 7}, {"begin": 47361, "end": 47363, "target": "#foot_8", "idx": 8}, {"begin": 51769, "end": 51771, "target": "#foot_9", "idx": 9}], "SectionFootnote": [{"begin": 51833, "end": 52917, "idx": 0}], "ReferenceString": [{"begin": 52934, "end": 53039, "id": "b0", "idx": 0}, {"begin": 53041, "end": 53372, "id": "b1", "idx": 1}, {"begin": 53376, "end": 53677, "id": "b2", "idx": 2}, {"begin": 53681, "end": 54122, "id": "b3", "idx": 3}, {"begin": 54126, "end": 54484, "id": "b4", "idx": 4}, {"begin": 54488, "end": 55614, "id": "b5", "idx": 5}, {"begin": 55618, "end": 55723, "id": "b6", "idx": 6}, {"begin": 55727, "end": 56059, "id": "b7", "idx": 7}, {"begin": 56063, "end": 56485, "id": "b8", "idx": 8}, {"begin": 56489, "end": 56893, "id": "b9", "idx": 9}, {"begin": 56897, "end": 57266, "id": "b10", "idx": 10}, {"begin": 57270, "end": 57604, "id": "b11", "idx": 11}, {"begin": 57608, "end": 57916, "id": "b12", "idx": 12}, {"begin": 57920, "end": 58357, "id": "b13", "idx": 13}, {"begin": 58361, "end": 59433, "id": "b14", "idx": 14}, {"begin": 59437, "end": 59868, "id": "b15", "idx": 15}, {"begin": 59872, "end": 60002, "id": "b16", "idx": 16}, {"begin": 60006, "end": 60560, "id": "b17", "idx": 17}, {"begin": 60564, "end": 60934, "id": "b18", "idx": 18}, {"begin": 60938, "end": 61218, "id": "b19", "idx": 19}, {"begin": 61222, "end": 61274, "id": "b20", "idx": 20}, {"begin": 61278, "end": 61626, "id": "b21", "idx": 21}, {"begin": 61630, "end": 62091, "id": "b22", "idx": 22}, {"begin": 62095, "end": 62360, "id": "b23", "idx": 23}, {"begin": 62364, "end": 62960, "id": "b24", "idx": 24}, {"begin": 62964, "end": 63280, "id": "b25", "idx": 25}, {"begin": 63284, "end": 63667, "id": "b26", "idx": 26}, {"begin": 63671, "end": 63998, "id": "b27", "idx": 27}, {"begin": 64002, "end": 64595, "id": "b28", "idx": 28}, {"begin": 64599, "end": 64760, "id": "b29", "idx": 29}, {"begin": 64764, "end": 65090, "id": "b30", "idx": 30}, {"begin": 65094, "end": 65611, "id": "b31", "idx": 31}, {"begin": 65615, "end": 65879, "id": "b32", "idx": 32}, {"begin": 65883, "end": 66279, "id": "b33", "idx": 33}, {"begin": 66283, "end": 66686, "id": "b34", "idx": 34}, {"begin": 66690, "end": 67185, "id": "b35", "idx": 35}, {"begin": 67189, "end": 67759, "id": "b36", "idx": 36}, {"begin": 67763, "end": 68143, "id": "b37", "idx": 37}, {"begin": 68147, "end": 68843, "id": "b38", "idx": 38}, {"begin": 68847, "end": 69188, "id": "b39", "idx": 39}, {"begin": 69192, "end": 69541, "id": "b40", "idx": 40}], "ReferenceToTable": [{"begin": 12770, "end": 12771, "idx": 0}, {"begin": 13340, "end": 13341, "idx": 1}, {"begin": 19391, "end": 19392, "target": "#tab_2", "idx": 2}, {"begin": 19877, "end": 19878, "target": "#tab_2", "idx": 3}, {"begin": 20388, "end": 20389, "target": "#tab_2", "idx": 4}, {"begin": 20808, "end": 20809, "target": "#tab_2", "idx": 5}, {"begin": 21636, "end": 21637, "target": "#tab_2", "idx": 6}, {"begin": 22720, "end": 22721, "target": "#tab_2", "idx": 7}, {"begin": 25996, "end": 25997, "target": "#tab_4", "idx": 8}, {"begin": 31373, "end": 31374, "target": "#tab_7", "idx": 9}, {"begin": 33278, "end": 33279, "target": "#tab_8", "idx": 10}, {"begin": 33999, "end": 34000, "target": "#tab_11", "idx": 11}, {"begin": 34257, "end": 34258, "idx": 12}, {"begin": 34645, "end": 34646, "idx": 13}, {"begin": 45396, "end": 45397, "idx": 14}, {"begin": 49097, "end": 49098, "target": "#tab_8", "idx": 15}, {"begin": 51131, "end": 51132, "idx": 16}], "Footnote": [{"begin": 51844, "end": 51986, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 51987, "end": 52078, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 52079, "end": 52181, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 52182, "end": 52220, "id": "foot_3", "n": "5", "idx": 3}, {"begin": 52221, "end": 52386, "id": "foot_4", "n": "6", "idx": 4}, {"begin": 52387, "end": 52419, "id": "foot_5", "n": "7", "idx": 5}, {"begin": 52420, "end": 52542, "id": "foot_6", "n": "8", "idx": 6}, {"begin": 52543, "end": 52773, "id": "foot_7", "n": "9", "idx": 7}, {"begin": 52774, "end": 52875, "id": "foot_8", "n": "10", "idx": 8}, {"begin": 52876, "end": 52917, "id": "foot_9", "n": "11", "idx": 9}], "ReferenceToFormula": [{"begin": 9505, "end": 9506, "idx": 0}, {"begin": 14353, "end": 14354, "idx": 1}, {"begin": 15734, "end": 15738, "idx": 2}, {"begin": 21058, "end": 21062, "idx": 3}, {"begin": 24478, "end": 24482, "idx": 4}, {"begin": 26711, "end": 26715, "idx": 5}, {"begin": 30263, "end": 30267, "idx": 6}, {"begin": 46494, "end": 46498, "idx": 7}], "Paragraph": [{"begin": 96, "end": 1273, "idx": 0}, {"begin": 1291, "end": 1987, "idx": 1}, {"begin": 1988, "end": 2042, "idx": 2}, {"begin": 2043, "end": 2906, "idx": 3}, {"begin": 2907, "end": 3764, "idx": 4}, {"begin": 3765, "end": 4343, "idx": 5}, {"begin": 4344, "end": 5306, "idx": 6}, {"begin": 5323, "end": 6236, "idx": 7}, {"begin": 6237, "end": 7206, "idx": 8}, {"begin": 7207, "end": 8000, "idx": 9}, {"begin": 8038, "end": 9020, "idx": 10}, {"begin": 9056, "end": 9331, "idx": 11}, {"begin": 9332, "end": 9474, "idx": 12}, {"begin": 9475, "end": 10079, "idx": 13}, {"begin": 10080, "end": 10397, "idx": 14}, {"begin": 10432, "end": 11534, "idx": 15}, {"begin": 11572, "end": 12162, "idx": 16}, {"begin": 12163, "end": 12829, "idx": 17}, {"begin": 12830, "end": 13220, "idx": 18}, {"begin": 13231, "end": 13240, "idx": 19}, {"begin": 13334, "end": 13390, "idx": 20}, {"begin": 13391, "end": 13996, "idx": 21}, {"begin": 14035, "end": 14725, "idx": 22}, {"begin": 14726, "end": 14902, "idx": 23}, {"begin": 14903, "end": 15246, "idx": 24}, {"begin": 15283, "end": 16351, "idx": 25}, {"begin": 16393, "end": 16983, "idx": 26}, {"begin": 16984, "end": 17078, "idx": 27}, {"begin": 17079, "end": 17364, "idx": 28}, {"begin": 17365, "end": 17900, "idx": 29}, {"begin": 17901, "end": 18380, "idx": 30}, {"begin": 18414, "end": 18813, "idx": 31}, {"begin": 18814, "end": 19341, "idx": 32}, {"begin": 19385, "end": 19812, "idx": 33}, {"begin": 19813, "end": 20390, "idx": 34}, {"begin": 20391, "end": 21356, "idx": 35}, {"begin": 21357, "end": 22489, "idx": 36}, {"begin": 22490, "end": 23066, "idx": 37}, {"begin": 23067, "end": 24254, "idx": 38}, {"begin": 24255, "end": 24283, "idx": 39}, {"begin": 24318, "end": 24668, "idx": 40}, {"begin": 24704, "end": 25723, "idx": 41}, {"begin": 25724, "end": 26505, "idx": 42}, {"begin": 26536, "end": 27691, "idx": 43}, {"begin": 27692, "end": 28432, "idx": 44}, {"begin": 28447, "end": 29105, "idx": 45}, {"begin": 29145, "end": 30023, "idx": 46}, {"begin": 30162, "end": 31016, "idx": 47}, {"begin": 31017, "end": 31051, "idx": 48}, {"begin": 31052, "end": 31089, "idx": 49}, {"begin": 31090, "end": 31126, "idx": 50}, {"begin": 31127, "end": 31195, "idx": 51}, {"begin": 31196, "end": 31264, "idx": 52}, {"begin": 31265, "end": 31375, "idx": 53}, {"begin": 31376, "end": 31871, "idx": 54}, {"begin": 31872, "end": 33280, "idx": 55}, {"begin": 33329, "end": 33710, "idx": 56}, {"begin": 33734, "end": 34001, "idx": 57}, {"begin": 34023, "end": 34339, "idx": 58}, {"begin": 34340, "end": 34601, "idx": 59}, {"begin": 34602, "end": 34709, "idx": 60}, {"begin": 34710, "end": 34875, "idx": 61}, {"begin": 34876, "end": 35208, "idx": 62}, {"begin": 35209, "end": 35413, "idx": 63}, {"begin": 35414, "end": 35991, "idx": 64}, {"begin": 36028, "end": 38465, "idx": 65}, {"begin": 38466, "end": 42192, "idx": 66}, {"begin": 42193, "end": 42275, "idx": 67}, {"begin": 42340, "end": 43257, "idx": 68}, {"begin": 43258, "end": 43558, "idx": 69}, {"begin": 43590, "end": 44396, "idx": 70}, {"begin": 44397, "end": 44684, "idx": 71}, {"begin": 44685, "end": 45381, "idx": 72}, {"begin": 45382, "end": 45963, "idx": 73}, {"begin": 45964, "end": 46500, "idx": 74}, {"begin": 46501, "end": 47226, "idx": 75}, {"begin": 47290, "end": 47907, "idx": 76}, {"begin": 47908, "end": 48228, "idx": 77}, {"begin": 48229, "end": 48934, "idx": 78}, {"begin": 48935, "end": 48983, "idx": 79}, {"begin": 48984, "end": 49438, "idx": 80}, {"begin": 49470, "end": 49674, "idx": 81}, {"begin": 49675, "end": 49875, "idx": 82}, {"begin": 49919, "end": 50137, "idx": 83}, {"begin": 50177, "end": 50989, "idx": 84}, {"begin": 51008, "end": 51496, "idx": 85}, {"begin": 51517, "end": 51605, "idx": 86}, {"begin": 51606, "end": 51831, "idx": 87}], "SectionHeader": [{"begin": 0, "end": 1273, "idx": 0}], "SectionReference": [{"begin": 52919, "end": 69543, "idx": 0}], "Sentence": [{"begin": 96, "end": 267, "idx": 0}, {"begin": 268, "end": 417, "idx": 1}, {"begin": 418, "end": 564, "idx": 2}, {"begin": 565, "end": 709, "idx": 3}, {"begin": 710, "end": 875, "idx": 4}, {"begin": 876, "end": 1108, "idx": 5}, {"begin": 1109, "end": 1273, "idx": 6}, {"begin": 1291, "end": 1525, "idx": 7}, {"begin": 1526, "end": 1698, "idx": 8}, {"begin": 1699, "end": 1796, "idx": 9}, {"begin": 1797, "end": 1886, "idx": 10}, {"begin": 1887, "end": 1987, "idx": 11}, {"begin": 1988, "end": 1999, "idx": 12}, {"begin": 2000, "end": 2042, "idx": 13}, {"begin": 2043, "end": 2090, "idx": 14}, {"begin": 2091, "end": 2181, "idx": 15}, {"begin": 2182, "end": 2193, "idx": 16}, {"begin": 2194, "end": 2286, "idx": 17}, {"begin": 2287, "end": 2408, "idx": 18}, {"begin": 2409, "end": 2510, "idx": 19}, {"begin": 2511, "end": 2559, "idx": 20}, {"begin": 2560, "end": 2672, "idx": 21}, {"begin": 2673, "end": 2794, "idx": 22}, {"begin": 2795, "end": 2906, "idx": 23}, {"begin": 2907, "end": 3033, "idx": 24}, {"begin": 3034, "end": 3168, "idx": 25}, {"begin": 3169, "end": 3392, "idx": 26}, {"begin": 3393, "end": 3495, "idx": 27}, {"begin": 3496, "end": 3635, "idx": 28}, {"begin": 3636, "end": 3764, "idx": 29}, {"begin": 3765, "end": 4010, "idx": 30}, {"begin": 4011, "end": 4343, "idx": 31}, {"begin": 4344, "end": 4501, "idx": 32}, {"begin": 4502, "end": 4696, "idx": 33}, {"begin": 4697, "end": 4773, "idx": 34}, {"begin": 4774, "end": 4959, "idx": 35}, {"begin": 4960, "end": 5192, "idx": 36}, {"begin": 5193, "end": 5306, "idx": 37}, {"begin": 5323, "end": 5507, "idx": 38}, {"begin": 5508, "end": 5773, "idx": 39}, {"begin": 5774, "end": 5913, "idx": 40}, {"begin": 5914, "end": 6134, "idx": 41}, {"begin": 6135, "end": 6236, "idx": 42}, {"begin": 6237, "end": 6672, "idx": 43}, {"begin": 6673, "end": 6838, "idx": 44}, {"begin": 6839, "end": 7041, "idx": 45}, {"begin": 7042, "end": 7206, "idx": 46}, {"begin": 7207, "end": 7441, "idx": 47}, {"begin": 7442, "end": 7730, "idx": 48}, {"begin": 7731, "end": 7852, "idx": 49}, {"begin": 7853, "end": 8000, "idx": 50}, {"begin": 8038, "end": 8118, "idx": 51}, {"begin": 8119, "end": 8317, "idx": 52}, {"begin": 8318, "end": 8502, "idx": 53}, {"begin": 8503, "end": 8684, "idx": 54}, {"begin": 8685, "end": 9020, "idx": 55}, {"begin": 9056, "end": 9331, "idx": 56}, {"begin": 9332, "end": 9474, "idx": 57}, {"begin": 9475, "end": 9502, "idx": 58}, {"begin": 9503, "end": 9598, "idx": 59}, {"begin": 9599, "end": 9744, "idx": 60}, {"begin": 9745, "end": 9927, "idx": 61}, {"begin": 9928, "end": 10079, "idx": 62}, {"begin": 10080, "end": 10319, "idx": 63}, {"begin": 10320, "end": 10397, "idx": 64}, {"begin": 10432, "end": 10601, "idx": 65}, {"begin": 10602, "end": 10867, "idx": 66}, {"begin": 10868, "end": 11091, "idx": 67}, {"begin": 11092, "end": 11229, "idx": 68}, {"begin": 11230, "end": 11450, "idx": 69}, {"begin": 11451, "end": 11534, "idx": 70}, {"begin": 11572, "end": 11663, "idx": 71}, {"begin": 11664, "end": 11793, "idx": 72}, {"begin": 11794, "end": 12162, "idx": 73}, {"begin": 12163, "end": 12394, "idx": 74}, {"begin": 12395, "end": 12465, "idx": 75}, {"begin": 12466, "end": 12653, "idx": 76}, {"begin": 12654, "end": 12772, "idx": 77}, {"begin": 12773, "end": 12829, "idx": 78}, {"begin": 12830, "end": 12920, "idx": 79}, {"begin": 12921, "end": 13091, "idx": 80}, {"begin": 13092, "end": 13220, "idx": 81}, {"begin": 13231, "end": 13240, "idx": 82}, {"begin": 13334, "end": 13390, "idx": 83}, {"begin": 13391, "end": 13532, "idx": 84}, {"begin": 13533, "end": 13592, "idx": 85}, {"begin": 13593, "end": 13775, "idx": 86}, {"begin": 13776, "end": 13891, "idx": 87}, {"begin": 13892, "end": 13996, "idx": 88}, {"begin": 14035, "end": 14167, "idx": 89}, {"begin": 14168, "end": 14357, "idx": 90}, {"begin": 14358, "end": 14427, "idx": 91}, {"begin": 14428, "end": 14657, "idx": 92}, {"begin": 14658, "end": 14725, "idx": 93}, {"begin": 14726, "end": 14773, "idx": 94}, {"begin": 14774, "end": 14902, "idx": 95}, {"begin": 14903, "end": 14965, "idx": 96}, {"begin": 14966, "end": 15119, "idx": 97}, {"begin": 15120, "end": 15246, "idx": 98}, {"begin": 15283, "end": 15441, "idx": 99}, {"begin": 15442, "end": 15578, "idx": 100}, {"begin": 15579, "end": 15661, "idx": 101}, {"begin": 15662, "end": 15996, "idx": 102}, {"begin": 15997, "end": 16105, "idx": 103}, {"begin": 16106, "end": 16177, "idx": 104}, {"begin": 16178, "end": 16265, "idx": 105}, {"begin": 16266, "end": 16351, "idx": 106}, {"begin": 16393, "end": 16402, "idx": 107}, {"begin": 16403, "end": 16576, "idx": 108}, {"begin": 16577, "end": 16678, "idx": 109}, {"begin": 16679, "end": 16869, "idx": 110}, {"begin": 16870, "end": 16983, "idx": 111}, {"begin": 16984, "end": 17078, "idx": 112}, {"begin": 17079, "end": 17238, "idx": 113}, {"begin": 17239, "end": 17364, "idx": 114}, {"begin": 17365, "end": 17458, "idx": 115}, {"begin": 17459, "end": 17550, "idx": 116}, {"begin": 17551, "end": 17730, "idx": 117}, {"begin": 17731, "end": 17801, "idx": 118}, {"begin": 17802, "end": 17900, "idx": 119}, {"begin": 17901, "end": 17920, "idx": 120}, {"begin": 17921, "end": 18028, "idx": 121}, {"begin": 18029, "end": 18147, "idx": 122}, {"begin": 18148, "end": 18237, "idx": 123}, {"begin": 18238, "end": 18332, "idx": 124}, {"begin": 18333, "end": 18380, "idx": 125}, {"begin": 18414, "end": 18561, "idx": 126}, {"begin": 18562, "end": 18676, "idx": 127}, {"begin": 18677, "end": 18813, "idx": 128}, {"begin": 18814, "end": 18975, "idx": 129}, {"begin": 18976, "end": 19164, "idx": 130}, {"begin": 19165, "end": 19341, "idx": 131}, {"begin": 19385, "end": 19500, "idx": 132}, {"begin": 19501, "end": 19687, "idx": 133}, {"begin": 19688, "end": 19812, "idx": 134}, {"begin": 19813, "end": 19858, "idx": 135}, {"begin": 19859, "end": 19960, "idx": 136}, {"begin": 19961, "end": 19990, "idx": 137}, {"begin": 19991, "end": 20139, "idx": 138}, {"begin": 20140, "end": 20244, "idx": 139}, {"begin": 20245, "end": 20310, "idx": 140}, {"begin": 20311, "end": 20390, "idx": 141}, {"begin": 20391, "end": 20519, "idx": 142}, {"begin": 20520, "end": 20725, "idx": 143}, {"begin": 20726, "end": 20861, "idx": 144}, {"begin": 20862, "end": 21001, "idx": 145}, {"begin": 21002, "end": 21105, "idx": 146}, {"begin": 21106, "end": 21252, "idx": 147}, {"begin": 21253, "end": 21356, "idx": 148}, {"begin": 21357, "end": 21524, "idx": 149}, {"begin": 21525, "end": 21638, "idx": 150}, {"begin": 21639, "end": 21796, "idx": 151}, {"begin": 21797, "end": 21880, "idx": 152}, {"begin": 21881, "end": 21942, "idx": 153}, {"begin": 21943, "end": 22110, "idx": 154}, {"begin": 22111, "end": 22273, "idx": 155}, {"begin": 22274, "end": 22489, "idx": 156}, {"begin": 22490, "end": 22517, "idx": 157}, {"begin": 22518, "end": 22591, "idx": 158}, {"begin": 22592, "end": 22713, "idx": 159}, {"begin": 22714, "end": 22794, "idx": 160}, {"begin": 22795, "end": 22875, "idx": 161}, {"begin": 22876, "end": 23066, "idx": 162}, {"begin": 23067, "end": 23172, "idx": 163}, {"begin": 23173, "end": 23251, "idx": 164}, {"begin": 23252, "end": 23457, "idx": 165}, {"begin": 23458, "end": 23610, "idx": 166}, {"begin": 23611, "end": 23761, "idx": 167}, {"begin": 23762, "end": 23853, "idx": 168}, {"begin": 23854, "end": 24079, "idx": 169}, {"begin": 24080, "end": 24177, "idx": 170}, {"begin": 24178, "end": 24254, "idx": 171}, {"begin": 24255, "end": 24283, "idx": 172}, {"begin": 24318, "end": 24433, "idx": 173}, {"begin": 24434, "end": 24578, "idx": 174}, {"begin": 24579, "end": 24668, "idx": 175}, {"begin": 24704, "end": 24878, "idx": 176}, {"begin": 24879, "end": 25132, "idx": 177}, {"begin": 25133, "end": 25329, "idx": 178}, {"begin": 25330, "end": 25429, "idx": 179}, {"begin": 25430, "end": 25695, "idx": 180}, {"begin": 25696, "end": 25723, "idx": 181}, {"begin": 25724, "end": 25732, "idx": 182}, {"begin": 25733, "end": 25869, "idx": 183}, {"begin": 25870, "end": 25977, "idx": 184}, {"begin": 25978, "end": 26130, "idx": 185}, {"begin": 26131, "end": 26244, "idx": 186}, {"begin": 26245, "end": 26390, "idx": 187}, {"begin": 26391, "end": 26505, "idx": 188}, {"begin": 26536, "end": 26570, "idx": 189}, {"begin": 26571, "end": 26682, "idx": 190}, {"begin": 26683, "end": 26888, "idx": 191}, {"begin": 26889, "end": 27039, "idx": 192}, {"begin": 27040, "end": 27482, "idx": 193}, {"begin": 27483, "end": 27621, "idx": 194}, {"begin": 27622, "end": 27691, "idx": 195}, {"begin": 27692, "end": 27723, "idx": 196}, {"begin": 27724, "end": 27848, "idx": 197}, {"begin": 27849, "end": 27944, "idx": 198}, {"begin": 27945, "end": 28056, "idx": 199}, {"begin": 28057, "end": 28134, "idx": 200}, {"begin": 28135, "end": 28285, "idx": 201}, {"begin": 28286, "end": 28432, "idx": 202}, {"begin": 28447, "end": 28565, "idx": 203}, {"begin": 28566, "end": 28746, "idx": 204}, {"begin": 28747, "end": 28892, "idx": 205}, {"begin": 28893, "end": 29005, "idx": 206}, {"begin": 29006, "end": 29105, "idx": 207}, {"begin": 29145, "end": 29310, "idx": 208}, {"begin": 29311, "end": 29440, "idx": 209}, {"begin": 29441, "end": 29498, "idx": 210}, {"begin": 29499, "end": 29656, "idx": 211}, {"begin": 29657, "end": 29953, "idx": 212}, {"begin": 29954, "end": 30023, "idx": 213}, {"begin": 30162, "end": 30313, "idx": 214}, {"begin": 30314, "end": 30409, "idx": 215}, {"begin": 30410, "end": 30461, "idx": 216}, {"begin": 30462, "end": 30565, "idx": 217}, {"begin": 30566, "end": 30637, "idx": 218}, {"begin": 30638, "end": 30810, "idx": 219}, {"begin": 30811, "end": 30917, "idx": 220}, {"begin": 30918, "end": 31016, "idx": 221}, {"begin": 31017, "end": 31051, "idx": 222}, {"begin": 31052, "end": 31089, "idx": 223}, {"begin": 31090, "end": 31126, "idx": 224}, {"begin": 31127, "end": 31165, "idx": 225}, {"begin": 31166, "end": 31195, "idx": 226}, {"begin": 31196, "end": 31264, "idx": 227}, {"begin": 31265, "end": 31311, "idx": 228}, {"begin": 31312, "end": 31375, "idx": 229}, {"begin": 31376, "end": 31587, "idx": 230}, {"begin": 31588, "end": 31641, "idx": 231}, {"begin": 31642, "end": 31738, "idx": 232}, {"begin": 31739, "end": 31789, "idx": 233}, {"begin": 31790, "end": 31871, "idx": 234}, {"begin": 31872, "end": 32051, "idx": 235}, {"begin": 32052, "end": 32118, "idx": 236}, {"begin": 32119, "end": 32193, "idx": 237}, {"begin": 32194, "end": 32356, "idx": 238}, {"begin": 32357, "end": 32518, "idx": 239}, {"begin": 32519, "end": 32664, "idx": 240}, {"begin": 32665, "end": 32734, "idx": 241}, {"begin": 32735, "end": 33020, "idx": 242}, {"begin": 33021, "end": 33205, "idx": 243}, {"begin": 33206, "end": 33280, "idx": 244}, {"begin": 33329, "end": 33468, "idx": 245}, {"begin": 33469, "end": 33516, "idx": 246}, {"begin": 33517, "end": 33675, "idx": 247}, {"begin": 33676, "end": 33710, "idx": 248}, {"begin": 33734, "end": 33907, "idx": 249}, {"begin": 33908, "end": 34001, "idx": 250}, {"begin": 34023, "end": 34143, "idx": 251}, {"begin": 34144, "end": 34250, "idx": 252}, {"begin": 34251, "end": 34292, "idx": 253}, {"begin": 34293, "end": 34339, "idx": 254}, {"begin": 34340, "end": 34508, "idx": 255}, {"begin": 34509, "end": 34601, "idx": 256}, {"begin": 34602, "end": 34709, "idx": 257}, {"begin": 34710, "end": 34875, "idx": 258}, {"begin": 34876, "end": 34934, "idx": 259}, {"begin": 34935, "end": 35038, "idx": 260}, {"begin": 35039, "end": 35156, "idx": 261}, {"begin": 35157, "end": 35208, "idx": 262}, {"begin": 35209, "end": 35309, "idx": 263}, {"begin": 35310, "end": 35413, "idx": 264}, {"begin": 35414, "end": 35552, "idx": 265}, {"begin": 35553, "end": 35843, "idx": 266}, {"begin": 35844, "end": 35991, "idx": 267}, {"begin": 36028, "end": 36198, "idx": 268}, {"begin": 36199, "end": 36450, "idx": 269}, {"begin": 36451, "end": 36529, "idx": 270}, {"begin": 36530, "end": 36725, "idx": 271}, {"begin": 36726, "end": 36894, "idx": 272}, {"begin": 36895, "end": 37406, "idx": 273}, {"begin": 37407, "end": 37621, "idx": 274}, {"begin": 37622, "end": 37689, "idx": 275}, {"begin": 37690, "end": 37883, "idx": 276}, {"begin": 37884, "end": 38025, "idx": 277}, {"begin": 38026, "end": 38160, "idx": 278}, {"begin": 38161, "end": 38385, "idx": 279}, {"begin": 38386, "end": 38391, "idx": 280}, {"begin": 38392, "end": 38465, "idx": 281}, {"begin": 38466, "end": 38510, "idx": 282}, {"begin": 38511, "end": 38569, "idx": 283}, {"begin": 38570, "end": 38804, "idx": 284}, {"begin": 38805, "end": 38884, "idx": 285}, {"begin": 38885, "end": 39146, "idx": 286}, {"begin": 39147, "end": 39205, "idx": 287}, {"begin": 39206, "end": 39529, "idx": 288}, {"begin": 39530, "end": 39684, "idx": 289}, {"begin": 39685, "end": 39962, "idx": 290}, {"begin": 39963, "end": 40142, "idx": 291}, {"begin": 40143, "end": 40277, "idx": 292}, {"begin": 40278, "end": 40478, "idx": 293}, {"begin": 40479, "end": 40635, "idx": 294}, {"begin": 40636, "end": 40794, "idx": 295}, {"begin": 40795, "end": 41020, "idx": 296}, {"begin": 41021, "end": 41356, "idx": 297}, {"begin": 41357, "end": 41394, "idx": 298}, {"begin": 41395, "end": 41722, "idx": 299}, {"begin": 41723, "end": 42192, "idx": 300}, {"begin": 42193, "end": 42275, "idx": 301}, {"begin": 42340, "end": 42425, "idx": 302}, {"begin": 42426, "end": 42569, "idx": 303}, {"begin": 42570, "end": 42761, "idx": 304}, {"begin": 42762, "end": 42925, "idx": 305}, {"begin": 42926, "end": 42993, "idx": 306}, {"begin": 42994, "end": 43115, "idx": 307}, {"begin": 43116, "end": 43257, "idx": 308}, {"begin": 43258, "end": 43458, "idx": 309}, {"begin": 43459, "end": 43558, "idx": 310}, {"begin": 43590, "end": 43621, "idx": 311}, {"begin": 43622, "end": 43812, "idx": 312}, {"begin": 43813, "end": 44011, "idx": 313}, {"begin": 44012, "end": 44137, "idx": 314}, {"begin": 44138, "end": 44297, "idx": 315}, {"begin": 44298, "end": 44396, "idx": 316}, {"begin": 44397, "end": 44421, "idx": 317}, {"begin": 44422, "end": 44622, "idx": 318}, {"begin": 44623, "end": 44684, "idx": 319}, {"begin": 44685, "end": 44768, "idx": 320}, {"begin": 44769, "end": 44982, "idx": 321}, {"begin": 44983, "end": 45085, "idx": 322}, {"begin": 45086, "end": 45141, "idx": 323}, {"begin": 45142, "end": 45272, "idx": 324}, {"begin": 45273, "end": 45381, "idx": 325}, {"begin": 45382, "end": 45389, "idx": 326}, {"begin": 45390, "end": 45526, "idx": 327}, {"begin": 45527, "end": 45663, "idx": 328}, {"begin": 45664, "end": 45858, "idx": 329}, {"begin": 45859, "end": 45963, "idx": 330}, {"begin": 45964, "end": 46165, "idx": 331}, {"begin": 46166, "end": 46284, "idx": 332}, {"begin": 46285, "end": 46368, "idx": 333}, {"begin": 46369, "end": 46500, "idx": 334}, {"begin": 46501, "end": 46627, "idx": 335}, {"begin": 46628, "end": 46839, "idx": 336}, {"begin": 46840, "end": 46896, "idx": 337}, {"begin": 46897, "end": 47040, "idx": 338}, {"begin": 47041, "end": 47181, "idx": 339}, {"begin": 47182, "end": 47226, "idx": 340}, {"begin": 47290, "end": 47400, "idx": 341}, {"begin": 47401, "end": 47459, "idx": 342}, {"begin": 47460, "end": 47553, "idx": 343}, {"begin": 47554, "end": 47599, "idx": 344}, {"begin": 47600, "end": 47790, "idx": 345}, {"begin": 47791, "end": 47873, "idx": 346}, {"begin": 47874, "end": 47907, "idx": 347}, {"begin": 47908, "end": 48021, "idx": 348}, {"begin": 48022, "end": 48144, "idx": 349}, {"begin": 48145, "end": 48228, "idx": 350}, {"begin": 48229, "end": 48408, "idx": 351}, {"begin": 48409, "end": 48495, "idx": 352}, {"begin": 48496, "end": 48610, "idx": 353}, {"begin": 48611, "end": 48731, "idx": 354}, {"begin": 48732, "end": 48846, "idx": 355}, {"begin": 48847, "end": 48934, "idx": 356}, {"begin": 48935, "end": 48983, "idx": 357}, {"begin": 48984, "end": 49115, "idx": 358}, {"begin": 49116, "end": 49241, "idx": 359}, {"begin": 49242, "end": 49344, "idx": 360}, {"begin": 49345, "end": 49438, "idx": 361}, {"begin": 49470, "end": 49486, "idx": 362}, {"begin": 49487, "end": 49616, "idx": 363}, {"begin": 49617, "end": 49674, "idx": 364}, {"begin": 49675, "end": 49696, "idx": 365}, {"begin": 49697, "end": 49787, "idx": 366}, {"begin": 49788, "end": 49834, "idx": 367}, {"begin": 49835, "end": 49875, "idx": 368}, {"begin": 49919, "end": 49975, "idx": 369}, {"begin": 49976, "end": 50058, "idx": 370}, {"begin": 50059, "end": 50137, "idx": 371}, {"begin": 50177, "end": 50304, "idx": 372}, {"begin": 50305, "end": 50419, "idx": 373}, {"begin": 50420, "end": 50484, "idx": 374}, {"begin": 50485, "end": 50611, "idx": 375}, {"begin": 50612, "end": 50655, "idx": 376}, {"begin": 50656, "end": 50805, "idx": 377}, {"begin": 50806, "end": 50886, "idx": 378}, {"begin": 50887, "end": 50989, "idx": 379}, {"begin": 51008, "end": 51038, "idx": 380}, {"begin": 51039, "end": 51133, "idx": 381}, {"begin": 51134, "end": 51261, "idx": 382}, {"begin": 51262, "end": 51360, "idx": 383}, {"begin": 51361, "end": 51437, "idx": 384}, {"begin": 51438, "end": 51496, "idx": 385}, {"begin": 51517, "end": 51605, "idx": 386}, {"begin": 51606, "end": 51725, "idx": 387}, {"begin": 51726, "end": 51831, "idx": 388}], "ReferenceToFigure": [{"begin": 2300, "end": 2301, "idx": 0}, {"begin": 3031, "end": 3032, "idx": 1}, {"begin": 12365, "end": 12366, "idx": 2}], "Div": [{"begin": 96, "end": 1273, "idx": 0}, {"begin": 1276, "end": 5306, "idx": 1}, {"begin": 5308, "end": 8000, "idx": 2}, {"begin": 8002, "end": 9020, "idx": 3}, {"begin": 9022, "end": 10397, "idx": 4}, {"begin": 10399, "end": 11534, "idx": 5}, {"begin": 11536, "end": 13220, "idx": 6}, {"begin": 13222, "end": 13996, "idx": 7}, {"begin": 13998, "end": 15246, "idx": 8}, {"begin": 15248, "end": 15262, "idx": 9}, {"begin": 15264, "end": 16351, "idx": 10}, {"begin": 16353, "end": 18380, "idx": 11}, {"begin": 18382, "end": 19341, "idx": 12}, {"begin": 19343, "end": 24283, "idx": 13}, {"begin": 24285, "end": 24668, "idx": 14}, {"begin": 24670, "end": 26505, "idx": 15}, {"begin": 26507, "end": 28432, "idx": 16}, {"begin": 28434, "end": 29105, "idx": 17}, {"begin": 29107, "end": 30023, "idx": 18}, {"begin": 30025, "end": 33280, "idx": 19}, {"begin": 33282, "end": 33710, "idx": 20}, {"begin": 33712, "end": 34001, "idx": 21}, {"begin": 34003, "end": 35991, "idx": 22}, {"begin": 35993, "end": 42275, "idx": 23}, {"begin": 42277, "end": 43558, "idx": 24}, {"begin": 43560, "end": 47226, "idx": 25}, {"begin": 47228, "end": 49438, "idx": 26}, {"begin": 49440, "end": 50137, "idx": 27}, {"begin": 50139, "end": 50989, "idx": 28}, {"begin": 50991, "end": 51831, "idx": 29}], "SectionMain": [{"begin": 1273, "end": 51831, "idx": 0}]}}