{"text": "PLAD: Learning to Infer Shape Programs with Pseudo-Labels and Approximate Distributions\n\nAbstract:\nInferring programs which generate 2D and 3D shapes is important for reverse engineering, editing, and more. Training models to perform this task is complicated because paired (shape, program) data is not readily available for many domains, making exact supervised learning infeasible. However, it is possible to get paired data by compromising the accuracy of either the assigned program labels or the shape distribution. Wake-sleep methods use samples from a generative model of shape programs to approximate the distribution of real shapes. In self-training, shapes are passed through a recognition model, which predicts programs that are treated as 'pseudo-labels' for those shapes. Related to these approaches, we introduce a novel self-training variant unique to program inference, where program pseudo-labels are paired with their executed output shapes, avoiding label mismatch at the cost of an approximate shape distribution. We propose to group these regimes under a single conceptual framework, where training is performed with maximum likelihood updates sourced from either Pseudo-Labels or an Approximate Distribution (PLAD). We evaluate these techniques on multiple 2D and 3D shape program inference domains. Compared with policy gradient reinforcement learning, we show that PLAD techniques infer more accurate shape programs and converge significantly faster. Finally, we propose to combine updates from different PLAD methods within the training of a single model, and find that this approach outperforms any individual technique.\n\nMain:\n\n\n\n1. Introduction\nHaving access to a procedure which generates a visual datum reveals its underlying structure, facilitating high-level manipulation and editing by a person or autonomous agent. Thus, inferring such programs from visual data is an important problem. In R 2 , inferring shape programs has applications in the design of diagrams, icons, and other 2D graphics. In R 3 , it has applications in reverse engineering of CAD models, procedural modeling for 3D games, and 3D structure understanding for autonomous agents.\nWe formally define shape program inference as obtaining a latent program z which generates a given observed shape x. We model p(z|x) with deep neural networks that train over a distribution of real shapes in order to amortize the cost of shape program inference on unseen shapes (e.g. a test set). This is a challenging problem: it is a structured prediction problem whose output is high-dimensional and can feature both discrete and continuous components (i.e. program control flow vs. program parameters). Nevertheless, learning p(z|x) becomes tractable provided that one has access to paired (X, Z) data (i.e. a dataset of shapes and the programs which generate them) [34]. Unfortunately, while shape data is increasingly available in large quantities [2], these shapes do not typically come with their generating program.\nTo circumvent this data problem, researchers have typically synthesized paired data by generating synthetic programs and pairing them with the shapes they output [25, 31]. However, as there is typically significant distributional mismatch between these synthetic shapes and \"real\" shapes from the distribution of interest, S * , various techniques must be employed to fine-tune p(z|x) models towards S * .\nA number of these fine-tuning strategies attempt to directly propagate gradients from geometric similarity measures back to p(z|x). When the program executor is not a black-box, it may be possible to do this by implementing a differentiable relaxation of its behavior [16], but this requires knowledge of its functional form. One can also try learning a differentiable proxy of the executor's behavior [31], but this approximation introduces errors. Moreover, as shape programs typically involve many discrete structural decisions, training such a model end-to-end is usually infeasible in many domains. Thus, many prior works often resort to general-purpose policy gradient reinforcement learning [8, 25], which treats the program executor as a (nondifferentiable) black-box. The downside of this strategy is that RL is notoriously unstable and slow to converge.\nIn this paper, we study a collection of methods that create (shape, program) data pairs used to train p(z|x) models with maximum likelihood estimation (MLE) updates while treating the program executor as a black-box. As discussed, ground-truth (shape, program) pairs are often unavailable, so these techniques must make compromises in how they formulate paired data. In wake-sleep, a generative model p(z) is trained to convergence on alternating cycles with respect to p(z|x). When training p(z|x), paired data can be created by sampling from p(z). Each program label z is valid with respect to its associated x shape, but there is often a distributional mismatch between the generated set of shapes, X, and shapes from the target distribution, S * . In self-training, one uses p(z|x) to infer latent z's for unlabeled input x's; these z's then become \"pseudo-labels\" which are treated as ground truth for another round of supervised training. In this paradigm, there is no distributional shift between X and S * , but each z is only an approximately correctly label with respect to its paired x. We observe that shape program inference has a unique property that makes it especially well-suited for self-training: the distribution p(x|z) is known a priori-this is a delta distribution defined by the program executor. When using a model p(z|x) to infer a program z from some shape x * of interest, one can use this executor to produce a shape x that is consistent with the program z: in the terminology of self-training, z is guaranteed to be the \"correct label\" for x. However, similar to wake-sleep, formulating X as shape executions produced by model inferred programs can cause a distributional shift between X and S * . Since this variant of self-training involves executing the inferred latent program z, we call this procedure latent execution self-training (LEST).\nAs all of the aforementioned fine-tuning regimes use either Pseudo-Labels or Approximate Distributions to formulate (shape, program) pairs, we group them under a single conceptual framework: PLAD. We evaluate PLAD methods experimentally, using them to fine-tune shape program inference models in multiple shape domains: 2D and 3D constructive solid geometry (CSG), and assembly-based modeling with ShapeAssembly, a domain-specific language for structures of manufactured 3D objects [14]. We find that PLAD training regimes offer substantial advantages over the de-facto approach of policy gradient reinforcement learning, achieving better shape reconstruction performance while requiring significantly less computation time. Further, we explore combining training updates from a mixture of PLAD methods, and find that this approach leads to better performance compared with any individual method. Code for our method and experiments can be found at found at https://github.com/rkjones4/PLAD .\nIn summary, our contributions are: 1. Proposing the PLAD conceptual framework to group a family of related self-supervised learning techniques for shape program inference.\n\n2. Related Work\nProgram synthesis is a broad field that has employed many techniques throughout its history. A program synthesizer takes as input a domain-specific language (DSL) and a specification; it outputs a program in the DSL that meets the specification. Machine learning has been used to improve performance on program synthesis tasks by e.g. performing a neurally-guided search over all possible programs or letting a recurrent network predict program text directly [1, 5, 7, 23, 30, 35].\nIn this work, we are interested in the sub-problem of shape program inference, which is a type of visual program induction problem [4]. In our case, the specification is a visual representation of an object which the inferred program's output must geometrically match. Here, we discuss prior work that has attacked this problem, organized by methodology used to learn p(z|x); see Table 1 for an overview. As discussed, a common practice in this prior work is to start with a model that has been pretrained on synthetically generated (shape, program) pairs with supervised learning, and then perform fine-tuning towards a distribution of interest [8, 9, 20, 25, 31].\nPolicy Gradient Reinforcement Learning The most general method for fine-tuning a pretrained p(z|x) is reinforcement learning: treating p(z|x) as a policy network and using policy gradient methods [33]. The geometric similarity of the inferred program's output to its input is the reward function; the program executor p(x|z) can be treated as a (non-differentiable) black-box. CSG-Net uses RL for finetuning [25, 26], as does other recent work on inferring CSG programs from input geometry [8]. While CSG-Net has been improved to allow it to converge without supervised pretraining [38], not starting from the supervised model results in worse performance. The main problem with policy gradient RL is its instability due to high variance gradients, leading to slow convergence. Like RL, PLAD methods treat the program executor as a black-box, but as we show experimentally, they converge faster and achieve better reconstruction performance.\nDifferentiable Executor If the functional form of the program executor p(x|z) is known and differentiable, then the gradient of the reward with respect to the parameters of p(z|x) can be computed, making policy gradient unnecessary. Shape programs are typically not fully differentiable, as they often involve discrete choices (e.g. which type of primitives to create). UCSGNet uses a differentiable relaxation to circumvent this issue [16]. Other work trains a differentiable network to approximate the behavior of the program executor [31], which introduces errors. PLAD regimes do not require the program executor to be differentiable, yet they perform better than other approaches (e.g. policy gradient RL) that share this desirable property.\nGenerative Model Learning Shape program inference has also been explored in the context of learning a generative model p(x, z) of programs and the shapes they produce. The most popular approach for training such models is variational Bayes, in particular the variational autoencoder [18]. This method simultaneously trains a generative model p(x, z) and a recognition model p(z|x) by optimizing a lower bound on the marginal likelihood p(x). When the z's are shape programs, the program executor is p(x|z), so learning the generative model reduces to learning a prior over programs p(z). Training such models with gradient descent requires that the executor p(x|z) be differentiable. When this is not possible, the wake-sleep algorithm is a viable alternative [13]. This approach alternates training steps of the generative and recognition models, training one on samples produced by the other. Recent work has used wake-sleep for visual program induction [10, 12]. If one trains the generative model and the inference model to convergence before switching to training the other, this is equivalent to expectation maximization (viewed as alternating maximization [22]).\n\nSelf-Training\nTraditionally, self-training has been employed in weakly-supervised learning paradigms to increase the predictive accuracy of simple classification models [21, 24, 36]. Recently, renewed interest in self-traininginspired data augmentation approaches have demonstrated empirical performance improvements for neural models in domains such as large-scale image classification, machine translation, and speech recognition [11, 15, 39]. But while self-training has been shown to yield practical gains for some domains, for others it can actually lead to worse perfor-mance, as training on too many incorrect pseudo-labels can cause learning to degrade [3, 28]. For self-training within the PLAD framework, the assigned pseudo-label for each example changes during fine-tuning whenever the inference model discovers a program that better explains the input shape; similar techniques have been proposed for learning programs that perform semantic parsing under the view of iterative maximum likelihood [19]. To our knowledge, selftraining has not been applied for fine-tuning visual program inference models, likely because it is somewhat unintuitive to view a program as a \"label\" for a visual datum.\n\n3. Method\nIn this section, we describe the PLAD framework: a conceptual grouping of fine-tuning methods for shape program inference models. Our formulation assumes three inputs: a training dataset of shapes from the distribution of interest, S * , a program inference model, p(z|x), and a program executor that converts programs into shapes, E. Throughout this paper, we assume that the p(z|x) passed as input has undergone supervised pretraining on a distribution of synthetically generated shapes. Methods within the PLAD framework return a fine-tuned p(z|x) specialized to the distribution of interest from which S * was sampled.\nWe depict the PLAD procedure both algorithimcally and pictorially in Figure 1. To fine-tune p(z|x) towards S * , PLAD methods iterate through the following steps: (1) use p(z|x) to find visually similar programs to S * , (2) construct a dataset of shape and program pairs (X, Z) using the inferred programs, and (3) fine-tune p(z|x) with maximum likelihood estimation updates on batches from (X, Z). Through successive iterations, these steps bootstrap one another, forming a virtuous cycle: improvements to p(z|x) create (X, Z) pairs that more closely match the statistics of S * , and training on better (X, Z) pairs specializes p(z|x) to S * .\nMethods that fall within the PLAD framework differ in how the paired (X, Z) data is created within each round. We detail this process for wake-sleep (Section 3.  (Section 3.2), and latent execution self-training (Section 3.3). In Section 3.4 we explain our program inference procedure (inferProgs, Fig 1). Finally, in Section 3.5 we discuss how a single p(z|x) can be fine-tuned by multiple PLAD methods.Z \u2190 P BEST X \u2190 S * else if LEST then Z \u2190 P BEST X \u2190 {E(z) | z \u2208 Z} else if Wake-Sleep then p(z) \u2190 trainGenerative(P BEST ) Z \u2190 sample(p(z), |S * |) X \u2190 {E(z) | z \u2208 Z} end if // Train inference model p(z|x) \u2190 trainMLE(X,\n\n3.1. Wake-Sleep (X, Z) Construction\nWake-sleep uses a generative model, p(z) to construct (X, Z). In our implementation, we choose to model p(z) as a variational auto-encoder (VAE) [18], where the encoder consumes visual data and the decoder outputs a program. To create data for p(z), we take the current best programs discovered for S * , P BEST , and execute each program to form a set of shapes X G . p(z) is then trained on pairs from (X G , P BEST ) in the typical VAE framework. Note that the design space for p(z) is quite flexible, for instance, p(z) can trained without access to X G if implemented with a program encoder.\nOnce p(z) has converged, we use it to sample |S * | programs by decoding normally distributed random vectors. This set of programs becomes Z, and X is formed by executing each program in Z. In this set of (X, Z) programs, Z is always the correct label for X, so the gradient estimates during p(z|x) training will be low-variance and unbiased. However, X is not guaranteed to be close to S * , it is only an approximate distribution. Note though, that as P BEST better approximates S * , the distributional mismatch should become smaller, as long as the generative model has enough capacity to properly model p(z).\n\n3.2. Self-Training (X, Z) Construction\nSelf-training constructs (X, Z) by assigning labels from the current best program set, P BEST , to shape instances from S * . Formally, X \u2190 S * and Z \u2190 P BEST . This framing maintains the nice property that X = S * , so there will never be distributional mismatch between these two sets. The downside is that unless programs from P BEST exactly recreate their paired shapes from S * when executed, we know that the pseudo-labels from P BEST are 'incorrect'. From this perspective, we can consider gradient estimates that come from such (X, Z) pairs to be biased. However, as we will show experimentally, when X forms a good approximation to S * , sourcing gradient estimates from these pseudo-labels leads to strong reconstruction performance.\n\n3.3. LEST (X, Z) Construction\nA unique property of shape program inference is that the distribution p(x|z) is readily available in the form of the program executor E. We leverage this property to propose LEST, a variant of self-training that does not create mismatch between pseudo-labels and their associated visual data. Similar to the self-training paradigm, LEST first constructs Z as the current best program set, P BEST . Then, differing from self-training, LEST constructs X as the executed version of each program in Z. By construction, the labels in Z will now be correct for their paired shapes in X. The downside is that, like wake-sleep, LEST may introduce a distributional mismatch between X and S * . But once again, as P BEST better approximates S * , the mismatch between the two distributions will decrease.\n\n3.4. Inferring Programs with p(z|x)\nDuring each round of fine-tuning, PLAD methods rely on p(z|x) to infer programs that approximate S * . We propose to train PLAD methods such that the best matching inferred programs for S * are maintained across rounds. Specifically, we construct a data structure P BEST that maintains a program for each training shape in S * . In this way, as more iterations are run, P BEST always forms a closer approximation to S * . There is an alternative framing, where P BEST is reset each epoch, but we show experimental results in the supplemental material that this can lead to worse generalization.\nTo update P BEST each round, we employ an inner-loop search procedure. For each shape in S * , p(z|x) suggests high-likelihood programs, and the P BEST entry is updated to keep the program whose execution obtains the highest similarity to the input shape; the specific similarity metric varies by domain. While there are many ways to structure this inner-loop search, we choose beam-search, as we find it offers a good trade-off between speed and performance. Experimentally, we demonstrate that PLAD methods are capable of performing well even as the time spent on innerloop search is varied (Section 4.4).\n\n3.5. Training p(z|x) with multiple PLAD methods\nAs detailed in the preceding sections, the main difference between PLAD approaches is in how they construct the (X, Z) dataset used for fine-tuning p(z|x). However, there is no strict requirement that these different (X, Z) distributions be kept separate. We explore how p(z|x) behaves under finetuning from multiple PLAD methods, such as combining LEST and self-training. We implement these mixtures on a per-batch basis. Before p(z|x) training, we construct distinct (X, Z) distributions for each method in the combination. Then, during training, each batch is randomly sampled from one of the (X, Z) distributions. We experimentally validate the effectiveness of this approach in the next section.\n\n4. Results\nWe evaluate a series of methods on their ability to finetune shape program inference models across multiple domains. We describe the different domains in Section 4.1 and details of our experimental design in Section 4.2. In Section 4.3, we compare the reconstruction accuracy of each method, and study how they are affected by varying the time spent on inner-loop search (Section 4.4) and the size of the training set (Section 4.5). Finally, we explore the convergence speed of each method in Section 4.6.\n\n4.1. Shape Program Domains\nWe run experiments across three shape program domains: 2D Constructive Solid Geometry (CSG), 3D CSG, and Sha-peAssembly. Details can be found in the supplemental.\nIn CSG, shapes are created by declaring parametric primitives (e.g. circles, boxes) and combining them with Boolean operations (union, intersection, difference). CSG inference is non-trivial: as CSG uses non-additive operations (intersection, difference), inferring a CSG program does not simply reduce to primitive detection. For 2D CSG, we follow the grammar defined by CSGNet [25], using 400 shape tokens that correspond to randomly placed circles, triangles and rectangles on a 64 x 64 grid. For 3D CSG, we use a grammar that has individual tokens for defining primitives (ellipsoids and cuboids), setting primitive attributes (position and scales), and the three Boolean operators. Attributes are discretized into 32 bins.\nShapeAssembly is designed for specifying the part structure of manufactured 3D objects. It creates objects by declaring cuboid part geometries and assembling those parts together via attachment and symmetry operators. Our grammar contains tokens for each command type and parameter value; to handle continuous values, we discretize them into 32 bins.\n\n4.2. Experimental Design\nFine-Tuning Methods We compare the ability of the following training schemes to fine-tune a model on a specific domain of interest:\n\u2022 SP: p(z|x) with supervised pretraining.\n\u2022 RL: Fine-tuning with REINFORCE.\n\u2022 WS: Fine-tuning with wake-sleep.\n\u2022 ST: Fine-tuning with self-training.\n\u2022 LEST: Fine-tuning with latent execution self-training.\n\u2022 LEST+ST: combining LEST and ST.\n\u2022 LEST+ST+WS: combining LEST, ST and WS.\nShape Datasets Fine-tuning methods learn to specialize p(z|x) against a distribution of real shapes S * . For each domain, we construct a dataset of shapes S * , and split it into train, validation, and test sets. We perform early-stopping with respect to the validation set. For 2DCSG, we use the CAD dataset from CSGNet [25], which consists of front and side views of chairs, desks, and lamps from the Trimble 3D warehouse. We split the dataset into 10K shapes for training, 3K shapes for validation, and 3K shapes for testing. For 3D CSG and ShapeAssembly, we use CAD shapes from the Model Architectures For all experiments, we model p(z|x) in an encoder-decoder framework, although the particular architectures vary by domain. In all cases, the encoder is a CNN that converts visual data into a latent variable, and the decoder is an auto-regressive model that decodes the latent variables into a sequence of tokens. For 2D CSG, we use the same p(z|x) architecture as CSGNet. A CNN consumes 64 \u00d7 64 binary mask shape images to produce a latent code that initializes a GRU-based recurrent decoder. For 3D CSG and ShapeAssembly, we use a 3D CNN that consumes 32\u00d732\u00d732 occupancy voxels. This CNN outputs a latent code that is attended to by a Transformer decoder network [32]; this network also attends over token sequences in a typical auto-regressive fashion.\nSupervised Pretraining Before fine-tuning, p(z|x) undergoes supervised pretraining on synthetically generated programs until it has converged on that set. For 2D CSG, we follow CSGNet's approach. For 3D CSG, we construct valid programs by (i) sampling a set of primitives within the allotted grid (ii) identifying potential overlaps (iii) constructing a binary tree of boolean operations using these overlaps. For ShapeAssembly, we propose programs by sampling random grammar expansions according to the language's typing system. We then employ a validation step where a program is rejected if any of its part are not the sole occupying part of at least 8 voxels (to discourage excessive part overlaps). For 3D CSG and ShapeAssembly we sample 2 million synthetic programs and train until convergence on a validation set of 1000 programs. Full details provided in the supplemental.\n\n4.3. Reconstruction Accuracy\nWe evaluate the performance of each fine-tuning method according to reconstruction accuracy: how closely the output of a shape program matches the input shape from which it was inferred, on a held out set of test shapes. The specific metric varies by domain. For 2D CSG, we follow CSGNet and use Chamfer Distance (CD), where lower distances indicate more similar shapes. For 3D CSG and ShapeAssembly, we use volumetric intersection over union (IoU).\nFor each domain, we run each fine-tuning method to convergence, starting with the same p(z|x) model that has undergone supervised pretraining. The reward for RL models follows the similarity metric in each domain: CD for 2D CSG; IoU for 3D CSG and ShapeAssembly. For PLAD fine-tuning methods, the similiarity metric in each domain determines which program is kept during updates to P BEST . At inference time, when evaluating the reconstruction performance of each p(z|x), we employ a beam search procedure, decoding multiple programs in parallel, and choosing the program that achieves the highest similarity to the target shape. We use a beam size of 10, unless otherwise stated.\nWe present quantitative results in Table 2. Looking at the middle four rows, the two self-training variants (ST and LEST) outperform RL as a fine-tuning method in all the domains we studied. The wake-sleep variant (WS) also outperforms RL for both 3D CSG and ShapeAssembly. These are more challenging domains with larger token spaces, posing difficulties for policy gradient fine-tuning. As demonstrated by the last two rows, further improvement can be had by combining multiple methods: for each domain, the best performance is achieved by LEST+ST+WS. In fact, for 2DCSG, the test-set reconstruction accuracy achieved by LEST+ST+WS (0.811) outperforms previous state-of-the-art results, CSGNet (1.14) [25] and CSGNetStack (1.02) [26], for paradigms where the executor is treated as a black-box.\nMixing updates from multiple PLAD methods is beneficial because, in this joint paradigm, each method can cover the other's weaknesses. For instance, employing ST ensures that some samples of X are sourced from S * , and employing LEST ensures that some samples of X have paired Z programs which are exact labels. We present qualitative results in Figure 3. The reconstructions from the PLAD combina- tion methods better reflect the input shapes, reinforcing the quantitative trends.\n\n4.4. Inner-loop Search Time\nPLAD methods make use of P BEST to generate (X, Z) datasets that train p(z|x). To study how time spent on innerloop search affects each technique, we ran an experiment using different beam sizes to update P BEST . We present results in Figure 2, left. On the X-axis we plot beam size; on the Y-axis we plot test-set reconstruction Chamfer distance. Unsurprisingly, spending more time on the inner-loop search leads to better performance; finding better programs for training shapes improves test time generalization. That said, across all beam sizes, we find that it is always best to train under a combination of PLAD methods; the LEST+ST and the LEST+ST+WS variants always outperform any individual fine-tuning scheme. Note, RL is not included in this experiment because REINFORCE, as defined, has no inner-loop search mechanism. In this way, PLAD provides an additional control lever, where time spent on inner-loop search modulates a trade-off between convergence speed and test-set reconstruction performance.\n\n4.5. Number of Training Shapes from S *\nAll the fine-tuning methods make use of a training distribution of shapes that are sampled from S * . For some domains, the size of available samples from S * may be limited. We run an experiment on 2D CSG to see how different fine-tuning methods are affected by training data size. We present the results of this experiment in Figure 2, middle. We plot the number of training shapes on the X-axis and test set reconstruction accuracy on the Y-axis. All fine-tuning methods improve as the training size of S * increases, but once again, combining multiple PLAD methods leads to the best performance in all regimes. This study also demonstrates the sample efficiency of PLAD combinations: LEST+ST and LEST+ST+WS trained on 1,000 shapes achieve better test set generalization than RL trained on 10,000 shapes.\n\n4.6. Convergence Speed\nBeyond reconstruction accuracy, we are also interested in the convergence properties of a fine-tuning method. Policy gradient RL is notoriously unstable and slow to converge, which is undesirable. For 2D CSG, we record the convergence speed of each method and present these results in Figure 2, right. We plot reconstruction accuracy (Y-axis) as a function of training wall-clock time (X-axis); all timing information was collected on a machine with a GeForce RTX 2080 Ti GPU and an Intel i9-9900K CPU. All PLAD techniques converge faster than policy gradient RL. For instance, RL took 36 hours to reach its converged test-set CD of 1.097, while LEST matched this performance at 1.1 hours (32x faster) and LEST+ST matched this performance at 0.85 hours (42x faster).\n\n5. Conclusion\nWe presented the PLAD framework to group a family of techniques for fine-tuning shape program inference models with Pseudo-Labels and Approximate Distributions. Within this framework, we proposed LEST: a self-training variant that creates a shape distribution X approximating the real distribution S * by executing inferred latent programs. Experiments on 2D CSG, 3D CSG, and ShapeAssembly demonstrate that PLAD methods achieve better reconstruction accuracy and converge faster than policy gradient RL, the current standard approach for black-box fine-tuning. Finally, we found that combining updates from multiple PLAD methods outperforms any individual technique.\nWhile fine-tuning p(z|x), PLAD methods construct (X, Z) sets approximating the statistics of S * , specializing p(z|x) towards S * . As a consequence, p(z|x) may not generalize as well to shapes outside of S * ; we explore this phenomenon in the supplemental material. Training a generalpurpose inference model for all shapes expressible under the grammar is an interesting line of future work.\nWhile our work focuses on reconstruction quality, producing programs with 'good' structure matters just as much, if the program is to be used for editing tasks. Currently, the synthetic pretraining data is the only place where knowledge about what constitutes \"good program structure\" can be injected. Such knowledge must be expressed in procedural form, which may be harder to elicit from domain experts than declarative knowledge (i.e. \"a good program has these properties\" vs. \"this is how you write a good program\"). Finding efficient ways to elicit and inject such knowledge is an important future direction.\nFinally, we believe that ideas from the PLAD framework are applicable to a broader class of program inference problems than those we evaluated in this paper. In principle, these approaches can be used to train an approximate inference model p(z|x) for any domain in which (1) an executor p(x|z) is available and (2) the executed output x takes the form of some concrete artifact which can be encoded via neural network (image, audio, text, etc.). For example, one could imagine using PLAD techniques to infer graphics shader programs which produce certain textures or audio synthesizers which sound like certain real-world instruments. developing 3D technology to build immersive virtual copies of the real world with applications in various fields, including games and architecture\n\nA. Details of Domain Grammars\n2D CSG We follow the grammar from CSGNet [25]. This grammar contains 3 Boolean operations (intersect, union, subtract), 3 primitive types (square, circle, triangle), and parameters to initialize each primitive (L and R tuples). Please refer to the CSGNet paper for details.S \u2192 E; E \u2192 EET | P (L, R); T \u2192 intersect | union | subtract; P \u2192 square | circle | triangle;\nL \u2192 8 : 8 : 56\n2 ; R \u2192 8 : 4 : 32 .\n3D CSG We design our own grammar for 3D CSG similar in spirit to the grammar of CSGNet. While CSGNet does contain a 3D CSG grammar, we find that it overly discretizes the possible spacing and positioning of primitives. Therefore in our grammar, we allow each primitive to be parameterized at the same granularity as the voxel grid (32 bins). In this way, each primitive takes in 6 parameters (instead of 2 parameter tuples), where the 6 parameters control the position and scaling of the primitive.S \u2192 E; E \u2192 EET | P (F, F, F, F, F, F ); T \u2192 intersect | union | subtract; P \u2192 cuboid | ellipsoid; F \u2192 1 : 32\nShapeAssembly ShapeAssembly is a domain-specific language for creating structures of 3D Shapes [14]T ranslate \u2212 \u2192 translate(axis, m, x) f \u2212 \u2192 right | lef t | top | bot | f ront | back axis \u2212 \u2192 X | Y | Z x \u2208 [1, 32]/32. uv \u2208 [1, 10] 2 /10. n \u2208 [0, 10] m \u2208 [1, 4]\n\nB. Details of Synthetic Pretraining\n2DCSG We follow the synthetic pretraining steps from CSGNet and directly use their released pretrained model weights. Please refer to their paper and code for further details.\n3DCSG We generate synthetic programs for 3D CSG with the following procedure. First, we sample K primitives, where K is randomly chosen between 2 and 12. To sample a primitive, we sample a center position within the voxel space, and then we sample a scale, such that the scale is constrained so that the primitive will not extend past the borders of the voxel grid. We then find if the bounding boxes of any two primitives overlap in space (using the position and scale of each primitive). We then construct a binary tree of Boolean operations by randomly merging the K primitives together, until only one group remains.\n\nC. Experiment Hyperparameters\n3D Experiments For 3D CSG and ShapeAssembly, we use the following model hyper-parameters. The encoder for both cases is a 3D CNN that consumes a 32 x 32 x 32 voxel grid. It has four layers of convolution, ReLU, max-pooling, and dropout. Each convolution layer uses kernel size of 4, stride of 1, padding of 2, with channels (32, 64, 128, 256). The output of the CNN is a (2x2x2x256) dimensional vector, which we transform into a (8 x 256) vector. This vector is then sent through a 3-layer MLP with ReLU and dropout to produce a final (8 x 256) vector that acts as an 8-token embedding of the voxel grid.\nThe decoder for both cases is a Transformer Decoder module [32]. It uses 8 layers and 16 heads, with a hidden dimension size of 256. It attends over the 8-token CNN voxel encoding and up to 100 additional sequence tokens, with an auto-regressive attention mask. We use a learned positional embedding for each sequence position. An embedding layer lifts each token into an embedding space, consumed by the transformer, and a 2-layer MLP converts Transformer outputs into a probability distribution over tokens.\nIn all cases we set dropout to 0.1 . We use a learning rate of 0.0005 with the Adam optimizer [17] for all training modes, except for RL, where following CSGNet we use SGD with a learning rate of 0.01 . During supervised pretraining we use a batch size of 400. During PLAD method fine-tuning we use batch size of 100. During RL fine-tuning we use a batch size of 4, due to memory limitations (a batch size of 4 takes up 10GB of GPU memory). Early stopping on the validation set is performed to determine when to end each round and when to stop introducing additional rounds. For deciding when to stop introducing additional rounds, we use a patience of 100 epochs. For deciding when to stop each round, we use a patience of 10 epochs. In both cases we employ a patience threshold of 0.001 IoU improvement (e.g. we must see at least this much improvement to reset the patience). Within each round of PLAD training, we check validation set reconstruction performance with a beam size of 3; between rounds of PLAD training we check validation set reconstruction performance with a beam size of 5; final reconstruction performance of converged models is computed with a beam size of 10.\nFor RL runs, we make a gradient update after every 10 batches, following CSGNet. For runs that involve VAE training (all Wake-Sleep runs), we add an additional module in-between the encoder and the decoder. This module uses an MLP to convert the output of the encoder into a 128 x 2 latent vector (representing 128 means and standard deviations). This module then samples an 128 dimensional vector from a normal distribution described by the means and standard deviations, and further lifts this encoding into the dimension that the decoder expects with a sequence of linear layers. For each round of VAE training, we allow the VAE to update for no more than 100 epochs. We perform early-stopping for VAE training with respect to its loss, where the loss is a combination of reconstruction (cross-entropy on token predictions) and KL divergence, both weighed equally.\n2D Experiments For 2DCSG, we follow the network architecture and hyper-parameters of CSGNet. All training regimes use a dropout of 0.2 and a batch size of 100. PLAD methods use the Adam optimizer with a learning rate of 0.001. For deciding when to stop introducing additional rounds, we use a patience of 1000 epochs. For deciding when to stop each round, we use a patience of 10 epochs.  3. Different ways to update P BEST data structure. In the \"Per round\" row, the data structure is cleared in between rounds. In the \"All-time\" row, the data structure maintains the best program for each input shape across multiple rounds.\nIn both cases we employ a patience threshold of 0.005 CD improvement. The parameters for the RL runs and VAE training are the same as in the 3D Experiments.\n\nD. P Best Update mode\nDuring updates to P BEST , we choose to update each entry in P BEST according to which inferred program has achieved the best reconstruction similarity with respect to the input shape. The entries of this data structure are maintained across rounds. There is another framing where the entries of this data structure are reset each round, so that the best program for each shape is reset each epoch. This is similar to traditional self-training framing.\nWe run experiments on 2D CSG with this variant of P BEST update and present results in Table 3. When the best program is maintained across rounds (All-time, bottom row) each fine-tuning strategy reaches a better converged reconstruction accuracy compared with when the best program is reset after each round (Per round, top row).\n\nE. Failure to generalize beyond S *\nAs demonstrated by our experiments, PLAD fine-tuning methods are able to successfully specialize p(z|x) towards a distribution of interest S * . Unfortunately, this specialization comes at a cost; the fine-tuned p(z|x) may actually generalize worse to out of distribution samples. To demonstrate this, we collected a small dataset of 2D icons from the The Noun Project 1. We tested the shape program inference abilities of the initial p(z|x) trained under supervised pretraining (SP) and of the fine-tuned p(z|x) trained under PLAD regimes (LEST+ST+WS) and specialized to CAD shapes. We show qualitative examples of this experiment in Figure 4. While both methods fail to accurately represent the 2D icons, finetuning p(z|x) on CAD shapes lowers the reconstruction accuracy significantly; the SP variant achieves an average CD of 1.9 while the LEST+ST+WS variant achieves a CD of 4.1 Developing p(z|x) models capable of out-of-domain generalization is an important area of future research.\n\nSP LEST+ST+WS Target\n\n\nF. Potential Societal Impacts\nFine-tuning our deep neural networks p(z|x) requires a relatively large amount of electricity, which can have a significant environmental impact [29]. Reducing the energy consumption of deep learning is an active research area [27, 37]. Notably, PLAD techniques place no restrictions on the inference model, making it easy to adopt more efficient deep learning techniques. Moreover, shape program inference procedures may also allow the reverse engineering of protected intellectual property. Thus, improvements in shape program inference may impact the content and enforcement of copyright law.\n\nG. Additional Qualitative Results\nWe present additional qualitative results comparing various fine-tuning methods in Figure 5\n\nFootnotes:\n1: https://thenounproject.com\nFigure 5. 2DCSG qualitative examples.\nFigure 6. 3DCSG qualitative examples.\nFigure 7. ShapeAssembly qualitative examples.\n\nReferences:\n\n- Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and rein- forcement learning for neural program synthesis. ICLR, 2018. 2- Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repos- itory. arXiv:1512.03012, 2015. 1\n\n- Eugene Charniak. Statistical parsing with a context-free gram- mar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Con- ference on Innovative Applications of Artificial Intelligence, AAAI'97/IAAI'97, page 598-603. AAAI Press, 1997. 3\n\n- Siddhartha Chaudhuri, Daniel Ritchie, Jiajun Wu, Kai Xu, and Hao Zhang. Learning Generative Models of 3D Structures. Computer Graphics Forum, 2020. 2\n\n- Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019. 2\n\n- Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, and Hao Zhang. Bae-net: Branched autoencoder for shape co-segmentation. Proceedings of International Con- ference on Computer Vision (ICCV), 2019. 6\n\n- Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Ro- bustfill: Neural program learning under noisy i/o. In Pro- ceedings of the 34th International Conference on Machine Learning -Volume 70, ICML'17, page 990-998. JMLR.org, 2017. 2\n\n- Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenen- baum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a repl. In Advances in Neural Infor- mation Processing Systems (NeurIPS), 2019. 1, 2\n\n- Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to Infer Graphics Programs from Hand-Drawn Images. In Advances in Neural Information Processing Systems (NeurIPS), 2018. 2\n\n- Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable- Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake- sleep bayesian program learning, 2020. 3\n\n- Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ran- zato. Revisiting self-training for neural sequence generation. In International Conference on Learning Representations, 2020. 3\n\n- Luke B. Hewitt, Tuan Anh Le, and Joshua B. Tenenbaum. Learning to learn generative programs with memoised wake- sleep, 2020. 3\n\n- Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Rad- ford M Neal. The \"wake-sleep\" algorithm for unsupervised neural networks. Science, 268(5214):1158-1161, 1995. 3\n\n- R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), Siggraph Asia 2020, 39(6):Article 234, 2020. 2, 10\n\n- Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7084-7088. IEEE, 2020. 3\n\n- Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz. Ucsg- net -unsupervised discovering of constructive solid geometry tree. In arXiv, 2020. 1, 3\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR 2015, 2015. 12\n\n- Diederik P. Kingma and Max Welling. Auto-Encoding Varia- tional Bayes. In International Conference on Learning Rep- resentations (ICLR), 2014. 3, 4\n\n- Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 23-33, 2017. 3\n\n- Yunchao Liu, Zheng Wu, Daniel Ritchie, William T. Free- man, Joshua B. Tenenbaum, and Jiajun Wu. Learning to Describe Scenes with Programs. In International Conference on Learning Representations (ICLR), 2019. 2\n\n- David McClosky, Eugene Charniak, and Mark Johnson. Ef- fective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Con- ference, pages 152-159, New York City, USA, June 2006. Association for Computational Linguistics. 3\n\n- Radford M. Neal and Geoffrey E. Hinton. A new view of the em algorithm that justifies incremental and other variants. In Learning in Graphical Models, pages 355-368. Kluwer Academic Publishers, 1993. 3\n\n- Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro- symbolic program synthesis. ICLR, 2017. 2\n\n- H. Scudder. Probability of error of some adaptive pattern- recognition machines. IEEE Transactions on Information Theory, 11(3):363-371, 1965. 3\n\n- Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kaloger- akis, and Subhransu Maji. CSGNet: Neural Shape Parser for Constructive Solid Geometry. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2018. 1, 2, 5, 6, 10\n\n- Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kaloger- akis, and Subhransu Maji. Neural shape parsers for construc- tive solid geometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 2, 6\n\n- Ryan Spring and Anshumali Shrivastava. Scalable and sus- tainable deep learning via randomized hashing. In Proceed- ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 445-454, 2017. 13\n\n- Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. Bootstrapping statistical parsers from small datasets. In 10th Conference of the European Chapter of the Association for Computational Linguistics, Bu- dapest, Hungary, Apr. 2003. Association for Computational Linguistics. 3\n\n- Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. 13\n\n- Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. Neural program synthesis from diverse demon- stration videos. In Proceedings of the 35th International Conference on Machine Learning, 2018. 2\n\n- Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to Infer and Execute 3D Shape Programs. In In- ternational Conference on Learning Representations (ICLR), 2019. 1, 2, 3\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Pro- cessing Systems, NIPS'17, page 6000-6010, Red Hook, NY, USA, 2017. Curran Associates Inc. 6, 12\n\n- Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8, 1992. 2\n\n- Karl D. D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G. Lambourne, Armando Solar-Lezama, and Wojciech Matusik. Fusion 360 gallery: A dataset and environment for programmatic cad construction from human design sequences. ACM Transactions on Graphics (TOG), 40(4), 2021. 1\n\n- Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\n- David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, Massachusetts, USA, June 1995. Association for Computational Linguistics. 3\n\n- Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019. 13\n\n- Chenghui Zhou, Chun-Liang Li, and Barnabas Poczos. Unsu- pervised program synthesis for images using tree-structured lstm, 2020. 2\n\n- Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, and Quoc V. Le. Rethinking pre-training and self-training, 2020. 3\n\n", "annotations": {"ReferenceToTable": [{"begin": 8205, "end": 8206, "target": "#tab_1", "idx": 0}, {"begin": 24981, "end": 24982, "target": "#tab_3", "idx": 1}, {"begin": 37099, "end": 37100, "idx": 2}, {"begin": 38063, "end": 38064, "idx": 3}], "ReferenceToFootnote": [{"begin": 38706, "end": 38707, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 1654, "end": 40103, "idx": 0}], "SectionReference": [{"begin": 40269, "end": 48621, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1654, "idx": 0}], "Div": [{"begin": 99, "end": 1646, "idx": 0}, {"begin": 1657, "end": 7319, "idx": 1}, {"begin": 7321, "end": 11343, "idx": 2}, {"begin": 11345, "end": 12553, "idx": 3}, {"begin": 12555, "end": 14458, "idx": 4}, {"begin": 14460, "end": 15706, "idx": 5}, {"begin": 15708, "end": 16490, "idx": 6}, {"begin": 16492, "end": 17316, "idx": 7}, {"begin": 17318, "end": 18556, "idx": 8}, {"begin": 18558, "end": 19306, "idx": 9}, {"begin": 19308, "end": 19824, "idx": 10}, {"begin": 19826, "end": 21094, "idx": 11}, {"begin": 21096, "end": 23777, "idx": 12}, {"begin": 23779, "end": 26218, "idx": 13}, {"begin": 26220, "end": 27262, "idx": 14}, {"begin": 27264, "end": 28111, "idx": 15}, {"begin": 28113, "end": 28902, "idx": 16}, {"begin": 28904, "end": 31376, "idx": 17}, {"begin": 31378, "end": 32678, "idx": 18}, {"begin": 32680, "end": 33512, "idx": 19}, {"begin": 33514, "end": 37493, "idx": 20}, {"begin": 37495, "end": 38299, "idx": 21}, {"begin": 38301, "end": 39326, "idx": 22}, {"begin": 39328, "end": 39349, "idx": 23}, {"begin": 39351, "end": 39976, "idx": 24}, {"begin": 39978, "end": 40103, "idx": 25}], "Head": [{"begin": 1657, "end": 1672, "n": "1.", "idx": 0}, {"begin": 7321, "end": 7336, "n": "2.", "idx": 1}, {"begin": 11345, "end": 11358, "idx": 2}, {"begin": 12555, "end": 12564, "n": "3.", "idx": 3}, {"begin": 14460, "end": 14495, "n": "3.1.", "idx": 4}, {"begin": 15708, "end": 15746, "n": "3.2.", "idx": 5}, {"begin": 16492, "end": 16521, "n": "3.3.", "idx": 6}, {"begin": 17318, "end": 17353, "n": "3.4.", "idx": 7}, {"begin": 18558, "end": 18605, "n": "3.5.", "idx": 8}, {"begin": 19308, "end": 19318, "n": "4.", "idx": 9}, {"begin": 19826, "end": 19852, "n": "4.1.", "idx": 10}, {"begin": 21096, "end": 21120, "n": "4.2.", "idx": 11}, {"begin": 23779, "end": 23807, "n": "4.3.", "idx": 12}, {"begin": 26220, "end": 26247, "n": "4.4.", "idx": 13}, {"begin": 27264, "end": 27303, "n": "4.5.", "idx": 14}, {"begin": 28113, "end": 28135, "n": "4.6.", "idx": 15}, {"begin": 28904, "end": 28917, "n": "5.", "idx": 16}, {"begin": 31378, "end": 31407, "idx": 17}, {"begin": 32680, "end": 32715, "idx": 18}, {"begin": 33514, "end": 33543, "idx": 19}, {"begin": 37495, "end": 37516, "idx": 20}, {"begin": 38301, "end": 38336, "idx": 21}, {"begin": 39328, "end": 39348, "idx": 22}, {"begin": 39351, "end": 39380, "idx": 23}, {"begin": 39978, "end": 40011, "idx": 24}], "Paragraph": [{"begin": 99, "end": 1646, "idx": 0}, {"begin": 1673, "end": 2183, "idx": 1}, {"begin": 2184, "end": 3009, "idx": 2}, {"begin": 3010, "end": 3415, "idx": 3}, {"begin": 3416, "end": 4279, "idx": 4}, {"begin": 4280, "end": 6154, "idx": 5}, {"begin": 6155, "end": 7147, "idx": 6}, {"begin": 7148, "end": 7319, "idx": 7}, {"begin": 7337, "end": 7818, "idx": 8}, {"begin": 7819, "end": 8484, "idx": 9}, {"begin": 8485, "end": 9426, "idx": 10}, {"begin": 9427, "end": 10173, "idx": 11}, {"begin": 10174, "end": 11343, "idx": 12}, {"begin": 11359, "end": 12553, "idx": 13}, {"begin": 12565, "end": 13187, "idx": 14}, {"begin": 13188, "end": 13834, "idx": 15}, {"begin": 13835, "end": 14239, "idx": 16}, {"begin": 14496, "end": 15092, "idx": 17}, {"begin": 15093, "end": 15706, "idx": 18}, {"begin": 15747, "end": 16490, "idx": 19}, {"begin": 16522, "end": 17316, "idx": 20}, {"begin": 17354, "end": 17948, "idx": 21}, {"begin": 17949, "end": 18556, "idx": 22}, {"begin": 18606, "end": 19306, "idx": 23}, {"begin": 19319, "end": 19824, "idx": 24}, {"begin": 19853, "end": 20015, "idx": 25}, {"begin": 20016, "end": 20743, "idx": 26}, {"begin": 20744, "end": 21094, "idx": 27}, {"begin": 21121, "end": 21252, "idx": 28}, {"begin": 21253, "end": 21294, "idx": 29}, {"begin": 21295, "end": 21328, "idx": 30}, {"begin": 21329, "end": 21363, "idx": 31}, {"begin": 21364, "end": 21401, "idx": 32}, {"begin": 21402, "end": 21458, "idx": 33}, {"begin": 21459, "end": 21492, "idx": 34}, {"begin": 21493, "end": 21533, "idx": 35}, {"begin": 21534, "end": 22896, "idx": 36}, {"begin": 22897, "end": 23777, "idx": 37}, {"begin": 23808, "end": 24257, "idx": 38}, {"begin": 24258, "end": 24939, "idx": 39}, {"begin": 24940, "end": 25735, "idx": 40}, {"begin": 25736, "end": 26218, "idx": 41}, {"begin": 26248, "end": 27262, "idx": 42}, {"begin": 27304, "end": 28111, "idx": 43}, {"begin": 28136, "end": 28902, "idx": 44}, {"begin": 28918, "end": 29584, "idx": 45}, {"begin": 29585, "end": 29979, "idx": 46}, {"begin": 29980, "end": 30593, "idx": 47}, {"begin": 30594, "end": 31376, "idx": 48}, {"begin": 31408, "end": 31681, "idx": 49}, {"begin": 31774, "end": 31788, "idx": 50}, {"begin": 31789, "end": 31809, "idx": 51}, {"begin": 31810, "end": 32308, "idx": 52}, {"begin": 32417, "end": 32516, "idx": 53}, {"begin": 32716, "end": 32891, "idx": 54}, {"begin": 32892, "end": 33512, "idx": 55}, {"begin": 33544, "end": 34148, "idx": 56}, {"begin": 34149, "end": 34658, "idx": 57}, {"begin": 34659, "end": 35841, "idx": 58}, {"begin": 35842, "end": 36709, "idx": 59}, {"begin": 36710, "end": 37336, "idx": 60}, {"begin": 37337, "end": 37493, "idx": 61}, {"begin": 37517, "end": 37969, "idx": 62}, {"begin": 37970, "end": 38299, "idx": 63}, {"begin": 38337, "end": 39326, "idx": 64}, {"begin": 39381, "end": 39976, "idx": 65}, {"begin": 40012, "end": 40103, "idx": 66}], "ReferenceToBib": [{"begin": 2855, "end": 2859, "target": "#b33", "idx": 0}, {"begin": 2939, "end": 2942, "target": "#b1", "idx": 1}, {"begin": 3172, "end": 3176, "target": "#b24", "idx": 2}, {"begin": 3177, "end": 3180, "target": "#b30", "idx": 3}, {"begin": 3684, "end": 3688, "target": "#b15", "idx": 4}, {"begin": 3818, "end": 3822, "target": "#b30", "idx": 5}, {"begin": 4114, "end": 4117, "target": "#b7", "idx": 6}, {"begin": 4118, "end": 4121, "target": "#b24", "idx": 7}, {"begin": 6637, "end": 6641, "target": "#b13", "idx": 8}, {"begin": 7796, "end": 7799, "target": "#b0", "idx": 9}, {"begin": 7800, "end": 7802, "target": "#b4", "idx": 10}, {"begin": 7803, "end": 7805, "target": "#b6", "idx": 11}, {"begin": 7806, "end": 7809, "target": "#b22", "idx": 12}, {"begin": 7810, "end": 7813, "target": "#b29", "idx": 13}, {"begin": 7814, "end": 7817, "target": "#b34", "idx": 14}, {"begin": 7950, "end": 7953, "target": "#b3", "idx": 15}, {"begin": 8465, "end": 8468, "target": "#b7", "idx": 16}, {"begin": 8469, "end": 8471, "target": "#b8", "idx": 17}, {"begin": 8472, "end": 8475, "target": "#b19", "idx": 18}, {"begin": 8476, "end": 8479, "target": "#b24", "idx": 19}, {"begin": 8480, "end": 8483, "target": "#b30", "idx": 20}, {"begin": 8681, "end": 8685, "target": "#b32", "idx": 21}, {"begin": 8893, "end": 8897, "target": "#b24", "idx": 22}, {"begin": 8898, "end": 8901, "target": "#b25", "idx": 23}, {"begin": 8975, "end": 8978, "target": "#b7", "idx": 24}, {"begin": 9067, "end": 9071, "target": "#b37", "idx": 25}, {"begin": 9863, "end": 9867, "target": "#b15", "idx": 26}, {"begin": 9964, "end": 9968, "target": "#b30", "idx": 27}, {"begin": 10457, "end": 10461, "target": "#b17", "idx": 28}, {"begin": 10934, "end": 10938, "target": "#b12", "idx": 29}, {"begin": 11130, "end": 11134, "target": "#b9", "idx": 30}, {"begin": 11135, "end": 11138, "target": "#b11", "idx": 31}, {"begin": 11337, "end": 11341, "target": "#b21", "idx": 32}, {"begin": 11514, "end": 11518, "target": "#b20", "idx": 33}, {"begin": 11519, "end": 11522, "target": "#b23", "idx": 34}, {"begin": 11523, "end": 11526, "target": "#b35", "idx": 35}, {"begin": 11777, "end": 11781, "target": "#b10", "idx": 36}, {"begin": 11782, "end": 11785, "target": "#b14", "idx": 37}, {"begin": 11786, "end": 11789, "target": "#b38", "idx": 38}, {"begin": 12006, "end": 12009, "target": "#b2", "idx": 39}, {"begin": 12010, "end": 12013, "target": "#b27", "idx": 40}, {"begin": 12354, "end": 12358, "target": "#b18", "idx": 41}, {"begin": 14133, "end": 14139, "idx": 42}, {"begin": 14641, "end": 14645, "target": "#b17", "idx": 43}, {"begin": 20395, "end": 20399, "target": "#b24", "idx": 44}, {"begin": 21856, "end": 21860, "target": "#b24", "idx": 45}, {"begin": 22806, "end": 22810, "target": "#b31", "idx": 46}, {"begin": 25642, "end": 25646, "target": "#b24", "idx": 47}, {"begin": 25670, "end": 25674, "target": "#b25", "idx": 48}, {"begin": 31449, "end": 31453, "target": "#b24", "idx": 49}, {"begin": 32512, "end": 32516, "target": "#b13", "idx": 50}, {"begin": 33868, "end": 33872, "target": "#b31", "idx": 51}, {"begin": 33873, "end": 33876, "idx": 52}, {"begin": 33877, "end": 33881, "idx": 53}, {"begin": 33882, "end": 33886, "idx": 54}, {"begin": 34208, "end": 34212, "target": "#b31", "idx": 55}, {"begin": 34753, "end": 34757, "target": "#b16", "idx": 56}, {"begin": 39526, "end": 39530, "target": "#b28", "idx": 57}, {"begin": 39608, "end": 39612, "target": "#b26", "idx": 58}, {"begin": 39613, "end": 39616, "target": "#b36", "idx": 59}], "ReferenceString": [{"begin": 40284, "end": 40456, "id": "b0", "idx": 0}, {"begin": 40458, "end": 40716, "id": "b1", "idx": 1}, {"begin": 40720, "end": 41016, "id": "b2", "idx": 2}, {"begin": 41020, "end": 41169, "id": "b3", "idx": 3}, {"begin": 41173, "end": 41319, "id": "b4", "idx": 4}, {"begin": 41323, "end": 41535, "id": "b5", "idx": 5}, {"begin": 41539, "end": 41827, "id": "b6", "idx": 6}, {"begin": 41831, "end": 42054, "id": "b7", "idx": 7}, {"begin": 42058, "end": 42260, "id": "b8", "idx": 8}, {"begin": 42264, "end": 42523, "id": "b9", "idx": 9}, {"begin": 42527, "end": 42712, "id": "b10", "idx": 10}, {"begin": 42716, "end": 42842, "id": "b11", "idx": 11}, {"begin": 42846, "end": 43014, "id": "b12", "idx": 12}, {"begin": 43018, "end": 43302, "id": "b13", "idx": 13}, {"begin": 43306, "end": 43526, "id": "b14", "idx": 14}, {"begin": 43530, "end": 43675, "id": "b15", "idx": 15}, {"begin": 43679, "end": 43778, "id": "b16", "idx": 16}, {"begin": 43782, "end": 43929, "id": "b17", "idx": 17}, {"begin": 43933, "end": 44234, "id": "b18", "idx": 18}, {"begin": 44238, "end": 44449, "id": "b19", "idx": 19}, {"begin": 44453, "end": 44725, "id": "b20", "idx": 20}, {"begin": 44729, "end": 44930, "id": "b21", "idx": 21}, {"begin": 44934, "end": 45083, "id": "b22", "idx": 22}, {"begin": 45087, "end": 45231, "id": "b23", "idx": 23}, {"begin": 45235, "end": 45473, "id": "b24", "idx": 24}, {"begin": 45477, "end": 45692, "id": "b25", "idx": 25}, {"begin": 45696, "end": 45928, "id": "b26", "idx": 26}, {"begin": 45932, "end": 46293, "id": "b27", "idx": 27}, {"begin": 46297, "end": 46448, "id": "b28", "idx": 28}, {"begin": 46452, "end": 46659, "id": "b29", "idx": 29}, {"begin": 46663, "end": 46901, "id": "b30", "idx": 30}, {"begin": 46905, "end": 47234, "id": "b31", "idx": 31}, {"begin": 47238, "end": 47377, "id": "b32", "idx": 32}, {"begin": 47381, "end": 47666, "id": "b33", "idx": 33}, {"begin": 47670, "end": 47825, "id": "b34", "idx": 34}, {"begin": 47829, "end": 48087, "id": "b35", "idx": 35}, {"begin": 48091, "end": 48336, "id": "b36", "idx": 36}, {"begin": 48340, "end": 48470, "id": "b37", "idx": 37}, {"begin": 48474, "end": 48619, "id": "b38", "idx": 38}], "Sentence": [{"begin": 99, "end": 206, "idx": 0}, {"begin": 207, "end": 383, "idx": 1}, {"begin": 384, "end": 520, "idx": 2}, {"begin": 521, "end": 641, "idx": 3}, {"begin": 642, "end": 784, "idx": 4}, {"begin": 785, "end": 1033, "idx": 5}, {"begin": 1034, "end": 1237, "idx": 6}, {"begin": 1238, "end": 1321, "idx": 7}, {"begin": 1322, "end": 1474, "idx": 8}, {"begin": 1475, "end": 1646, "idx": 9}, {"begin": 1673, "end": 1848, "idx": 10}, {"begin": 1849, "end": 1920, "idx": 11}, {"begin": 1921, "end": 2028, "idx": 12}, {"begin": 2029, "end": 2183, "idx": 13}, {"begin": 2184, "end": 2300, "idx": 14}, {"begin": 2301, "end": 2481, "idx": 15}, {"begin": 2482, "end": 2691, "idx": 16}, {"begin": 2692, "end": 2860, "idx": 17}, {"begin": 2861, "end": 3009, "idx": 18}, {"begin": 3010, "end": 3181, "idx": 19}, {"begin": 3182, "end": 3415, "idx": 20}, {"begin": 3416, "end": 3547, "idx": 21}, {"begin": 3548, "end": 3741, "idx": 22}, {"begin": 3742, "end": 3865, "idx": 23}, {"begin": 3866, "end": 4019, "idx": 24}, {"begin": 4020, "end": 4192, "idx": 25}, {"begin": 4193, "end": 4279, "idx": 26}, {"begin": 4280, "end": 4496, "idx": 27}, {"begin": 4497, "end": 4646, "idx": 28}, {"begin": 4647, "end": 4757, "idx": 29}, {"begin": 4758, "end": 4829, "idx": 30}, {"begin": 4830, "end": 5031, "idx": 31}, {"begin": 5032, "end": 5224, "idx": 32}, {"begin": 5225, "end": 5377, "idx": 33}, {"begin": 5378, "end": 5599, "idx": 34}, {"begin": 5600, "end": 5851, "idx": 35}, {"begin": 5852, "end": 6006, "idx": 36}, {"begin": 6007, "end": 6154, "idx": 37}, {"begin": 6155, "end": 6351, "idx": 38}, {"begin": 6352, "end": 6642, "idx": 39}, {"begin": 6643, "end": 6879, "idx": 40}, {"begin": 6880, "end": 7051, "idx": 41}, {"begin": 7052, "end": 7147, "idx": 42}, {"begin": 7148, "end": 7319, "idx": 43}, {"begin": 7337, "end": 7429, "idx": 44}, {"begin": 7430, "end": 7582, "idx": 45}, {"begin": 7583, "end": 7671, "idx": 46}, {"begin": 7672, "end": 7818, "idx": 47}, {"begin": 7819, "end": 7954, "idx": 48}, {"begin": 7955, "end": 8087, "idx": 49}, {"begin": 8088, "end": 8223, "idx": 50}, {"begin": 8224, "end": 8484, "idx": 51}, {"begin": 8485, "end": 8686, "idx": 52}, {"begin": 8687, "end": 8861, "idx": 53}, {"begin": 8862, "end": 8979, "idx": 54}, {"begin": 8980, "end": 9141, "idx": 55}, {"begin": 9142, "end": 9262, "idx": 56}, {"begin": 9263, "end": 9426, "idx": 57}, {"begin": 9427, "end": 9659, "idx": 58}, {"begin": 9660, "end": 9796, "idx": 59}, {"begin": 9797, "end": 9868, "idx": 60}, {"begin": 9869, "end": 9994, "idx": 61}, {"begin": 9995, "end": 10173, "idx": 62}, {"begin": 10174, "end": 10341, "idx": 63}, {"begin": 10342, "end": 10462, "idx": 64}, {"begin": 10463, "end": 10615, "idx": 65}, {"begin": 10616, "end": 10761, "idx": 66}, {"begin": 10762, "end": 10857, "idx": 67}, {"begin": 10858, "end": 10939, "idx": 68}, {"begin": 10940, "end": 11068, "idx": 69}, {"begin": 11069, "end": 11139, "idx": 70}, {"begin": 11140, "end": 11343, "idx": 71}, {"begin": 11359, "end": 11527, "idx": 72}, {"begin": 11528, "end": 11790, "idx": 73}, {"begin": 11791, "end": 12014, "idx": 74}, {"begin": 12015, "end": 12359, "idx": 75}, {"begin": 12360, "end": 12553, "idx": 76}, {"begin": 12565, "end": 12694, "idx": 77}, {"begin": 12695, "end": 13054, "idx": 78}, {"begin": 13055, "end": 13187, "idx": 79}, {"begin": 13188, "end": 13266, "idx": 80}, {"begin": 13267, "end": 13587, "idx": 81}, {"begin": 13588, "end": 13834, "idx": 82}, {"begin": 13835, "end": 13945, "idx": 83}, {"begin": 13946, "end": 14061, "idx": 84}, {"begin": 14062, "end": 14140, "idx": 85}, {"begin": 14141, "end": 14239, "idx": 86}, {"begin": 14496, "end": 14557, "idx": 87}, {"begin": 14558, "end": 14720, "idx": 88}, {"begin": 14721, "end": 14864, "idx": 89}, {"begin": 14865, "end": 14945, "idx": 90}, {"begin": 14946, "end": 15092, "idx": 91}, {"begin": 15093, "end": 15202, "idx": 92}, {"begin": 15203, "end": 15282, "idx": 93}, {"begin": 15283, "end": 15435, "idx": 94}, {"begin": 15436, "end": 15525, "idx": 95}, {"begin": 15526, "end": 15706, "idx": 96}, {"begin": 15747, "end": 15872, "idx": 97}, {"begin": 15873, "end": 15907, "idx": 98}, {"begin": 15908, "end": 16034, "idx": 99}, {"begin": 16035, "end": 16204, "idx": 100}, {"begin": 16205, "end": 16309, "idx": 101}, {"begin": 16310, "end": 16490, "idx": 102}, {"begin": 16522, "end": 16814, "idx": 103}, {"begin": 16815, "end": 16919, "idx": 104}, {"begin": 16920, "end": 17019, "idx": 105}, {"begin": 17020, "end": 17102, "idx": 106}, {"begin": 17103, "end": 17206, "idx": 107}, {"begin": 17207, "end": 17316, "idx": 108}, {"begin": 17354, "end": 17456, "idx": 109}, {"begin": 17457, "end": 17573, "idx": 110}, {"begin": 17574, "end": 17682, "idx": 111}, {"begin": 17683, "end": 17775, "idx": 112}, {"begin": 17776, "end": 17948, "idx": 113}, {"begin": 17949, "end": 18019, "idx": 114}, {"begin": 18020, "end": 18253, "idx": 115}, {"begin": 18254, "end": 18408, "idx": 116}, {"begin": 18409, "end": 18556, "idx": 117}, {"begin": 18606, "end": 18761, "idx": 118}, {"begin": 18762, "end": 18861, "idx": 119}, {"begin": 18862, "end": 18978, "idx": 120}, {"begin": 18979, "end": 19028, "idx": 121}, {"begin": 19029, "end": 19131, "idx": 122}, {"begin": 19132, "end": 19223, "idx": 123}, {"begin": 19224, "end": 19306, "idx": 124}, {"begin": 19319, "end": 19435, "idx": 125}, {"begin": 19436, "end": 19539, "idx": 126}, {"begin": 19540, "end": 19751, "idx": 127}, {"begin": 19752, "end": 19824, "idx": 128}, {"begin": 19853, "end": 19973, "idx": 129}, {"begin": 19974, "end": 20015, "idx": 130}, {"begin": 20016, "end": 20083, "idx": 131}, {"begin": 20084, "end": 20177, "idx": 132}, {"begin": 20178, "end": 20342, "idx": 133}, {"begin": 20343, "end": 20511, "idx": 134}, {"begin": 20512, "end": 20702, "idx": 135}, {"begin": 20703, "end": 20743, "idx": 136}, {"begin": 20744, "end": 20831, "idx": 137}, {"begin": 20832, "end": 20961, "idx": 138}, {"begin": 20962, "end": 21094, "idx": 139}, {"begin": 21121, "end": 21252, "idx": 140}, {"begin": 21253, "end": 21294, "idx": 141}, {"begin": 21295, "end": 21328, "idx": 142}, {"begin": 21329, "end": 21363, "idx": 143}, {"begin": 21364, "end": 21401, "idx": 144}, {"begin": 21402, "end": 21458, "idx": 145}, {"begin": 21459, "end": 21492, "idx": 146}, {"begin": 21493, "end": 21533, "idx": 147}, {"begin": 21534, "end": 21639, "idx": 148}, {"begin": 21640, "end": 21747, "idx": 149}, {"begin": 21748, "end": 21809, "idx": 150}, {"begin": 21810, "end": 21959, "idx": 151}, {"begin": 21960, "end": 22063, "idx": 152}, {"begin": 22064, "end": 22264, "idx": 153}, {"begin": 22265, "end": 22454, "idx": 154}, {"begin": 22455, "end": 22513, "idx": 155}, {"begin": 22514, "end": 22634, "idx": 156}, {"begin": 22635, "end": 22721, "idx": 157}, {"begin": 22722, "end": 22896, "idx": 158}, {"begin": 22897, "end": 23051, "idx": 159}, {"begin": 23052, "end": 23092, "idx": 160}, {"begin": 23093, "end": 23306, "idx": 161}, {"begin": 23307, "end": 23426, "idx": 162}, {"begin": 23427, "end": 23600, "idx": 163}, {"begin": 23601, "end": 23734, "idx": 164}, {"begin": 23735, "end": 23777, "idx": 165}, {"begin": 23808, "end": 24028, "idx": 166}, {"begin": 24029, "end": 24066, "idx": 167}, {"begin": 24067, "end": 24178, "idx": 168}, {"begin": 24179, "end": 24257, "idx": 169}, {"begin": 24258, "end": 24400, "idx": 170}, {"begin": 24401, "end": 24520, "idx": 171}, {"begin": 24521, "end": 24648, "idx": 172}, {"begin": 24649, "end": 24888, "idx": 173}, {"begin": 24889, "end": 24939, "idx": 174}, {"begin": 24940, "end": 25130, "idx": 175}, {"begin": 25131, "end": 25213, "idx": 176}, {"begin": 25214, "end": 25327, "idx": 177}, {"begin": 25328, "end": 25492, "idx": 178}, {"begin": 25493, "end": 25735, "idx": 179}, {"begin": 25736, "end": 25870, "idx": 180}, {"begin": 25871, "end": 26048, "idx": 181}, {"begin": 26049, "end": 26092, "idx": 182}, {"begin": 26093, "end": 26218, "idx": 183}, {"begin": 26248, "end": 26326, "idx": 184}, {"begin": 26327, "end": 26461, "idx": 185}, {"begin": 26462, "end": 26499, "idx": 186}, {"begin": 26500, "end": 26596, "idx": 187}, {"begin": 26597, "end": 26764, "idx": 188}, {"begin": 26765, "end": 26968, "idx": 189}, {"begin": 26969, "end": 27079, "idx": 190}, {"begin": 27080, "end": 27262, "idx": 191}, {"begin": 27304, "end": 27405, "idx": 192}, {"begin": 27406, "end": 27478, "idx": 193}, {"begin": 27479, "end": 27586, "idx": 194}, {"begin": 27587, "end": 27649, "idx": 195}, {"begin": 27650, "end": 27753, "idx": 196}, {"begin": 27754, "end": 27918, "idx": 197}, {"begin": 27919, "end": 28111, "idx": 198}, {"begin": 28136, "end": 28245, "idx": 199}, {"begin": 28246, "end": 28332, "idx": 200}, {"begin": 28333, "end": 28437, "idx": 201}, {"begin": 28438, "end": 28638, "idx": 202}, {"begin": 28639, "end": 28699, "idx": 203}, {"begin": 28700, "end": 28902, "idx": 204}, {"begin": 28918, "end": 29078, "idx": 205}, {"begin": 29079, "end": 29258, "idx": 206}, {"begin": 29259, "end": 29478, "idx": 207}, {"begin": 29479, "end": 29584, "idx": 208}, {"begin": 29585, "end": 29717, "idx": 209}, {"begin": 29718, "end": 29853, "idx": 210}, {"begin": 29854, "end": 29979, "idx": 211}, {"begin": 29980, "end": 30140, "idx": 212}, {"begin": 30141, "end": 30281, "idx": 213}, {"begin": 30282, "end": 30417, "idx": 214}, {"begin": 30418, "end": 30500, "idx": 215}, {"begin": 30501, "end": 30593, "idx": 216}, {"begin": 30594, "end": 30751, "idx": 217}, {"begin": 30752, "end": 31040, "idx": 218}, {"begin": 31041, "end": 31229, "idx": 219}, {"begin": 31230, "end": 31376, "idx": 220}, {"begin": 31408, "end": 31454, "idx": 221}, {"begin": 31455, "end": 31635, "idx": 222}, {"begin": 31636, "end": 31681, "idx": 223}, {"begin": 31774, "end": 31788, "idx": 224}, {"begin": 31789, "end": 31809, "idx": 225}, {"begin": 31810, "end": 31897, "idx": 226}, {"begin": 31898, "end": 32028, "idx": 227}, {"begin": 32029, "end": 32151, "idx": 228}, {"begin": 32152, "end": 32308, "idx": 229}, {"begin": 32417, "end": 32516, "idx": 230}, {"begin": 32716, "end": 32833, "idx": 231}, {"begin": 32834, "end": 32891, "idx": 232}, {"begin": 32892, "end": 32969, "idx": 233}, {"begin": 32970, "end": 33045, "idx": 234}, {"begin": 33046, "end": 33257, "idx": 235}, {"begin": 33258, "end": 33381, "idx": 236}, {"begin": 33382, "end": 33512, "idx": 237}, {"begin": 33544, "end": 33633, "idx": 238}, {"begin": 33634, "end": 33713, "idx": 239}, {"begin": 33714, "end": 33780, "idx": 240}, {"begin": 33781, "end": 33887, "idx": 241}, {"begin": 33888, "end": 33990, "idx": 242}, {"begin": 33991, "end": 34148, "idx": 243}, {"begin": 34149, "end": 34213, "idx": 244}, {"begin": 34214, "end": 34281, "idx": 245}, {"begin": 34282, "end": 34410, "idx": 246}, {"begin": 34411, "end": 34476, "idx": 247}, {"begin": 34477, "end": 34658, "idx": 248}, {"begin": 34659, "end": 34695, "idx": 249}, {"begin": 34696, "end": 34861, "idx": 250}, {"begin": 34862, "end": 34919, "idx": 251}, {"begin": 34920, "end": 34976, "idx": 252}, {"begin": 34977, "end": 35099, "idx": 253}, {"begin": 35100, "end": 35233, "idx": 254}, {"begin": 35234, "end": 35323, "idx": 255}, {"begin": 35324, "end": 35393, "idx": 256}, {"begin": 35394, "end": 35469, "idx": 257}, {"begin": 35470, "end": 35536, "idx": 258}, {"begin": 35537, "end": 35841, "idx": 259}, {"begin": 35842, "end": 35922, "idx": 260}, {"begin": 35923, "end": 36048, "idx": 261}, {"begin": 36049, "end": 36188, "idx": 262}, {"begin": 36189, "end": 36424, "idx": 263}, {"begin": 36425, "end": 36512, "idx": 264}, {"begin": 36513, "end": 36709, "idx": 265}, {"begin": 36710, "end": 36802, "idx": 266}, {"begin": 36803, "end": 36869, "idx": 267}, {"begin": 36870, "end": 36936, "idx": 268}, {"begin": 36937, "end": 37027, "idx": 269}, {"begin": 37028, "end": 37097, "idx": 270}, {"begin": 37098, "end": 37149, "idx": 271}, {"begin": 37150, "end": 37222, "idx": 272}, {"begin": 37223, "end": 37336, "idx": 273}, {"begin": 37337, "end": 37406, "idx": 274}, {"begin": 37407, "end": 37493, "idx": 275}, {"begin": 37517, "end": 37701, "idx": 276}, {"begin": 37702, "end": 37766, "idx": 277}, {"begin": 37767, "end": 37915, "idx": 278}, {"begin": 37916, "end": 37969, "idx": 279}, {"begin": 37970, "end": 38065, "idx": 280}, {"begin": 38066, "end": 38299, "idx": 281}, {"begin": 38337, "end": 38481, "idx": 282}, {"begin": 38482, "end": 38617, "idx": 283}, {"begin": 38618, "end": 38708, "idx": 284}, {"begin": 38709, "end": 38920, "idx": 285}, {"begin": 38921, "end": 38981, "idx": 286}, {"begin": 38982, "end": 39326, "idx": 287}, {"begin": 39381, "end": 39531, "idx": 288}, {"begin": 39532, "end": 39617, "idx": 289}, {"begin": 39618, "end": 39753, "idx": 290}, {"begin": 39754, "end": 39873, "idx": 291}, {"begin": 39874, "end": 39976, "idx": 292}, {"begin": 40012, "end": 40103, "idx": 293}], "ReferenceToFigure": [{"begin": 13264, "end": 13265, "target": "#fig_0", "idx": 0}, {"begin": 26090, "end": 26091, "idx": 1}, {"begin": 26491, "end": 26492, "idx": 2}, {"begin": 27639, "end": 27640, "idx": 3}, {"begin": 28428, "end": 28429, "idx": 4}, {"begin": 38979, "end": 38980, "target": "#fig_2", "idx": 5}, {"begin": 40102, "end": 40103, "idx": 6}], "Abstract": [{"begin": 89, "end": 1646, "idx": 0}], "SectionFootnote": [{"begin": 40105, "end": 40267, "idx": 0}], "Footnote": [{"begin": 40116, "end": 40145, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 40146, "end": 40183, "id": "foot_1", "idx": 1}, {"begin": 40184, "end": 40221, "id": "foot_2", "idx": 2}, {"begin": 40222, "end": 40267, "id": "foot_3", "idx": 3}]}}