{"text": "Hollow-tree Super: a directional and scalable approach for feature importance in boosted tree models\n\nAbstract:\nPurpose\nCurrent limitations in methodologies used throughout machine-learning to investigate feature importance in boosted tree modelling prevent the effective scaling to datasets with a large number of features, particularly when one is investigating both the magnitude and directionality of various features on the classification into a positive or negative class. This manuscript presents a novel methodology, \"Hollow-tree Super\" (HOTS), designed to resolve and visualize feature importance in boosted tree models involving a large number of features.\nFurther, this methodology allows for accurate investigation of the directionality and magnitude various features have on classification, and incorporates cross-validation to improve the accuracy and validity of the determined features of importance.\nMethods\nUsing the Iris dataset, we first highlight the characteristics of HOTS by comparing it to other commonly used techniques for feature importance, including Gini Importance, Partial Dependence Plots, and Permutation Importance, and explain how HOTS resolves the weaknesses present in these three strategies for investigating feature importance. We then demonstrate how HOTS can be utilized in high dimensional spaces such as neuroscientific setting, by taking 60 Schizophrenic subjects from the publically available SchizConnect database and applying the method to determine which regions of the brain were most important for the positive and negative classification of schizophrenia as determined by the positive and negative syndrome scale (PANSS).\n\nMain:\n\n\n\nResults\nHOTS effectively replicated and supported the findings of feature importance for classification of the Iris dataset when compared to Gini importance, Partial Dependence Plots and Permutation importance, determining 'petal length' as the most important feature for positive and negative classification. When applied to the Schizconnect dataset, HOTS was able to resolve from 379 independant features, the top 10 most important features for classification, as well as their directionality for classification and magnitude compared to other features. Cross-validation supported that these same 10 features were consistently used in the decision-making process across multiple trees, and these features were localised primarily to the occipital and parietal cortices, commonly disturbed brain regions in those afflicted with Schizophrenia.\n\nConclusion\nHOTS effectively overcomes previous challenges of identifying feature importance at scale, and can be utilized across a swathe of disciplines. As computational power and data quantity continues to expand, it is imperative that a methodology is developed that is able to handle the demands of working with large datasets that contain a large number of features. This approach represents a unique way to investigate both the directionality and magnitude of feature importance when working at scale within a boosted tree model that can be easily visualized within commonly used software.\n\n1. Introduction\nTree based models, a category of supervised machine learning algorithms, have become widely used to perform regression or classification. Among the reasons for their popularity is the ability to perform predictions on data with high dimensionality, mixed type variables and complex, non-linear relationships -better so than linear methods [1].\nIn many real-life applications however, model interpretation is equally as valuable as the prediction output. Yet understanding why a prediction was made can be a non-trivial exercise given that tree-based models can become extremely complex (e.g. deep trees) and difficult to interpret at scale. Interpretability becomes even more complex in the case of boosted trees such as XGBoost [2] where numerous different trees are bagged together and weighted with different importance (boosting). Some methods exist to understand why a model makes these predictions, such as Gini importance, partial dependence plots and permutation analysis [3] [4] [5]. Whilst those techniques provide a certain degree of useful insights into the model, they each lack one or more important properties for explainability of the model, specifically: (1) the direction in the relationship between features and the response variable (e.g., whether feature X1 is predictive of the negative/positive outcome) and ( 2) magnitude (e.g., how much feature X1 influences the prediction towards the positive or negative outcome). It is important to note that a successful technique would need to achieve (1) and ( 2) in a way that would scale to a larger number of features, thus providing a truly commensurable understanding of datasets, particularly those that have high dimensionality.\nRecently, a method was proposed to linearize tree-based model nodes to provide an answer to both (1) and (2) [6]. However, this method has some limitations when it comes to applying it on boosted trees as each instance of the model added to the ensemble can have a different tree structure.\nIn this paper, we present a feature contribution method which extends the linearization methods available for single and ensemble trees to include boosted trees. Further, we demonstrate and provide an example how this new method can be used to analyse highdimensionality data, such as in the case of human neuroimaging datasets investigating brain pathology.\n\n1.1 Common approaches to feature importance for single trees\nBefore demonstrating our feature contribution method on boosted trees, it is important to first discuss the most common methods currently available for calculating feature importance. We examine three methods: gini importance, partial dependence plots and permutation importance. We do this on single decision trees -the simplest of the tree-based approaches and thus the most readily interpretable. For this (and throughout the worked example), we use the well-known iris dataset [7] -a simple classification problem with four input features relating to plant dimensions: petal length, petal width, sepal length and sepal width.\nTypically, the iris dataset categorizes plants as one of; iris versicolor, iris virginica and iris setosa. To keep things simple, we removed the iris setosa group to make this a binary classification problem (iris versicolor being the negative class (=0) and iris virginica as the positive class (=1).\nUsing the sklearn DecisionTreeClassifier package [8] we constructed a single decision tree with a max depth of 4 (Fig 1a). Importance for our decision tree using sklearn's \"feature_importances_\" property. This revealed 'petal length' to be the most important feature in the model for classification into the positive or negative class. (c) A one feature partial dependence plot (PDP) for 'petal length' revealed that positive classification as Iris Virginica (partial dependence) was non-linearly related to 'petal length', with a critical point at 5cm marking certain negative (Iris Versicolor) classification. (d) By introducing 'petal width' and conducting a two feature PDP, we were able to determine that these two features were increasingly dependent for positive classification at values lower than 5cm and 1.6cm for petal length and width respectively, giving us directional and magnitudinal inferences between two features (For two feature PDP's, a colour map is added to help visualise dependencies such that green indicates a greater partial dependency than purple). (e) When performing permutation importance on our four features included in our simple decision tree, we found that permuting the values of petal width had the greatest impact on the model prediction error when attempting to classify data to the positive or negative class. Unlike Gini importance, this analysis revealed that petal length was also significantly important in the classification process, whilst sepal length and width again were found to be relatively unimportant in the decision making process, and permutation of these features did not greatly impact model prediction error.\n\n1.1.1 Gini Importance\nA standard approach to determining feature importance is to score features based on the number of times or probability a variable is utilized by the model for splitting, weighted by some other value. This could be the criterion used to select split points (Gini or entropy), or some other metric such as the squared improvement to the model's F-score. Fig 1b shows the feature importances for our binarized decision tree, computed using sklearn's \"feature_importances_\" property. This function calculates feature importance using the \"normalized\" total reduction of the criterion brought by that feature, also referred to as the \"Gini importance\".\nThis approach attributes a score to each feature, where a higher value indicates greater influence on the output prediction. From our simple decision tree, the most important feature for outcome prediction was petal length, with petal width, sepal length and sepal width showing no significant difference in their influence for predicting outcomes through this tree.\nWhilst this approach is an informative metric for assessing the importance of the different features used in the model, particularly by providing a relative ranking for each of these, and is scalable, it lacks directionality. As a matter of fact, nothing is said for the variable \"petal length\" to be more predictive of the positive or negative outcome.\n\n1.1.2 Partial Dependence Plots\nAs an alternative, partial dependence plots (PDP) are often used to visualize decision boundaries. They help to describe relationships with non-linear effects, and show interactions between features. To construct such a plot, the PDP function is calculated at each possible value of a feature, representing the average model prediction output at that value. Fig 1c shows the PDP for 'petal length' alone, and fig 1d demonstrates the interaction between 'petal length' and 'petal width' and the predicted outcome of the constructed simple decision tree.\nThese two PDP's together reveal that petal length and petal width become dependent features for classification at values less than 5cm for petal length and 1.6cm for petal width respectively, and at greater values for either feature they are largely independent for classification. The benefits of PDP's are that they are easy to implement, interpret and provide a measure of directionality for feature importance. This would scale nicely for the iris dataset, which has four features. However, the main disadvantage is that the 2D representation of PDPs limits observations to two variables at a time. This makes PDPs difficult to interpret at the scale of a dataset which comprises hundreds of variables, as each feature, or pair of features at most, would require their own plot and subsequent analysis to ultimately determine and inform feature importance.\n\n1.1.3 Permutation Importance\nFinally, permutation importance of features can be used to measure the change in the model's prediction error as the value of the feature is 'permuted'. Permutation is the process of shuffling the data points for one feature whilst retaining the order for all other features to measure which variable most greatly affects model prediction error [9]. From this perspective, a feature is unimportant if changing it's values has little effect on the model's error -this implies that the model did not rely strongly on this feature to make predictions.\nConversely, an 'important' feature would increase the model error when its values are shuffled.\n\nWe can use sklearn's permutation importance package to perform this calculation on the Iris dataset (Fig 1e).\nWhilst this is another useful approach to determine the magnitude of feature importance, much like Gini importance, this method lacks information about directionality. We are able to glean that permutation of 'petal width' had the largest effect on the model's prediction error, however, we are again unable to say whether 'petal width' was more predictive of the positive or negative outcome.\n\n1.2 Direction and magnitude in feature importance coefficients\nAs shown, none of the three methods already described are able to provide information about both outcome directionality and magnitude, in a way which could be efficiently scaled to a large number of features.\nLinearizing decision trees [6] offers the advantage of giving feature importance coefficients that have a direction, which magnitude can be compared and further scaled to larger datasets.\nA general linear equation can be derived from the model by considering that each decision in the tree stems from a feature and these decisions either increase or decrease the value from the parent node. Thus, it is possible to consider the final prediction as the sum of each feature's contribution within the tree plus a bias value (typically the topmost sample average).\nTo achieve this for a given prediction, the decision tree that led to that prediction is navigated and the local increments of feature contributions at each node (positive or negative) are identified. In this way, each prediction can be mathematically described using equation ( 1)\ud835\udc53(\ud835\udc65) = \ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60 + \u2211 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc4f\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b (x, k) \ud835\udc3e \ud835\udc58=1 (1)\nWhere; K is the number of features, bias is the value at the root of the node and contribution(x, k) is the contribution from the k-th feature in the feature vector x [10]. This approach is similar to linear regression in a dynamic sense, and so by borrowing from regression models in this way we can achieve a similar level of interpretability as linear models. More specifically, this view of trees enables us to isolate the contributions of each feature for each prediction. It should be noted though that this linearization technique is limited in use to boosted trees whose input feature vectors only contain linearizable variables.\nHere we provide a worked example, showing how the feature contributions are calculated for one prediction. Fig 2 depicts the decision tree, and Table 1 outlines the values of each feature for a sample iris plant, as well as the feature contribution score. Such score can easily be calculated programmatically though the Python package eli5 package [11] If we have a flower with the attribute values described in Table 1 (sepal length = 6.9, sepal width = 3.1 and petal length = 4.9), the model estimates the likelihood of this being in the positive class (y = 1, an iris virginica) at 1.0 (i.e. 100%).\nSince this is a simple single decision tree, we can easily follow the path through the tree (Fig\nThe model is telling us that sepal length of 6.9cm makes it much more likely that this is a iris virginica (85.7% more likely, to be exact), whilst the petal length of 4.9 somewhat lowers the chances of this being a iris virginica. This has made our single decision tree fully interpretable, in the sense that we can readily say exactly how each feature has influenced the prediction.\nThe final prediction (probability of being in the positive class) is the sum of the feature contributions towards the prediction (0.857-0.107-0.243=0.507) plus the bias value (=0.493), in this case, 1.\nSince we don't want to study feature importance at a prediction level, we can average the contributions of each feature across all predictions to obtain a global picture of importance.\nThe method here is to;\n1. Filter out incorrect and low probability predictions.\n2. Separate the feature contributions towards the positive and negative classes.\n3. Sum the contributions of each feature across all predictions, and normalize these values by dividing by the number of predictions made.\nAfter performing this process on the Iris dataset, we found the top feature for predicting both the positive and negative classes to be petal length (Fig 2). Unsurprisingly, petal length also had the greatest gini importance and the second greatest permutation importance, suggesting this method correctly captures feature importance within a decision model and supports the findings of alternative, less robust, analyses.\n\n1.3 Scaling feature extraction from single tree to boosted ensembles\nEnsemble trees and particularly boosted ensemble trees often provide a superior prediction ability than single trees [12]. Unfortunately, the linearization method described above falls short as each tree of the boosted ensemble leads to a different succession of split and group mean for each node due to the random start for the generation of each tree used in the ensemble. As a matter of fact, running the prediction function provided in the eli5 package would lead to a different linear equation for each tree of the ensemble.\nTo circumvent this issue, we propose here an aggregation method across several boosted tree instances within a model to provide directional, proportional, and interpretable feature importance. In addition, this method has the advantage of working across multiple cycles of cross-validation dealing more positively with boosted trees' tendency to overfit.\n\n2. Methods\nWe use XGBClassifier [2] to fit the model. For each subject, we can use eli5 explain_prediction [11] to obtain each feature's contribution to each prediction (ignoring the bias value). Note that when extracting feature contribution from a boosted tree model, the 'weights' become the log odds contribution of each feature (as opposed to the probabilities shown under a single tree model).\n2. We separate the weights contributing towards the positive and negative class cases.\n3. Incorrect predictions, and those with a prediction probability of less than 70% are filtered out, keeping only the subjects that were correctly predicted by the model with confidence.\n4. The weights across all the remaining predictions are aggregated by feature and divided by the number of predictions, obtaining an average weight of each feature per prediction.\n5. As mentioned in (1), the weights provided are the log odds of being in the class that is ultimately predicted (i.e. for positive class predictions, the weights are the log odds of being in the positive class). Additionally, the weights are the log odds at the value of the feature currently being predicted. Thus, to extract the directionality desired here, it is necessary to infer the sign of these log odds for each feature. This is achieved by identifying whether the mean value for each feature in this positive class is greater or less than the mean of each corresponding feature in the negative class. The log odds of each feature in the positive class are multiplied by the sign of the mean value for the positive class less the mean value for the negative class, whilst the log odds of each feature in the negative class are multiplied by the inverse sign of this same calculation. The 'weights' now become the log odds with a standard view of directionality. However, it is important to note that this approach assumes linearity between the feature values.\n6. As mentioned, one challenge to this approach is the inherent instability of feature importance over different runs caused by the overfitting and random factor dependence of gradient boosted tree models. Different cuts of data for training and testing produce different results. This problem can be solved by performing crossvalidation to arrive at a stable set of features. This allows for greater confidence when interpreting the model as it highlights only features that are consistent across runs.\nHere we use 5 fold cross-validation to obtain the mean feature importances, with an average accuracy across all 5 folds = 0.94.\n\n3. Results\n\n\n3.1 Iris dataset\nUsing HOTS, A high degree of concordance with the features determined by gini and permutation importance is maintained, with petal length and petal width both identified as the top two most predictive features (Fig 3). Since these weights are effectively the log odds for the respective classes, we can interpret them as such. The large negative value for petal length in the positive class (= -0.45) indicates that a petal length increases, the odds of being in the positive class decreases. A similar conclusion can be made for petal width. Further, we can see that the sepal dimensions had little to no predictive power. entering the Iris dataset into our linearized decision tree, we found the most important feature for successfully determining a positive or negative class to be 'petal length'. Importantly, the outputs provided in this analysis are given a magnitude and direction for their respective involvement in classification, offering a significant improvement on previous analyses, whilst remaining consistent with the findings of gini and permutation importance, where 'petal length' had the highest and second highest weightings respectively. Note that this represents the output data from a single decision tree (fold) prior to the cross-validation process.\nWe are also able to perform a count of how many folds each feature appears during the crossvalidation process (Fig 4). We see that all four features of the iris dataset were used to make predictions in each of the 5 folds (Fig 5b). This methodology unveils a way to achieve a similar outcome as calculating feature importance by Gini importance or improvement to F-score, whilst also making use of ensemble methods like gradient boosting which provide a superior fit. Further, it does this in a way that better quantifies the impact of the variable since the weights in Fig 5a represent the averaged feature importance contributions after cross-validation towards the respective final predictions.\nThis method is optimized for approximating the contribution of each feature to the classification outcome and while it is limited to one observation at a time, it can be easily scaled.\n\n3.2 Case study: HOTS feature importance in Schizophrenic brain data\nFurthermore, we propose this method as a suitable way of tying clinically observed behaviours to functional brain regions, or \"parcellations\".\nA \"parcellation atlas\" in simple terms is a map of the brain. It delineates regions of the neocortex that exhibit similar properties across individuals, such as functional activity, structural connectivity, or cellular composition. Thus, a 'parcellation' is a region of the brain that expresses similar properties in a population, even if the exact boundaries or topological location may differ between individuals.\nParcellation atlases are particularly useful when analyzing functional magnetic resonance imaging (fMRI) data, which through recording changes in blood flow over time, can produce a representation of neural activity. This information can further be used to show functional connectivity between regions of the brain if the activity recorded between them exhibits a statistical relationship. A parcellation atlas can be used to reduce this complexity of pairwise correlations by reducing the comparisons to a finite number of regions, assumed to perform somewhat uniform functions.\nRecently, Doyen et al. (forthcoming) developed a machine learning-based technique for parcellating the brain in a way that is both subject specific, and comparable between subjects [2, 13]. Using this technique, we are able to generate adjacency matrices representing the correlation between every pair of parcellations -described for the remainder of this paper as 'connectomic features'.\nThe applied atlas contains 379 parcellations in Glassian nomenclature [14] equating to 71,631 input features, and so any method used to tie clinically observed behaviour to these functional brain areas would not only need to provide direction and magnitude, but also be scalable to a large number of features.\nHere we present a case example of the feature contribution for boosted trees method described throughout this manuscript, demonstrating its ability to effectively tie functional regions of the brain to clinically observed behaviours.\nWe performed this analysis using the SchizConnect Center for Biomedical Research Excellence (COBRE) dataset [15]. The dataset consists of the necessary brain MRI data for mapping, as well as neuropsychological assessment scores for 60 patients diagnosed with Schizophrenia. The analysis was designed as a binary classification problem -predicting the presence or absence of a specific symptom. Specifically, we use item N4 from the patient's\nPositive and Negative Syndrome Scale (PANSS) [16], which measures the degree to which patient's show \"passive/apathetic social withdrawal\". Each patient recorded their response on a 7 point Likert scale, representing increasing levels of psychopathology (1 = absent, 7=extreme), with a sensible binarization point determined at a score of 2 (i.e. subjects with a N4 score > 2 were in the positive class, and \u2264 2 were in the negative class). As described above, the input features used to predict the presence or absence of this symptom were the subject's pairwise functional correlation between the 379 regions of the brain atlas which together form a large adjacency matrix.\nAs with the iris dataset, we use XGBClassifier [1] to fit the model and perform 5-fold crossvalidation with an average accuracy of 0.74. Using the same feature contribution method described for the iris dataset, we are able to generate a list of the features (parcellations) that were most predictive of the positive and negative class symptom (Fig 6)  Further, cross-validation revealed that the R_POS2 was used by the boosted tree model to make predictions in 3 out of 5 folds (Fig 7). This suggests that this feature was consistently important for making predictions towards the positive class. (R_IFSa, R_6mp, R_PFcm, R_10r, L_V8, L_Cerebellum, R_MIP, L_LO3, L_LIPd) or 3 (R_POS2, L_DVT, R_AAIC, R_FST ) out of the 5 folds, suggesting that these same features were regularly used throughout the decision making process. Note that this plot was abbreviated to only show features with a count of greater than 1.\n\n4. Discussion\nWe have described a method which accurately provides metrics of directionality and magnitude between features in boosted tree models. Further, the ability to discern between a large number of features and be scaled to a large dataset makes HOTS an easily implementable method which can be applied to a range of data-driven disciplines. When applied to the Iris dataset, HOTS showed high concordance with other commonly used methods for investigation feature importance: Gini importance, partial dependence plots and permutation importance. In a neuroscientific setting, HOTS was able to accurately identify between 379 features, the most responsible brain regions for positive classification of Schizophrenia from item N4 on the PANSS. Cross-validation supported that these same features were consistently utilized throughout the decision-making process, improving our confidence that the chosen model had an accurate fit for diagnosis. Considering the literature suggesting abnormal activity and connectivity between occipital and parietal brain regions during Schizophrenia [17] [18], HOTS supports and adds value to the current understanding of this pathology.\nAs with any machine-learning model, increasing the size of the dataset would improve the ability of HOTS to make accurate inferences about feature contribution and classification into positive and negative classes [19]. Further investigations should test the applicability of HOTS to other types of data, and in the case of neuroscience, which brain regions are most responsible for other disorders such as depression, Alzheimer's and Parkinson's disease.\narise due to random factor dependence which occurs between trees. However, the incorporation of cross-validation when performing HOTS we believe provides an effective regulating step to best minimize this, and further promotes this method as superior for investigating feature importance in boosted-tree models.\n\n5. Conclusion\nThese results indicate that HOTS offers superior metrics for investigating feature importance than previously used methods. At least in the case of boosted-tree models, we suggest that HOTS could be well incorporated into the modelling process, and provide thorough and easily interpretable metrics for the most responsible features which are also consistently utilized to make classifications. Overall, the potential to visualise directionality and magnitude of feature contribution within a single methodology streamlines the process of performing boosted-tree modelling, and offers a unique approach which can be applied to a range of different types of data.\n\nFootnotes:\n\nReferences:\n\n- Klosterman, S. Why decision trees are more flexible than linear models, explains Stephen Klosterman. 2019 Dec. Available from https://hub.packtpub.com/why-decision-trees- are-more-flexible-than-linear-models-explains-stephen-klosterman/- Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting Method. Association for Computing Machinery, NY, USA. 2016. DOI: 10.1145/2939672.2939785\n\n- Breiman L, Friedman JH, Olshen RA, and Stone CJ., Classification and regression trees: Wadsworth, Inc. 1984\n\n- Greenwell BM. pdp: An R Package for Constructing Partial Dependence Plots. R J., 2017;9(1), 421.\n\n- Ojala M, Garriga GC. Permutation tests for studying classifier performance. Journal of Machine Learning Research, 2010;11(6).\n\n- Palczewska A, Palczewski J, Robinson R, Neagu D. Interpreting random forest models using a feature contribution method. 2013 IEEE 14th International Conference on Information Reuse & Integration (IRI), 2013; 112-119.\n\n- Fisher RA.. The Use Of Multiple Measurements in Taxonomic Problems. Annals of Eugenics, 1936;7(2), 179-188. https://doi.org/https://doi.org/10.1111/j.1469- 1809.1936.tb02137.x\n\n- Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 2011;12, 2825-2830.\n\n- Altmann A, Tolo\u015fi L, Sander O, Lengauer T. Permutation importance: a corrected feature importance measure. Bioinformatics, 2010;26(10), 1340-1347. https://doi.org/10.1093 /bioinformatics/btq134.\n\n- Saabas A. Interpreting Random Forests. 2014 Oct. Available from http://blog.datadive.net/interpreting-random-forests/\n\n- Tulio Ribeiro M, Singh S, Guestrin C. \" Why Should I Trust You?\": Explaining the Predictions of Any Classifier. arXiv e-prints, 2016;arXiv-1602.\n\n- Friedman J. Greedy Function Approximation: A Gradient Boosting Machine. The Annals of Statistics, 2000;29. https://doi.org/10.1214/aos/1013203451\n\n- Omniscient Neurotechnology Pty. Ltd. (o8t). Infinitome [Software] 2020. Available from: https://www.o8t.com/neurologistresearchers.\n\n- Glasser MF, Coalson TS, Robinson EC, Hacker CD, Harwell J, Yacoub E, Ugurbil K, Andersson J, Beckmann CF, Jenkinson M, Smith SM, Van Essen DC. A multi-modal parcellation of human cerebral cortex. Nature, 2016;536(7615), 171-178. https://doi.org/10.1038/nature18933\n\n- Mayer AR, Ruhl D, Merideth F, Ling J, Hanlon FM, Bustillo J, Ca\u00f1ive J. Functional imaging of the hemodynamic sensory gating response in schizophrenia. Human brain mapping, 2013;34(9), 2302-2312. https://doi.org/10.1002/hbm.22065\n\n- Kay SR, Fiszbein A, Opler LA. The positive and negative syndrome scale (PANSS) for schizophrenia. Schizophr Bull. 1987;13(2):261-76. doi: 10.1093/schbul/13.2.261. PMID: 3616518.\n\n- Danckert J, Saoud M, Maruff P. Attention, motor control and motor imagery in schizophrenia: implications for the role of the parietal cortex. Schizophrenia Research, 2004;70(2), 241-261. https://doi.org/https://doi.org/10.1016/j.schres.2003.12.007\n\n- Tohid H, Faizan M, Faizan U. Alterations of the occipital lobe in schizophrenia. Neurosciences (Riyadh, Saudi Arabia), 2015;20(3), 213-224. https://doi.org/10.17712/ nsj.2015.3.20140757\n\n- Mola F. Classification and Regression Trees Software and New Developments. In: Rizzi A., Vichi M., Bock HH. (eds) Advances in Data Science and Classification. Studies in Classification, Data Analysis, and Knowledge Organization. Springer, Berlin, Heidelberg. 1998. https://doi.org/10.1007/978-3-642-72253-0_42\n\n", "annotations": {"ReferenceToTable": [{"begin": 14124, "end": 14125, "target": "#tab_0", "idx": 0}, {"begin": 14392, "end": 14393, "target": "#tab_0", "idx": 1}], "SectionMain": [{"begin": 1681, "end": 28669, "idx": 0}], "ReferenceToFormula": [{"begin": 4475, "end": 4476, "idx": 0}, {"begin": 4668, "end": 4669, "idx": 1}, {"begin": 13288, "end": 13289, "idx": 2}], "SectionReference": [{"begin": 28683, "end": 32299, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1681, "idx": 0}], "Div": [{"begin": 112, "end": 916, "idx": 0}, {"begin": 917, "end": 1673, "idx": 1}, {"begin": 1684, "end": 2527, "idx": 2}, {"begin": 2529, "end": 3124, "idx": 3}, {"begin": 3126, "end": 5492, "idx": 4}, {"begin": 5494, "end": 8156, "idx": 5}, {"begin": 8158, "end": 9548, "idx": 6}, {"begin": 9550, "end": 10994, "idx": 7}, {"begin": 10996, "end": 11669, "idx": 8}, {"begin": 11671, "end": 12174, "idx": 9}, {"begin": 12176, "end": 16167, "idx": 10}, {"begin": 16169, "end": 17123, "idx": 11}, {"begin": 17125, "end": 19680, "idx": 12}, {"begin": 19682, "end": 19693, "idx": 13}, {"begin": 19695, "end": 21870, "idx": 14}, {"begin": 21872, "end": 26044, "idx": 15}, {"begin": 26046, "end": 27991, "idx": 16}, {"begin": 27993, "end": 28669, "idx": 17}], "Head": [{"begin": 112, "end": 119, "idx": 0}, {"begin": 917, "end": 924, "idx": 1}, {"begin": 1684, "end": 1691, "idx": 2}, {"begin": 2529, "end": 2539, "idx": 3}, {"begin": 3126, "end": 3141, "n": "1.", "idx": 4}, {"begin": 5494, "end": 5554, "n": "1.1", "idx": 5}, {"begin": 8158, "end": 8179, "n": "1.1.1", "idx": 6}, {"begin": 9550, "end": 9580, "n": "1.1.2", "idx": 7}, {"begin": 10996, "end": 11024, "n": "1.1.3", "idx": 8}, {"begin": 11671, "end": 11780, "idx": 9}, {"begin": 12176, "end": 12238, "n": "1.2", "idx": 10}, {"begin": 16169, "end": 16237, "n": "1.3", "idx": 11}, {"begin": 17125, "end": 17135, "n": "2.", "idx": 12}, {"begin": 19682, "end": 19692, "n": "3.", "idx": 13}, {"begin": 19695, "end": 19711, "n": "3.1", "idx": 14}, {"begin": 21872, "end": 21939, "n": "3.2", "idx": 15}, {"begin": 26046, "end": 26059, "n": "4.", "idx": 16}, {"begin": 27993, "end": 28006, "n": "5.", "idx": 17}], "Paragraph": [{"begin": 120, "end": 666, "idx": 0}, {"begin": 667, "end": 916, "idx": 1}, {"begin": 925, "end": 1673, "idx": 2}, {"begin": 1692, "end": 2527, "idx": 3}, {"begin": 2540, "end": 3124, "idx": 4}, {"begin": 3142, "end": 3485, "idx": 5}, {"begin": 3486, "end": 4842, "idx": 6}, {"begin": 4843, "end": 5133, "idx": 7}, {"begin": 5134, "end": 5492, "idx": 8}, {"begin": 5555, "end": 6184, "idx": 9}, {"begin": 6185, "end": 6486, "idx": 10}, {"begin": 6487, "end": 8156, "idx": 11}, {"begin": 8180, "end": 8827, "idx": 12}, {"begin": 8828, "end": 9194, "idx": 13}, {"begin": 9195, "end": 9548, "idx": 14}, {"begin": 9581, "end": 10133, "idx": 15}, {"begin": 10134, "end": 10994, "idx": 16}, {"begin": 11025, "end": 11573, "idx": 17}, {"begin": 11574, "end": 11669, "idx": 18}, {"begin": 11781, "end": 12174, "idx": 19}, {"begin": 12239, "end": 12447, "idx": 20}, {"begin": 12448, "end": 12635, "idx": 21}, {"begin": 12636, "end": 13008, "idx": 22}, {"begin": 13009, "end": 13290, "idx": 23}, {"begin": 13336, "end": 13973, "idx": 24}, {"begin": 13974, "end": 14575, "idx": 25}, {"begin": 14576, "end": 14672, "idx": 26}, {"begin": 14673, "end": 15057, "idx": 27}, {"begin": 15058, "end": 15259, "idx": 28}, {"begin": 15260, "end": 15444, "idx": 29}, {"begin": 15445, "end": 15467, "idx": 30}, {"begin": 15468, "end": 15524, "idx": 31}, {"begin": 15525, "end": 15605, "idx": 32}, {"begin": 15606, "end": 15744, "idx": 33}, {"begin": 15745, "end": 16167, "idx": 34}, {"begin": 16238, "end": 16768, "idx": 35}, {"begin": 16769, "end": 17123, "idx": 36}, {"begin": 17136, "end": 17524, "idx": 37}, {"begin": 17525, "end": 17611, "idx": 38}, {"begin": 17612, "end": 17798, "idx": 39}, {"begin": 17799, "end": 17978, "idx": 40}, {"begin": 17979, "end": 19048, "idx": 41}, {"begin": 19049, "end": 19552, "idx": 42}, {"begin": 19553, "end": 19680, "idx": 43}, {"begin": 19712, "end": 20987, "idx": 44}, {"begin": 20988, "end": 21685, "idx": 45}, {"begin": 21686, "end": 21870, "idx": 46}, {"begin": 21940, "end": 22082, "idx": 47}, {"begin": 22083, "end": 22498, "idx": 48}, {"begin": 22499, "end": 23078, "idx": 49}, {"begin": 23079, "end": 23468, "idx": 50}, {"begin": 23469, "end": 23778, "idx": 51}, {"begin": 23779, "end": 24012, "idx": 52}, {"begin": 24013, "end": 24454, "idx": 53}, {"begin": 24455, "end": 25130, "idx": 54}, {"begin": 25131, "end": 26044, "idx": 55}, {"begin": 26060, "end": 27223, "idx": 56}, {"begin": 27224, "end": 27679, "idx": 57}, {"begin": 27680, "end": 27991, "idx": 58}, {"begin": 28007, "end": 28669, "idx": 59}], "ReferenceToBib": [{"begin": 3481, "end": 3484, "target": "#b0", "idx": 0}, {"begin": 3871, "end": 3874, "target": "#b1", "idx": 1}, {"begin": 4122, "end": 4125, "target": "#b2", "idx": 2}, {"begin": 4126, "end": 4129, "target": "#b3", "idx": 3}, {"begin": 4130, "end": 4133, "target": "#b4", "idx": 4}, {"begin": 4314, "end": 4317, "target": "#b0", "idx": 5}, {"begin": 4952, "end": 4955, "target": "#b5", "idx": 6}, {"begin": 6036, "end": 6039, "target": "#b6", "idx": 7}, {"begin": 6536, "end": 6539, "target": "#b7", "idx": 8}, {"begin": 11370, "end": 11373, "target": "#b8", "idx": 9}, {"begin": 12475, "end": 12478, "target": "#b5", "idx": 10}, {"begin": 13503, "end": 13507, "target": "#b9", "idx": 11}, {"begin": 14322, "end": 14326, "target": "#b10", "idx": 12}, {"begin": 16355, "end": 16359, "target": "#b11", "idx": 13}, {"begin": 17157, "end": 17160, "target": "#b1", "idx": 14}, {"begin": 17232, "end": 17236, "target": "#b10", "idx": 15}, {"begin": 23260, "end": 23263, "target": "#b1", "idx": 16}, {"begin": 23264, "end": 23267, "target": "#b12", "idx": 17}, {"begin": 23539, "end": 23543, "target": "#b13", "idx": 18}, {"begin": 24121, "end": 24125, "target": "#b14", "idx": 19}, {"begin": 24500, "end": 24504, "target": "#b15", "idx": 20}, {"begin": 25178, "end": 25181, "target": "#b0", "idx": 21}, {"begin": 27136, "end": 27140, "target": "#b16", "idx": 22}, {"begin": 27141, "end": 27145, "target": "#b17", "idx": 23}, {"begin": 27438, "end": 27442, "target": "#b18", "idx": 24}], "Sentence": [{"begin": 120, "end": 478, "idx": 0}, {"begin": 479, "end": 666, "idx": 1}, {"begin": 667, "end": 916, "idx": 2}, {"begin": 925, "end": 1267, "idx": 3}, {"begin": 1268, "end": 1673, "idx": 4}, {"begin": 1692, "end": 1993, "idx": 5}, {"begin": 1994, "end": 2239, "idx": 6}, {"begin": 2240, "end": 2527, "idx": 7}, {"begin": 2540, "end": 2682, "idx": 8}, {"begin": 2683, "end": 2900, "idx": 9}, {"begin": 2901, "end": 3124, "idx": 10}, {"begin": 3142, "end": 3279, "idx": 11}, {"begin": 3280, "end": 3485, "idx": 12}, {"begin": 3486, "end": 3595, "idx": 13}, {"begin": 3596, "end": 3733, "idx": 14}, {"begin": 3734, "end": 3782, "idx": 15}, {"begin": 3783, "end": 3976, "idx": 16}, {"begin": 3977, "end": 4134, "idx": 17}, {"begin": 4135, "end": 4583, "idx": 18}, {"begin": 4584, "end": 4842, "idx": 19}, {"begin": 4843, "end": 4956, "idx": 20}, {"begin": 4957, "end": 5133, "idx": 21}, {"begin": 5134, "end": 5295, "idx": 22}, {"begin": 5296, "end": 5492, "idx": 23}, {"begin": 5555, "end": 5738, "idx": 24}, {"begin": 5739, "end": 5834, "idx": 25}, {"begin": 5835, "end": 5954, "idx": 26}, {"begin": 5955, "end": 6184, "idx": 27}, {"begin": 6185, "end": 6291, "idx": 28}, {"begin": 6292, "end": 6486, "idx": 29}, {"begin": 6487, "end": 6609, "idx": 30}, {"begin": 6610, "end": 6691, "idx": 31}, {"begin": 6692, "end": 6822, "idx": 32}, {"begin": 6823, "end": 7098, "idx": 33}, {"begin": 7099, "end": 7564, "idx": 34}, {"begin": 7565, "end": 7838, "idx": 35}, {"begin": 7839, "end": 8156, "idx": 36}, {"begin": 8180, "end": 8379, "idx": 37}, {"begin": 8380, "end": 8531, "idx": 38}, {"begin": 8532, "end": 8659, "idx": 39}, {"begin": 8660, "end": 8827, "idx": 40}, {"begin": 8828, "end": 8952, "idx": 41}, {"begin": 8953, "end": 9194, "idx": 42}, {"begin": 9195, "end": 9420, "idx": 43}, {"begin": 9421, "end": 9548, "idx": 44}, {"begin": 9581, "end": 9679, "idx": 45}, {"begin": 9680, "end": 9780, "idx": 46}, {"begin": 9781, "end": 9938, "idx": 47}, {"begin": 9939, "end": 10133, "idx": 48}, {"begin": 10134, "end": 10415, "idx": 49}, {"begin": 10416, "end": 10548, "idx": 50}, {"begin": 10549, "end": 10619, "idx": 51}, {"begin": 10620, "end": 10736, "idx": 52}, {"begin": 10737, "end": 10994, "idx": 53}, {"begin": 11025, "end": 11177, "idx": 54}, {"begin": 11178, "end": 11374, "idx": 55}, {"begin": 11375, "end": 11573, "idx": 56}, {"begin": 11574, "end": 11669, "idx": 57}, {"begin": 11781, "end": 11948, "idx": 58}, {"begin": 11949, "end": 12174, "idx": 59}, {"begin": 12239, "end": 12447, "idx": 60}, {"begin": 12448, "end": 12635, "idx": 61}, {"begin": 12636, "end": 12838, "idx": 62}, {"begin": 12839, "end": 13008, "idx": 63}, {"begin": 13009, "end": 13209, "idx": 64}, {"begin": 13210, "end": 13290, "idx": 65}, {"begin": 13336, "end": 13508, "idx": 66}, {"begin": 13509, "end": 13698, "idx": 67}, {"begin": 13699, "end": 13813, "idx": 68}, {"begin": 13814, "end": 13973, "idx": 69}, {"begin": 13974, "end": 14080, "idx": 70}, {"begin": 14081, "end": 14229, "idx": 71}, {"begin": 14230, "end": 14568, "idx": 72}, {"begin": 14569, "end": 14575, "idx": 73}, {"begin": 14576, "end": 14672, "idx": 74}, {"begin": 14673, "end": 14904, "idx": 75}, {"begin": 14905, "end": 15057, "idx": 76}, {"begin": 15058, "end": 15212, "idx": 77}, {"begin": 15213, "end": 15259, "idx": 78}, {"begin": 15260, "end": 15444, "idx": 79}, {"begin": 15445, "end": 15467, "idx": 80}, {"begin": 15468, "end": 15524, "idx": 81}, {"begin": 15525, "end": 15605, "idx": 82}, {"begin": 15606, "end": 15744, "idx": 83}, {"begin": 15745, "end": 15902, "idx": 84}, {"begin": 15903, "end": 16167, "idx": 85}, {"begin": 16238, "end": 16360, "idx": 86}, {"begin": 16361, "end": 16613, "idx": 87}, {"begin": 16614, "end": 16768, "idx": 88}, {"begin": 16769, "end": 16961, "idx": 89}, {"begin": 16962, "end": 17123, "idx": 90}, {"begin": 17136, "end": 17178, "idx": 91}, {"begin": 17179, "end": 17320, "idx": 92}, {"begin": 17321, "end": 17524, "idx": 93}, {"begin": 17525, "end": 17611, "idx": 94}, {"begin": 17612, "end": 17798, "idx": 95}, {"begin": 17799, "end": 17978, "idx": 96}, {"begin": 17979, "end": 18191, "idx": 97}, {"begin": 18192, "end": 18289, "idx": 98}, {"begin": 18290, "end": 18409, "idx": 99}, {"begin": 18410, "end": 18590, "idx": 100}, {"begin": 18591, "end": 18872, "idx": 101}, {"begin": 18873, "end": 18950, "idx": 102}, {"begin": 18951, "end": 19048, "idx": 103}, {"begin": 19049, "end": 19051, "idx": 104}, {"begin": 19052, "end": 19254, "idx": 105}, {"begin": 19255, "end": 19329, "idx": 106}, {"begin": 19330, "end": 19425, "idx": 107}, {"begin": 19426, "end": 19552, "idx": 108}, {"begin": 19553, "end": 19680, "idx": 109}, {"begin": 19712, "end": 19930, "idx": 110}, {"begin": 19931, "end": 20038, "idx": 111}, {"begin": 20039, "end": 20204, "idx": 112}, {"begin": 20205, "end": 20254, "idx": 113}, {"begin": 20255, "end": 20335, "idx": 114}, {"begin": 20336, "end": 20512, "idx": 115}, {"begin": 20513, "end": 20871, "idx": 116}, {"begin": 20872, "end": 20987, "idx": 117}, {"begin": 20988, "end": 21106, "idx": 118}, {"begin": 21107, "end": 21219, "idx": 119}, {"begin": 21220, "end": 21455, "idx": 120}, {"begin": 21456, "end": 21685, "idx": 121}, {"begin": 21686, "end": 21870, "idx": 122}, {"begin": 21940, "end": 22082, "idx": 123}, {"begin": 22083, "end": 22144, "idx": 124}, {"begin": 22145, "end": 22314, "idx": 125}, {"begin": 22315, "end": 22498, "idx": 126}, {"begin": 22499, "end": 22715, "idx": 127}, {"begin": 22716, "end": 22888, "idx": 128}, {"begin": 22889, "end": 23078, "idx": 129}, {"begin": 23079, "end": 23268, "idx": 130}, {"begin": 23269, "end": 23468, "idx": 131}, {"begin": 23469, "end": 23778, "idx": 132}, {"begin": 23779, "end": 24012, "idx": 133}, {"begin": 24013, "end": 24126, "idx": 134}, {"begin": 24127, "end": 24286, "idx": 135}, {"begin": 24287, "end": 24406, "idx": 136}, {"begin": 24407, "end": 24454, "idx": 137}, {"begin": 24455, "end": 24594, "idx": 138}, {"begin": 24595, "end": 24801, "idx": 139}, {"begin": 24802, "end": 24895, "idx": 140}, {"begin": 24896, "end": 25130, "idx": 141}, {"begin": 25131, "end": 25267, "idx": 142}, {"begin": 25268, "end": 25618, "idx": 143}, {"begin": 25619, "end": 25728, "idx": 144}, {"begin": 25729, "end": 25954, "idx": 145}, {"begin": 25955, "end": 26044, "idx": 146}, {"begin": 26060, "end": 26193, "idx": 147}, {"begin": 26194, "end": 26395, "idx": 148}, {"begin": 26396, "end": 26599, "idx": 149}, {"begin": 26600, "end": 26795, "idx": 150}, {"begin": 26796, "end": 26996, "idx": 151}, {"begin": 26997, "end": 27223, "idx": 152}, {"begin": 27224, "end": 27443, "idx": 153}, {"begin": 27444, "end": 27679, "idx": 154}, {"begin": 27680, "end": 27745, "idx": 155}, {"begin": 27746, "end": 27991, "idx": 156}, {"begin": 28007, "end": 28130, "idx": 157}, {"begin": 28131, "end": 28401, "idx": 158}, {"begin": 28402, "end": 28669, "idx": 159}], "ReferenceToFigure": [{"begin": 6605, "end": 6608, "target": "#fig_0", "idx": 0}, {"begin": 8536, "end": 8538, "target": "#fig_0", "idx": 1}, {"begin": 9943, "end": 9945, "target": "#fig_0", "idx": 2}, {"begin": 15899, "end": 15900, "target": "#fig_2", "idx": 3}, {"begin": 19927, "end": 19928, "target": "#fig_4", "idx": 4}, {"begin": 21103, "end": 21104, "idx": 5}, {"begin": 21215, "end": 21217, "idx": 6}, {"begin": 21565, "end": 21574, "idx": 7}, {"begin": 25480, "end": 25481, "target": "#fig_7", "idx": 8}, {"begin": 25615, "end": 25616, "target": "#fig_8", "idx": 9}], "Abstract": [{"begin": 102, "end": 1673, "idx": 0}], "SectionFootnote": [{"begin": 28671, "end": 28681, "idx": 0}], "ReferenceString": [{"begin": 28698, "end": 28934, "id": "b0", "idx": 0}, {"begin": 28936, "end": 29078, "id": "b1", "idx": 1}, {"begin": 29082, "end": 29189, "id": "b2", "idx": 2}, {"begin": 29193, "end": 29289, "id": "b3", "idx": 3}, {"begin": 29293, "end": 29418, "id": "b4", "idx": 4}, {"begin": 29422, "end": 29638, "id": "b5", "idx": 5}, {"begin": 29642, "end": 29817, "id": "b6", "idx": 6}, {"begin": 29821, "end": 30112, "id": "b7", "idx": 7}, {"begin": 30116, "end": 30310, "id": "b8", "idx": 8}, {"begin": 30314, "end": 30431, "id": "b9", "idx": 9}, {"begin": 30435, "end": 30579, "id": "b10", "idx": 10}, {"begin": 30583, "end": 30728, "id": "b11", "idx": 11}, {"begin": 30732, "end": 30863, "id": "b12", "idx": 12}, {"begin": 30867, "end": 31131, "id": "b13", "idx": 13}, {"begin": 31135, "end": 31363, "id": "b14", "idx": 14}, {"begin": 31367, "end": 31544, "id": "b15", "idx": 15}, {"begin": 31548, "end": 31795, "id": "b16", "idx": 16}, {"begin": 31799, "end": 31984, "id": "b17", "idx": 17}, {"begin": 31988, "end": 32297, "id": "b18", "idx": 18}]}}