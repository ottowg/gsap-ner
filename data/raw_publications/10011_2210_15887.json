{"text": "NONPARALLEL HIGH-QUALITY AUDIO SUPER RESOLUTION WITH DOMAIN ADAPTATION AND RESAMPLING CYCLEGANS\n\nAbstract:\nNeural audio super-resolution models are typically trained on low-and high-resolution audio signal pairs. Although these methods achieve highly accurate super-resolution if the acoustic characteristics of the input data are similar to those of the training data, challenges remain: the models suffer from quality degradation for out-of-domain data, and paired data are required for training. To address these problems, we propose Dual-CycleGAN, a high-quality audio super-resolution method that can utilize unpaired data based on two connected cycle consistent generative adversarial networks (CycleGAN). Our method decomposes the super-resolution method into domain adaptation and resampling processes to handle acoustic mismatch in the unpaired low-and high-resolution signals. The two processes are then jointly optimized within the CycleGAN framework. Experimental results verify that the proposed method significantly outperforms conventional methods when paired data are not available. Code and audio samples are available from https: //chomeyama.github.io/DualCycleGAN-Demo/.\n\nMain:\n\n\n\n1. INTRODUCTION\nAudio super-resolution (SR) (also called bandwidth extension) is a technique used to predict a high-resolution (HR) audio signal (e.g., at 48 kHz) from a low-resolution (LR) signal (e.g., at 16 kHz). The potential applications of audio SR are wide-ranging and include refining old audio or video recordings, enhancement of speech generated by a text-to-speech (TTS) systems [1], and restoring frequency-bands missing from low-resolution data in recognition tasks; other uses include automatic speech recognition [2] [3] [4], speaker recognition [5], speaker identification, and speaker verification [6, 7].\nRecent neural audio SR models based on generative adversarial networks (GANs) [1, 8], normalizing flows [9], and diffusion probabilistic models [10, 11] were all trained on the pairs of corresponding LR and HR audio signals. Although these methods achieve accurate SR when the acoustic characteristics of the input data are similar to those of training data, they suffer from quality degradation for the out-of-domain data. This is problematic in the real-world applications to which SR is applied outside the training *Work performed during an internship at LINE Corporation.\ndomain, e.g., speech recorded in different acoustic environments and synthesized speech from TTS systems.\nOne possible way to solve this problem is to train SR models using data from the target domain to familiarize the model with it. However, conventional SR methods [1, [8] [9] [10] [11] [12] [13] [14] require pairs of LR and HR data (i.e., parallel data), and HR audio signals from the target domain are generally unavailable. Therefore, it is inherently difficult to solve these out-of-domain issues using conventional methods, and thus its applications are limited.\nTo address these problems, we propose Dual-CycleGAN, a high-quality nonparallel audio SR method based on two connected cycle consistent generative adversarial networks (Cy-cleGAN) [15]. In contrast to the conventional methods, Dual-CycleGAN uses both nonparallel and parallel data and learns an implicit mapping between the LR and HR signals, even when HR signals are unavailable for the target domain. Furthermore, we decompose SR into domain adaptation and resampling (i.e., up and downsampling processes) to solve the mismatch between the acoustic characteristics of the unpaired LR and HR signals. These two processes are modeled using two connected CycleGANs and are optimized via a two-stage training approach for stable optimization: joint pre-training and fine-tuning. Experimental results show that the Dual-CycleGAN outperforms the conventional methods under two challenging SR conditions: 1) SR across different recording environments and 2) SR on TTS-generated speech.\nNote that there is a concurrent study that aims to solve for domain adaptation and SR jointly based on two time-domain Cycle-GANs for speaker verification [7]. The researchers investigated several optimization strategies and applied their method successfully to upsampling of telephone signals from 8 kHz to 16 kHz. We tackle more challenging SR conditions, i.e., upsampling of 16 kHz signals to 48 kHz for high-quality SR, which is essential for applications such as enhancement of TTS-generated speech.\n\n2. PROPOSED METHOD\n\n\n2.1. Overview\nFigure 1 shows an overview of the proposed Dual-CycleGAN. Our method comprises two components: domain adaptation and resampling, where each is based on a CycleGAN [15]. We denote the generator and discriminator pairs for the domain adaptation CycleGAN by {G 1 , G 2 } and {D 1 , D 2 }, and the corresponding pairs for the resampling CycleGAN are {G 3 , G 4 } and {D 2 , D 3 }, respectively. Note that the two CycleGANs share D 2 to save their parameters.\nThe resampling CycleGAN learns the conversion between LR and HR speech waveforms in the same domain T , where parallel data are available. We define the LR and HR domains of T as T LR and T HR , respectively. To generate a high quality HR speech waveform from LR speech from the different domain S LR , the domain adaptation CycleGAN learns mappings between S LR and T LR . The inference is performed as the composition mapping S LR \u2192 T LR \u2192 T HR by G 3 (G 1 (\u2022)).\n\n2.2. Two-stage optimization\n\n\n2.2.1. Joint pre-training\nIn the joint pre-training process, the two conversions S LR \u2194 T LR and T LR \u2194 T HR are trained simultaneously following the general CycleGAN training approach based on least-squares GANs [16]. In the domain adaptation CycleGAN, D 1 learns to identify the generated samples from G 2 as fake and the ground-truth (GT) samples x \u2208 S LR as real, while D 2 learns to identify the generated samples from G 1 as fake and the GT samples z \u2208 T LR as real. In the resampling CycleGAN, D 2 learns to identify the generated samples from G 4 as fake and the GT samples z \u2208 T LR as real, while D 3 learns to identify the generated samples from G 3 as fake, and the GT samples y \u2208 T HR as real.\nThe generators learn to minimize the weighted sum of three losses: the adversarial loss L adv , the cycle-consistency loss Lcyc, and the identity mapping loss L idt . These losses are formulated as follows:L adv (G 1 , G 2 , G 3 , G 4 ; D 1 , D 2 , D 3 ) = Ex,z (1 \u2212 D 1 (G 2 (z))) 2 + (1 \u2212 D 2 (G 1 (x))) 2 + Ey,z (1 \u2212 D 2 (G 4 (y))) 2 + (1 \u2212 D 3 (G 3 (z))) 2 ,\n(1)Lcyc(G 1 , G 2 , G 3 , G 4 ) = Ex,z [ x \u2212 G 2 (G 1 (x)) 1 + z \u2212 G 1 (G 2 (z)) 1 ] + Ey,z [ y \u2212 G 3 (G 4 (y)) 1 + z \u2212 G 4 (G 3 (z)) 1 ] ,L idt (G 1 , G 2 , G 3 , G 4 ) = Ex,z [ x \u2212 G 2 (x) 1 + z \u2212 G 1 (z) 1 ] + Ey,z [ y \u2212 G 3 (z) 1 + z \u2212 G 4 (y) 1 ] .\nWe set the weights of L adv , Lcyc and L idt empirically to 1, 10, and 10, respectively.\nThe discriminators learn to identify the fake and real samples by learning to minimize the sum of two adversarial losses: the fake loss L fake and the real loss L real .L fake (D 1 , D 2 , D 3 ; G 1 , G 2 , G 3 , G 4 ) = Ex,z D 1 (G 2 (z)) 2 + D 2 (G 1 (x)) 2 / 2 + Ey,z D 2 (G 4 (y)) 2 / 2 + D 3 (G 3 (z)) 2 ,L real (D 1 , D 2 , D 3 ) (5) = Ex,y,z (1 \u2212 D 1 (x)) 2 + (1 \u2212 D 2 (z)) 2 + (1 \u2212 D 3 (y)) 2 .\nAs described in Section 2.2.2, we can optimize the entire network in an end-to-end manner, but joint pre-training makes the entire training process stable.\n\n2.2.2. Fine-tuning\nThe fine-tuning process enables a direct optimization of the composite mappings S LR \u2192 T LR \u2192 T HR and its inverse T HR \u2192 T LR \u2192 S LR to ease the effect of the domain shift between the real and fake samples of T LR , induced by the imperfect domain adaptation performed by G 1 .\nWe rewrite three of the loss functions used in the joint pretraining process. In Equations ( 1) and ( 4), the generated samples G 2 (z) and G 3 (z) are replaced by G 2 (G 4 (y)) and G 3 (G 1 (x)), respectively. Furthermore, Equation ( 2) is replaced with a new cycle-consistency loss that considers the entire cycle between the four generators and is defined as follows:Lcyc(G 1 , G 2 , G 3 , G 4 ) = Ex [ x \u2212 G 2 (G 4 (G 3 (G 1 (x)))) 1 ] + Ex [ G 1 (x) \u2212 G 4 (G 3 (G 1 (x))) 1 ] + Ey [ y \u2212 G 3 (G 1 (G 2 (G 4 (y)))) 1 ] + Ey [ G 4 (y) \u2212 G 1 (G 2 (G 4 (y))) 1 ] . (6)\nThe fine-tuning is performed as the same manner as the joint pretraining process, except that the three newly defined loss functions are used in this case.\n\n2.3. Perceptual cycle consistency\nIt is possible to use the waveform-domain L1 distance to compute the cycle-consistency loss functions [17, 18]. However, because there are multiple possible HR waveforms for what corresponds to the same LR waveform due to the ill-posed nature of SR, the waveform-domain L1 loss would make it difficult for the CycleGANs to learn cycle-consistent functions that should be bijections [15]. To address this issue, we compute a perceptually meaningful cycle-consistency loss based on the mel-spectrogram. Specifically, we replace the waveform-domain L1 loss in Equations (2) and ( 6) with the mel-scale time-frequency domain L1 loss. Similarly, we replace the identity mapping loss in Equation (3) with the proposed perceptual loss. Because the melspectral matching brings flexibility to the high-frequency bands, the CycleGANs can learn conversions that consider the ill-posed nature of SR implicitly.\n\n2.4. Network architecture\nAll generators of the Dual-CycleGAN have the same architecture, which is based on the WaveCycleGAN2 generator [18]. We replaced the first and last linear projection layers of the base architecture with a one-dimensional convolutional neural network (CNN) followed by gated linear units (GLUs) [19]. To perform resampling, we add a sinc-based interpolation with 151 kernel size before the first layer only for G 3 and G 4 .\nSeveral studies have shown the importance of combining multiple types of discriminators for speech generation tasks [18, 20, 21]. Inspired by these studies, we use multi-domain discriminators for the waveform and the log amplitude linear spectral domains. For the waveform domain discriminator, we use the Parallel WaveGAN's one [22]. For the spectral domain discriminator, we use the multi-band grouped discriminators of the NU-GAN [12]. Each sub-discriminator of the NU-GAN captures band-wise spectral features independently according to its unique grouping size, thus helping the generators learn all the frequency bands effectively. To balance the loss ratio of the two domains of the discriminators, we use a fully connected layer to summarize the multiple outputs from the spectral discriminators into one scalar value.\n\n3. EXPERIMENTAL EVALUATIONS\nTo evaluate the performance of the proposed Dual-CycleGAN, we conducted two experiments: SR across different recording environments and SR on TTS-generated speech. In this section, we first describe the details of the dataset and model configurations. We then describe the results from the two experiments.\n\n3.1. Experimental setups\n\n\n3.1.1. Database\nWe used the two publicly available databases LJ Speech [23] and VCTK [24] (mic2, version 0.92), as the nonparallel and parallel datasets, respectively. LJ Speech contains approximately 24 hours of data recorded by a single English female speaker. These audio signals were downsampled from 22.05 kHz to 16 kHz, and were then used as data for the domain S LR . VCTK consists of approximately 44 hours of clean speech signals recorded by 110 English speakers. The VCTK audio signals were provided at 48 kHz. We performed high-pass filtering with a cutoff frequency of 70 Hz to remove the low-frequency noise. Each audio signal was normalized to -26 dB. To prepare paired LR and HR signals for VCTK, audio signals at 16 kHz were constructed by performing downsampling using the librosa [25] resample function. Then, the paired data of the 16 kHz and 48 kHz signals were used as samples of S LR and T LR . We split the datasets into training, validation, and test sets. For LJ Speech, we used 100 and 250 utterances for the validation and test sets, respectively, and the rest were used for training. For VCTK, we split the datasets for each speaker: we used 200 and 400 utterances for the validation and test sets, respectively, and the rest were kept for training.\n\n3.1.2. Model details\nThe proposed Dual-CycleGAN model was trained for the first 400 K iterations with the joint pre-training approach and then fine-tuned for 200 K iterations, using the Adam optimizer [26] ( = 10 \u22128 , \u03b2 = [0.5, 0.999]). The identity mapping loss was used for up to 100 K iterations. We set the minibatch size to four, the length of each audio clip to 12 K time samples (0.75 s at 16 kHz), and the initial learning rates were set to 0.0002 and 0.0001 for the generators and discriminators, respectively. These learning rates were reduced by half after every 200 K iterations. We applied weight normalization [27] to all convolutional layers and clipped the gradient norms at ten for all networks.\nAs the baselines, the HiFi-GAN+ [1] and WSRGlow [9] models were used. Because these baseline models cannot use unpaired datasets, we only used the VCTK dataset for training. HiFi-GAN+ consists of a feed-forward WaveNet [28] architecture trained using a GAN-based deep feature loss. We trained the HiFi-GAN+ model using an open-source implementation 1. The model was trained for 1000 K steps, followed by an additional 100 K steps of joint training using the additive noise data augmentation [1] with the DNS challenge dataset [29]. WSRGlow uses a Glow-based generative model to perform audio SR [30]. Because the official implementation 2 does not support SR from 16 kHz to 48 kHz, we re-implemented the WSRGlow model based on the official version. The hyperparameters were the same as those described in the original paper [9], with the exception of those related to the resampling factor.\nWe also prepared a single CycleGAN [15] based nonparallel SR model to investigate the effectiveness of the explicit domain adaptation. The model architecture was the same as that of the Dual-CycleGAN, except that its generators have three more layers than the Dual-CycleGAN to match the number of parameters approximately to that of the Dual-CycleGAN. To calculate the identity mapping loss, which requires parallel data, we trained the model on the VCTK parallel data for the first 100 K iterations. We then trained it for 500 K iterations using all data.\n\n3.2. SR across different recording environments\n\n\n3.2.1. Comparison with baselines\nWe conducted subjective preference tests 3 to evaluate the performance of the proposed method. We asked 20 subjects to evaluate their preference from a randomly selected pair of two samples from the test set. Specifically, 48 kHz samples were generated from LJ Speech's 16 kHz speech using the trained SR models, and they were then used to create pairs of samples. Note that each pair consisted of samples of the proposed and baseline methods. We also included the recorded samples at 22.05 kHz for comparison. The subjects were asked to listen to the entire utterances. In total, 200 samples for each method pair were evaluated.\nTable 1 shows the results of the preference tests, which can be summarized as follows: (1) the proposed method outperformed Note that we observed that the proposed method is inferior to the 22.05 kHz recorded speech. We hypothesize that these results occurred because some of the VCTK data contain audible noise. We believe that the quality gap between the recorded and generated speech can be reduced if higher quality clean data from the T HR domain are used.\n\n3.2.2. Ablation study\nWe performed an ablation study to investigate the effectiveness of the proposed fine-tuning process and the mel-spectral-domain cyclic and identity mapping loss functions. We prepared two models for comparison; one model was jointly pre-trained for 600 K iterations, and the other model was trained with both the cycle-consistency and identity mapping losses in the waveform domain. Preference tests were performed as per Section 3.2.1 under the same test conditions. Table 2 shows the preference test results. These results show that the fine-tuning procedure improved the speech quality from the jointly pre-trained model. Furthermore, a comparison between the proposed model and the proposed model without the mel-spectral losses confirmed the importance of the mel-spectral cycle consistency and identity mapping losses.\n\n3.3. SR on TTS-generated speech\n\n\n3.3.1. TTS setup\nTo evaluate the effectiveness of the proposed method further, we performed additional experiments for SR on TTS-generated speech. In these experiments, we used the VCTK dataset only. We defined the 16 kHz TTS-generated speech as samples of the domain S LR . T LR and T HR were the domains of the 16 kHz and 48 kHz recorded samples of VCTK, respectively. The test set comprises two seen male and two seen female speakers (designated p351, p361, p363, and p364) and the four unseen speakers (designated p362, p374, p376, and s5), where the numbers of utterances by each speaker are identical. For the TTS system, we used a multi-speaker version of a high-quality end-to-end TTS model (i.e., VITS [31]). We trained the model on the 16 kHz VCTK dataset for 1000 K steps using the AdamW optimizer [32]. We also prepared another VITS model that was trained on the same corpus with the 48 kHz sampling rate. The detailed model architecture and training setups were the same as those in the original paper [31], although the upsampling factors were adjusted for the 16 kHz and 48 kHz sampling rates. To enhance the TTS-generated speech, the proposed Dual-CycleGAN was trained using both recorded and synthetic data, while the HiFi-GAN+ and WSRGlow models were trained using the recorded data only. The details of the model architecture and training configurations were the same as those in Section 3.1.2.\n\n3.3.2. Subjective listening tests\nTo analyze the performances of TTS systems with the proposed SR method, we performed five scaled mean opinion score (MOS) tests. We evaluated three TTS systems that were enhanced by HiFi-GAN+, WSRGlow, and our proposed model. We also evaluated two VITS-based TTS systems and 48 kHz clean recordings for comparison. We asked 20 subjects to judge the sound quality of 20 randomly selected speech samples from the test set. The subjects listened to the entire utterances of each sample.\nTable 3 shows the results of these MOS tests. These results showed that (1) our proposed TTS system obtained the best score of 4.51 among all the enhanced TTS systems, significantly outperforming the two baseline SR methods; and (2) a TTS system trained on LR speech (i.e., VITS trained on 16 kHz speech) can be improved significantly to match the quality of a system trained on HR data (i.e., VITS trained on 48 kHz speech).\n\n4. CONCLUSION\nThis paper introduced the Dual-CycleGAN, a nonparallel audio SR method to perform high-quality SR across different domains. We demonstrated that the Dual-CycleGAN outperformed the baseline SR methods in two challenging situations in which parallel data were unavailable. The intended future direction is to develop an any-to-one domain adaptation method to handle input speech from arbitrary domains. Moreover, investigation of crosslingual SR will be valuable for low-resource languages without sufficient amounts of studio-quality data.\n\nFootnotes:\n1: https://github.com/brentspell/hifi-gan-bwe/\n2: https://github.com/zkx06111/WSRGlow\n3: Although previous works [1, 9] used objective tests based on spectral distances, we do not adopt them for lack of target speech.\n\nReferences:\n\n- REFERENCES- J. Su, Y. Wang, A. Finkelstein, et al., \"Bandwidth Extension is All You Need,\" in Proc. ICASSP, 2021, pp. 696-700.\n\n- M. L. Seltzer and A. Acero, \"Training Wideband Acoustic Models Using Mixed-Bandwidth Training Data for Speech Recognition,\" IEEE Trans. on Audio, Speech, and Lang. Pro- cess., vol. 15, no. 1, pp. 235-245, 2007.\n\n- D. Haws and X. Cui, \"Cyclegan Bandwidth Extension Acous- tic Modeling for Automatic Speech Recognition,\" in Proc. ICASSP, 2019, pp. 6780-6784.\n\n- X. Li, V. Chebiyyam, and K. Kirchhoff, \"Speech Audio Super-Resolution for Speech Recognition,\" in Proc. Inter- speech, 2019, pp. 3416-3420.\n\n- H. Yamamoto, K. A. Lee, K. Okabe, et al., \"Speaker Aug- mentation and Bandwidth Extension for Deep Speaker Em- bedding,\" in Proc. Interspeech, 2019, pp. 406-410.\n\n- H. Miyamoto, S. Shiota, and H. Kiya, \"Application of Band- width Extension with No Learning to Data Augmentation for Speaker Verification,\" in Odyssey, 2020, pp. 446-450.\n\n- S. Kataria, J. Villalba, L. Moro-Vel\u00e1zquez, et al., \"Joint do- main adaptation and speech bandwidth extension using time- domain GANs for speaker verification,\" in Proc. Interspeech, 2022, pp. 615-619.\n\n- S. Kim and V. Sathe, \"Bandwidth extension on raw au- dio via generative adversarial networks,\" arXiv preprint arXiv:1903.09027, 2019.\n\n- K. Zhang, Y. Ren, C. Xu, et al., \"WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution,\" in Proc. Interspeech, 2021, pp. 1649-1653.\n\n- J. Lee and S. Han, \"NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling,\" in Proc. Interspeech, 2021, pp. 1634-1638.\n\n- S. Han and J. Lee, \"NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates,\" in Proc. Interspeech, 2022, pp. 4401-4405.\n\n- R. Kumar, K. Kumar, V. Anand, Y. Bengio, and A. Courville, \"NU-GAN: High resolution neural upsampling with GAN,\" arXiv preprint arXiv:2010.11362, 2020.\n\n- H. Liu, W. Choi, X. Liu, et al., \"Neural Vocoder is All You Need for Speech Super-resolution,\" in Proc. Interspeech, 2022, pp. 4227-4231.\n\n- N. Viet Anh, A. Nguyen, and A. Khong, \"TUNet: A Block- Online Bandwidth Extension Model Based On Transformers And Self-Supervised Pretraining,\" in Proc. ICASSP, 2022, pp. 161-165.\n\n- J.-Y. Zhu, T. Park, P. Isola, et al., \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,\" in Proc. ICCV, 2017, pp. 2223-2232.\n\n- X. Mao, Q. Li, H. Xie, et al., \"Least Squares Generative Ad- versarial Networks,\" in Proc. ICCV, 2017, pp. 2794-2802.\n\n- K. Tanaka, T. Kaneko, N. Hojo, and H. Kameoka, \"Synthetic- to-natural speech waveform conversion using cycle-consistent adversarial networks,\" in Proc. SLT, 2018, pp. 632-639.\n\n- K. Tanaka, H. Kameoka, T. Kaneko, and N. Hojo, \"WaveCy- cleGAN2: Time-domain Neural Post-filter for Speech Wave- form Generation,\" arXiv preprint arXiv:1904.02892, 2019.\n\n- Y. N. Dauphin, A. Fan, M. Auli, et al., \"Language modeling with gated convolutional networks,\" in Proc. ICML, 2017, pp. 933-941.\n\n- J. You, D. Kim, G. Nam, G. Hwang, and G. Chae, \"GAN Vocoder: Multi-Resolution Discriminator Is All You Need,\" in Proc. Interspeech, 2021, pp. 2177-2181.\n\n- W. Jang, D. Lim, J. Yoon, et al., \"UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High- Fidelity Waveform Generation,\" in Proc. Interspeech, 2021, pp. 2207-2211.\n\n- R. Yamamoto, E. Song, and J.-M. Kim, \"Parallel WaveGAN: A fast waveform generation model based on generative adver- sarial networks with multi-resolution spectrogram,\" in Proc. ICASSP, 2020, pp. 6199-6203.\n\n- K. Ito and L. Johnson, \"The LJ Speech Dataset,\" https: //keithito.com/LJ-Speech-Dataset/, 2017.\n\n- J. Yamagishi, C. Veaux, K. MacDonald, et al., \"CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),\" 2019.\n\n- B. McFee, A. Metsai, M. McVicar, et al., \"librosa/librosa: 0.9.1,\" Feb. 2022.\n\n- D. Kingma and J. Ba, \"Adam: A Method for Stochastic Op- timization,\" in Proc. ICLR, 2015.\n\n- T. Salimans and D. P. Kingma, \"Weight normalization: A simple reparameterization to accelerate training of deep neu- ral networks,\" in Proc. NIPS, 2016, pp. 901-909.\n\n- A. v. d. Oord, S. Dieleman, H. Zen, et al., \"WaveNet: A Generative Model for Raw Audio,\" in Proc. SSW, 2016.\n\n- C. K. Reddy, V. Gopal, R. Cutler, et al., \"The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjec- tive Testing Framework, and Challenge Results,\" in Proc. In- terspeech, 2020, pp. 2492-2496.\n\n- D. P. Kingma and P. Dhariwal, \"Glow: Generative Flow with Invertible 1x1 Convolutions,\" in Proc. NeurIPS, 2018, vol. 31.\n\n- J. Kim, J. Kong, and J. Son, \"Conditional Variational Au- toencoder with Adversarial Learning for End-to-End Text-to- Speech,\" in Proc. ICML, 2021, pp. 5530-5540.\n\n- I. Loshchilov and F. Hutter, \"Decoupled Weight Decay Reg- ularization,\" in Proc. ICLR, 2019.\n\n", "annotations": {"ReferenceToTable": [{"begin": 15174, "end": 15175, "target": "#tab_0", "idx": 0}, {"begin": 16127, "end": 16128, "target": "#tab_1", "idx": 1}, {"begin": 18452, "end": 18453, "target": "#tab_2", "idx": 2}], "ReferenceToFootnote": [{"begin": 13355, "end": 13356, "target": "#foot_0", "idx": 0}, {"begin": 13643, "end": 13644, "target": "#foot_1", "idx": 1}, {"begin": 14579, "end": 14580, "target": "#foot_2", "idx": 2}], "SectionMain": [{"begin": 1197, "end": 19425, "idx": 0}], "ReferenceToFormula": [{"begin": 7808, "end": 7809, "idx": 0}, {"begin": 7817, "end": 7818, "target": "#formula_3", "idx": 1}, {"begin": 7950, "end": 7951, "target": "#formula_1", "idx": 2}, {"begin": 9052, "end": 9053, "idx": 3}], "SectionReference": [{"begin": 19657, "end": 24533, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1197, "idx": 0}], "Div": [{"begin": 107, "end": 1189, "idx": 0}, {"begin": 1200, "end": 4457, "idx": 1}, {"begin": 4459, "end": 4478, "idx": 2}, {"begin": 4480, "end": 5413, "idx": 3}, {"begin": 5415, "end": 5443, "idx": 4}, {"begin": 5445, "end": 7415, "idx": 5}, {"begin": 7417, "end": 8439, "idx": 6}, {"begin": 8441, "end": 9373, "idx": 7}, {"begin": 9375, "end": 10649, "idx": 8}, {"begin": 10651, "end": 10985, "idx": 9}, {"begin": 10987, "end": 11012, "idx": 10}, {"begin": 11014, "end": 12291, "idx": 11}, {"begin": 12293, "end": 14453, "idx": 12}, {"begin": 14455, "end": 14503, "idx": 13}, {"begin": 14505, "end": 15629, "idx": 14}, {"begin": 15631, "end": 16477, "idx": 15}, {"begin": 16479, "end": 16511, "idx": 16}, {"begin": 16513, "end": 17926, "idx": 17}, {"begin": 17928, "end": 18871, "idx": 18}, {"begin": 18873, "end": 19425, "idx": 19}], "Head": [{"begin": 1200, "end": 1215, "n": "1.", "idx": 0}, {"begin": 4459, "end": 4477, "n": "2.", "idx": 1}, {"begin": 4480, "end": 4493, "n": "2.1.", "idx": 2}, {"begin": 5415, "end": 5442, "n": "2.2.", "idx": 3}, {"begin": 5445, "end": 5470, "n": "2.2.1.", "idx": 4}, {"begin": 7417, "end": 7435, "n": "2.2.2.", "idx": 5}, {"begin": 8441, "end": 8474, "n": "2.3.", "idx": 6}, {"begin": 9375, "end": 9400, "n": "2.4.", "idx": 7}, {"begin": 10651, "end": 10678, "n": "3.", "idx": 8}, {"begin": 10987, "end": 11011, "n": "3.1.", "idx": 9}, {"begin": 11014, "end": 11029, "n": "3.1.1.", "idx": 10}, {"begin": 12293, "end": 12313, "n": "3.1.2.", "idx": 11}, {"begin": 14455, "end": 14502, "n": "3.2.", "idx": 12}, {"begin": 14505, "end": 14537, "n": "3.2.1.", "idx": 13}, {"begin": 15631, "end": 15652, "n": "3.2.2.", "idx": 14}, {"begin": 16479, "end": 16510, "n": "3.3.", "idx": 15}, {"begin": 16513, "end": 16529, "n": "3.3.1.", "idx": 16}, {"begin": 17928, "end": 17961, "n": "3.3.2.", "idx": 17}, {"begin": 18873, "end": 18886, "n": "4.", "idx": 18}], "Paragraph": [{"begin": 107, "end": 1189, "idx": 0}, {"begin": 1216, "end": 1822, "idx": 1}, {"begin": 1823, "end": 2399, "idx": 2}, {"begin": 2400, "end": 2505, "idx": 3}, {"begin": 2506, "end": 2971, "idx": 4}, {"begin": 2972, "end": 3952, "idx": 5}, {"begin": 3953, "end": 4457, "idx": 6}, {"begin": 4494, "end": 4948, "idx": 7}, {"begin": 4949, "end": 5413, "idx": 8}, {"begin": 5471, "end": 6150, "idx": 9}, {"begin": 6151, "end": 6357, "idx": 10}, {"begin": 6514, "end": 6517, "idx": 11}, {"begin": 6768, "end": 6856, "idx": 12}, {"begin": 6857, "end": 7026, "idx": 13}, {"begin": 7260, "end": 7415, "idx": 14}, {"begin": 7436, "end": 7714, "idx": 15}, {"begin": 7715, "end": 8085, "idx": 16}, {"begin": 8284, "end": 8439, "idx": 17}, {"begin": 8475, "end": 9373, "idx": 18}, {"begin": 9401, "end": 9823, "idx": 19}, {"begin": 9824, "end": 10649, "idx": 20}, {"begin": 10679, "end": 10985, "idx": 21}, {"begin": 11030, "end": 12291, "idx": 22}, {"begin": 12314, "end": 13005, "idx": 23}, {"begin": 13006, "end": 13896, "idx": 24}, {"begin": 13897, "end": 14453, "idx": 25}, {"begin": 14538, "end": 15167, "idx": 26}, {"begin": 15168, "end": 15629, "idx": 27}, {"begin": 15653, "end": 16477, "idx": 28}, {"begin": 16530, "end": 17926, "idx": 29}, {"begin": 17962, "end": 18445, "idx": 30}, {"begin": 18446, "end": 18871, "idx": 31}, {"begin": 18887, "end": 19425, "idx": 32}], "ReferenceToBib": [{"begin": 1590, "end": 1593, "target": "#b1", "idx": 0}, {"begin": 1728, "end": 1731, "target": "#b2", "idx": 1}, {"begin": 1732, "end": 1735, "target": "#b3", "idx": 2}, {"begin": 1736, "end": 1739, "target": "#b4", "idx": 3}, {"begin": 1761, "end": 1764, "idx": 4}, {"begin": 1815, "end": 1818, "target": "#b6", "idx": 5}, {"begin": 1819, "end": 1821, "target": "#b7", "idx": 6}, {"begin": 1901, "end": 1904, "target": "#b1", "idx": 7}, {"begin": 1905, "end": 1907, "target": "#b8", "idx": 8}, {"begin": 1927, "end": 1930, "target": "#b9", "idx": 9}, {"begin": 1967, "end": 1971, "target": "#b10", "idx": 10}, {"begin": 1972, "end": 1975, "target": "#b11", "idx": 11}, {"begin": 2668, "end": 2671, "target": "#b1", "idx": 12}, {"begin": 2672, "end": 2675, "target": "#b8", "idx": 13}, {"begin": 2676, "end": 2679, "target": "#b9", "idx": 14}, {"begin": 2680, "end": 2684, "target": "#b10", "idx": 15}, {"begin": 2685, "end": 2689, "target": "#b11", "idx": 16}, {"begin": 2690, "end": 2694, "target": "#b12", "idx": 17}, {"begin": 2695, "end": 2699, "target": "#b13", "idx": 18}, {"begin": 2700, "end": 2704, "target": "#b14", "idx": 19}, {"begin": 3152, "end": 3156, "target": "#b15", "idx": 20}, {"begin": 4108, "end": 4111, "target": "#b7", "idx": 21}, {"begin": 4657, "end": 4661, "target": "#b15", "idx": 22}, {"begin": 5658, "end": 5662, "target": "#b16", "idx": 23}, {"begin": 8577, "end": 8581, "target": "#b17", "idx": 24}, {"begin": 8582, "end": 8585, "target": "#b18", "idx": 25}, {"begin": 8857, "end": 8861, "target": "#b15", "idx": 26}, {"begin": 9511, "end": 9515, "target": "#b18", "idx": 27}, {"begin": 9694, "end": 9698, "target": "#b19", "idx": 28}, {"begin": 9940, "end": 9944, "target": "#b18", "idx": 29}, {"begin": 9945, "end": 9948, "target": "#b20", "idx": 30}, {"begin": 9949, "end": 9952, "target": "#b21", "idx": 31}, {"begin": 10153, "end": 10157, "target": "#b22", "idx": 32}, {"begin": 10257, "end": 10261, "target": "#b12", "idx": 33}, {"begin": 11085, "end": 11089, "target": "#b23", "idx": 34}, {"begin": 11099, "end": 11103, "target": "#b24", "idx": 35}, {"begin": 11812, "end": 11816, "target": "#b25", "idx": 36}, {"begin": 12494, "end": 12498, "target": "#b26", "idx": 37}, {"begin": 12917, "end": 12921, "target": "#b27", "idx": 38}, {"begin": 13038, "end": 13041, "target": "#b1", "idx": 39}, {"begin": 13225, "end": 13229, "target": "#b28", "idx": 40}, {"begin": 13497, "end": 13500, "target": "#b1", "idx": 41}, {"begin": 13532, "end": 13536, "target": "#b29", "idx": 42}, {"begin": 13601, "end": 13605, "target": "#b30", "idx": 43}, {"begin": 13830, "end": 13833, "target": "#b9", "idx": 44}, {"begin": 13932, "end": 13936, "target": "#b15", "idx": 45}, {"begin": 17224, "end": 17228, "target": "#b31", "idx": 46}, {"begin": 17322, "end": 17326, "target": "#b32", "idx": 47}, {"begin": 17528, "end": 17532, "target": "#b31", "idx": 48}, {"begin": 19551, "end": 19554, "target": "#b1", "idx": 49}, {"begin": 19555, "end": 19557, "target": "#b9", "idx": 50}], "ReferenceString": [{"begin": 19672, "end": 19682, "id": "b0", "idx": 0}, {"begin": 19684, "end": 19798, "id": "b1", "idx": 1}, {"begin": 19802, "end": 20012, "id": "b2", "idx": 2}, {"begin": 20016, "end": 20158, "id": "b3", "idx": 3}, {"begin": 20162, "end": 20301, "id": "b4", "idx": 4}, {"begin": 20305, "end": 20466, "id": "b5", "idx": 5}, {"begin": 20470, "end": 20640, "id": "b6", "idx": 6}, {"begin": 20644, "end": 20845, "id": "b7", "idx": 7}, {"begin": 20849, "end": 20982, "id": "b8", "idx": 8}, {"begin": 20986, "end": 21139, "id": "b9", "idx": 9}, {"begin": 21143, "end": 21276, "id": "b10", "idx": 10}, {"begin": 21280, "end": 21422, "id": "b11", "idx": 11}, {"begin": 21426, "end": 21577, "id": "b12", "idx": 12}, {"begin": 21581, "end": 21718, "id": "b13", "idx": 13}, {"begin": 21722, "end": 21901, "id": "b14", "idx": 14}, {"begin": 21905, "end": 22061, "id": "b15", "idx": 15}, {"begin": 22065, "end": 22182, "id": "b16", "idx": 16}, {"begin": 22186, "end": 22361, "id": "b17", "idx": 17}, {"begin": 22365, "end": 22534, "id": "b18", "idx": 18}, {"begin": 22538, "end": 22666, "id": "b19", "idx": 19}, {"begin": 22670, "end": 22822, "id": "b20", "idx": 20}, {"begin": 22826, "end": 23019, "id": "b21", "idx": 21}, {"begin": 23023, "end": 23228, "id": "b22", "idx": 22}, {"begin": 23232, "end": 23327, "id": "b23", "idx": 23}, {"begin": 23331, "end": 23478, "id": "b24", "idx": 24}, {"begin": 23482, "end": 23559, "id": "b25", "idx": 25}, {"begin": 23563, "end": 23652, "id": "b26", "idx": 26}, {"begin": 23656, "end": 23821, "id": "b27", "idx": 27}, {"begin": 23825, "end": 23933, "id": "b28", "idx": 28}, {"begin": 23937, "end": 24145, "id": "b29", "idx": 29}, {"begin": 24149, "end": 24269, "id": "b30", "idx": 30}, {"begin": 24273, "end": 24435, "id": "b31", "idx": 31}, {"begin": 24439, "end": 24531, "id": "b32", "idx": 32}], "Sentence": [{"begin": 107, "end": 212, "idx": 0}, {"begin": 213, "end": 498, "idx": 1}, {"begin": 499, "end": 711, "idx": 2}, {"begin": 712, "end": 886, "idx": 3}, {"begin": 887, "end": 962, "idx": 4}, {"begin": 963, "end": 1098, "idx": 5}, {"begin": 1099, "end": 1189, "idx": 6}, {"begin": 1216, "end": 1415, "idx": 7}, {"begin": 1416, "end": 1822, "idx": 8}, {"begin": 1823, "end": 2047, "idx": 9}, {"begin": 2048, "end": 2246, "idx": 10}, {"begin": 2247, "end": 2399, "idx": 11}, {"begin": 2400, "end": 2505, "idx": 12}, {"begin": 2506, "end": 2634, "idx": 13}, {"begin": 2635, "end": 2830, "idx": 14}, {"begin": 2831, "end": 2971, "idx": 15}, {"begin": 2972, "end": 3157, "idx": 16}, {"begin": 3158, "end": 3374, "idx": 17}, {"begin": 3375, "end": 3573, "idx": 18}, {"begin": 3574, "end": 3748, "idx": 19}, {"begin": 3749, "end": 3952, "idx": 20}, {"begin": 3953, "end": 4112, "idx": 21}, {"begin": 4113, "end": 4268, "idx": 22}, {"begin": 4269, "end": 4457, "idx": 23}, {"begin": 4494, "end": 4551, "idx": 24}, {"begin": 4552, "end": 4662, "idx": 25}, {"begin": 4663, "end": 4884, "idx": 26}, {"begin": 4885, "end": 4948, "idx": 27}, {"begin": 4949, "end": 5087, "idx": 28}, {"begin": 5088, "end": 5157, "idx": 29}, {"begin": 5158, "end": 5322, "idx": 30}, {"begin": 5323, "end": 5413, "idx": 31}, {"begin": 5471, "end": 5663, "idx": 32}, {"begin": 5664, "end": 5917, "idx": 33}, {"begin": 5918, "end": 6150, "idx": 34}, {"begin": 6151, "end": 6317, "idx": 35}, {"begin": 6318, "end": 6357, "idx": 36}, {"begin": 6514, "end": 6517, "idx": 37}, {"begin": 6768, "end": 6856, "idx": 38}, {"begin": 6857, "end": 7026, "idx": 39}, {"begin": 7260, "end": 7415, "idx": 40}, {"begin": 7436, "end": 7714, "idx": 41}, {"begin": 7715, "end": 7792, "idx": 42}, {"begin": 7793, "end": 7925, "idx": 43}, {"begin": 7926, "end": 8085, "idx": 44}, {"begin": 8284, "end": 8439, "idx": 45}, {"begin": 8475, "end": 8586, "idx": 46}, {"begin": 8587, "end": 8862, "idx": 47}, {"begin": 8863, "end": 8975, "idx": 48}, {"begin": 8976, "end": 9104, "idx": 49}, {"begin": 9105, "end": 9203, "idx": 50}, {"begin": 9204, "end": 9373, "idx": 51}, {"begin": 9401, "end": 9516, "idx": 52}, {"begin": 9517, "end": 9699, "idx": 53}, {"begin": 9700, "end": 9823, "idx": 54}, {"begin": 9824, "end": 9953, "idx": 55}, {"begin": 9954, "end": 10079, "idx": 56}, {"begin": 10080, "end": 10158, "idx": 57}, {"begin": 10159, "end": 10262, "idx": 58}, {"begin": 10263, "end": 10460, "idx": 59}, {"begin": 10461, "end": 10649, "idx": 60}, {"begin": 10679, "end": 10842, "idx": 61}, {"begin": 10843, "end": 10930, "idx": 62}, {"begin": 10931, "end": 10985, "idx": 63}, {"begin": 11030, "end": 11181, "idx": 64}, {"begin": 11182, "end": 11276, "idx": 65}, {"begin": 11277, "end": 11388, "idx": 66}, {"begin": 11389, "end": 11486, "idx": 67}, {"begin": 11487, "end": 11534, "idx": 68}, {"begin": 11535, "end": 11635, "idx": 69}, {"begin": 11636, "end": 11679, "idx": 70}, {"begin": 11680, "end": 11835, "idx": 71}, {"begin": 11836, "end": 11930, "idx": 72}, {"begin": 11931, "end": 11994, "idx": 73}, {"begin": 11995, "end": 12125, "idx": 74}, {"begin": 12126, "end": 12291, "idx": 75}, {"begin": 12314, "end": 12529, "idx": 76}, {"begin": 12530, "end": 12592, "idx": 77}, {"begin": 12593, "end": 12812, "idx": 78}, {"begin": 12813, "end": 12884, "idx": 79}, {"begin": 12885, "end": 13005, "idx": 80}, {"begin": 13006, "end": 13075, "idx": 81}, {"begin": 13076, "end": 13179, "idx": 82}, {"begin": 13180, "end": 13287, "idx": 83}, {"begin": 13288, "end": 13357, "idx": 84}, {"begin": 13358, "end": 13537, "idx": 85}, {"begin": 13538, "end": 13606, "idx": 86}, {"begin": 13607, "end": 13754, "idx": 87}, {"begin": 13755, "end": 13896, "idx": 88}, {"begin": 13897, "end": 14031, "idx": 89}, {"begin": 14032, "end": 14248, "idx": 90}, {"begin": 14249, "end": 14397, "idx": 91}, {"begin": 14398, "end": 14453, "idx": 92}, {"begin": 14538, "end": 14632, "idx": 93}, {"begin": 14633, "end": 14746, "idx": 94}, {"begin": 14747, "end": 14902, "idx": 95}, {"begin": 14903, "end": 14981, "idx": 96}, {"begin": 14982, "end": 15048, "idx": 97}, {"begin": 15049, "end": 15108, "idx": 98}, {"begin": 15109, "end": 15167, "idx": 99}, {"begin": 15168, "end": 15384, "idx": 100}, {"begin": 15385, "end": 15480, "idx": 101}, {"begin": 15481, "end": 15629, "idx": 102}, {"begin": 15653, "end": 15824, "idx": 103}, {"begin": 15825, "end": 16035, "idx": 104}, {"begin": 16036, "end": 16120, "idx": 105}, {"begin": 16121, "end": 16163, "idx": 106}, {"begin": 16164, "end": 16277, "idx": 107}, {"begin": 16278, "end": 16477, "idx": 108}, {"begin": 16530, "end": 16659, "idx": 109}, {"begin": 16660, "end": 16712, "idx": 110}, {"begin": 16713, "end": 16787, "idx": 111}, {"begin": 16788, "end": 16883, "idx": 112}, {"begin": 16884, "end": 17120, "idx": 113}, {"begin": 17121, "end": 17230, "idx": 114}, {"begin": 17231, "end": 17327, "idx": 115}, {"begin": 17328, "end": 17430, "idx": 116}, {"begin": 17431, "end": 17621, "idx": 117}, {"begin": 17622, "end": 17819, "idx": 118}, {"begin": 17820, "end": 17926, "idx": 119}, {"begin": 17962, "end": 18090, "idx": 120}, {"begin": 18091, "end": 18187, "idx": 121}, {"begin": 18188, "end": 18276, "idx": 122}, {"begin": 18277, "end": 18382, "idx": 123}, {"begin": 18383, "end": 18445, "idx": 124}, {"begin": 18446, "end": 18491, "idx": 125}, {"begin": 18492, "end": 18871, "idx": 126}, {"begin": 18887, "end": 19010, "idx": 127}, {"begin": 19011, "end": 19157, "idx": 128}, {"begin": 19158, "end": 19287, "idx": 129}, {"begin": 19288, "end": 19425, "idx": 130}], "ReferenceToFigure": [{"begin": 4501, "end": 4502, "target": "#fig_0", "idx": 0}], "Abstract": [{"begin": 97, "end": 1189, "idx": 0}], "SectionFootnote": [{"begin": 19427, "end": 19655, "idx": 0}], "Footnote": [{"begin": 19438, "end": 19484, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 19485, "end": 19523, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 19524, "end": 19655, "id": "foot_2", "n": "3", "idx": 2}]}}