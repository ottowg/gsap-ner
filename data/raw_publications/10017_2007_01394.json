{"text": "Robust Linear Regression: Optimal Rates in Polynomial Time\n\nAbstract:\nWe obtain robust and computationally efficient estimators for learning several linear models that achieve statistically optimal convergence rate under minimal distributional assumptions. Concretely, we assume our data is drawn from a k-hypercontractive distribution and an \u01eb-fraction is adversarially corrupted. We then describe an estimator that converges to the optimal least-squares minimizer for the true distribution at a rate proportional to \u01eb 2\u22122/k , when the noise is independent of the covariates. We note that no such estimator was known prior to our work, even with access to unbounded computation. The rate we achieve is informationtheoretically optimal and thus we resolve the main open question in Klivans, Kothari and Meka [COLT'18].\nOur key insight is to identify an analytic condition that serves as a polynomial relaxation of independence of random variables. In particular, we show that when the moments of the noise and covariates are negatively-correlated, we obtain the same rate as independent noise. Further, when the condition is not satisfied, we obtain a rate proportional to \u01eb 2\u22124/k , and again match the information-theoretic lower bound. Our central technical contribution is to algorithmically exploit independence of random variables in the \"sum-of-squares\" framework by formulating it as the aforementioned polynomial inequality.\n\nMain:\n\n\n\n1 Introduction\nWhile classical statistical theory has focused on designing statistical estimators assuming access to i.i.d. samples from a nice distribution, estimation in the presence of adversarial outliers has been a challenging problem since it was formalized by Huber [Hub64]. A long and influential line of work in high-dimensional robust statistics has since focused on studying the trade-off between sample complexity, accuracy and more recently, computational complexity for basic tasks such as estimating mean, covariance [LRV16, DKK + 16, CSV17, KS17b, SCV17, CDG19, DKK + 17, DKK + 18a, CDGW19], moment tensors of distributions [KS17b] and regression [DKS17, KKM18b, DKK + 19, PSBR20, KKK19, RY20a].\nRegression continues to be extensively studied under various models, including realizable regression (no noise), true linear models (independent noise), asymmetric noise, agnostic regression and generalized linear models (see [Wei05] and references therein). In each model, a variety of distributional assumptions are considered over the covariates and the noise. As a consequence, there exist innumerable estimators for regression achieving various trade-offs between sample complexity, running time and rate of convergence. The presence of adversarial outliers adds yet another dimension to design and compare estimators.\nSeminal works on robust regression focused on designing non-convex loss functions, including M-estimators [Hub11], Theil-Sen estimators [The92, Sen68], R-estimators [Jae72], Least-Median-Squares [Rou84] and S-estimators [RY84]. These estimators have desirable statistical properties under disparate assumptions, yet remain computationally intractable in high dimensions. Further, recent works show that it is information-theoretically impossible to design robust estimators for linear regression without distributional assumptions [KKM18b].\nAn influential recent line of work showed that when the data is drawn from the well studied and highly general class of hypercontractive distributions (see Definition 1.3), there exist robust and computationally efficient estimators for regression [KKM18b, PSBR20, DKS19]. Several families of natural distributions fall into this category, including Gaussians, strongly log-concave distributions and product distributions on the hypercube. However, both estimators converge to the the true hyperplane (in \u2113 2 -norm) at a sub-optimal rate, as a function of the fraction of corrupted points.\nGiven the vast literature on ad-hoc and often incomparable estimators for high-dimensional robust regression, the central question we address in this work is as follows: Does there exist a unified approach to design robust and computationally efficient estimators achieving optimal rates for all linear regression models under mild distributional assumptions?\nWe address the aforementioned question by introducing a framework to design robust estimators for linear regression when the input is drawn from a hypercontractive distribution. Our estimators converge to the true hyperplanes at the information-theoretically optimal rate (as a function of the fraction of corrupted data) under various well-studied noise models, including independent and agnostic noise. Further, we show that our estimators can be computed in polynomial time using the sum-of-squares convex hierarchy.\nWe note that, despite decades of progress, prior to our work, estimators achieving optimal convergence rate in terms of the fraction of corrupted points were not known, even with independent noise and access to unbounded computation.\n\n1.1 Our Results\nWe begin by formalizing the regression model we work with. In classical regression, we assume D is a distribution over \u00ca d \u00d7 \u00ca and for a vector \u0398 \u2208 \u00ca d , the least-squares loss is given by err D (\u0398) = Ex,y\u223cD y \u2212 x \u22a4 \u0398 2 . The goal is to learn \u0398 * = arg min \u0398 err D (\u0398). We assume sample access to D, and given n i.i.d. samples, we want to obtain a vector \u0398 that approximately achieves optimal error, err D (\u0398 * ).\nIn contrast to the classical setting, we work in the strong contamination model. Here, an adversary has access to the input samples and is allowed to corrupt an \u01eb-fraction arbitrarily. Note, the adversary has access to unbounded computation and has knowledge of the estimators we design. We note that this is the most stringent corrupt model and captures Huber contamination, additive corruption, label noise, agnostic learning etc (see [DK19]). Formally, Model 1.1 (Robust Regression Model). Let D be a distribution over \u00ca d \u00d7 R such that the marginal dis- tribution over \u00ca d is centered and has covariance \u03a3 * and let \u0398 * = arg min \u0398 Ex,y\u223cD (y \u2212 \u0398, x ) 2 be the optimal hyperplane for D. Let {(x * 1 , y * 1 ), (x * 2 , y * 2 ), . . . (x * n , y * n )} be n i.i.d. random variables drawn from D. Given \u01eb > 0, the robust regression model R D (\u01eb, \u03a3 * , \u0398 * ) outputs a set of n samples {(x 1 , y 1 ), . . . (x n , y n )} such that for at least (1 \u2212 \u01eb)n points x i = x * i and y i = y * i . The remaining \u01ebn points are arbitrary, and potentially adversarial w.r.t. the input and estimator.\nA natural starting point is to assume that the marginal distribution over the covariates (the x's above) is heavy-tailed and has bounded, finite covariance. However, we show that there is no robust estimator in this setting, even when the linear model has no noise and the uncorrupted points lie on a line. Theorem 1.2 (Bounded Covariance does not suffice, Theorem 7.1 informal). For all \u01eb > 0, there exist two distributions D 1 , D 2 over \u00ca d \u00d7 \u00ca such that the marginal distribution over the covariates has bounded covariance, denoted by \u03a3 2 = \u0398(1), yet \u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = \u2126(1), where \u0398 1 and \u0398 2 are the optimal hyperplanes for D 1 and D 2 .\nThe aforementioned result precludes any statistical estimator that converges to the true hyperplane as the fraction of corrupted points tends to 0. Therefore, we strengthen the distributional assumption and hypercontractive distributions instead.\n\nDefinition 1.3 ((C, k)-Hypercontractivity\n). A distribution D over R d is (C, k)-hypercontractive for an even integer k 4, if for all r \u2208 [k/2], for all v \u2208 R d , E x\u223cD v, x \u2212 E [x] 2r E x\u223cD C v, x \u2212 E [x]In this work we focus on the rate of convergence of our estimators to the true hyperplane, \u0398 * , as a function of the fraction of corrupted points, denoted by \u01eb. We measure convergence in both parameter distance (\u2113 2 -distance between the hyperplanes) and least-squares error on the true distribution (err D ).\nWe introduce a simple analytic condition on the relationship between the noise (marginal distribution over y \u2212 x \u22a4 \u0398 * ) and covariates (marginal distribution over x) that can be considered as a proxy for independence of y \u2212 x \u22a4 \u0398 * and x : Definition 1.5 (Negatively Correlated Moments). Given a distribution D over \u00ca d \u00d7 \u00ca, such that the marginal distribution on \u00ca d is (c k , k)-hypercontractive, the corresponding regression instance has negatively correlated moments if for all r k, and for all v,E x,y\u223cD v, x r y \u2212 x \u22a4 \u0398 * r O(1) E x\u223cD v, x r E x,y\u223cD y \u2212 x \u22a4 \u0398 * r\nInformally, the negatively correlated moments condition can be viewed as a polynomial relaxation of independence of random variables. Note, it is easy to see that when the noise is independent of the covariates, the above definition is satisfied.\nRemark 1.6. We show that when this condition is satisfied by the true distribution, D, we obtain rates that match the information theoretically optimal rate in a true linear model, where the noise (marginal distribution over y \u2212 x \u22a4 \u0398 * ) is independent of the covariates (marginal distribution over x). Further, when this condition is not satisfied, we show that there exist distributions for which obtaining rates matching the true linear model is impossible.\nWhen the distribution over the input is hypercontractive and has negatively correlated moments, we obtain an estimator achieving rate proportional to \u01eb 1\u22121/k for parameter recovery. Further, our estimator can be computed efficiently. Thus, our main algorithmic result is as follows: Theorem 1.7 (Robust Regresssion with Negatively Correlated Noise, Theorem 5.1 informal). Given \u01eb > 0, k 4, and n (d log(d)) O(k) samples from R D (\u01eb, \u03a3 * , \u0398 * ), such that D is (c, k)-certifiably hypercontractive and has negatively correlated moments, there exists an algorithm that runs in n O(k) time and outputs an estimator \u0398 such that with high probability,(\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 O \u01eb 1\u22121/k err D (\u0398 * ) 1/2 and, err D ( \u0398) 1 + O \u01eb 2\u22122/k err D (\u0398 * )\nRemark 1.8. We note that prior work does not draw a distinction between the independent and dependent noise models. In comparison (see Table 1), Klivans, Kothari and Meka [KKM18b] obtained a sub-optimal least-squares error scales proportional to \u01eb 1\u22122/k . For the special case of k = 4, Prasad et. al. [PSBR20] obtain least squares error proportional to O(\u01eb\u03ba 2 (\u03a3)), where \u03ba is the condition number. In very recent independent work Zhu, Jiao and Steinhardt [ZJS20] obtained a sub-optimal least-squares error scales proportional to \u01eb 2\u22124/k .\nFurther, we show that the rate we obtained in Theorem 1.7 is information-theoretically optimal, even when the noise and covariates are independent:[KKM18b] \u01eb 1\u22122/k \u01eb 1\u22122/k Zhu, Jiao and Steinhardt [ZJS20] \u01eb 2\u22124/k \u01eb 2\u22124/k Our Work Thm 1.7, Cor 1.10 \u01eb 2\u22122/k \u01eb 2\u22124/k\nLower Bounds Thm 1.9, Thm 1.11\u01eb 2\u22122/k \u01eb 2\u22124/k(c k , k)- hypercontractive.\nTheorem 1.9 (Lower Bound for Independent Noise, Theorem 6.1 informal ). For any \u01eb > 0, there exist two distributions D 1 , D 2 over \u00ca 2 \u00d7 \u00ca such that the marginal distribution over \u00ca 2 has covariance \u03a3 and is (c, k)-hypercontractive for both distributions, and yet\u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = \u2126 \u01eb 1\u22121/k \u03c3 , where \u0398 1 ,\n\u0398 2 are the optimal hyperplanes for D 1 and D 2 respectively, \u03c3 = max(errD 1 (\u0398 1 ), err D 2 (\u0398 2 )) and the noise is uniform over [\u2212\u03c3, \u03c3]. Further, |err D 1 (\u0398 2 ) \u2212 err D 1 (\u0398 1 )| = \u2126 \u01eb 2\u22122/k \u03c3 2 .\nNext, we consider the setting where the noise is allowed to arbitrary, and need not have negatively correlated moments with the covariates. A simple modification to our algorithm and analysis yields an efficient estimator that obtains rate proportional to \u01eb 1\u22122/k for parameter recovery.\nCorollary 1.10 (Robust Regresssion with Dependent Noise, Corollary 4.2 informal). Given \u01eb > 0, k 4 and n (d log(d)) O(k) samples from R D (\u01eb, \u03a3 * , \u0398 * ), such that D is (c, k)-certifiably hypercontractive, there exists an algorithm that runs in n O(k) time and outputs an estimator \u0398 such that with probability 9/10,(\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 O \u01eb 1\u22122/k err D (\u0398 * ) 1/2 and, err D ( \u0398) 1 + O \u01eb 2\u22124/k err D (\u0398 * )\nFurther, we show that the dependence on \u01eb is again information-theoretically optimal:\nTheorem 1.11 (Lower Bound for Dependent Noise, Theorem 6.2 informal). For any \u01eb > 0, there exist two distributions D 1 , D 2 over \u00ca 2 \u00d7 \u00ca such that the marginal distribution over \u00ca 2 has covariance \u03a3 and is (c, k)-hypercontractive for both distributions, and yet\u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = \u2126 \u01eb 1\u22122/k \u03c3 , where \u0398 1 , \u0398 2\nbe the optimal hyperplanes for D 1 and D 2 respectively and \u03c3 = max(errD 1 (\u0398 1 ), err D 2 (\u0398 2 )). Further, |err D 1 (\u0398 2 ) \u2212 err D 1 (\u0398 1 )| = \u2126 \u01eb 2\u22124/k \u03c3 2 .\nApplications for Gaussian Covariates. The special case where the marginal distribution over x is Gaussian has received considerable interest recently [DKS18, DKK + 18b]. We note that our estimators extend to the setting of Gaussian covariates, since the uniform distribution over samples from N (0, \u03a3) are (O(k) , O(k))-certifiably hypercontractive for all k (see Section 5 in Kothari and Steurer [KS17c]). As a consequence, instantiating Corollary 1.10 with k = log(1/\u01eb) yields the following:\nCorollary 1.12 (Robust Regression with Gaussian Covariates). Given \u01eb > 0 and n (d log(d)) O(log(1/\u01eb)) samples from R N (\u01eb, \u03a3 * , \u0398 * ), such that the marginal distribution over the x's is N (0, \u03a3 * ), there exists an algorithm that runs in n O(log(1/\u01eb) time and outputs an estimator \u0398 such that with high probability,(\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 O(\u01eb log(1/\u01eb)) (err N (\u0398 * )) 1/2\nand, errN ( \u0398) 1 + O (\u01eb log(1/\u01eb)) 2 err N (\u0398 * )\nWe note that our estimators obtain the rate matching recent work for Gaussians, albeit in quasipolynomial time. In comparison, Diakonikolas, Kong and Stewart [DKS18] obtain the same rate in polynomial time, when the noise is independent of the covariates. We note that obtaining the optimal rate for Gaussian covariates (shaving the additional log(1/\u01eb) factor) remains an outstanding open question.\n\n1.2 Related Work.\nRobust Statistics. Computationally efficient estimators for robust statistics in high dimension have been extensively studied, following the initial work on robust mean estimation [DKK + 16, LRV16]. We focus on literature regarding robust regression and sum-of-squares. We refer the reader to recent surveys and theses for an extensive discussion of the literature on robust statistics [RSS18, Li18, Ste18, DK19].\n\nRobust Regression.\nComputationally efficient estimators for robust linear regression were proposed by [PSBR20, KKM18b, DKK + 19, DKS19]. While [PSBR20] and [DKK + 19] obtained estimators for the more general case of distributions with bounded 4th moment. However, their estimators suffer an error of O err D (\u0398 * )\u01eb\u03ba 2 (\u03a3) , where \u03ba(\u03a3) is the condition number of the covariance matrix of X. Hence, these estimators don't obtain the optimal dependence on \u01eb in the negatively correlated noise setting, and also suffer an additional condition number dependence in the dependent noise setting. [DKS19] obtained improved bounds under the restrictive assumption that X is distributed according to a Gaussian.  [KKM18b] obtained polynomial-time estimators for distributions with certifiably bounded distributions, however, their estimators obtain a sub-optimal error of O err D (\u0398 * )\u01eb 1\u22122/k . In very recent and independent work, Zhu, Jiao and Steinhardt [ZJS20] obtained polynomial time estimators for the dependent noise setting, but their estimators are suboptimal for the negatively correlated setting.\nThere has been significant work in more restrictive noise models as well. For instance, a series of works [BJK15, BJKK17, SBRJ19] consider a noise model where the adversary is only allowed to corrupt the labels and obtain consistent estimators in this regime (error goes to zero with more samples). In comparison, our estimators do not obtain For a comprehensive overview we refer the reader to references in the aforementioned papers.\n\nSum-of-Squares Algorithms.\nIn recent years, there has been a significant progress in applying the Sum-of-Squares framework to design efficient algorithms for several fundamental computational problems. Starting with the work of Barak, Kelner and Steurer [BKS15], sum-of-squares algorithms have the best known running time for dictionary learning and tensor decomposition [MSS16a, SS17, HSS19], optimizing random tensors over the sphere [BGL17] and refuting CSPs below the spectral threshold [RRS17].\nIn the context of high-dimensional estimation, sum-of-squares algorithms achieved state-ofthe-art performance for robust moment estimation [KS17c], robust regression [KKM18b], robustly learning mixtures of spherical [KS17a, HL18] and arbitrary Gaussians [BK20b, DHKK20], heavytailed estimation [Hop18, CHK + 20] and list-decodable variants of these problems [KKK19, RY20a, RY20b, BK20a, CMY20].\n\nConcurrent Work.\nWe note that a statistical estimator achieving rate proportional to \u01eb 1 \u22121/k can be obtained from combining ideas in [ZJS19] and [ZJS20] 1 . However, this approach remains computationally intractable. Finally, Cherapanamjeri et al. [CHK + 20] consider the special case of k = 4 and obtain nearly linear sample complexity and running time. However, their running time and rate incurs a condition number dependence. Further, their rate scales proportional to \u01eb 1/2 , even when the noise is indpendent of the covariates (as opposed to \u01eb 3/4 ).\nWe emphasize that the bottleneck in all prior and concurrent work remains algorithmically exploiting the independence of the noise and covariates, which we achieve via the negatively correlated moments condition (Definition 1.5).\n\n2 Technical Overview\nIn this section, we provide an overview of our approach, the new algorithmic ideas we introduce and the corresponding technical challenges. At a high level, we build on several recent works that study Sum-of-Squares relaxations for solving algorithmic problems arising in robust statistics Following the proofs-to-algorithms paradigm arising from the aforementioned works, we show that given two distributions over regression instances that are close in total variation distance (definition 3.1), any hyperplane minimizing the least-squares loss on one distribution must be close (in \u2113 2 distance) to any other hyperplane minimizing the loss on the second distribution.\nThis information-theoretic statement immediately yields a robust estimator achieving optimal rate, albeit given access to unbounded computation. To see this, let D 1 be the uniform distribution over n i.i.d samples from the true distribution, and D 2 be the uniform distribution over n corrupted samples from R D (\u01eb, \u03a3 * , \u0398 * ), denoted by D 2 . It is easy to check that the total variation distance between D 1 and D 2 is at most \u01eb. Therefore, the two hyperplanes must be close in \u2113 2 norm. In order to make this strategy algorithmic, we show that we can distilled a set of polynomial constraints from the information theoretic proof and can efficiently optimize over them using the Sum-of-Squares framework. We note that we crucially require several new constraints, including the gradient condition and the negatively-correlated moments condition (see Section 2.1), which did not appear in any prior works. We describe each step in more detail subsequently.\n\n2.1 Total Variation Closeness implies Hyperplane Closeness\nConsider two distributions D 1 and D 2 over \u00ca d \u00d7 \u00ca such that the total variation distance between D 1 and D 2 is \u01eb and the marginals for both distributions over \u00ca d are (c k , k)-hypercontractive and have covariance \u03a3. Ignoring computational and sample complexity concerns, we can obtain the optimal hyperplanes corresponding to each distribution. Note, these hyperplanes need not be unique and are simply charecterized as minimzers of the least-squares loss : for i \u2208 {1, 2},\u0398 i = arg min \u0398 E x,y\u223cD i y \u2212 x \u22a4 \u0398 2\nOur central contribution is to obtain an information theoretic proof that the optimal hyperplanes are indeed close in scaled \u2113 2 norm, i.e.\u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 O \u01eb 1\u22121/k E x,y\u223cD 1 y \u2212 x \u22a4 \u0398 1 2 1/2 + E x,y\u223cD 2 y \u2212 x \u22a4 \u0398 2 2 1/2\nWe refer the reader to Theorem 4.1 for a precise statement. Further, we show that the \u01eb 1\u22121/k dependence can be achieved even when the noise is not completely independent of the covariates but satisfies an analytic condition which we refer to as negatively correlated moments (see Definition 1.5). We provide an outline of the proof as it illustrates where the techniques we introduced in this work.\nCoupling and Decoupling. We begin by considering a maximal coupling, G, between distributions D 1 and D 2 such that they disagree on at most an \u01eb-measure support (\u01eb-fraction of the points for a discrete distribution). Let (x, y) \u223c D 1 and (x \u2032 , y \u2032 ) \u223c D 2 . Then, observe for any vector v,v, \u03a3(\u0398 1 \u2212 \u0398 2 ) = v, E G xx \u22a4 (\u0398 1 \u2212 \u0398 2 ) = E G v, x x \u22a4 \u0398 1 \u2212 y + E G v, x y \u2212 x \u22a4 \u0398 2\nWhile the first term in Equation (1) depends completely on D 1 , the second term requires using the properties of the maximal coupling. Since 1 = 1 (x,y)=(x \u2032 ,y \u2032 ) + 1 (x,y) =(x \u2032 ,y \u2032 ) , we can rewrite the second term in Equation (1) as follows:E G v, x y \u2212 x \u22a4 \u0398 2 = E G v, x \u2032 y \u2032 \u2212 (x \u2032 ) \u22a4 \u0398 2 1 (x,y)=(x \u2032 ,y \u2032 ) + E G v, x y \u2212 x \u22a4 \u0398 2 1 (x,y) =(x \u2032 ,y \u2032 )\nWith a bit of effort, we can combine Equations ( 1) and (2), and upper bound them as follows (see Theorem 4.1 for details):v, \u03a3(\u0398 1 \u2212 \u0398 2 ) O(1) E G v, x x \u22a4 \u0398 1 \u2212 y (i) + E G v, x \u2032 (x \u2032 ) \u22a4 \u0398 2 \u2212 y \u2032 (ii) + E G v, x y \u2212 x \u22a4 \u0398 1 1 (x,y) =(x \u2032 ,y \u2032 ) + E G v, x \u2032 y \u2032 \u2212 (x \u2032 ) \u22a4 \u0398 2 1 (x,y) =(x \u2032 ,y \u2032 )\nObserve, since we have a maximal coupling, the last two terms appearing in Equation (3) are nonzero only on an \u01eb-measure support. To bound them, we decouple the indicator using H \u00f6lder's inequality,E G v, x(y \u2212 x \u22a4 \u0398 1 ) 1 (x,y) =(x \u2032 ,y \u2032 ) E 1 (x,y) =(x \u2032 ,y \u2032 ) k\u22121 k E v, x k y \u2212 x \u22a4 \u0398 1 k 1 k \u01eb 1\u22121/k \u2022 E v, x k y \u2212 x \u22a4 \u0398 1 k 1 k (iii)\nwhere we used the maximality of the coupling G to bound E 1 (x,y) =(x \u2032 ,y \u2032 ) \u01eb. The above analysis can be repeated verbatim for the second term in (3) as well. Going forward, we focus on bounding terms (i), (ii) and (iii).\nGradient Conditions. To bound terms (i) and (ii) in Equation (3), we crucially rely on gradient information provided by the least-squares objective. Concretely, a key observation in our informationtheoretic proof is that the candidate hyperplanes are locally optimal: given least-squares loss, for i \u2208 {1, 2} for all vectors v,\u2207 E x,y\u223cD i y \u2212 x \u22a4 \u0398 i 2 , v = E x,y\u223cD i v, xx \u22a4 \u0398 i \u2212 xy = 0\nwhere \u0398 1 and \u0398 2 are the optimal hyperplanes for D 1 and D 2 respectively. Therefore, both (i) and (ii) are identically 0. It remains to bound (iii).\nIndependence and Negatively Correlated Moments. We observe that term (iii) can be interpreted as the k-th order correlation between the distribution of the covariates projected along v and the distribution of the noise in the linear model. Here, we observe that if the linear model satisfies the negatively correlated moments condition (Definition 1.5), we can decouple the expectation and bound each term independently:E v, x k y \u2212 x \u22a4 \u0398 1 k 1/k E v, x k 1/k E y \u2212 x \u22a4 \u0398 1 k 1/k (5)\nObserve, when the underlying linear model has independent noise, Equation (5) follows for any k.\nWe thus crucially exploit the structure of the noise and require a considerably weaker notion than independence. Further, if the negatively correlated moments property is not satisfied, we can use Cauchy-Schwarz to decouple the expectation in Equation (5) and incur a \u01eb 1\u22122/k dependence (see Corollary 4.2). Conceptually, we emphasize that the negatively correlated moments condition may be of independent interest to design estimators that exploit independence in various statistics problems.\nHypercontractivity. To bound the RHS in Equation (5), we use our central distributional assumption of hypercontractive k-th moments (Definition 1.3) of the covariates :E v, x k 1/k \u221a c k E v, x 2 1/2 = \u221a c k v, \u03a3v 1/2\nWe can bound the noise similarly, by assuming that the noise is hypercontractive and this considerably simplifies our statements. However, hypercontractivity of the noise is not a necessary assumption and prior work indeed incurs a term proportional to the k-th moment of the noise. Assuming boundedness of the regression vectors, Klivans, Kothari and Meka [KKM18b] obtained a uniform upper bound on k-th moment of the noise by truncating large samples. We note that the same holds for our estimators and we refer the reader to Section 5.2.3 in their paper. Finally, substituting v = \u0398 1 \u2212 \u0398 2 and rearranging, completes the information-theoretic proof. We note that our approach already differs from prior work [KKM18b, PSBR20, ZJS19] and to our knowledge, we obtain the first information theoretic proof that being \u01eb-close in TV distance implies that the optimal hyperplanes are O \u01eb 1\u22121/k close in \u2113 2 distance. Next, we use this proof as motivation to construct a robust estimator. We do this by explicitly enforcing the gradient condition, negatively correlated moments, and hypercontractivity as constraints in the description of the robust estimator. In contrast, prior work only uses hypercontractivity or the gradient information in the analysis of their robust estimators. We describe these details below.\n\n2.2 Proofs to Inefficient Algorithms: Distilling Constraints\nGiven an \u01eb-corrupted sample generated by Model 1.1, a natural approach to design a robust (albeit inefficient) estimator is to search over all subsets of size (1 \u2212 \u01eb)n, minimize the least squares objective on each one and output the hyperplane that achieves the minimal least-squares objective. Klivans, Kothari and Meka [KKM18b] implicitly analyze this estimator and show that it obtains rate \u01eb 1/2\u22121/k , which is sub-optimal. Instead, we search over all subsets of size (1 \u2212 \u01eb)n and in addition to minimizing the least-squares objective, we check if the uniform distribution over the samples is hypercontractive, the corresponding hyperplane satisfies the gradient condition and that the distribution over the covariates and noise satisfies the negatively correlated moments condition. Then, picking the hyperplane that achieves minimal least-squares cost and satisfies the aforementioned constraints obtains the optimal rate.\nSince we work in the strong contamination model, where the adversary is allowed to both add and delete samples, there may not be any i.i.d. subset of (1 \u2212 \u01eb)n samples and thus enforcing constraints directly on the samples does not suffice. Instead, we create variables 2n variables denoted by {(x \u2032 1 , y \u2032 1 ), . . . (x \u2032 n , y \u2032 n )} that serve as a proxy for the uncorrupted samples. We can now enforce constraints on the variables with impunity since a there exists a feasible assignment, namely the uncorrupted samples. With the goal of obtaining an efficiently computable estimator, we restrict to enforcing only polynomial constraints, instead of arbitrary ones.\nIntersection Constraints. The discrete analogue of the coupling argument is to ensure high intersection between the variables of our polynomial program and the uncorrupted samples. We know that at least a (1 \u2212 \u01eb)-fraction of the samples we observe agree with the uncorrupted samples. To this end, we create indicator variables w i , for i \u2208 [n] such that :\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2211 i\u2208[n] w i = (1 \u2212 \u01eb)n \u2200i \u2208 [n]. w 2 i = w i \u2200i \u2208 [n] w i (x \u2032 i \u2212 x i ) = 0 \u2200i \u2208 [n] w i (y \u2032 i \u2212 y i ) = 0 \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe\nThe intersection constraints ensure that our polynomial system variables agree with the observed samples on (1 \u2212 \u01eb)n points. We note that such constraints are now standard in the literature, and indeed are the only constraints explicitly enforced by [KKM18b].\nIndependence as a Polynomial Inequality. The central challenge in obtaining optimal rates for robust regression is to leverage the independence of the noise and covariates. Since independence is a property of the marginals of D, it is not immediately clear how to leverage it while designing a robust estimator. However, recall that we do not require independence in full generality and use negatively correlated moments as a proxy for independence. Ideally, we would want to enforce the polynomial inequality corresponding to negatively correlated moments directly on the variables of our polynomial program as follows:\u2200r k/2, 1 n \u2211 i\u2208[n] v \u22a4 x \u2032 i y \u2032 i \u2212 (x \u2032 i ) \u22a4 \u0398 2r O(1) 1 n \u2211 i\u2208[n] (v \u22a4 x \u2032 i ) 2r 1 n \u2211 i\u2208[n] y \u2032 i \u2212 (x \u2032 i ) \u22a4 \u0398 2r\nwhere \u0398 is a variable corresponding to the true hyperplane. To demonstrate feasibility of this constraint, we would require a finite sample analysis, showing that uncorrupted samples from a hypercontractive distribution satisfy the above inequality. Observe, when r = k/2, the LHS is a degree-k polynomial and our distribution may be too heavy-tailed to achieve any concentration. Instead, we observe that since hypercontractivity is preserved under sampling, we can relax our polynomial constraint by applying hypercontractivity to the terms in the RHS above :\u2200r k/2, 1 n \u2211 i\u2208[n] v \u22a4 x \u2032 i y \u2032 i \u2212 (x \u2032 i ) \u22a4 \u0398 2r O(1) 1 n \u2211 i\u2208[n] (v \u22a4 x \u2032 i ) 2 r 1 n \u2211 i\u2208[n] y \u2032 i \u2212 (x \u2032 i ) \u22a4 \u0398 2 r\nIn Lemma 5.4 we show that the above inequality is feasible for the uncorrupted samples. In particular, given at least, d \u2126(k) i.i.d. samples from D, the above inequality holds on the samples with high probability.\nPerhaps surprisingly, the dependence on \u01eb achieved when D has negatively correlated moments matches the information theoretically optimal rate for independent noise. We thus expect the notion of negatively correlated moments to lead to new estimators for problems where independence of random variables requires to be formulated as a polynomial inequality. Hypercontractivity Constraints. Since hypercontractivity is a already a polynomial identity relating the k-th moment to the variance of a distribution, it can easily enforced as a constraint. Conveniently, if the underlying distribution D is hypercontractive, then the uniform distribution over a sufficiently large sample is also sampling also hypercontractive (see Lemma 5.7 in [KKM18b]). Therefore, the following constraints are feasible:\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2200r k/2 1 n \u2211 i\u2208[n] x \u2032 i , v 2r c r n \u2211 i\u2208[n] x \u2032 i , v 2 r \u2200r k/2 1 n \u2211 i\u2208[n] y \u2032 i \u2212 \u0398, x \u2032 i 2r \u03b7 r n \u2211 i\u2208[n] y \u2032 i \u2212 \u0398, x \u2032 i 2 r \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe\nWe note that Klivans, Kothari and Meka [KKM18b] use hypercontractivity of the uncorrupted samples in their analysis but do not explicitly enforce this as a constraint. Enforcing hypercontractivity explicitly on the samples was used by Kothari and Steurer [KS17c] in the context of robust moment estimation and Kothari and Steinhardt [KS17a] in the context of robustly clustering a mixture of spherical Gaussians.\nGradient Constraints. Finally, it is crucial in our analysis to enforce that the minimizer we are searching for, \u0398, has gradient 0. For the least-squares loss, the gradient has a simple analytic form: for all v \u2208 \u00ca d , \uf8f1 \uf8f2 \uf8f3 v, 1 n \u2211 i\u2208[n] x \u2032 i x \u2032 i , \u0398 \u2212 y \u2032 i k = 0 \uf8fc \uf8fd \uf8fe\nWhile such optimality conditions are often used in the analysis of estimators (as done in [PSBR20]), we emphasize that we hardcode the gradient condition into the description of our robust estimator.\nTo the best of our knowledge, no estimator for robust/high-dimensional statistics includes explicit optimality constraints as a part of a polynomial system. Solving the least-squares objective on the samples subject to the polynomial system described by the aforementioned constraints results in an estimator for robust regression that achieves optimal rate. Recall, this follows immidiately from our robust certifiability proof. However, optimizing polynomial systems can be NP-Hard in the worse case, and thus we briefly describe how to avoid this computational intractability next.\n\n2.3 Efficient Algorithms via Sum-of-Squares\nWe use the sum-of-squares method to make the aforementioned polynomial system efficiently computable and provide an outline of this approach (see Section 3 for a formal treatment of sumof-squares proofs). Instead of directly solving the polynomial program, let us instead consider finding a distribution, \u00b5, over feasible solutions w, x \u2032 , y \u2032 and \u0398 that minimizesE w,x \u2032 ,y \u2032 ,\u0398\u223c\u00b5 1 n \u2211 i\u2208[n] w i y \u2032 i \u2212 w i x \u2032 i , \u0398 2\nand satisfies the constraints above. Then, it follows from our information-theoretic proof (Theorem 4.1) thatE \u00b5 \u03a3 1/2 (\u0398 * \u2212 \u0398) 2 O \u01eb 1\u22121/k err D (\u0398 * ) 1/2 (6)\nwhere \u0398 * is the optimal hyperplane. We now face two challenges: finding a distribution over solutions is at least as hard as the original problem and we no longer recover a unique hyperplane. The latter is easy to address by observing that the hyperplane obtained by averaging over the distribution, \u00b5, suffices:\u03a3 1/2 \u0398 * \u2212 E \u00b5 [\u0398] 2 E \u00b5 \u03a3 1/2 (\u0398 * \u2212 \u0398) 2\nwhere the inequality follows from convexity of the loss.\nFollowing prior works, it is now natural to instead consider searching for a \"pseudo-distribution\", \u03b6, over feasible solutions. A pseudo-distribution is an object similar to a real distribution, but relaxed to allow negative mass on its support (see Subsection 3.1 for a formal treatment). Crucially, a pseudo-distribution over the polynomial program can be computed efficiently by formulating it as a large SDP. To see why this helps, note any polynomial inequality that can be derived using \"sum-of-squares\" proofs from a set of polynomial constraints using a low-degree sum-of-squares proof remains valid if we replace distributions by \"pseudo-distribution\".\nFor instance, if Equation 6 were a polynomial inequality in w, x \u2032 , y \u2032 and \u0398, obtained by applying simple transformations that admit sum-of-squares proofs, we could replace \u00b5 by \u03b6, and obtain an efficient estimator. However, Equation 6 is not a polynomial inequality and the proof outlined in Subsection 2.1 is not a low-degree sum-of-squares proof. Therefore, a central technical contribution of our work is to formulate the right polynomial identity bounding the distance between \u0398 * and \u0398 in terms of the least-squares error incurred by by \u0398 * , and deriving this bound from the polynomial constraints using a low-degree sum-of-squares proof. We do this in Section 5.\n\n2.4 Distribution Families\nWe note that our statistical estimator applies to all distributions, D, that are (c k , k)-hypercontractive and the rate is completely determined by whether D has negatively correlated moments. In particular, for the important special case of heavy-tailed regression with independent noise, we obtain rate proportional to \u01eb 1\u22121/k for parameter recovery.\nHowever, similar to prior work on hypercontractive distributions, our efficient estimators apply to a more restrictive class, i.e. certifiably hypercontractive distributions (Definition 3.5). Intuitively, this condition captures the criteria that information about degree-k moment upper bounds is \"algorithmically accessible\". Certifiably hypercontractive distributions are a broad class and include affine transformations of isotropic distributions satisfying Poincar\u00e9 inequalities and all strongly log-concave distributions. For a detailed discussion of distributions satisfying Poincar\u00e9 inequalities and their closure properties, we refer the reader to [KS17a, KS17c].\nSurprisingly, while we enforce a constraint corresponding to negatively correlated moments, we do not require a certifiable variant of this condition. Therefore, our efficient estimators hold for regression instances where the true distribution satisfies this condition, including the special case where the noise is independent from the covariates. Finally, our estimators unify various noise models and imply that even in the agnostic setting, the rate degrades only when the noise is positively correlated with the covariates.\n\n3 Preliminaries\nThroughout this paper, for a vector v, we use v 2 to denote the Euclidean norm of v. For a n \u00d7 m matrix M, we use M 2 = max x 2 =1 Mx 2 to denote the spectral norm of M and M F = \u2211 i,j M 2 i,j to denote the Frobenius norm of M. For symmetric matrices we use to denote the PSD/Loewner ordering over eigenvalues of M. Recall, the definition of total variation distance between probability measures: Definition 3.1 (Total Variation Distance). The TV distance between distributions with PDFs p, q is defined as1 2 \u221e \u2212\u221e |p(x) \u2212 q(x)|dx.\nGiven a distribution D over \u00ca d \u00d7 \u00ca, we consider the least squares error of a vector \u0398 w.r.t. D to be err D (\u0398) = Ex,y\u223cD (y \u2212 x, \u0398 ) 2 . The linear regression problem minimizes the error over all \u0398. The minimizer, \u0398 D of the aformentioned error satisfies the following \"gradient condition\" :for all v \u2208 \u00ca d , E x,y\u223cD v, xx \u22a4 \u0398 D \u2212 xy = 0 Fact 3.2 (Convergence of Empirical Moments, implicit in Lemma 5.5 [KS17c] ). Let D be a (c k , k)- hypercontractive distribution with covariance \u03a3 and let X = {x 1 , . . . x n } be n = \u2126((d log(d)/\u03b4) k/2 ) i.i.d. samples from D. Then, with probability at least 1 \u2212 \u03b4, (1 \u2212 0.1)\u03a3 1 n n \u2211 i x i x \u22a4 i (1 + 0.1)\u03a3 Fact 3.3 (TV Closeness to Covariance Closeness, Lemma 2.2 [KS17c]). Let D 1 , D 2 be (c k , k)-hypercontractive distributions over \u00ca d such that D \u2212 D \u2032 TV \u01eb, where 0 < \u01eb < O (1/c k ) k k\u22121 . Let \u03a3 1 , \u03a3 2 be the corre- sponding covariance matrices. Then, for \u03b4 O c k \u01eb 1\u22121/k < 1, (1 \u2212 \u03b4)\u03a3 2 \u03a3 1 (1 + \u03b4)\u03a3 2\nLemma 3.4 (L \u00f6wner Ordering for Hypercontractive Samples). Let D be a (c k , k)-hypercontractive distribution with covariance \u03a3 and and let U be the uniform distribution over n samples. Then, with probability 1 \u2212 \u03b4,\u03a3 \u22121/2 \u03a3\u03a3 \u22121/2 \u2212 I F C 4 d 2 \u221a n \u221a \u03b4 , where \u03a3 = 1 n \u2211 i\u2208[n] x i x \u22a4 i .\nNext, we define the technical conditions required for efficient estimators. Formally,Definition 3.5 (Certifiable Hypercontractivity). A distribution D on \u00ca d is (c k , k)-certifiably hyper-\ncontractive if for all r k/2, there exists a degree O(k) sum-of-squares proof (defined below) of the following inequality in the variable vE x\u223cD x, v 2r E x\u223cD c r x, v 2 r such that c r c k .\nNext, we note that if a distribution D is certifiably hypercontractive, the uniform distribution over n i.i.d. samples from D is also certifiably hypercontractive.\nFact 3.6 (Sampling Preserves Certifiable Hypercontractivity, Lemma 5.5 [KS17c]). Let D be a (c k , k)certifiably hypercontractive distribution on \u00ca d . Let X be a set of n = \u2126 (d log(d/\u03b4)) k/2 /\u03b3 2 i.i.d. samples from D. Then, with probability 1 \u2212 \u03b4, the uniform distribution over X is (c k + \u03b3, k)-certifiably hypercontractive.\nWe also note that certifiably hypercontractivity is preserved under Affine transformations of the distribution. Let x \u2208 \u00ca d be a random variable drawn from a (c k , k)-certifiably hypercontractive distribution. Then, for matrix A and vector b, the distribution over the random variable Ax + b is also (c k , k)-certifiably hypercontractive.\nNext, we formally define the condition on the moments and noise that we require to obtain efficient algorithms. We note that for technical reasons it is not simply a polynomial identity encoding Definition 1.5.\n\nDefinition 3.8 (Certifiable Negatively Correlated Moments\n). A distribution D on \u00ca d \u00d7 \u00ca has O(1)- certifiable negatively correlated moments if for all r k/2 there exists a degree O(k) sum-of- squares proof of the following inequality E x,y\u223cD v \u22a4 x y \u2212 x \u22a4 \u0398 2r O(\u03bb r r ) E (v \u22a4 x) 2 r E y \u2212 x \u22a4 \u0398 2 rfor a fixed vector \u0398.\n\n3.1 SoS Background.\nIn this subsection, we provide the necessary background for the sum-of-squares proof system. We follow the exposition as it appears in lecture notes by Barak [Bar], the Appendix of Ma, Shi and Steurer [MSS16b], and the preliminary sections of several recent works [KS17c, KKK19, KKM18a, BK20a, BK20b].\nPseudo-Distributions. We can represent a discrete probability distribution over \u00ca n by its prob- ability mass function D : \u00ca n \u2192 \u00ca such that D 0 and \u2211 x\u2208supp(D) D(x) = 1. Similarly, we can describe a pseudo-distribution by its mass function by relaxing the non-negativity constraint, while still passing certain low-degree non-negativity tests.\nDefinition 3.9 (Pseudo-distribution). A level-\u2113 pseudo-distribution is a finitely-supported function D : \u00ca n \u2192 \u00ca such that \u2211 x D(x) = 1 and \u2211 x D(x) f (x) 2 0 for every polynomial f of degree at most \u2113/2, where the summation is over all x in the support of D.\nNext, we define the notion of pseudo-expectation.\nDefinition 3.10 (Pseudo-expectation). The pseudo-expectation of a function f on \u00ca d with respect to a pseudo-distribution D, denoted by \u1ebcD(x) [ f (x)], is defined as\u1ebcD(x) [ f (x)] = \u2211 x D(x) f (x)\nWe use the notation \u1ebcD(x) [(1, x 1 , x 2 , . . . , x n ) \u2297\u2113 ] to denote the degree-\u2113 moment tensor of the pseudo-distribution D. In particular, each entry in the moment tensor corresponds to the pseudoexpectation of a monomial of degree at most \u2113 in x. Crucially, there's an efficient separation oracle for moment tensors of pseudo-distributions.\nFact 3.11 ([Sho87, Par00, Nes00, Las01]). For any n, \u2113 \u2208 AE, the following set has a n O(\u2113) -time weak separation oracle (in the sense of [GLS81]):\u1ebcD(x) (1, x 1 , x 2 , . . . , x n ) \u2297\u2113 | degree-\u2113 pseudo-distribution D over \u00ca n (8)\nThis fact, together with the equivalence of weak separation and optimization [GLS81] forms the basis of the sum-of-squares algorithm, as it allows us to efficiently approximately optimize over pseudo-distributions.\nGiven a system of polynomial constraints, denoted by A, we say that it is explicitly bounded if it contains a constraint of the form { x 2 M}. Then, the following fact follows from Fact 3.11 and [GLS81]: Fact 3.12 (Efficient Optimization over Pseudo-distributions). There exists an (n + m) O(\u2113) -time algorithm that, given any explicitly bounded and satisfiable system 2 A of m polynomial constraints in n variables, outputs a level-\u2113 pseudo-distribution that satisfies A approximately.\nWe now define basic facts for pseudo-distributions, which extend facts that hold for standard probability distributions, which can be found in several prior works listed above.\u1ebc \u03b6 [ f 2 ] \u1ebc \u03b6 [g 2 ].\nFact 3.14 (H \u00f6lder's Inequality for Pseudo-Distributions). Let f , g be polynomials of degree at mostd in indeterminate x \u2208 \u00ca d . Fix t \u2208 AE. Then, for any degree dt pseudo-distribution \u03b6, \u1ebc \u03b6 [ f t\u22121 g] \u1ebc \u03b6 [ f t ] t\u22121 t \u1ebc \u03b6 [g t ] 1/t\n. In particular, for all even integers k,\u1ebc \u03b6 [ f ] k \u1ebc \u03b6 [ f k ].\nSum-of-squares proofs. Let f 1 , f 2 , . . . , f r and g be multivariate polynomials in x. A sum-of-squares proof that the constraints { f 1 0, . . . , f m 0} imply the constraint {g 0} consists of polynomials (p S ) S\u2286 [m] such thatg = \u2211 S\u2286[m] p 2 S \u2022 \u03a0 i\u2208S f i (9)\nWe say that this proof has degree \u2113 if for every set S \u2286 [m], the polynomial p 2 S \u03a0 i\u2208S f i has degree at most \u2113. If there is a degree \u2113 SoS proof that { f i 0 | i r} implies {g 0}, we write:{ f i 0 | i r} \u2113 {g 0}\nFor all polynomials f , g : \u00ca n \u2192 \u00ca and for all functionsF : \u00ca n \u2192 \u00ca m , G : \u00ca n \u2192 \u00ca k , H : \u00ca p \u2192\n\u00ca n such that each of the coordinates of the outputs are polynomials of the inputs, we have the following inference rules. The first one derives new inequalities by addition/multiplication:A \u2113 { f 0, g 0} A \u2113 { f + g 0} , A \u2113 { f 0}, A \u2113 \u2032 {g 0} A \u2113+\u2113 \u2032 { f \u2022 g 0} (Addition/Multiplication Rule)\nThe next one derives new inequalities by transitivity:A \u2113 B, B \u2113 \u2032 C A \u2113\u2022\u2113 \u2032 C (Transitivity Rule)\nFinally, the last rule derives new inequalities via substitution:{F 0} \u2113 {G 0} {F(H) 0} \u2113\u2022deg(H) {G(H) 0} (Substitution Rule)\nLow-degree sum-of-squares proofs are sound and complete if we take low-level pseudo-distributions as models. Concretely, sum-of-squares proofs allow us to deduce properties of pseudo-distributions that satisfy some constraints.\n\nFact 3.15 (Soundness). If D r A for a level-\u2113 pseudo-distribution D and there exists a sum-of-squares\nproof A r \u2032 B, then D r\u2022r \u2032 +r \u2032 B.If the pseudo-distribution D satisfies A only approximately, soundness continues to hold if we require an upper bound on the bit-complexity of the sum-of-squares A r \u2032 B (number of bits required to write down the proof). In our applications, the bit complexity of all sum of squares proofs will be n O(\u2113) (assuming that all numbers in the input have bit complexity n O (1) ). This bound suffices in order to argue about pseudo-distributions that satisfy polynomial constraints approximately.\nThe following fact shows that every property of low-level pseudo-distributions can be derived by low-degree sum-of-squares proofs. Fact 3.17 (Operator norm Bound). Let A be a symmetric d \u00d7 d matrix and v be a vector in R d . Then,2 v v \u22a4 Av A 2 v 2 2 .\nFact 3.18 (SoS Almost Triangle Inequality). Let f 1 , f 2 , . . . , f r be indeterminates. Then,2t f 1 , f 2 ,..., f r \uf8f1 \uf8f2 \uf8f3 \u2211 i r f i 2t r 2t\u22121 r \u2211 i=1 f 2t i \uf8fc \uf8fd \uf8fe .\nFact 3.19 (SoS H \u00f6lder's Inequality). Let w 1 , . . . w n be indeterminates and let f 1 , . . . f n be polynomials of degree m in vector valued variable x. Let k be a power of 2. Then,w 2 i = w i , \u2200i \u2208 [n] 2km x,w \uf8f1 \uf8f2 \uf8f3 1 n n \u2211 i=1 w i f i k 1 n n \u2211 i=1 w i k\u22121 1 n n \u2211 i=1 f k i \uf8fc \uf8fd \uf8fe .\nWe also use the following fact that allows us to cancel powers of indeterminates within the sum-of-squares proof system. Fact 3.20 (Cancellation Within SoS, Lemma 9.4 [BK20b]). Let a, C be indeterminates. Then,{a 0} \u222a a 2t Ca t 2t a,C a 2t C 2 .\n\n4 Robust Certifiability and Information Theoretic Estimators\nIn this section, we provide an estimator that obtains the information theoretically optimal rate for robust regression. We note that we consider the setting where both the covariates and the noise are hypercontractive and the are independent of each other. This setting displays all the key ideas of our estimator. Further, our estimator extends to the remaining settings, such as bounded dependent noise, by simple modifications to the subsequent analysis.\u2212 x, E xx \u22a4 \u22121 E [xy] be (\u03b7 k , k)-hypercontractive Then, \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 O \u221a c k \u03b7 k \u01eb 1\u22121/k err D (\u0398 D ) 1/2 + err D \u2032 (\u0398 D \u2032 ) 1/2 Further, err D (\u0398 D \u2032 ) 1 + O c k \u03b7 k \u01eb 2\u22122/k err D (\u0398 D ) + O c k \u03b7 k \u01eb 2\u22122/k err D \u2032 (\u0398 D \u2032 ) Proof. Consider a maximal coupling of D, D \u2032 over (x, y) \u00d7 (x \u2032 , y \u2032 ), denoted by G, such that the marginal of G (x, y) is D, the marginal on (x \u2032 , y \u2032 ) is D \u2032 and P G [I {(x, y) = (x \u2032 , y \u2032 )}] = 1 \u2212 \u01eb. Then, for all v, v, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) = E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) + xy \u2212 xy = E G [ v, x ( x, \u0398 D \u2212 y) ] + E G [ v, x (y \u2212 x, \u0398 D \u2032 ) ]\nSince \u0398 D is the minimizer for the least squares loss, we have the following gradient condition : for all v \u2208 R d , E(x,y)\u223cD [ v, ( x, \u0398 D \u2212 y)x ] = 0 (12)\nSince G is a coupling, using the gradient condition (12) and using that 1 = I {(x, y) = (x \u2032 , y \u2032 )} +I {(x, y) = (x \u2032 , y \u2032 )}, we can rewrite equation (11) asv, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) = E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) = E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 )\nConsider the first term in the last equality above. Using the gradient condition for \u0398 D \u2032 along with H \u00f6lder's Inequality, we haveE G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) = E D \u2032 v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 \u2212 E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) = E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k/(k\u22121) (k\u22121)/k \u2022 E D \u2032 v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k 1/k (14)\nObserve, since G is a maximal coupling EG [I {(x, y) = (x \u2032 , y \u2032 )}] (k\u22121)/k \u01eb 1\u22121/k . Further, since D \u2032 has negatively correlated moments,E D \u2032 v, x \u2032 k \u2022 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k = E D \u2032 v, x \u2032 k E D \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k\nBy hypercontractivity of the covariates and the noise, we haveE D \u2032 v, x \u2032 k 1/k E D \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k 1/k O( \u221a c k \u03b7 k ) v \u22a4 \u03a3 D \u2032 v 1/2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1/2\nTherefore, we can restate (14) as followsE G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) O \u221a c k \u03b7 k \u01eb k\u22121 k v \u22a4 \u03a3 D \u2032 v 1 2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1 2\nIt remains to bound the second term in the last equality of equation ( 13), and we proceed as follows :E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) = E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D ) I (x, y) = (x \u2032 , y \u2032 ) (16)\nWe bound the two terms above separately. Observe, applying H \u00f6lder's Inequality to the first term, we haveE G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k (17)\nTo bound the second term in equation 16, we again use H \u00f6lder's Inequality followed D having negatively correlated moments,E G v, x (y \u2212 x, \u0398 D ) I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k\u22121 k E G v, x (y \u2212 x, \u0398 D ) k 1 k \u01eb k\u22121 k E x\u223cD v, x k 1/k E x,y\u223cD (y \u2212 x, \u0398 D ) k 1/k \u01eb k\u22121 k \u221a c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2 (18)\nwhere the last inequality follows from hypercontractivity of the covariates and noise. Substituting the upper bounds obtained in Equations ( 17) and (18) back in to ( 16),E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k + \u01eb k\u22121 k \u221a c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2\nTherefore, we can now upper bound both terms in Equation (13) as follows:v, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) O c k \u03b7 k \u01eb k\u22121 k v \u22a4 \u03a3 D \u2032 v 1/2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1/2 + O \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k/2 2/k + O \u01eb k\u22121 k \u221a c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2 (19) Recall,\u03a3 D \u2032 \u03a3 D (1 + 0.1) \u03a3 D \u2032 (20) when \u01eb O (1/c k k) k/k\u22121 . Now, consider the substitution v = \u0398 D \u2212 \u0398 D \u2032 . Observe, E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k/2 2/k = E D x, (\u0398 D \u2212 \u0398 D \u2032 ) k 2/k c k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 2 (21)\nThen, using the bounds in (20) and ( 21)along with v = \u0398 D \u2212 \u0398 D \u2032 in Equation 19, we have 1 \u2212 O \u01eb k\u22122 k c k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 2 O \u221a c k \u03b7 k \u01eb k\u22121 k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1 2 + E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1 2 (22) Dividing out (22) by 1 \u2212 O \u01eb k\u22122 k c k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 2\nand observing that O \u01eb k\u22122 k c k is upper bounded by a fixed constant less than 1 yields the parameter recovery bound.\nGiven the parameter recovery result above, we bound the least-squares loss between the two hyperplanes on D as follows:err D (\u0398 D ) \u2212 err D (\u0398 D \u2032 ) = E (x,y)\u223cD y \u2212 x \u22a4 \u0398 D 2 \u2212 y \u2212 x \u22a4 \u0398 D \u2032 + x \u22a4 \u0398 D \u2212 x \u22a4 \u0398 D 2 = E (x,y)\u223cD x, (\u0398 D \u2212 \u0398 D \u2032 ) 2 + 2(y \u2212 x \u22a4 \u0398 D )x \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) O c k \u03b7 k \u01eb 2\u22122/k E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 + E x,y\u223cD (y \u2212 x, \u0398 D ) 2 (23)\nwhere the last inequality follows from observing E \u0398 D \u2212 \u0398 D \u2032 , x(y \u2212 x \u22a4 \u0398 D ) = 0 (gradient condition) and squaring the parameter recovery bound.\nNext, we consider the setting where the noise is allowed to dependent arbitrarily on the covariates, which captures the well-studied agnostic model. With a slightly modification in our certifiability proof above (using Cauchy-Schwarz instead of independence), we obtain the optimal rate in this setting. We defer the details to Appendix A.\n\nCorollary 4.2 (Robust Regression with Dependent Noise\n). Let D, D \u2032 be distributions over \u00ca d \u00d7 \u00ca and let R D (\u01eb, \u03a3 D , \u0398 D ), R D \u2032 (\u01eb, \u03a3 D \u2032 , \u0398 D \u2032) be robust regression instances satisfying the hypothesis in Theorem 4.1 such that the negatively correlated moments condition is not satisfied. Then,\u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 O \u221a c k \u03b7 k \u01eb 1\u22122/k err D (\u0398 D ) 1/2 + err D \u2032 (\u0398 D \u2032 ) 1/2 Further, err D (\u0398 D \u2032 ) 1 + O c k \u03b7 k \u01eb 2\u22124/k err D (\u0398 D ) + O c k \u03b7 k \u01eb 2\u22124/k err D \u2032 (\u0398 D \u2032 )\n\n5 Robust Regression in Polynomial Time\nIn this section, we describe an algorithm to compute our robust estimator for linear regression efficiently. We consider a polynomial system that encodes our robust estimator. We then consider a sum-of-squares relaxation of this program and compute an approximately optimal solution for our relaxation. To analyze our algorithm, we consider the dual of the sum-of-squares relaxation and show that the sum-of-squares proof system caputures a variant of our robust identifiability proof.\nWe begin by recalling notation: let D be a distribution over\u00ca d \u00d7 \u00ca such that it is (\u03bb k , k)- certifiably hypercontractive. Let X = {(x * 1 , y * 1 ), (x * 2 , y * 2 ) . . . (x * n , y * n )} denote n uncorrupted i.\ni.d samples from D and let X \u01eb = .{(x 1 , y 1 ), (x 2 , y 2 ) . . . (x n , y n )} be an \u01eb-corruption of the samples X , drawn from a Robust Regression model, R D (\u01eb, \u03a3 * , \u0398 * ) (Model 1.1). We consider a polynomial system in the variables A \u01eb,\u03bb k :X \u2032 = {(x \u2032 1 , y \u2032 1 ), (x \u2032 2 , y \u2032 2 ) . . . (x \u2032 n , y \u2032 n )} and\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2211 i\u2208[n] w i = (1 \u2212 \u01eb)n \u2200i \u2208 [n]. w 2 i = w i \u2200i \u2208 [n] w i (x \u2032 i \u2212 x i ) = 0 \u2200i \u2208 [n] w i (y \u2032 i \u2212 y i ) = 0 v, 1 n \u2211 i\u2208[n] x \u2032 i x \u2032 i , \u0398 \u2212 y i k = 0 \u2200r k/2 1 n \u2211 i\u2208[n] x \u2032 i , v 2r \u03bb r n \u2211 i\u2208[n] x \u2032 i , v 2 r \u2200r k/2 1 n \u2211 i\u2208[n] y \u2032 i \u2212 \u0398, x \u2032 i 2r \u03bb r n \u2211 i\u2208[n] y \u2032 i \u2212 \u0398, x \u2032 i 2 r \u2200r k/2 E v \u22a4 x \u2032 i y \u2032 i \u2212 (x \u2032 i ) \u22a4 \u0398 2r O \u03bb 2r r E v, x \u2032 i 2 r E y \u2032 i \u2212 x \u2032 i , \u0398 2 r \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe\nWe show that optimizing an appropriate convex function subject to the aforementioned constraint system results in an efficiently computable robust estimator for regression, achieving the information-theoretically optimal rate. Formally, Theorem 5.1 (Robust Regression with Negatively Correlated Moments, Theorem 1.7 restated).Given k \u2208 N, \u01eb > 0 and n n 0 samples X \u01eb = {(x 1 , y 1 ), . . . (x n , y n )} from R D (\u01eb, \u03a3 * , \u0398 * ), where D is a (\u03bb k , k)-certifiably hypercontractive distribution over \u00ca d \u00d7 \u00ca.(\u03a3 * ) 1/2 \u0398 * \u2212 \u1ebc \u03b6 [\u0398] 2 O \u03bb k \u01eb 1\u22121/k + \u03bb k \u03b3 err D (\u0398 * ) 1/2 Further, err D \u1ebc \u03b6 [\u0398] 1 + O \u03bb 2 k \u01eb 2\u22122/k + \u03bb 2 k \u03b3 2 err D (\u0398 * ).\nEfficient Estimator for Arbitrary Noise. We note that an argument similar to the one presented for Theorem 5.1 results in a polynomial time estimator when the regression instance does not have negatively correlated moments (definition 1.5), albeit at a slightly worse rate. Formally, Corollary 5.2 (Robust Regression with Arbitrary Noise). Consider the hypothesis of Theorem 5.1, without the negatively correlated moments assumption. Then, there exists an algorithm that runs in time n O(k)  outputs an estimator \u0398 such that when n 0 = (d log(d)) \u2126(k) /\u03b3 2 , with probability 1 \u2212 1/poly(d) (over the draw of the input),(\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 O \u03bb k \u01eb 1\u22122/k + c 2 \u03b7 2 \u03b3 err D (\u0398 * ) 1/2 Further, err D \u0398 1 + O \u03bb 2 k \u01eb 2\u22124/k + \u03bb 2 2 \u03b3 err D (\u0398 * )\nAlgorithm 5.3 (Optimal Robust Regression in Polynomial Time).\nInput: n samples X \u01eb from the robust regression model R D (\u01eb, \u0398 * , \u03a3 * ).\nOperation:1. Find a degree-O(k) pseudo-distribution \u03b6 satisfying A \u01eb,\u03bb k and minimizing min w,x \u2032 ,y \u2032 ,\u0398 \u1ebc \u03b6 \uf8ee \uf8f0 1 n \u2211 i\u2208[n] w i y \u2032 i \u2212 \u0398, x \u2032 2 k \uf8f9 \uf8fb .\n\n2. Round the pseudo-distribution to obtain an estimator \u1ebc \u03b6 [\u0398].\nOutput: A vector \u1ebc \u03b6 [\u0398] such that the recovery guarantee in Theorem 5.1 is satisfied.\nAt a high level, we simply do not enforce the negatively correlated moments constraint in our polynomial system A \u01eb,\u03bb k and instead use the SoS Cauchy-Schwarz inequality in our key technical lemma (Lemma 5.5). For completeness, we provide the proof of the SoS lemma in Appendix B.\n\n5.1 Analysis\nWe begin by observing that we can efficiently optimize the polynomial program above since it admits a compact representation. In particular, A \u01eb,\u03bb k can be represented as a system of poly(n k ) constraints in n O(k) variables. We refer the reader to [FKP + 19] for a detailed overview on how to efficiently implement the aforementioned constraints.\nLemma 5.4 (Soundness of the Constraint System). Given n n 0 samples from R D (\u01eb, \u0398 * , \u03a3), with probability at least 1 \u2212 1/poly(d) over the draw of the samples, there exists an assignment for w, x \u2032 , y \u2032 and \u0398 such that A \u01eb,\u03bb k is feasible when n 0 = (d log(d)) \u2126(k) .\nProof. Consider the following assignment: for all i \u2208 [n] the w i 's indicate the set of uncorrupted points in X \u01eb , i.e.w i = 1 if (x i , y i ) = (x * i , y * i ), x \u2032 i = x i and y \u2032 i = y i .\nFurther, \u0398 = \u0398 * , the true hyperplane. It is easy to see that the first four constraints (intersection constraints) are satisfied.\nWe observe that the marginal distribution over the covariates and the noise are both (\u03bb k , k)certifiably hypercontractive since they are Affine transformations of D (Fact 3.7). Next, it follows from Fact 3.6, that for n 0 = \u2126 d log(d) O(k) , the uniform distribution over the samples x i , is (2 \u03bb k , k)-certifiably hypercontractive with probability at least 1 \u2212 1/poly(d). Similarly, the uniform distribution ony i \u2212 x i , \u0398 * is (2 \u03bb k , k)-certifiably hypercontractive.\nIt remains to show that sampling preserves certifiable negatively correlated moments. Recall, since the joint distribution is hypercontractive, by Fact 3.6 we know that there's a degree O(k) proof of1 n \u2211 i\u2208[n] v, x i k (y i \u2212 x i , \u0398 * ) k O \u03bb k k 1 n \u2211 i\u2208[n] v, x i 2 (y i \u2212 x i , \u0398 * ) 2 k/2 = O \u03bb k k 1 n \u2211 i\u2208[n] v \u22a4 x i (x i ) \u22a4 (y i \u2212 x i , \u0398 * ) 2 v k/2 (24)\nIt thus suffices to bound the Operator norm of1 n \u2211 i\u2208[n] x i x \u22a4 i (y i \u2212 x i , \u0398 * ) 2 .\nIt follows from Lemma 3.4 that with probability at least 1 \u2212 1/poly(d),1 n \u2211 i\u2208[n] x i x \u22a4 i (y i \u2212 x i , \u0398 * ) 2 O(1) E x,y\u223cD xx \u22a4 (y \u2212 x, \u0398 * ) 2 (25)\nwhen n n 0 . Using that D has negatively correlated moments,E x,y\u223cD xx \u22a4 (y \u2212 x, \u0398 * ) 2 E x\u223cD xx \u22a4 E x,y\u223cD (y \u2212 x, \u0398 * ) 2 (26)\nUsing Lemma 3.4 on xx \u22a4 and (y \u2212 x, \u0398 * ) 2 , we can bound (26) as follows:E x\u223cD xx \u22a4 E x,y\u223cD (y \u2212 x, \u0398 * ) 2 O(1) E x i x \u22a4 i (y i \u2212 x i , \u0398 * ) 2 (27)\nCombining Equations ( 25), (26), and (27), and substituting in (24), we have1 n \u2211 i\u2208[n] v, x i k (y i \u2212 x i , \u0398 * ) k O \u03bb k k 1 n \u2211 i\u2208[n] x i , v 2 k 2 1 n \u2211 i\u2208[n] (y i \u2212 x i , \u0398 * ) 2 k 2\nwhich concludes the proof.\nLet \u03a3 be the empirical covariance of the uncorrupted samples X and let \u0398 be an optimizer for the empirical loss. Applying Theorem 4.1 with D being the uniform distribution on the uncorrupted samples X and D \u2032 being the uniform distribution onx \u2032 i , we get \u03a31/2 \u0398 \u2212 \u0398 2 O \u03bb k \u01eb 1\u22121/k err D (\u0398 * ) 1/2\nObserve, the aforementioned bound is not a polynomial identity and thus cannot be expressed in the SoS framework. Therefore, we provide a low-degree SoS proof of a slightly modified version of the inequality above, that is inspired by our information theoretic identifiability proof in Theorem 4.1.\n\nLemma 5.5 (Robust Identifiability in SoS)\n. Consider the hypothesis of Theorem 5.1. Let w, x \u2032 , y \u2032 and \u0398 be feasible solutions for the polynomial constraint system A.Let \u0398 = arg min \u0398 1 n \u2211 i\u2208[n] (y * i \u2212 x * i , \u0398\n) 2 be the empirical loss minimizer on the uncorrupted samples and let \u03a3 = E x * i (x * i ) \u22a4 be the covariance of the uncorrupted samples. Then,A 4k w,x \u2032 ,y \u2032 ,\u0398 \u03a31/2 \u0398 \u2212 \u0398 2k 2 2 3k (2\u01eb) k\u22121 \u03bb k k \u03c3 k/2 E x \u2032 i (x \u2032 i ) \u22a4 1/2 \u0398 \u2212 \u0398 k 2 + 2 3k (2\u01eb) k\u22122 \u03bb 2k k \u03a31/2 \u0398 \u2212 \u0398 2k 2 + 2 3k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 \u03a31/2 \u0398 \u2212 \u0398 k 2\nProof. Consider the empirical covariance of the uncorrupted set given by \u03a3 = E x * i (x * i ) \u22a4 . Then, using the Substitution Rule, along with SoS Almost Triangle Inequality (Fact 3.18),2k \u0398 v, \u03a3 \u0398 \u2212 \u0398 k = v, E x * i (x * i ) \u22a4 \u0398 \u2212 \u0398 + x * i y * i \u2212 x * i y * i k = v, E x * i x * i , \u0398 \u2212 y * i + E [x * i (y * i \u2212 x * i , \u0398 )] k 2 k v, E x * i x * i , \u0398 \u2212 y * i k + 2 k v, E [x * i (y * i \u2212 x * i , \u0398 )] k (28)\nObserve, the first term in (28) only consists of constants of the proof system. Since \u0398 is the min-imizer of E ( x * i , \u0398 \u2212 y * i ) 2\n, the gradient condition on the samples (appearing in Equation (12) of the indentifiability proof) implies this term is 0. Therefore, applying the Substitution Rule it suffices to bound the second term.\nTo this end, we introduce the following auxiliary variables : for all i \u2208 [n], let w \u2032 i = w i iff the i-th sample is uncorrupted in X \u01eb , i.e. x i = x * i . Then, it is easy to see that\u2211 i w \u2032 i (1 \u2212 2\u01eb)n. Further, since A 2 w (1 \u2212 w \u2032 i w i ) 2 = (1 \u2212 w \u2032 i w i ) , A 2 w 1 n \u2211 i\u2208[n] (1 \u2212 w \u2032 i w i ) 2 = 1 n \u2211 i\u2208[n] (1 \u2212 w \u2032 i w i ) 2\u01eb\nThe above equation bounds the uncorrupted points in X \u01eb that are not indicated by w. Then, using the Substitution Rule, along with the SoS Almost Triangle Inequality (Fact 3.18),A 2k \u0398,w \u2032 v, E [x * i (y * i \u2212 x * i , \u0398 )] k = v, E x * i y * i \u2212 x * i , \u0398 (w \u2032 i + 1 \u2212 w \u2032 i ) k = v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) + E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k 2 k v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) k + 2 k v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k (30)\nConsider the first term of the last inequality in (30). Observe, sincew \u2032 i x * i = w i w \u2032 i x \u2032 i and similarly, w \u2032 i y * i = w i w \u2032 i y \u2032 i , A 4 \u0398,w \u2032 E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) = E w \u2032 i w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398\nFor the sake of brevity, the subsequent statements hold for relevant SoS variables and have degree O(k) proofs. Using the Substitution Rule,A v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) k = v, E w \u2032 i w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = v, E x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 + E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k 2 k v, E x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k + 2 k v, E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k (31)\nObserve, the first term in the last inequality above is identically 0, since we enforce the gradient condition on the SoS variables x \u2032 , y \u2032 and \u0398. We can then rewrite the second term using linearity of expectation, followed by applying SoS H \u00f6lder's Inequality (Fact 3.19) combined with A 2w (1 \u2212 w \u2032 i w i ) 2 = 1 \u2212 w \u2032 i w i to get A v, E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = E v, (1 \u2212 w \u2032 i )w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = E (1 \u2212 w \u2032 i w i ) v, x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k E (1 \u2212 w \u2032 i w i ) k\u22121 E v, x \u2032 i k y \u2032 i \u2212 x \u2032 i , \u0398 k (2\u01eb) k\u22121 E v, x \u2032 i k y \u2032 i \u2212 x \u2032 i , \u0398 k ()\nwhere the last inequality follows from Equation (29). Next, we use the certifiable negatively correlated moments constraint with the Substitution Rule,A E v, x \u2032 i k y \u2032 i \u2212 x \u2032 i , \u0398 k O \u03bb k k E v, x \u2032 i 2 k 2 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k 2\nFor brevity, let \u03c3 = E (y\u2032 i \u2212 x \u2032 i , \u0398 ) 2 .\nUsing the Substitution Rule, plugging Equation (33) back into (32), we getA v, E (1 \u2212 w \u2032 i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k (2\u01eb) k\u22121 \u03bb k k \u03c3 k/2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 (34)\nRecall, we have now bounded the first term of the last inequality in (30). Therefore, it remains to bound the second term of the last inequality in (30). Using the Substitution Rule, we haveA v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k = v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 \u2212 \u0398 + \u0398 k 2 k v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k + 2 k v, E (1 \u2212 w \u2032 i )x * i x * i , \u0398 \u2212 \u0398 k (35)\nWe again handle each term separately. Observe, the first term when decoupled is a statement about the uncorrupted samples. Therefore, using the SoS H \u00f6lder's Inequality (Fact 3.19),A v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k = E (1 \u2212 w \u2032 i ) v, x * i y * i \u2212 x * i , \u0398 k E (1 \u2212 w \u2032 i ) k\u22121 E v, x * i y * i \u2212 x * i , \u0398 k (2\u01eb) k\u22121 E v, x * i k y * i \u2212 x * i , \u0398 k (36)\nObserve, the uncorrupted samples have negatively correlated moments, and thusE v, x * i k y * i \u2212 x * i , \u0398 k O \u03bb k k E v, x * i 2 k/2 E y * i \u2212 x * i , \u0398 2 k/2\nThen, by the Substitution Rule, we can bound (36) as follows:A v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2\nIn order to bound the second term in (35), we use the SoS H \u00f6lder's Inequality,A v, E (1 \u2212 w \u2032 i )x * i x * i , \u0398 \u2212 \u0398 k = E (1 \u2212 w \u2032 i ) k\u22122 v, x * i x * i , \u0398 \u2212 \u0398 E 1 \u2212 w \u2032 i k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 ()\nCombining the bounds obtained in (37) and (38), we can restate Equation (35) as followsA v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k 2 k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2 + 2 k (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k/2 2\nCombining (39) with (34), we obtain an upper bound for the last inequality in Equation (30). Therefore, using the Substitution Rule, we obtainA v, E [x * i (y * i \u2212 x * i , \u0398 )] k 2 k (2\u01eb) k\u22121 \u03bb k k \u03c3 k/2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 + 2 2k (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 + 2 2k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2 (40)\nRecall, an upper bound on Equation (28) suffices to obtain an upper bound on v, \u03a3 \u0398 \u2212 \u0398 as follows:A v, \u03a3 \u0398 \u2212 \u0398 k 2 2k (2\u01eb) k\u22121 \u03bb k k \u03c3 k/2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 + 2 3k (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 + 2 3k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2 (41) Consider the substitution v \u2192 \u0398 \u2212 \u0398 . Then, v, \u03a3 \u0398 \u2212 \u0398 k = \u03a31/2 \u0398 \u2212 \u0398 2k 2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 = E x \u2032 i (x \u2032 i ) \u22a4 1/2 \u0398 \u2212 \u0398 k 2 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 = E x * i , \u0398 \u2212 \u0398 k 2 \u03bb 2k k \u03a31/2 \u0398 \u2212 \u0398 2k 2 v, \u03a3v k/2 = \u03a31/2 \u0398 \u2212 \u0398 k 2\nCombining the above with (41), we concludeA \u03a31/2 \u0398 \u2212 \u0398 2k 2 2 3k (2\u01eb) k\u22121 \u03bb k k \u03c3 k/2 E x \u2032 i (x \u2032 i ) \u22a4 1/2 \u0398 \u2212 \u0398 k 2 + 2 3k (2\u01eb) k\u22122 \u03bb 2k k \u03a31/2 \u0398 \u2212 \u0398 2k 2 + 2 3k (2\u01eb) k\u22121 \u03bb k k E y * i \u2212 x * i , \u0398 2 k/2 \u03a31/2 \u0398 \u2212 \u0398 k 2\nNext, we relate the covariance of the samples indicated by w to the covariance on the uncorrupted points. Observe, a real world proof of this follows simply from Fact 3.3. Lemma 5.6 (Bounding Sample Covariance). Consider the hypothesis of Theorem 5.1. Let w, x \u2032 , y \u2032 and \u0398 be feasible solutions for the polynomial constraint system A.Then, for \u03b4 O \u03bb k \u01eb 1\u22121/k < 1, A 2k w,x \u2032 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 1 + O \u03b4 k/2 v, \u03a3v k/2\nProof. Our proof closely follows Lemma 4.5 in [KS17c]. For i \u2208 [n], let z i be an indicator variable such z i (x* i \u2212 x \u2032 i ) = 0.\nObserve, there exists an assignment to z i such that \u2211 i\u2208[n] z i = (1 \u2212 \u01eb)n, since at most \u01ebn points were corrupted. Further, z 2 i = z i and 1 n z i = \u01eb. Then, using the Substitution Rule,A 2k w,x \u2032 v, E x \u2032 i (x \u2032 i ) \u22a4 \u2212 \u03a3 v k = v, E (1 + z i \u2212 z i ) x \u2032 i (x \u2032 i ) \u22a4 \u2212 x * i (x * i ) \u22a4 v k = E (1 \u2212 z i ) v, x \u2032 i (x \u2032 i ) \u22a4 \u2212 x * i (x * i ) \u22a4 , v k \u01eb k\u22122 \u2022 E v, x \u2032 i 2 \u2212 v, x * i 2 k/2 2 \u01eb k\u22122 E 2 k/2 v, x \u2032 i k + 2 k/2 v, x * i k 2 \u01eb k\u22122 2 k c k k E v, x \u2032 i 2 k/2 + \u03bb k k E v, x * i 2 k/2 2\nwhere the first inequality follows from applying the SoS H \u00f6lder's Inequality, the second follows from the SoS Almost Triangle Inequality and the third inequality follows from certifiable hypercontractivity of the SoS variables and the uncorrupted samples. Using the SoS Almost Triangle Inequality again, we haveA c k k E v, x \u2032 i 2 k/2 + \u03bb k k E v, x * i 2 k/2 2 \u03bb 2k k 2 2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k + v, \u03a3v k (44) Combining Equations 43, 44, we obtain A v, E x \u2032 i (x \u2032 i ) \u22a4 \u2212 \u03a3 v k \u01eb k\u22122 \u03bb 2k k 2 k+2 v, E x \u2032 i (x \u2032 i ) \u22a4 + \u03a3 v k (45)\nUsing Lemma A.4 from [KS17c], rearranging and setting k = k/2 yields the claim.\nLemma 5.7 (Rounding). Consider the hypothesis of Theorem 5.1. Let \u0398 = arg min \u0398 1 n \u2211 i\u2208[n] (y * i \u2212 x * i , \u0398\n) 2 be the empirical loss minimizer on the uncorrupted samples. Then,\u03a31/2 \u0398 \u2212 \u1ebc \u03b6 [\u0398] 2 O \u01eb 1\u2212 1 k \u03bb k \u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k 1 2k + E y * i \u2212 x * i , \u0398 2 1 2\nProof. Observe, combining Lemma 5.5 and Lemma 5.6, we obtainA \u03a31/2 \u0398 \u2212 \u0398 2k 2 O 2 3k \u01eb k\u22121 \u03bb k k 1 + 2 3k (2\u01eb) k\u22122 \u03bb 2k k \u03a31/2 \u0398 \u2212 \u0398 k 2 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k 2 + E y * i \u2212 x * i , \u0398 2 k 2\nUsing Cancellation within SoS (Fact 3.20) along with the SoS Almost Triangle Inequality, we can concludeA \u03a31/2 \u0398 \u2212 \u0398 2k 2 O 2 3k \u01eb k\u22121 \u03bb k k 2 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k + E y * i \u2212 x * i , \u0398 2 k (47)\nRecall, \u03b6 is a degree-O(k) pseudo-expectation satisfying A. Therefore, it follows from Fact 3.15 along with Equation 46,\u1ebc \u03b6 \u03a3 1 2 \u0398 \u2212 \u0398 2k 2 O 2 4k \u01eb k\u22121 \u03bb k k 2 \u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k + E y * i \u2212 x * i , \u0398 2 k (48)\nFurther, using Fact 3.14, we have\u03a3 1 2 \u0398 \u2212 \u1ebc \u03b6 \u0398 2k 2 \u1ebc \u03b6 \u03a3 1 2 \u0398 \u2212 \u0398 2k 2\n. Substituting above and taking the (1/2k)-th root,\u03a3 1 2 \u0398 \u2212 \u1ebc \u03b6 [\u0398] 2 O \u01eb 1\u2212 1 k \u03bb k \u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k + E y * i \u2212 x * i , \u0398 2 k 1/2k O \u01eb 1\u2212 1 k \u03bb k \u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k 1 2k + E y * i \u2212 x * i , \u0398 2 1 2\nwhich concludes the proof.\nLemma 5.8 (Bounding Optimization and Generalization Error). Under the hypothesis of Theorem 5.1,1. \u1ebc \u03b6 E (y \u2032 i \u2212 x \u2032 i , \u0398 ) 2 k 1 2k E y * i \u2212 x * i , \u0398 2 1 2 , and 2. For any \u03b6 > 0, if n n 0 , such that n 0 = \u2126 max{c 4 d/\u03b6 2 , d O(k) } , with probability at least 1 \u2212 1/poly(d), E y * i \u2212 x * i , \u0398 2 1 2 (1 + \u03b6) Ex,y\u223cD y \u2212 x, \u0398 * 2 1 2 .\nProof. We exhibit a degree-O(k) pseudo-distribution \u03b6 such that it is supported on a point mass and attains objective value at most E y * i \u2212 x * i , \u0398 2 1 2 . Since our objective function minimizes over all degree-O(k) pseudo-distributions, the resulting objective value w.r.t. \u03b6 can only be better. Let \u03b6 be the pseudo-distribution supported on (w, x * , y * , \u0398) such that w i = 1 if x i = x * i (i.e. the i-th sample is not corrupted.) It follows from n n 0 and Lemma 5.4 that this assignment satisfies the constraint system A \u01eb,\u03bb k . Then, the objective value satisfies\u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k \u1ebc \u03b6 E y \u2032 i \u2212 x \u2032 i , \u0398 2 k = E y * i \u2212 x * i , \u0398 2 k (50)\nTaking (1/2k)-th roots yields the first claim.\nTo bound the second claim, let U be the uniform distribution on the uncorrupted samples, x * i , y * i . Observe, by optimality of \u0398 on the uncorrupted samples, err U ( \u0398) err U (\u0398 * ). Consider the random variablez i = (y * i \u2212 x * i , \u0398 * ) 2 \u2212 Ex,y\u223cD (y \u2212 x, \u0398 * ) 2 .\nSince E [z i ] = 0, we apply Chebyschev's inequality to obtainPr 1 n \u2211 i\u2208[n] z i \u03b6 = E z 2 1 \u03b6 2 n E (y \u2212 x, \u0398 ) 4 \u03b6 2 n c 4 err D (\u0398 * ) 2 n\u03b6 2\nTherefore, with probability at least 1 \u2212 \u03b4,err U ( \u0398) 1 + c 4 n\u03b4 err D (\u0398 * )\nTherefore, setting n = \u2126(c 4 d/\u03b6 2 ), it follows that with probability 1 \u2212 1/poly(d), for any \u03b6 > 0,err U ( \u0398) (1 + \u03b6) err D (\u0398 * )\nTaking square-roots concludes the proof.\nProof of Theorem 5.1. Given n n 0 samples, it follows from Lemma 5.4, that with probability 1 \u2212 1/poly(d), the constraint system A \u01eb,\u03bb k is feasible. Let \u03be 1 be the event that the system is feasible and condition on it. Then, it follows from Lemma 5.7 and Lemma 5.8, with probability 1 \u2212 1/poly(d),\u03a31/2 \u1ebc \u03b6 [\u0398] \u2212 \u0398 2 O \u03bb k \u01eb 1\u22121/k err D (\u0398 * ) 1/2 (51)\nLet \u03be 2 be the event that (51) holds and condition on it. It then follows from Fact 3.2, with probability 1 \u2212 1/poly(d),(\u03a3 * ) 1/2 \u1ebc \u03b6 [\u0398] \u2212 \u0398 2 O \u03bb k \u01eb 1\u22121/k err D (\u0398 * ) 1/2 (52)\nLet \u03be 2 be the event that (52) holds and condition on it. It remains to relate the hyperplanes \u0398 and \u0398 * . By reverse triangle inequality,(\u03a3 * ) 1/2 \u1ebc \u03b6 [\u0398] \u2212 \u0398 * 2 \u2212 (\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 (\u03a3 * ) 1/2 \u1ebc \u03b6 [\u0398] \u2212 \u0398 2 Using normal equations, we have \u0398 = \u03a3\u22121 E [x i y i ] and \u0398 * = (\u03a3 * ) \u22121 E [xy]. Since \u03a3 (1 + 0.01)\u03a3 * , (\u03a3 * ) 1/2 \u0398 * \u2212 \u0398 2 = (\u03a3 * ) 1/2 \u03a3\u22121 \u03a3\u0398 * \u2212 \u03a3\u22121 E [x i y i ] 2 = (\u03a3 * ) 1/2 \u03a3\u22121 E x i y i \u2212 x \u22a4 i \u0398 * 2 1.01 E (\u03a3 * ) \u22121/2 x i y i \u2212 x \u22a4 i \u0398 * 2 (53)\nBy Jensen's inequalityE \uf8ee \uf8f0 1 n \u2211 i\u2208[n] (\u03a3 * ) \u22121/2 x i y i \u2212 x \u22a4 i \u0398 * 2 \uf8f9 \uf8fb E \uf8ee \uf8f0 1 n \u2211 i\u2208[n] (\u03a3 * ) \u22121/2 x i y i \u2212 x \u22a4 i \u0398 * 2 2 \uf8f9 \uf8fb Let z i = \u2211 i\u2208[n] (\u03a3 * ) \u22121/2 x i y i \u2212 x \u22a4 i \u0398 * . Let (\u2211 i\u2208[n] z i ) 1\ndenote the first coordinate of the vector. We bound the expectation of this coordinate as follows:E ( \u2211 i\u2208[n] z i ) 2 1 = 1 n 2 E \u2211 i,i \u2032 \u2208[n] (\u03a3 * ) \u22121 x i x i \u2032 1 y i \u2212 x \u22a4 i \u0398 * y i \u2032 \u2212 x \u22a4 i \u2032 \u0398 * = 1 n 2 E \u2211 i\u2208[n] (\u03a3 * ) \u22121 x 2 i 1 y i \u2212 x \u22a4 i \u0398 * 2 = 1 n E (\u03a3 * ) \u22121 (x) 2 1 y \u2212 x \u22a4 \u0398 *\nwhere the second equality follows from independence of the samples. Using negatively correlated moments, we haveE (\u03a3 * ) \u22121 (x) 2 1 y \u2212 x \u22a4 \u0398 * 2 E (\u03a3 * ) \u22121 (x) 2 1 E y \u2212 x \u22a4 \u0398 * 2\nSetting v = (\u03a3 * ) 1/2 e 1 and using Hypercontractivity of the covariates and the noise in the above equation,E \u03a3 \u22121 (x) 2 1 E y \u2212 x \u22a4 \u0398 * 2 O c 2 2 \u03b7 2 2 err D (\u0398 * )\nSumming over the coordinates, and combining (54), (55), we obtainE \uf8ee \uf8f0 1 n \u2211 i\u2208[n] (\u03a3 * ) \u22121/2 x i y i \u2212 x \u22a4 i \u0398 * 2 \uf8f9 \uf8fb O(c 2 \u03b7 2 ) d err D (\u0398 * ) n ()\nApplying Chebyschev's Inequality , with probability 1 \u2212 \u03b4(\u03a3 * ) 1/2 \u0398 * \u2212 E \u03b6 [\u0398] 2 O \u03bb k \u01eb 1\u22121/k + c 2 \u03b7 2 d \u03b4n err D (\u0398 * ) 1/2\nSince n n 0 , we can simplify the above bound and obtain the claim.\nThe running time of our algorithm is clearly dominated by computing a degree-O(k) pseudodistribution satisfying A \u01eb,\u03bb k . Given that our constraint system consists of O(n) variables and poly(n) constraints, it follows from Fact 3.12 that the pseudo-distribution \u03b6 can be computed in n O(k) time.\nObserve,E x k 1 = 1 \u22121 x k /2 = 1/(k + 1) and E x 2 1 = 1 \u22121 x 2 /2 = 1/3. Further, E[x k 2 ] = (1 \u2212 \u01eb) \u01eb\u03c3 (\u01eb\u03c3) k+1 k + 1 + \u01eb \u2022 1 \u01eb 1/k k = 1 + (1 \u2212 \u01eb) (k + 1) (\u01eb\u03c3) k E[x 2 2 ] = (1 \u2212 \u01eb) 3\u01eb\u03c3 (\u01eb\u03c3) 3 + \u01eb \u2022 1 \u01eb 1/k 2 = \u01eb 1\u22122/k + 1 \u2212 \u01eb 3 (\u01eb\u03c3) 2 Observe, E[x k 2 ] (1/(c\u01eb k/2\u22121 )) E[x 2 2 ] k/2\n, for a fixed constant c. Then, for any unit vector v,E x, v k E (2x 1 v 1 ) k + (2x 2 v 2 ) k c k/2 k E (x 1 v) 2 k/2 + E (x 2 v) 2 k/2 c k/2 k E x, v 2 k/2 where c k/2 k = 2 k /c\u01eb k/2\u22121 . Therefore, D 1 , D 2 are (c k , k)-hypercontractive over \u00ca 2 .\nNext, we com- pute the TV distance between the two distributions.d TV (D 1 , D 2 ) = 1 2 \u00ca 2 \u00d7\u00ca |D 1 (x 1 , x 2 , y) \u2212 D 2 (x 1 , x 2 , y)| = 1 2 \u00ca 2 D 1 (x 1 , x 2 ) \u00ca |D 1 (y | (x 1 , x 2 )) \u2212 D 2 (y | (x 1 , x 2 ))|\nwhere the last equality follows from the definition of conditional probability. It follows from Equation (57) that D 1 (y | (x 1 , x 2 )) = U (x 1 + x 2 \u2212 \u03c3, x 1 + x 2 + \u03c3) and D 2 (y | (x 1 , x 2 )) = U (x 1 \u2212 x 2 \u2212 \u03c3, x 1 \u2212 x 2 + \u03c3). If |x 2 | \u03c3 the intervals are disjoint and |D 1 (y | (x 1 , x 2 )) \u2212 D 2 (y | (x 1 , x 2 ))| = 2. If |x 2 | < \u03c3,d TV (D 1 , D 2 ) = 1 2 \u00ca 2I {|x 2 | \u03c3} + 2|x 2 | \u03c3 I {|x 2 | < \u03c3} = Pr [|x 2 | \u03c3] + 1 \u03c3 E x 2 \u223cD 1 [|x 2 |I {|x 2 | < \u03c3}] =\nFinally, we lower bound the parameter distance. Since the coordinates are independent, \u03a3 is a diagonal matrix with\u03a3 1,1 = E x 2 1 = 1/3 and \u03a3 2,2 = E x 2 2 = \u01eb 1\u22122/k + (\u01eb\u03c3) 2 /3. Further, \u0398 1 \u2212 \u0398 2 = (0, 2). Thus, \u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = 2\u03a3 1/2 2,2 2\u01eb 1/2\u22121/k . For any \u03c3 < 1/\u01eb 1/k , \u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 2 \u01eb 1/2\u22121/k > 2 \u03c3 \u01eb 1/2 2 \u221a c k \u03c3 \u01eb 1\u22121/k\nwhich concludes the proof.\n\n6.2 Agnostic Model\nNext, consider the setting where we simply observe samples from (x, y) \u223c D, and our goal is to return is to return the minimizer of the squared error, given by\u0398 * = E xx \u22a4 \u22121 E [xy].\nHere, the distribution of the noise is allowed to depend on the covariates arbitrarily. We further assume the noise is hypercontractive and obtain a lower bound proportional to \u01eb 1\u22122/k for recovering an estimator close to \u0398 * . This matches the upper boundd obtained in Corollary 4.2.\nTheorem 6.2 (Agnostic Model Lower Bound, Theorem 1.11 restated). For any \u01eb > 0, there exist two distributions D 1 , D 2 over \u00ca 2 \u00d7 \u00ca such that the marginal distribution over \u00ca 2 has covariance \u03a3 and is(c k , k)-hypercontractive yet \u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = \u2126 \u221a c k \u03c3 \u01eb 1\u22122/k\n, where \u0398 1 , \u0398 2 be the optimal hyper- planes for D 1 and D 2 respectively, \u03c3 = max(err D 1 (\u0398 1 ), err D 2 (\u0398 2 )) < 1/\u01eb 1/k and the noise is a function of the marginal distribution of \u00ca 2 .\nProof. We provide a proof for the special case of k = 4. The same proof extends to general k.\nWe again construct a 2-dimensional instance where the marginal distribution over covariates is identical for D 1 and D 2 . The pdf is given as follows: for q \u2208 {1, 2} on the first coordinate, x 1 ,D q (x 1 ) = 1/2, if x 1 \u2208 [\u22121, 1]\n0 otherwise and on the second coordinate, x 2 ,D q (x 2 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u01eb/2, if x 2 \u2208 {\u22121/\u01eb 1/4 , 1/\u01eb 1/4 } 1\u2212\u01eb 2 if x 2 \u2208 [\u22121, 1] 0 otherwise Observe, E x 4 1 = 1/5 and E x 2 1 = 1/3. Similarly, E x 4 2 = 1 + (1 \u2212 \u01eb)/5 and E x 2 2 = \u221a \u01eb + (1 \u2212 \u01eb)/3\n. Therefore, the marginal distribution over \u00ca 2 is (c, 4)-hypercontractive for a fixed constant c. Next, letD 1 (y | (x 1 , x 2 )) = x 2 and D 2 (y | (x 1 , x 2 )) = 0 if |x 2 | = 1/\u01eb 1/4\nx 2 otherwise (59)\nThen,d TV (D 1 , D 2 ) = 1 2 \u00ca 2 D 1 (x 1 , x 2 ) \u00ca |D 1 (y | (x 1 , x 2 )) \u2212 D 2 (y | (x 1 , x 2 ))| = 1 2 \u00ca |x 2 |I |x 2 | = 1/\u01eb 1/4 = \u01eb\nSince the coordinates over \u00ca 2 are independent the covariance matrix \u03a3 is diagonal, such that\u03a3 1,1 = E x 2 1 = 1/3 and \u03a3 2,2 = E x 2 2 = \u221a \u01eb + (1 \u2212 \u01eb)/3.\nWe can then compute the optimal hyperplanes using normal equations:\u0398 1 = E x\u223cD 1 xx \u22a4 \u22121 E x,y\u223cD 1 [xy] = \u03a3 \u22121 E\nx,y\u223cD 1\n\n[xy]\nObserve, using (59),E [x 1 y] = \u00ca x 1 yD 1 (x 1 y) = \u00ca\nx 1 yD 1 (x 1 )D 1 (y) = 0 since x 1 and y are independent. Further,E [x 2 y] = \u00ca x 2 yD(x 2 , y) = \u00ca x 2 2 D(x 2 ) = \u221a \u01eb + (1 \u2212 \u01eb)/3\nTherefore, \u0398 1 = (0, 1). Similarly,\u0398 2 = E x\u223cD 2 xx \u22a4 \u22121 E x,y\u223cD 2 [xy] = \u03a3 \u22121 E x,y\u223cD 2 [xy]\nFurther, E [x 1 y] = 0. However,E [x 2 y] = \u00ca x 2 yD 2 (x 2 , y) = \u00ca x 2 2 I {|x 2 | 1} D 2 (x 2 ) = 1 \u2212 \u01eb Therefore, \u0398 2 = 0, 1\u2212\u01eb 1+ \u221a \u01eb . Then, \u03a3 1/2 (\u0398 1 \u2212 \u0398 2 ) 2 = \u221a \u01eb + (1 \u2212 \u01eb)/3 \u2022 \u221a \u01eb + \u01eb 1 + \u221a \u01eb = \u2126( \u221a \u01eb)\nwhich concludes the proof.\n\n7 Bounded Covariance Distributions\nIn the heavy-tailed setting, the minimal assumption is to consider a distribution over the covariates with bounded covariance. In this setting, we show that robust estimators for linear regression do not exist, even when the underlying linear model has no noise, i.e. the uncorrupted samples are drawn as follows: y i = \u0398 * , x i . Our hard instance relies on the so called Student's t-distribution, which has heavy tails when the degrees of freedom are close to 2. Definition 7.2 (Student's t-distribution). Given \u03bd > 1, Student's t-distribution has the following probability density function:f \u03bd (t) = \u0393 \u03bd+1 2 \u221a \u03bd\u03c0 \u0393 \u03bd 2 1 + t 2 \u03bd \u2212 \u03bd+1 2 where \u0393(z) = \u221e 0 x z\u22121 e x dxE x\u223c f \u03bd x 2 = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u221e if 1 < \u03bd 2 \u03bd \u03bd\u22122 if 2 < \u03bd undefined otherwise\nThe intuition behind our lower bound is to construct a regression instance where the covariates are non-zero only on an \u01eb-measure support and are heavy tailed when non-zero. As a consequence, the adversary can introduce a distinct valid regression instance by changing a different \u01eb-measure of the support. It is then information-theoretically impossible to distinguish between the true and the planted models.\nProof of Theorem 7.1. We construct a 1-dimensional instance where the marginal distribution over covariates is identical for D 1 and D 2 . The pdf is given as follows: for q \u2208 {1, 2} the marginal distribution on the covariates is given as follows:D q (x) = 1 \u2212 \u01eb, if x = 0 \u01eb \u2022 f 2+\u01eb (x) otherwise\nThe distribution of the labels is gives as follows:D 1 (y | x) = x and D 2 (y | x) = \u2212x\nNext, we compute the total variation distance between D 1 and D 2 . Recall,d TV (D 1 , D 2 ) = 1 2 \u00ca\u00d7\u00ca |D 1 (x, y) \u2212 D 2 (x, y)| = 1 2 \u00ca D 1 (x) \u00ca |D 1 (y | x) \u2212 D 2 (y | x)| = 1 2 \u00ca |D 1 (y | x) \u2212 D 2 (y | x)| (I {x = 0} + I {x = 0}) = 1 2 \u00ca |2x|I {x = 0} \u01eb (60)\nObserve, since the regression instances have no noise, we can obtain a perfect fit by setting \u0398 1 = 1 and \u0398 2 = \u22121. Further, for q \u2208 {1, 2},E x\u223cD q [x] = (1 \u2212 \u01eb) \u2022 0 + \u01eb \u2022 E x\u223c f 2+\u01eb [x] = 0\nandE x\u223cD q x 2 = (1 \u2212 \u01eb) \u2022 0 + \u01eb \u2022 E x\u223c f 2+\u01eb x 2 = \u01eb \u2022 2 + \u01eb \u01eb (62)\nThus,E x\u223cD q x 2 1/2 (\u0398 1 \u2212 \u0398 2 ) = (2 + \u01eb) 1/2 \u2022 2 = 2\u03c3\nwhich completes the proof. We note that the 4-th moment of f 2+\u01eb (t) is infinite and thus it is not hypercontractive, even for k = 4.\n\nA Robust Identifiability for Arbitrary Noise\nProof of Corollary 4.2. Consider a maximal coupling of D, D \u2032 over (x, y) \u00d7 (x \u2032 , y \u2032 ), denoted by G, such that the marginal of G (x, y) is D, the marginal on (x \u2032 , y \u2032 ) is D \u2032 and P G [I {(x, y) = (x \u2032 , y \u2032 )}] = 1 \u2212 \u01eb. Then, for all v, v, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) = E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) + xy \u2212 xy = E G [ v, x ( x, \u0398 D \u2212 y) ] + E G [ v, x (y \u2212 x, \u0398 D \u2032 ) ]\nSince \u0398 D is the minimizer for the least squares loss, we have the following gradient condition : for allv \u2208 R d , E (x,y)\u223cD [ v, ( x, \u0398 D \u2212 y)x ] = 0 ()\nSince G is a coupling, using the gradient condition (65) and using that 1= I {(x, y) = (x \u2032 , y \u2032 )} +I {(x, y) = (x \u2032 , y \u2032 )}, we can rewrite equation (64) as v, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) = E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) = E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 )\nConsider the first term in the last equality above. Using the gradient condition for \u0398 D \u2032 along with H \u00f6lder's Inequality, we haveE G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) = E D \u2032 v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 \u2212 E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) = E G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k/(k\u22122) (k\u22122)/k \u2022 E D \u2032 v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k/2 2/k (67) Observe, since G is a maximal coupling EG [I {(x, y) = (x \u2032 , y \u2032 )}] (k\u22122)/k \u01eb 1\u22122/k .\nHere, we no longer have independence of the noise and the covariates, therefore using Cauchy-SchwarzE D \u2032 v, x \u2032 k/2 \u2022 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k/2 E D \u2032 v, x \u2032 k E D \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k 1/2\nBy hypercontractivity of the covariates and the noise, we haveE D \u2032 v, x \u2032 k 1/k E D \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 k 1/k O(c k \u03b7 k ) v \u22a4 \u03a3 D \u2032 v 1/2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1/2\nTherefore, we can restate (67) as followsE G v, x \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 I (x, y) = (x \u2032 , y \u2032 ) O c k \u03b7 k \u01eb k\u22122 k v \u22a4 \u03a3 D \u2032 v 1 2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1 2\nIt remains to bound the second term in the last equality of equation ( 66), and we proceed as follows :E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) = E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) + E G v, x (y \u2212 x, \u0398 D ) I (x, y) = (x \u2032 , y \u2032 )\nWe bound the two terms above separately. Observe, applying H \u00f6lder's Inequality to the first term, we haveE G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k (70)\nTo bound the second term in equation 69, we again use H \u00f6lder's Inequality followed by Cauchy-Schwarz noise and covariates.E G v, x (y \u2212 x, \u0398 D ) I (x, y) = (x \u2032 , y \u2032 ) E G I (x, y) = (x \u2032 , y \u2032 ) k\u22121 k E G v, x (y \u2212 x, \u0398 D ) k 1 k \u01eb k\u22122 k E x\u223cD v, x k/2 2/k E x,y\u223cD (y \u2212 x, \u0398 D ) k/2 2/k \u01eb k\u22122 k c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2 (71)\nwhere the last inequality follows from hypercontractivity of the covariates and noise. Substituting the upper bounds obtained in Equations ( 70) and (71) back in to (69),E G v, x (y \u2212 x, \u0398 D \u2032 ) I (x, y) = (x \u2032 , y \u2032 ) \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k 2 2 k + \u01eb k\u22122 k c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2\nTherefore, we can now upper bound both terms in Equation (66) as follows:v, \u03a3 D (\u0398 D \u2212 \u0398 D \u2032 ) O c k \u03b7 k \u01eb k\u22122 k v \u22a4 \u03a3 D \u2032 v 1/2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 1/2 + O \u01eb k\u22122 k E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k/2 2/k + O \u01eb k\u22122 k c k \u03b7 k v \u22a4 \u03a3 D v 1/2 E x,y\u223cD (y \u2212 x, \u0398 D ) 2 1/2 (72)\nRecall, since the marginals of D and D\u2032 on \u00ca d are (c k , k)-hypercontractive and D \u2212 D \u2032 TV \u01eb, it follows from Fact 3.3 that (1 \u2212 0.1) \u03a3 D \u2032 \u03a3 D (1 + 0.1) \u03a3 D \u2032 (73) when \u01eb O (1/c k k) k/(k\u22122) . Now, consider the substitution v = \u0398 D \u2212 \u0398 D \u2032 . Observe, E G v, xx \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) k/2 2/k = E D x, (\u0398 D \u2212 \u0398 D \u2032 ) k 2/k c 2 k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 2 (74)\nThen, using the bounds in ( 73) and (74 Given the parameter recovery result above, we bound the least-squares loss between the two hyperplanes on D as follows:) along with v = \u0398 D \u2212 \u0398 D \u2032 in Equation 72, we have 1 \u2212 O \u01eb k\u22122 k c 2 k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 2 O c k \u03b7 k \u01eb k\u22122 k \u03a3 1/2 D (\u0398 D \u2212 \u0398 D \u2032 ) 2 E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032err D (\u0398 D ) \u2212 err D (\u0398 D \u2032 ) = E (x,y)\u223cD y \u2212 x \u22a4 \u0398 D 2 \u2212 y \u2212 x \u22a4 \u0398 D \u2032 + x \u22a4 \u0398 D \u2212 x \u22a4 \u0398 D 2 = E (x,y)\u223cD x, (\u0398 D \u2212 \u0398 D \u2032 ) 2 + 2(y \u2212 x \u22a4 \u0398 D )x \u22a4 (\u0398 D \u2212 \u0398 D \u2032 ) O c 2 k \u03b7 2 k \u01eb 2\u22124/k E x \u2032 ,y \u2032 \u223cD \u2032 y \u2032 \u2212 x \u2032 , \u0398 D \u2032 2 + E x,y\u223cD (y \u2212 x, \u0398 D ) 2 (76)\nwhere the last inequality follows from observing E \u0398 D \u2212 \u0398 D \u2032 , x(y \u2212 x \u22a4 \u0398 D ) = 0 (gradient condition) and squaring the parameter recovery bound.\n\nB Efficient Estimator for Arbitrary Noise\nIn this section, we provide a proof of the key SoS lemma required to obtain a polynomial time estimator. The remainder of the proof, including the feasibility of the constraints and rounding is identical to the one presented in Section 5.\n\nLemma\n) k\u22122 c k k \u03b7 k k \u03c3 k/2 E x \u2032 i (x \u2032 i ) \u22a4 1/2 \u0398 \u2212 \u0398 k 2 + 2 3k (2\u01eb) k\u22122 c 2k k \u03a31/2 \u0398 \u2212 \u0398 2k 2 + 2 3k (2\u01eb) k\u22122 c k k \u03b7 k k E y * i \u2212 x * i , \u0398 2 k/2 \u03a31/2 \u0398 \u2212 \u0398 k 2Proof. Consider the empirical covariance of the uncorrupted set given by \u03a3 = E x * i (x * i ) \u22a4 . Then, using the Substitution Rule, along with Fact 3.182k \u0398 v, \u03a3 \u0398 \u2212 \u0398 k = v, E x * i (x * i ) \u22a4 \u0398 \u2212 \u0398 + x * i y * i \u2212 x * i y * i k = v, E x * i x * i , \u0398 \u2212 y * i + E [x * i (y * i \u2212 x * i , \u0398 )] k 2 k v, E x * i x * i , \u0398 \u2212 y * i k + 2 k v, E [x * i (y * i \u2212 x * i , \u0398 )] k ()\nSince \u0398 is the minimizer of E ( x * i , \u0398 \u2212 y * i ) 2 , the gradient condition (appearing in Equation (65) of the indentifiability proof) implies this term is 0. Therefore, it suffices to bound the second term.\nFor all i \u2208 [n], let w \u2032 i = w i iff the i-th sample is uncorrupted in X \u01eb , i.e. x i = x * i . Then, it is easy to see that\u2211 i w \u2032 i (1 \u2212 2\u01eb)n. Further, since A 2 w (1 \u2212 w \u2032 i w i ) 2 = (1 \u2212 w \u2032 i w i ) , A 2 w 1 n \u2211 i\u2208[n] (1 \u2212 w \u2032 i w i ) 2 = 1 n \u2211 i\u2208[n] (1 \u2212 w \u2032 i w i ) 2\u01eb\nThe above equation bounds the uncorrupted points in X \u01eb that are not indicated by w. Then, using the Substitution Rule, along with the SoS Almost Triangle Inequality (Fact 3.18),A 2k \u0398,w \u2032 v, E [x * i (y * i \u2212 x * i , \u0398 )] k = v, E x * i y * i \u2212 x * i , \u0398 (w \u2032 i + 1 \u2212 w \u2032 i ) k = v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) + E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k 2 k v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) k + 2 k v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k (79)\nConsider the first term of the last inequality in (79). Observe, since w \u2032 i x * i = w i w \u2032 i x \u2032 i and similarly,w \u2032 i y * i = w i w \u2032 i y \u2032 i , A 4 \u0398,w \u2032 E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) = E w \u2032 i w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398\nFor the sake of brevity, the subsequent statements hold for relevant SoS variables and have degree O(k) proofs. Using the Substitution Rule,A v, E w \u2032 i x * i (y * i \u2212 x * i , \u0398 ) k = v, E w \u2032 i w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = v, E x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 + E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k 2 k v, E x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k + 2 k v, E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k (80)\nObserve, the first term in the last inequality above is identically 0, since we enforce the gradient condition on the SoS variables x \u2032 , y \u2032 and \u0398. We can then rewrite the second term using linearity of expectation, followed by applying SoS H \u00f6lder's Inequality (Fact 3.19) combined with A 2w (1 \u2212 w \u2032 i w i ) 2 = 1 \u2212 w \u2032 i w i to get A v, E (1 \u2212 w \u2032 i w i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = E v, (1 \u2212 w \u2032 i )w i x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k = E (1 \u2212 w \u2032 i w i ) v, x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k E (1 \u2212 w \u2032 i w i ) k\u22122 E v, x \u2032 i k/2 y \u2032 i \u2212 x \u2032 i , \u0398 k/2 (2\u01eb) k\u22122 E v, x \u2032 i k E y \u2032 i \u2212 x \u2032 i , \u0398 k (81)\nwhere the last inequality follows from (78) and the SoS Cauchy Schwarz Inequality. Using the certifiable-hypercontractivity of the covariates,A 2k w,x \u2032 E v, x \u2032 i k c k k E v, x \u2032 i 2 k/2 = c k k v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2\n(82)\nFurther, using certifiable hypercontractivity of the noise,A E y \u2032 i \u2212 w i x \u2032 i , \u0398 k \u03b7 k k E (y \u2032 i \u2212 x \u2032 i , \u0398 ) 2 ) k/2 (83) Recall, \u03c3 = E (y \u2032 i \u2212 x \u2032 i , \u0398 ) 2 )\nCombining the upper bounds obtained in (82) and ( 83), and plug- ging this back into (81), we getA v, E (1 \u2212 w \u2032 i )x \u2032 i y \u2032 i \u2212 x \u2032 i , \u0398 k (2\u01eb) k\u22122 c k k \u03b7 k k \u03c3 k/2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2\n(84)\nRecall, we have now bounded the first term of the last inequality in (79). Therefore, it remains to bound the second term of the last inequality in (79). Using the Substitution Rule, we haveA v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k = v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 \u2212 \u0398 + \u0398 k 2 k v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k + 2 k v, E (1 \u2212 w \u2032 i )x * i x * i , \u0398 \u2212 \u0398 k ()\nWe again handle each term separately. Observe, the first term when decoupled is a statement about the uncorrupted samples. Therefore, using the SoS H \u00f6lder's Inequality (Fact 3.19),A v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k = E (1 \u2212 w \u2032 i ) v, x * i y * i \u2212 x * i , \u0398 k E (1 \u2212 w \u2032 i ) k\u22122 E v, x * i y * i \u2212 x * i , \u0398 k/2 (2\u01eb) k\u22122 E v, x * i k E y * i \u2212 x * i , \u0398 k (86)\nUsing certifiable hypercontractivity of the x * i s,E v, x * i k c k k E v, x * i 2 k/2 = c k k v, \u03a3v k/2\nwhere \u03a3 = E x * i (x * i ) \u22a4 and similarly using hypercontractivity of the noise,E y * i \u2212 x * i , \u0398 k \u03b7 k k E y * i \u2212 x * i , \u0398 2 k/2\nThen, by the Substitution Rule, we can bound (86) as follows:A v, E (1 \u2212 w \u2032 i )x * i y * i \u2212 x * i , \u0398 k (2\u01eb) k\u22121 c k k \u03b7 k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2\n(87) In order to bound the second term in (85), we use the SoS H \u00f6lder's Inequality,A v, E (1 \u2212 w \u2032 i )x * i x * i , \u0398 \u2212 \u0398 k = E (1 \u2212 w \u2032 i ) k\u22122 v, x * i x * i , \u0398 \u2212 \u0398 E 1 \u2212 w \u2032 i k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 ()\nCombining the bounds obtained in (87) and (88), we can restate Equation (85) as followsA v, E (1 \u2212 w \u2032 i )x * i (y * i \u2212 x * i , \u0398 ) k 2 k (2\u01eb) k\u22121 c k k \u03b7 k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2 + 2 k (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2\nCombining (89) with (84), we obtain an upper bound for the last inequality in Equation (79). Therefore, using the Substitution Rule, we obtainA v, E [x * i (y * i \u2212 x * i , \u0398 )] k 2 k (2\u01eb) k\u22121 c k k \u03b7 k k \u03c3 k/2 v, E x \u2032 i (x \u2032 i ) \u22a4 v k/2 + 2 2k (2\u01eb) k\u22122 E v \u22a4 x * i (x * i ) \u22a4 (\u0398 \u2212 \u0398) k 2 2 + 2 2k (2\u01eb) k\u22121 c k k \u03b7 k k E y * i \u2212 x * i , \u0398 2 k/2 v, \u03a3v k/2 (90)\nThe remaining proof is identical to Lemma 5.5.\n\nC Proof of Lemma 3.4\nLemma C.1 (L \u00f6wner Ordering for Hypercontractive Samples (restated)). Let D be a (c k , k)-hypercontractive distribution with covariance \u03a3 and and let U be the uniform distribution over n samples. Then, with probability 1 \u2212 \u03b4,\u03a3 \u22121/2 \u03a3\u03a3 \u22121/2 \u2212 I F C 4 d 2 \u221a n \u221a \u03b4 ,\nwhere \u03a3 = 1 n \u2211 i\u2208 [n] x i x \u22a4 i .\n\nFootnotes:\n1: We thank Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt for communicating their observation to us.\n2: Here, we assume that the bit complexity of the constraints in A is (n + m) O (1).\n\nReferences:\n\n- Boaz Barak. Proofs, beliefs, and algorithms through the lens of sum-of-squares. 14- Vijay V. S. P. Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Sum- of-squares certificates for maxima of random tensors on the sphere. In APPROX-RANDOM, volume 81 of LIPIcs, pages 31:1-31:20. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2017. 6\n\n- Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresh- olding. In Advances in Neural Information Processing Systems, pages 721-729, 2015. 5\n\n- Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar. Con- sistent robust regression. In Advances in Neural Information Processing Systems, pages 2110-2119, 2017. 5\n\n- Ainesh Bakshi and Pravesh Kothari. List-decodable subspace recovery via sum-of- squares. arXiv preprint arXiv:2002.05139, 2020. 6, 14\n\n- Ainesh Bakshi and Pravesh Kothari. Outlier-robust clustering of non-spherical mix- tures. arXiv preprint arXiv:2005.02970, 2020. 6, 14, 17\n\n- Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In STOC, pages 143-151. ACM, 2015. 6\n\n- Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estima- tion in nearly-linear time. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 2755-2771. SIAM, 2019. 1\n\n- Yu Cheng, Ilias Diakonikolas, Rong Ge, and David P. Woodruff. Faster algorithms for high-dimensional robust covariance estimation. In Alina Beygelzimer and Daniel Hsu, editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 727-757. PMLR, 2019. 1\n\n- CHK + 20] Yeshwanth Cherapanamjeri, Samuel B. Hopkins, Tarun Kathuria, Prasad Raghaven- dra, and Nilesh Tripuraneni. Algorithms for heavy-tailed statistics: Regression, co- variance estimation, and beyond. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, page 601-609, New York, NY, USA, 2020. Association for Computing Machinery. 6\n\n- Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau. List decodable mean estimation in nearly linear time. arXiv preprint arXiv:2005.09796, 2020. 6\n\n- Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In STOC, pages 47-60. ACM, 2017. 1\n\n- Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Ro- bustly learning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020. 6\n\n- Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high- dimensional robust statistics. arXiv preprint arXiv:1911.05911, 2019. 2, 5\n\n- DKK + 16] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 655-664. IEEE, 2016. 1, 5\n\n- DKK + 17] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. In ICML, vol- ume 70 of Proceedings of Machine Learning Research, pages 999-1008. PMLR, 2017.\n\n- DKK + 18a] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a gaussian: Getting optimal error, efficiently. In Artur Czumaj, editor, Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2683-2702. SIAM, 2018. 1\n\n- DKK + 18b] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018. 4\n\n- DKK + 19] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alis- tair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1596-1606. PMLR, 2019. 1, 5\n\n- Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In FOCS, pages 73-84. IEEE Computer Society, 2017. 1\n\n- Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. arXiv preprint arXiv:1806.00040, 2018. 4, 5\n\n- DKS19] Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for robust linear regression. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages 2745-2754. SIAM, 2019. 1, 5\n\n- FKP + 19] Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic Proofs and Efficient Algorithm Design. now the essence of knowledge, 2019. 16, 22\n\n- M. Gr \u00f6tschel, L. Lov\u00e1sz, and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1(2):169-197, 1981. 15\n\n- Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021-1034, 2018. 6\n\n- Samuel B Hopkins. Sub-gaussian mean estimation in polynomial time. arXiv preprint arXiv:1809.07425, 2018. 6\n\n- Samuel B Hopkins, Tselil Schramm, and Jonathan Shi. A robust spectral algorithm for overcomplete tensor decomposition. In Conference on Learning Theory, pages 1683-1722, 2019. 6\n\n- Peter J Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):73-101, 1964. 1\n\n- Peter J Huber. Robust statistics. In International Encyclopedia of Statistical Science, pages 1248-1251. Springer, 2011. 1\n\n- Louis A Jaeckel. Estimating regression coefficients by minimizing the dispersion of the residuals. The Annals of Mathematical Statistics, pages 1449-1458, 1972. 1\n\n- Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear re- gression. In Advances in Neural Information Processing Systems, pages 7423-7432, 2019. 1, 6, 14\n\n- Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier- robust regression. arXiv preprint arXiv:1803.03241, 2018. 14\n\n- Adam R. Klivans, Pravesh K. Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018, pages 1420-1430, 2018. 1, 3, 4, 5, 6, 9, 10, 11, 36\n\n- Pravesh K. Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms. 2017. 2, 6, 11, 12\n\n- Pravesh K. Kothari and David Steurer. Outlier-robust moment-estimation via sum- of-squares. CoRR, abs/1711.11581, 2017. 1\n\n- Pravesh K Kothari and David Steurer. Outlier-robust moment-estimation via sum-of- squares. arXiv preprint arXiv:1711.11581, 2017. 2, 5, 6, 11, 12, 13, 14, 28\n\n- Jean B. Lasserre. New positive semidefinite relaxations for nonconvex quadratic pro- grams. In Advances in convex analysis and global optimization (Pythagorion, 2000), volume 54 of Nonconvex Optim. Appl., pages 319-331. Kluwer Acad. Publ., Dor- drecht, 2001. 15\n\n- Jerry Zheng Li. Principled approaches to robust machine learning and beyond. PhD thesis, Massachusetts Institute of Technology, 2018. 5\n\n- Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 665-674. IEEE, 2016. 1, 5\n\n- Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decomposi- tions with sum-of-squares. In FOCS, pages 438-446. IEEE Computer Society, 2016. 6\n\n- Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decomposi- tions with sum-of-squares. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 438-446. IEEE, 2016. 14\n\n- Yurii Nesterov. Squared functional systems and optimization problems. In High performance optimization, volume 33 of Appl. Optim., pages 405-440. Kluwer Acad. Publ., Dordrecht, 2000. 15\n\n- Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization. PhD thesis, California Institute of Tech- nology, 2000. 15\n\n- Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation via robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(3):601-627, 2020. 1, 3, 4, 5, 9, 11\n\n- Peter J Rousseeuw. Least median of squares regression. Journal of the American statistical association, 79(388):871-880, 1984. 1\n\n- Prasad Raghavendra, Satish Rao, and Tselil Schramm. Strongly refuting random csps below the spectral threshold. In STOC, pages 121-131. ACM, 2017. 6\n\n- Prasad Raghavendra, Tselil Schramm, and David Steurer. High-dimensional estima- tion via sum-of-squares proofs. arXiv preprint arXiv:1807.11419, 6, 2018. 5\n\n- Peter Rousseeuw and Victor Yohai. Robust regression by means of s-estimators. In Robust and nonlinear time series analysis, pages 256-272. Springer, 1984. 1\n\n- Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 161-180. SIAM, 2020. 1, 6\n\n- Prasad Raghavendra and Morris Yau. List decodable subspace recovery, 2020. 6\n\n- Arun Sai Suggala, Kush Bhatia, Pradeep Ravikumar, and Prateek Jain. Adaptive hard thresholding for near-optimal consistent robust regression. arXiv preprint arXiv:1903.08192, 2019. 5\n\n- Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the presence of arbitrary outliers. CoRR, abs/1703.04940, 2017. 1\n\n- Pranab Kumar Sen. Estimates of the regression coefficient based on kendall's tau. Journal of the American statistical association, 63(324):1379-1389, 1968. 1\n\n- N. Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet., (1):128-139, 222, 1987. 15\n\n- Tselil Schramm and David Steurer. Fast and robust tensor decomposition with ap- plications to dictionary learning. In COLT, volume 65 of Proceedings of Machine Learning Research, pages 1760-1793. PMLR, 2017. 6\n\n- Jacob Steinhardt. ROBUST LEARNING: INFORMATION THEORY AND ALGORITHMS. PhD thesis, STANFORD UNIVERSITY, 2018. 5\n\n- Henri Theil. A rank-invariant method of linear and polynomial regression analy- sis. In Henri Theil's contributions to economics and econometrics, pages 345-381. Springer, 1992. 1\n\n- Sanford Weisberg. Applied linear regression, volume 528. John Wiley & Sons, 2005. 1\n\n- Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Generalized resilience and robust statistics. arXiv preprint arXiv:1909.08755, 2019. 6, 9\n\n- Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Robust estimation via generalized quasi-gradients. arXiv preprint arXiv:2005.14073, 2020. 3, 4, 5, 6\n\n", "annotations": {"ReferenceToTable": [{"begin": 10114, "end": 10115, "target": "#tab_1", "idx": 0}], "ReferenceToFootnote": [{"begin": 16892, "end": 16893, "target": "#foot_0", "idx": 0}, {"begin": 41358, "end": 41359, "target": "#foot_1", "idx": 1}], "SectionMain": [{"begin": 1440, "end": 88924, "idx": 0}], "ReferenceToFormula": [{"begin": 21246, "end": 21247, "target": "#formula_15", "idx": 0}, {"begin": 47838, "end": 47840, "target": "#formula_54", "idx": 1}, {"begin": 48792, "end": 48794, "idx": 2}, {"begin": 48818, "end": 48820, "idx": 3}, {"begin": 49522, "end": 49524, "idx": 4}, {"begin": 57391, "end": 57393, "idx": 5}, {"begin": 67110, "end": 67112, "target": "#formula_114", "idx": 6}, {"begin": 80405, "end": 80407, "target": "#formula_164", "idx": 7}, {"begin": 81356, "end": 81358, "idx": 8}, {"begin": 82205, "end": 82207, "idx": 9}, {"begin": 86361, "end": 86363, "idx": 10}], "SectionReference": [{"begin": 89126, "end": 100419, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1440, "idx": 0}], "Div": [{"begin": 70, "end": 1432, "idx": 0}, {"begin": 1443, "end": 5023, "idx": 1}, {"begin": 5025, "end": 7438, "idx": 2}, {"begin": 7440, "end": 13936, "idx": 3}, {"begin": 13938, "end": 14369, "idx": 4}, {"begin": 14371, "end": 15907, "idx": 5}, {"begin": 15909, "end": 16803, "idx": 6}, {"begin": 16805, "end": 17592, "idx": 7}, {"begin": 17594, "end": 19246, "idx": 8}, {"begin": 19248, "end": 25215, "idx": 9}, {"begin": 25217, "end": 31746, "idx": 10}, {"begin": 31748, "end": 34125, "idx": 11}, {"begin": 34127, "end": 35708, "idx": 12}, {"begin": 35710, "end": 38927, "idx": 13}, {"begin": 38929, "end": 39251, "idx": 14}, {"begin": 39253, "end": 43540, "idx": 15}, {"begin": 43542, "end": 45126, "idx": 16}, {"begin": 45128, "end": 50780, "idx": 17}, {"begin": 50782, "end": 51263, "idx": 18}, {"begin": 51265, "end": 54607, "idx": 19}, {"begin": 54609, "end": 55041, "idx": 20}, {"begin": 55043, "end": 58184, "idx": 21}, {"begin": 58186, "end": 73364, "idx": 22}, {"begin": 73366, "end": 75517, "idx": 23}, {"begin": 75519, "end": 76046, "idx": 24}, {"begin": 76048, "end": 78340, "idx": 25}, {"begin": 78342, "end": 82911, "idx": 26}, {"begin": 82913, "end": 83193, "idx": 27}, {"begin": 83195, "end": 88602, "idx": 28}, {"begin": 88604, "end": 88924, "idx": 29}], "Head": [{"begin": 1443, "end": 1457, "n": "1", "idx": 0}, {"begin": 5025, "end": 5040, "n": "1.1", "idx": 1}, {"begin": 7440, "end": 7481, "idx": 2}, {"begin": 13938, "end": 13955, "n": "1.2", "idx": 3}, {"begin": 14371, "end": 14389, "idx": 4}, {"begin": 15909, "end": 15935, "idx": 5}, {"begin": 16805, "end": 16821, "idx": 6}, {"begin": 17594, "end": 17614, "n": "2", "idx": 7}, {"begin": 19248, "end": 19306, "n": "2.1", "idx": 8}, {"begin": 25217, "end": 25277, "n": "2.2", "idx": 9}, {"begin": 31748, "end": 31791, "n": "2.3", "idx": 10}, {"begin": 34127, "end": 34152, "n": "2.4", "idx": 11}, {"begin": 35710, "end": 35725, "n": "3", "idx": 12}, {"begin": 38929, "end": 38986, "idx": 13}, {"begin": 39253, "end": 39272, "n": "3.1", "idx": 14}, {"begin": 43542, "end": 43643, "idx": 15}, {"begin": 45128, "end": 45188, "n": "4", "idx": 16}, {"begin": 50782, "end": 50835, "idx": 17}, {"begin": 51265, "end": 51303, "n": "5", "idx": 18}, {"begin": 54609, "end": 54673, "n": "2.", "idx": 19}, {"begin": 55043, "end": 55055, "n": "5.1", "idx": 20}, {"begin": 58186, "end": 58227, "idx": 21}, {"begin": 73366, "end": 73384, "n": "6.2", "idx": 22}, {"begin": 75519, "end": 75523, "idx": 23}, {"begin": 76048, "end": 76082, "n": "7", "idx": 24}, {"begin": 78342, "end": 78386, "idx": 25}, {"begin": 82913, "end": 82954, "idx": 26}, {"begin": 83195, "end": 83200, "idx": 27}, {"begin": 88604, "end": 88624, "idx": 28}], "Paragraph": [{"begin": 70, "end": 818, "idx": 0}, {"begin": 819, "end": 1432, "idx": 1}, {"begin": 1458, "end": 2154, "idx": 2}, {"begin": 2155, "end": 2778, "idx": 3}, {"begin": 2779, "end": 3319, "idx": 4}, {"begin": 3320, "end": 3909, "idx": 5}, {"begin": 3910, "end": 4269, "idx": 6}, {"begin": 4270, "end": 4789, "idx": 7}, {"begin": 4790, "end": 5023, "idx": 8}, {"begin": 5041, "end": 5454, "idx": 9}, {"begin": 5455, "end": 6543, "idx": 10}, {"begin": 6544, "end": 7191, "idx": 11}, {"begin": 7192, "end": 7438, "idx": 12}, {"begin": 7645, "end": 7955, "idx": 13}, {"begin": 7956, "end": 8458, "idx": 14}, {"begin": 8527, "end": 8773, "idx": 15}, {"begin": 8774, "end": 9235, "idx": 16}, {"begin": 9236, "end": 9882, "idx": 17}, {"begin": 9973, "end": 10513, "idx": 18}, {"begin": 10514, "end": 10661, "idx": 19}, {"begin": 10778, "end": 10808, "idx": 20}, {"begin": 10852, "end": 11116, "idx": 21}, {"begin": 11165, "end": 11238, "idx": 22}, {"begin": 11366, "end": 11653, "idx": 23}, {"begin": 11654, "end": 11971, "idx": 24}, {"begin": 12062, "end": 12147, "idx": 25}, {"begin": 12148, "end": 12410, "idx": 26}, {"begin": 12463, "end": 12534, "idx": 27}, {"begin": 12624, "end": 13117, "idx": 28}, {"begin": 13118, "end": 13435, "idx": 29}, {"begin": 13489, "end": 13497, "idx": 30}, {"begin": 13538, "end": 13936, "idx": 31}, {"begin": 13956, "end": 14369, "idx": 32}, {"begin": 14390, "end": 15471, "idx": 33}, {"begin": 15472, "end": 15907, "idx": 34}, {"begin": 15936, "end": 16408, "idx": 35}, {"begin": 16409, "end": 16803, "idx": 36}, {"begin": 16822, "end": 17362, "idx": 37}, {"begin": 17363, "end": 17592, "idx": 38}, {"begin": 17615, "end": 18284, "idx": 39}, {"begin": 18285, "end": 19246, "idx": 40}, {"begin": 19307, "end": 19784, "idx": 41}, {"begin": 19822, "end": 19961, "idx": 42}, {"begin": 20050, "end": 20449, "idx": 43}, {"begin": 20450, "end": 20741, "idx": 44}, {"begin": 20831, "end": 21080, "idx": 45}, {"begin": 21197, "end": 21320, "idx": 46}, {"begin": 21501, "end": 21699, "idx": 47}, {"begin": 21842, "end": 22066, "idx": 48}, {"begin": 22067, "end": 22394, "idx": 49}, {"begin": 22457, "end": 22607, "idx": 50}, {"begin": 22608, "end": 23028, "idx": 51}, {"begin": 23092, "end": 23188, "idx": 52}, {"begin": 23189, "end": 23682, "idx": 53}, {"begin": 23683, "end": 23851, "idx": 54}, {"begin": 23901, "end": 25215, "idx": 55}, {"begin": 25278, "end": 26206, "idx": 56}, {"begin": 26207, "end": 26876, "idx": 57}, {"begin": 26877, "end": 27233, "idx": 58}, {"begin": 27386, "end": 27645, "idx": 59}, {"begin": 27646, "end": 28266, "idx": 60}, {"begin": 28389, "end": 28950, "idx": 61}, {"begin": 29075, "end": 29288, "idx": 62}, {"begin": 29289, "end": 30087, "idx": 63}, {"begin": 30273, "end": 30685, "idx": 64}, {"begin": 30686, "end": 30885, "idx": 65}, {"begin": 30962, "end": 31161, "idx": 66}, {"begin": 31162, "end": 31746, "idx": 67}, {"begin": 31792, "end": 32157, "idx": 68}, {"begin": 32215, "end": 32324, "idx": 69}, {"begin": 32377, "end": 32690, "idx": 70}, {"begin": 32734, "end": 32790, "idx": 71}, {"begin": 32791, "end": 33452, "idx": 72}, {"begin": 33453, "end": 34125, "idx": 73}, {"begin": 34153, "end": 34506, "idx": 74}, {"begin": 34507, "end": 35178, "idx": 75}, {"begin": 35179, "end": 35708, "idx": 76}, {"begin": 35726, "end": 36232, "idx": 77}, {"begin": 36258, "end": 36549, "idx": 78}, {"begin": 37213, "end": 37428, "idx": 79}, {"begin": 37501, "end": 37586, "idx": 80}, {"begin": 37691, "end": 37830, "idx": 81}, {"begin": 37883, "end": 38046, "idx": 82}, {"begin": 38047, "end": 38375, "idx": 83}, {"begin": 38376, "end": 38716, "idx": 84}, {"begin": 38717, "end": 38927, "idx": 85}, {"begin": 39230, "end": 39251, "idx": 86}, {"begin": 39273, "end": 39574, "idx": 87}, {"begin": 39575, "end": 39919, "idx": 88}, {"begin": 39920, "end": 40179, "idx": 89}, {"begin": 40180, "end": 40229, "idx": 90}, {"begin": 40230, "end": 40395, "idx": 91}, {"begin": 40427, "end": 40773, "idx": 92}, {"begin": 40774, "end": 40921, "idx": 93}, {"begin": 41006, "end": 41220, "idx": 94}, {"begin": 41221, "end": 41707, "idx": 95}, {"begin": 41708, "end": 41884, "idx": 96}, {"begin": 41908, "end": 42009, "idx": 97}, {"begin": 42145, "end": 42186, "idx": 98}, {"begin": 42211, "end": 42444, "idx": 99}, {"begin": 42478, "end": 42670, "idx": 100}, {"begin": 42693, "end": 42750, "idx": 101}, {"begin": 42792, "end": 42981, "idx": 102}, {"begin": 43088, "end": 43142, "idx": 103}, {"begin": 43187, "end": 43252, "idx": 104}, {"begin": 43313, "end": 43540, "idx": 105}, {"begin": 43679, "end": 44170, "idx": 106}, {"begin": 44171, "end": 44401, "idx": 107}, {"begin": 44424, "end": 44520, "idx": 108}, {"begin": 44592, "end": 44776, "idx": 109}, {"begin": 44881, "end": 45091, "idx": 110}, {"begin": 45189, "end": 45646, "idx": 111}, {"begin": 46232, "end": 46349, "idx": 112}, {"begin": 46388, "end": 46549, "idx": 113}, {"begin": 46779, "end": 46910, "idx": 114}, {"begin": 47194, "end": 47335, "idx": 115}, {"begin": 47415, "end": 47477, "idx": 116}, {"begin": 47597, "end": 47638, "idx": 117}, {"begin": 47767, "end": 47870, "idx": 118}, {"begin": 48026, "end": 48132, "idx": 119}, {"begin": 48300, "end": 48423, "idx": 120}, {"begin": 48651, "end": 48822, "idx": 121}, {"begin": 48976, "end": 49049, "idx": 122}, {"begin": 49485, "end": 49525, "idx": 123}, {"begin": 49807, "end": 49925, "idx": 124}, {"begin": 49926, "end": 50045, "idx": 125}, {"begin": 50292, "end": 50440, "idx": 126}, {"begin": 50441, "end": 50780, "idx": 127}, {"begin": 50932, "end": 51083, "idx": 128}, {"begin": 51304, "end": 51789, "idx": 129}, {"begin": 51790, "end": 51850, "idx": 130}, {"begin": 52007, "end": 52256, "idx": 131}, {"begin": 52930, "end": 53256, "idx": 132}, {"begin": 53573, "end": 54192, "idx": 133}, {"begin": 54316, "end": 54377, "idx": 134}, {"begin": 54378, "end": 54452, "idx": 135}, {"begin": 54453, "end": 54463, "idx": 136}, {"begin": 54674, "end": 54760, "idx": 137}, {"begin": 54761, "end": 55041, "idx": 138}, {"begin": 55056, "end": 55404, "idx": 139}, {"begin": 55405, "end": 55674, "idx": 140}, {"begin": 55675, "end": 55796, "idx": 141}, {"begin": 55870, "end": 56001, "idx": 142}, {"begin": 56002, "end": 56416, "idx": 143}, {"begin": 56477, "end": 56676, "idx": 144}, {"begin": 56843, "end": 56889, "idx": 145}, {"begin": 56934, "end": 57005, "idx": 146}, {"begin": 57087, "end": 57147, "idx": 147}, {"begin": 57216, "end": 57291, "idx": 148}, {"begin": 57369, "end": 57445, "idx": 149}, {"begin": 57558, "end": 57584, "idx": 150}, {"begin": 57585, "end": 57827, "idx": 151}, {"begin": 57886, "end": 58184, "idx": 152}, {"begin": 58228, "end": 58354, "idx": 153}, {"begin": 58403, "end": 58548, "idx": 154}, {"begin": 58744, "end": 58931, "idx": 155}, {"begin": 59157, "end": 59256, "idx": 156}, {"begin": 59292, "end": 59494, "idx": 157}, {"begin": 59495, "end": 59681, "idx": 158}, {"begin": 59834, "end": 60012, "idx": 159}, {"begin": 60299, "end": 60369, "idx": 160}, {"begin": 60529, "end": 60669, "idx": 161}, {"begin": 60924, "end": 61216, "idx": 162}, {"begin": 61509, "end": 61660, "idx": 163}, {"begin": 61746, "end": 61771, "idx": 164}, {"begin": 61793, "end": 61867, "idx": 165}, {"begin": 61966, "end": 62156, "idx": 166}, {"begin": 62354, "end": 62535, "idx": 167}, {"begin": 62723, "end": 62800, "idx": 168}, {"begin": 62884, "end": 62945, "idx": 169}, {"begin": 63041, "end": 63120, "idx": 170}, {"begin": 63307, "end": 63394, "idx": 171}, {"begin": 63549, "end": 63691, "idx": 172}, {"begin": 63898, "end": 63997, "idx": 173}, {"begin": 64429, "end": 64471, "idx": 174}, {"begin": 64650, "end": 64986, "idx": 175}, {"begin": 65078, "end": 65190, "idx": 176}, {"begin": 65209, "end": 65398, "idx": 177}, {"begin": 65709, "end": 66021, "idx": 178}, {"begin": 66249, "end": 66328, "idx": 179}, {"begin": 66329, "end": 66388, "idx": 180}, {"begin": 66440, "end": 66509, "idx": 181}, {"begin": 66604, "end": 66664, "idx": 182}, {"begin": 66795, "end": 66899, "idx": 183}, {"begin": 66993, "end": 67113, "idx": 184}, {"begin": 67214, "end": 67247, "idx": 185}, {"begin": 67289, "end": 67340, "idx": 186}, {"begin": 67510, "end": 67536, "idx": 187}, {"begin": 67537, "end": 67633, "idx": 188}, {"begin": 67879, "end": 68453, "idx": 189}, {"begin": 68540, "end": 68586, "idx": 190}, {"begin": 68587, "end": 68801, "idx": 191}, {"begin": 68859, "end": 68921, "idx": 192}, {"begin": 69004, "end": 69047, "idx": 193}, {"begin": 69082, "end": 69182, "idx": 194}, {"begin": 69214, "end": 69254, "idx": 195}, {"begin": 69255, "end": 69553, "idx": 196}, {"begin": 69608, "end": 69728, "idx": 197}, {"begin": 69789, "end": 69927, "idx": 198}, {"begin": 70258, "end": 70280, "idx": 199}, {"begin": 70467, "end": 70565, "idx": 200}, {"begin": 70760, "end": 70872, "idx": 201}, {"begin": 70942, "end": 71052, "idx": 202}, {"begin": 71110, "end": 71175, "idx": 203}, {"begin": 71263, "end": 71320, "idx": 204}, {"begin": 71393, "end": 71460, "idx": 205}, {"begin": 71461, "end": 71756, "idx": 206}, {"begin": 71757, "end": 71765, "idx": 207}, {"begin": 72047, "end": 72101, "idx": 208}, {"begin": 72300, "end": 72365, "idx": 209}, {"begin": 72519, "end": 72627, "idx": 210}, {"begin": 72992, "end": 73106, "idx": 211}, {"begin": 73338, "end": 73364, "idx": 212}, {"begin": 73385, "end": 73544, "idx": 213}, {"begin": 73568, "end": 73852, "idx": 214}, {"begin": 73853, "end": 74054, "idx": 215}, {"begin": 74126, "end": 74318, "idx": 216}, {"begin": 74319, "end": 74412, "idx": 217}, {"begin": 74413, "end": 74610, "idx": 218}, {"begin": 74645, "end": 74692, "idx": 219}, {"begin": 74897, "end": 75005, "idx": 220}, {"begin": 75085, "end": 75103, "idx": 221}, {"begin": 75104, "end": 75109, "idx": 222}, {"begin": 75243, "end": 75336, "idx": 223}, {"begin": 75397, "end": 75464, "idx": 224}, {"begin": 75510, "end": 75517, "idx": 225}, {"begin": 75524, "end": 75544, "idx": 226}, {"begin": 75579, "end": 75647, "idx": 227}, {"begin": 75713, "end": 75748, "idx": 228}, {"begin": 75807, "end": 75839, "idx": 229}, {"begin": 76020, "end": 76046, "idx": 230}, {"begin": 76083, "end": 76677, "idx": 231}, {"begin": 76830, "end": 77240, "idx": 232}, {"begin": 77241, "end": 77488, "idx": 233}, {"begin": 77538, "end": 77589, "idx": 234}, {"begin": 77626, "end": 77701, "idx": 235}, {"begin": 77890, "end": 78030, "idx": 236}, {"begin": 78081, "end": 78084, "idx": 237}, {"begin": 78150, "end": 78155, "idx": 238}, {"begin": 78207, "end": 78340, "idx": 239}, {"begin": 78387, "end": 78549, "idx": 240}, {"begin": 78751, "end": 78856, "idx": 241}, {"begin": 78905, "end": 78978, "idx": 242}, {"begin": 79296, "end": 79427, "idx": 243}, {"begin": 79801, "end": 79901, "idx": 244}, {"begin": 79987, "end": 80049, "idx": 245}, {"begin": 80166, "end": 80207, "idx": 246}, {"begin": 80334, "end": 80437, "idx": 247}, {"begin": 80588, "end": 80694, "idx": 248}, {"begin": 80862, "end": 80985, "idx": 249}, {"begin": 81215, "end": 81385, "idx": 250}, {"begin": 81537, "end": 81610, "idx": 251}, {"begin": 81820, "end": 81858, "idx": 252}, {"begin": 82177, "end": 82336, "idx": 253}, {"begin": 82763, "end": 82911, "idx": 254}, {"begin": 82955, "end": 83193, "idx": 255}, {"begin": 83365, "end": 83518, "idx": 256}, {"begin": 83742, "end": 83952, "idx": 257}, {"begin": 83953, "end": 84077, "idx": 258}, {"begin": 84230, "end": 84408, "idx": 259}, {"begin": 84695, "end": 84810, "idx": 260}, {"begin": 84925, "end": 85065, "idx": 261}, {"begin": 85320, "end": 85612, "idx": 262}, {"begin": 85913, "end": 86055, "idx": 263}, {"begin": 86138, "end": 86142, "idx": 264}, {"begin": 86143, "end": 86202, "idx": 265}, {"begin": 86311, "end": 86408, "idx": 266}, {"begin": 86508, "end": 86512, "idx": 267}, {"begin": 86513, "end": 86703, "idx": 268}, {"begin": 86899, "end": 87080, "idx": 269}, {"begin": 87272, "end": 87324, "idx": 270}, {"begin": 87378, "end": 87459, "idx": 271}, {"begin": 87513, "end": 87574, "idx": 272}, {"begin": 87676, "end": 87760, "idx": 273}, {"begin": 87947, "end": 88034, "idx": 274}, {"begin": 88195, "end": 88337, "idx": 275}, {"begin": 88556, "end": 88602, "idx": 276}, {"begin": 88625, "end": 88851, "idx": 277}, {"begin": 88890, "end": 88924, "idx": 278}], "ReferenceToBib": [{"begin": 1716, "end": 1723, "target": "#b27", "idx": 0}, {"begin": 2083, "end": 2090, "target": "#b34", "idx": 1}, {"begin": 2140, "end": 2146, "target": "#b30", "idx": 2}, {"begin": 2147, "end": 2153, "target": "#b48", "idx": 3}, {"begin": 2381, "end": 2388, "target": "#b57", "idx": 4}, {"begin": 2885, "end": 2892, "target": "#b28", "idx": 5}, {"begin": 2915, "end": 2922, "target": "#b56", "idx": 6}, {"begin": 2923, "end": 2929, "target": "#b52", "idx": 7}, {"begin": 2944, "end": 2951, "target": "#b29", "idx": 8}, {"begin": 2974, "end": 2981, "target": "#b44", "idx": 9}, {"begin": 2999, "end": 3005, "target": "#b47", "idx": 10}, {"begin": 3310, "end": 3318, "target": "#b32", "idx": 11}, {"begin": 3568, "end": 3576, "target": "#b32", "idx": 12}, {"begin": 3577, "end": 3584, "target": "#b43", "idx": 13}, {"begin": 3585, "end": 3591, "idx": 14}, {"begin": 5892, "end": 5898, "target": "#b13", "idx": 15}, {"begin": 10144, "end": 10152, "target": "#b32", "idx": 16}, {"begin": 10275, "end": 10283, "target": "#b43", "idx": 17}, {"begin": 10430, "end": 10437, "target": "#b59", "idx": 18}, {"begin": 13021, "end": 13028, "target": "#b35", "idx": 19}, {"begin": 13696, "end": 13703, "target": "#b20", "idx": 20}, {"begin": 14342, "end": 14349, "target": "#b46", "idx": 21}, {"begin": 14350, "end": 14355, "target": "#b37", "idx": 22}, {"begin": 14356, "end": 14362, "target": "#b55", "idx": 23}, {"begin": 14363, "end": 14368, "target": "#b13", "idx": 24}, {"begin": 14514, "end": 14522, "target": "#b43", "idx": 25}, {"begin": 15075, "end": 15083, "target": "#b32", "idx": 26}, {"begin": 15320, "end": 15327, "target": "#b59", "idx": 27}, {"begin": 16163, "end": 16170, "target": "#b6", "idx": 28}, {"begin": 16345, "end": 16352, "target": "#b1", "idx": 29}, {"begin": 16400, "end": 16407, "target": "#b45", "idx": 30}, {"begin": 16548, "end": 16555, "target": "#b35", "idx": 31}, {"begin": 16575, "end": 16583, "target": "#b32", "idx": 32}, {"begin": 16625, "end": 16632, "target": "#b33", "idx": 33}, {"begin": 16633, "end": 16638, "target": "#b24", "idx": 34}, {"begin": 16663, "end": 16670, "target": "#b5", "idx": 35}, {"begin": 16671, "end": 16678, "target": "#b12", "idx": 36}, {"begin": 16939, "end": 16946, "target": "#b58", "idx": 37}, {"begin": 24258, "end": 24266, "target": "#b32", "idx": 38}, {"begin": 25599, "end": 25607, "target": "#b32", "idx": 39}, {"begin": 27636, "end": 27644, "target": "#b32", "idx": 40}, {"begin": 30026, "end": 30034, "target": "#b32", "idx": 41}, {"begin": 30312, "end": 30320, "target": "#b32", "idx": 42}, {"begin": 30528, "end": 30535, "target": "#b35", "idx": 43}, {"begin": 30606, "end": 30613, "target": "#b33", "idx": 44}, {"begin": 31052, "end": 31060, "target": "#b43", "idx": 45}, {"begin": 35163, "end": 35170, "target": "#b33", "idx": 46}, {"begin": 35171, "end": 35177, "target": "#b35", "idx": 47}, {"begin": 38118, "end": 38125, "target": "#b35", "idx": 48}, {"begin": 39425, "end": 39436, "idx": 49}, {"begin": 39474, "end": 39482, "target": "#b40", "idx": 50}, {"begin": 40912, "end": 40919, "target": "#b23", "idx": 51}, {"begin": 41083, "end": 41090, "target": "#b23", "idx": 52}, {"begin": 42431, "end": 42434, "idx": 53}, {"begin": 45048, "end": 45055, "target": "#b5", "idx": 54}, {"begin": 65124, "end": 65131, "target": "#b35", "idx": 55}, {"begin": 66270, "end": 66277, "target": "#b35", "idx": 56}, {"begin": 88909, "end": 88912, "idx": 57}, {"begin": 89120, "end": 89123, "target": "#b16", "idx": 58}], "ReferenceString": [{"begin": 89141, "end": 89223, "id": "b0", "idx": 0}, {"begin": 89225, "end": 89482, "id": "b1", "idx": 1}, {"begin": 89486, "end": 89653, "id": "b2", "idx": 2}, {"begin": 89657, "end": 89841, "id": "b3", "idx": 3}, {"begin": 89845, "end": 89978, "id": "b4", "idx": 4}, {"begin": 89982, "end": 90120, "id": "b5", "idx": 5}, {"begin": 90124, "end": 90287, "id": "b6", "idx": 6}, {"begin": 90291, "end": 90594, "id": "b7", "idx": 7}, {"begin": 90598, "end": 90935, "id": "b8", "idx": 8}, {"begin": 90939, "end": 91312, "id": "b9", "idx": 9}, {"begin": 91316, "end": 91470, "id": "b10", "idx": 10}, {"begin": 91474, "end": 91593, "id": "b11", "idx": 11}, {"begin": 91597, "end": 91769, "id": "b12", "idx": 12}, {"begin": 91773, "end": 91922, "id": "b13", "idx": 13}, {"begin": 91926, "end": 92221, "id": "b14", "idx": 14}, {"begin": 92225, "end": 92477, "id": "b15", "idx": 15}, {"begin": 92481, "end": 92845, "id": "b16", "idx": 16}, {"begin": 92849, "end": 93060, "id": "b17", "idx": 17}, {"begin": 93064, "end": 93505, "id": "b18", "idx": 18}, {"begin": 93509, "end": 93725, "id": "b19", "idx": 19}, {"begin": 93729, "end": 93895, "id": "b20", "idx": 20}, {"begin": 93899, "end": 94227, "id": "b21", "idx": 21}, {"begin": 94231, "end": 94391, "id": "b22", "idx": 22}, {"begin": 94395, "end": 94549, "id": "b23", "idx": 23}, {"begin": 94553, "end": 94742, "id": "b24", "idx": 24}, {"begin": 94746, "end": 94853, "id": "b25", "idx": 25}, {"begin": 94857, "end": 95034, "id": "b26", "idx": 26}, {"begin": 95038, "end": 95156, "id": "b27", "idx": 27}, {"begin": 95160, "end": 95282, "id": "b28", "idx": 28}, {"begin": 95286, "end": 95448, "id": "b29", "idx": 29}, {"begin": 95452, "end": 95627, "id": "b30", "idx": 30}, {"begin": 95631, "end": 95774, "id": "b31", "idx": 31}, {"begin": 95778, "end": 96013, "id": "b32", "idx": 32}, {"begin": 96017, "end": 96129, "id": "b33", "idx": 33}, {"begin": 96133, "end": 96254, "id": "b34", "idx": 34}, {"begin": 96258, "end": 96415, "id": "b35", "idx": 35}, {"begin": 96419, "end": 96680, "id": "b36", "idx": 36}, {"begin": 96684, "end": 96819, "id": "b37", "idx": 37}, {"begin": 96823, "end": 97023, "id": "b38", "idx": 38}, {"begin": 97027, "end": 97186, "id": "b39", "idx": 39}, {"begin": 97190, "end": 97402, "id": "b40", "idx": 40}, {"begin": 97406, "end": 97591, "id": "b41", "idx": 41}, {"begin": 97595, "end": 97770, "id": "b42", "idx": 42}, {"begin": 97774, "end": 98020, "id": "b43", "idx": 43}, {"begin": 98024, "end": 98152, "id": "b44", "idx": 44}, {"begin": 98156, "end": 98304, "id": "b45", "idx": 45}, {"begin": 98308, "end": 98463, "id": "b46", "idx": 46}, {"begin": 98467, "end": 98623, "id": "b47", "idx": 47}, {"begin": 98627, "end": 98820, "id": "b48", "idx": 48}, {"begin": 98824, "end": 98900, "id": "b49", "idx": 49}, {"begin": 98904, "end": 99086, "id": "b50", "idx": 50}, {"begin": 99090, "end": 99250, "id": "b51", "idx": 51}, {"begin": 99254, "end": 99411, "id": "b52", "idx": 52}, {"begin": 99415, "end": 99525, "id": "b53", "idx": 53}, {"begin": 99529, "end": 99738, "id": "b54", "idx": 54}, {"begin": 99742, "end": 99852, "id": "b55", "idx": 55}, {"begin": 99856, "end": 100035, "id": "b56", "idx": 56}, {"begin": 100039, "end": 100122, "id": "b57", "idx": 57}, {"begin": 100126, "end": 100264, "id": "b58", "idx": 58}, {"begin": 100268, "end": 100417, "id": "b59", "idx": 59}], "Sentence": [{"begin": 70, "end": 256, "idx": 0}, {"begin": 257, "end": 381, "idx": 1}, {"begin": 382, "end": 576, "idx": 2}, {"begin": 577, "end": 679, "idx": 3}, {"begin": 680, "end": 818, "idx": 4}, {"begin": 819, "end": 947, "idx": 5}, {"begin": 948, "end": 1093, "idx": 6}, {"begin": 1094, "end": 1237, "idx": 7}, {"begin": 1238, "end": 1432, "idx": 8}, {"begin": 1458, "end": 1566, "idx": 9}, {"begin": 1567, "end": 1724, "idx": 10}, {"begin": 1725, "end": 2154, "idx": 11}, {"begin": 2155, "end": 2413, "idx": 12}, {"begin": 2414, "end": 2518, "idx": 13}, {"begin": 2519, "end": 2680, "idx": 14}, {"begin": 2681, "end": 2778, "idx": 15}, {"begin": 2779, "end": 3006, "idx": 16}, {"begin": 3007, "end": 3149, "idx": 17}, {"begin": 3150, "end": 3319, "idx": 18}, {"begin": 3320, "end": 3592, "idx": 19}, {"begin": 3593, "end": 3759, "idx": 20}, {"begin": 3760, "end": 3909, "idx": 21}, {"begin": 3910, "end": 4269, "idx": 22}, {"begin": 4270, "end": 4447, "idx": 23}, {"begin": 4448, "end": 4674, "idx": 24}, {"begin": 4675, "end": 4789, "idx": 25}, {"begin": 4790, "end": 5023, "idx": 26}, {"begin": 5041, "end": 5099, "idx": 27}, {"begin": 5100, "end": 5262, "idx": 28}, {"begin": 5263, "end": 5310, "idx": 29}, {"begin": 5311, "end": 5359, "idx": 30}, {"begin": 5360, "end": 5454, "idx": 31}, {"begin": 5455, "end": 5535, "idx": 32}, {"begin": 5536, "end": 5639, "idx": 33}, {"begin": 5640, "end": 5742, "idx": 34}, {"begin": 5743, "end": 5900, "idx": 35}, {"begin": 5901, "end": 5947, "idx": 36}, {"begin": 5948, "end": 6191, "idx": 37}, {"begin": 6192, "end": 6221, "idx": 38}, {"begin": 6222, "end": 6361, "idx": 39}, {"begin": 6362, "end": 6444, "idx": 40}, {"begin": 6445, "end": 6543, "idx": 41}, {"begin": 6544, "end": 6700, "idx": 42}, {"begin": 6701, "end": 6850, "idx": 43}, {"begin": 6851, "end": 6923, "idx": 44}, {"begin": 6924, "end": 7191, "idx": 45}, {"begin": 7192, "end": 7438, "idx": 46}, {"begin": 7645, "end": 7806, "idx": 47}, {"begin": 7807, "end": 7955, "idx": 48}, {"begin": 7956, "end": 8244, "idx": 49}, {"begin": 8245, "end": 8458, "idx": 50}, {"begin": 8527, "end": 8660, "idx": 51}, {"begin": 8661, "end": 8773, "idx": 52}, {"begin": 8774, "end": 8785, "idx": 53}, {"begin": 8786, "end": 9077, "idx": 54}, {"begin": 9078, "end": 9235, "idx": 55}, {"begin": 9236, "end": 9417, "idx": 56}, {"begin": 9418, "end": 9469, "idx": 57}, {"begin": 9470, "end": 9607, "idx": 58}, {"begin": 9608, "end": 9882, "idx": 59}, {"begin": 9973, "end": 9984, "idx": 60}, {"begin": 9985, "end": 10088, "idx": 61}, {"begin": 10089, "end": 10228, "idx": 62}, {"begin": 10229, "end": 10270, "idx": 63}, {"begin": 10271, "end": 10372, "idx": 64}, {"begin": 10373, "end": 10513, "idx": 65}, {"begin": 10514, "end": 10661, "idx": 66}, {"begin": 10778, "end": 10808, "idx": 67}, {"begin": 10852, "end": 10923, "idx": 68}, {"begin": 10924, "end": 11116, "idx": 69}, {"begin": 11165, "end": 11238, "idx": 70}, {"begin": 11366, "end": 11505, "idx": 71}, {"begin": 11506, "end": 11653, "idx": 72}, {"begin": 11654, "end": 11735, "idx": 73}, {"begin": 11736, "end": 11971, "idx": 74}, {"begin": 12062, "end": 12147, "idx": 75}, {"begin": 12148, "end": 12217, "idx": 76}, {"begin": 12218, "end": 12410, "idx": 77}, {"begin": 12463, "end": 12534, "idx": 78}, {"begin": 12624, "end": 12661, "idx": 79}, {"begin": 12662, "end": 12793, "idx": 80}, {"begin": 12794, "end": 13030, "idx": 81}, {"begin": 13031, "end": 13117, "idx": 82}, {"begin": 13118, "end": 13178, "idx": 83}, {"begin": 13179, "end": 13435, "idx": 84}, {"begin": 13489, "end": 13497, "idx": 85}, {"begin": 13538, "end": 13649, "idx": 86}, {"begin": 13650, "end": 13793, "idx": 87}, {"begin": 13794, "end": 13936, "idx": 88}, {"begin": 13956, "end": 13974, "idx": 89}, {"begin": 13975, "end": 14154, "idx": 90}, {"begin": 14155, "end": 14225, "idx": 91}, {"begin": 14226, "end": 14369, "idx": 92}, {"begin": 14390, "end": 14507, "idx": 93}, {"begin": 14508, "end": 14625, "idx": 94}, {"begin": 14626, "end": 14761, "idx": 95}, {"begin": 14762, "end": 14960, "idx": 96}, {"begin": 14961, "end": 15073, "idx": 97}, {"begin": 15074, "end": 15257, "idx": 98}, {"begin": 15258, "end": 15471, "idx": 99}, {"begin": 15472, "end": 15545, "idx": 100}, {"begin": 15546, "end": 15770, "idx": 101}, {"begin": 15771, "end": 15907, "idx": 102}, {"begin": 15936, "end": 16110, "idx": 103}, {"begin": 16111, "end": 16408, "idx": 104}, {"begin": 16409, "end": 16803, "idx": 105}, {"begin": 16822, "end": 16962, "idx": 106}, {"begin": 16963, "end": 17022, "idx": 107}, {"begin": 17023, "end": 17160, "idx": 108}, {"begin": 17161, "end": 17235, "idx": 109}, {"begin": 17236, "end": 17362, "idx": 110}, {"begin": 17363, "end": 17592, "idx": 111}, {"begin": 17615, "end": 17754, "idx": 112}, {"begin": 17755, "end": 18284, "idx": 113}, {"begin": 18285, "end": 18429, "idx": 114}, {"begin": 18430, "end": 18631, "idx": 115}, {"begin": 18632, "end": 18719, "idx": 116}, {"begin": 18720, "end": 18777, "idx": 117}, {"begin": 18778, "end": 18995, "idx": 118}, {"begin": 18996, "end": 19195, "idx": 119}, {"begin": 19196, "end": 19246, "idx": 120}, {"begin": 19307, "end": 19655, "idx": 121}, {"begin": 19656, "end": 19784, "idx": 122}, {"begin": 19822, "end": 19961, "idx": 123}, {"begin": 20050, "end": 20109, "idx": 124}, {"begin": 20110, "end": 20347, "idx": 125}, {"begin": 20348, "end": 20449, "idx": 126}, {"begin": 20450, "end": 20474, "idx": 127}, {"begin": 20475, "end": 20667, "idx": 128}, {"begin": 20668, "end": 20709, "idx": 129}, {"begin": 20710, "end": 20741, "idx": 130}, {"begin": 20831, "end": 20966, "idx": 131}, {"begin": 20967, "end": 21080, "idx": 132}, {"begin": 21197, "end": 21320, "idx": 133}, {"begin": 21501, "end": 21630, "idx": 134}, {"begin": 21631, "end": 21699, "idx": 135}, {"begin": 21842, "end": 21923, "idx": 136}, {"begin": 21924, "end": 22003, "idx": 137}, {"begin": 22004, "end": 22066, "idx": 138}, {"begin": 22067, "end": 22087, "idx": 139}, {"begin": 22088, "end": 22215, "idx": 140}, {"begin": 22216, "end": 22394, "idx": 141}, {"begin": 22457, "end": 22532, "idx": 142}, {"begin": 22533, "end": 22607, "idx": 143}, {"begin": 22608, "end": 22655, "idx": 144}, {"begin": 22656, "end": 22847, "idx": 145}, {"begin": 22848, "end": 23028, "idx": 146}, {"begin": 23092, "end": 23188, "idx": 147}, {"begin": 23189, "end": 23301, "idx": 148}, {"begin": 23302, "end": 23496, "idx": 149}, {"begin": 23497, "end": 23682, "idx": 150}, {"begin": 23683, "end": 23702, "idx": 151}, {"begin": 23703, "end": 23851, "idx": 152}, {"begin": 23901, "end": 24030, "idx": 153}, {"begin": 24031, "end": 24183, "idx": 154}, {"begin": 24184, "end": 24354, "idx": 155}, {"begin": 24355, "end": 24458, "idx": 156}, {"begin": 24459, "end": 24554, "idx": 157}, {"begin": 24555, "end": 24814, "idx": 158}, {"begin": 24815, "end": 24885, "idx": 159}, {"begin": 24886, "end": 25057, "idx": 160}, {"begin": 25058, "end": 25182, "idx": 161}, {"begin": 25183, "end": 25215, "idx": 162}, {"begin": 25278, "end": 25572, "idx": 163}, {"begin": 25573, "end": 25705, "idx": 164}, {"begin": 25706, "end": 26065, "idx": 165}, {"begin": 26066, "end": 26206, "idx": 166}, {"begin": 26207, "end": 26346, "idx": 167}, {"begin": 26347, "end": 26446, "idx": 168}, {"begin": 26447, "end": 26524, "idx": 169}, {"begin": 26525, "end": 26593, "idx": 170}, {"begin": 26594, "end": 26731, "idx": 171}, {"begin": 26732, "end": 26876, "idx": 172}, {"begin": 26877, "end": 26902, "idx": 173}, {"begin": 26903, "end": 27057, "idx": 174}, {"begin": 27058, "end": 27160, "idx": 175}, {"begin": 27161, "end": 27233, "idx": 176}, {"begin": 27386, "end": 27510, "idx": 177}, {"begin": 27511, "end": 27645, "idx": 178}, {"begin": 27646, "end": 27686, "idx": 179}, {"begin": 27687, "end": 27818, "idx": 180}, {"begin": 27819, "end": 27957, "idx": 181}, {"begin": 27958, "end": 28095, "idx": 182}, {"begin": 28096, "end": 28266, "idx": 183}, {"begin": 28389, "end": 28448, "idx": 184}, {"begin": 28449, "end": 28638, "idx": 185}, {"begin": 28639, "end": 28769, "idx": 186}, {"begin": 28770, "end": 28950, "idx": 187}, {"begin": 29075, "end": 29162, "idx": 188}, {"begin": 29163, "end": 29207, "idx": 189}, {"begin": 29208, "end": 29288, "idx": 190}, {"begin": 29289, "end": 29454, "idx": 191}, {"begin": 29455, "end": 29645, "idx": 192}, {"begin": 29646, "end": 29677, "idx": 193}, {"begin": 29678, "end": 29837, "idx": 194}, {"begin": 29838, "end": 30036, "idx": 195}, {"begin": 30037, "end": 30087, "idx": 196}, {"begin": 30273, "end": 30440, "idx": 197}, {"begin": 30441, "end": 30685, "idx": 198}, {"begin": 30686, "end": 30707, "idx": 199}, {"begin": 30708, "end": 30885, "idx": 200}, {"begin": 30962, "end": 31161, "idx": 201}, {"begin": 31162, "end": 31318, "idx": 202}, {"begin": 31319, "end": 31520, "idx": 203}, {"begin": 31521, "end": 31591, "idx": 204}, {"begin": 31592, "end": 31746, "idx": 205}, {"begin": 31792, "end": 31996, "idx": 206}, {"begin": 31997, "end": 32157, "idx": 207}, {"begin": 32215, "end": 32251, "idx": 208}, {"begin": 32252, "end": 32324, "idx": 209}, {"begin": 32377, "end": 32413, "idx": 210}, {"begin": 32414, "end": 32569, "idx": 211}, {"begin": 32570, "end": 32690, "idx": 212}, {"begin": 32734, "end": 32790, "idx": 213}, {"begin": 32791, "end": 32918, "idx": 214}, {"begin": 32919, "end": 33080, "idx": 215}, {"begin": 33081, "end": 33203, "idx": 216}, {"begin": 33204, "end": 33452, "idx": 217}, {"begin": 33453, "end": 33670, "idx": 218}, {"begin": 33671, "end": 33804, "idx": 219}, {"begin": 33805, "end": 34100, "idx": 220}, {"begin": 34101, "end": 34125, "idx": 221}, {"begin": 34153, "end": 34346, "idx": 222}, {"begin": 34347, "end": 34506, "idx": 223}, {"begin": 34507, "end": 34698, "idx": 224}, {"begin": 34699, "end": 34833, "idx": 225}, {"begin": 34834, "end": 35033, "idx": 226}, {"begin": 35034, "end": 35178, "idx": 227}, {"begin": 35179, "end": 35329, "idx": 228}, {"begin": 35330, "end": 35528, "idx": 229}, {"begin": 35529, "end": 35708, "idx": 230}, {"begin": 35726, "end": 35810, "idx": 231}, {"begin": 35811, "end": 36165, "idx": 232}, {"begin": 36166, "end": 36232, "idx": 233}, {"begin": 36258, "end": 36351, "idx": 234}, {"begin": 36352, "end": 36394, "idx": 235}, {"begin": 36395, "end": 36456, "idx": 236}, {"begin": 36457, "end": 36549, "idx": 237}, {"begin": 37213, "end": 37271, "idx": 238}, {"begin": 37272, "end": 37398, "idx": 239}, {"begin": 37399, "end": 37428, "idx": 240}, {"begin": 37501, "end": 37576, "idx": 241}, {"begin": 37577, "end": 37586, "idx": 242}, {"begin": 37691, "end": 37830, "idx": 243}, {"begin": 37883, "end": 37993, "idx": 244}, {"begin": 37994, "end": 38046, "idx": 245}, {"begin": 38047, "end": 38127, "idx": 246}, {"begin": 38128, "end": 38198, "idx": 247}, {"begin": 38199, "end": 38251, "idx": 248}, {"begin": 38252, "end": 38375, "idx": 249}, {"begin": 38376, "end": 38487, "idx": 250}, {"begin": 38488, "end": 38586, "idx": 251}, {"begin": 38587, "end": 38716, "idx": 252}, {"begin": 38717, "end": 38828, "idx": 253}, {"begin": 38829, "end": 38927, "idx": 254}, {"begin": 39230, "end": 39251, "idx": 255}, {"begin": 39273, "end": 39365, "idx": 256}, {"begin": 39366, "end": 39574, "idx": 257}, {"begin": 39575, "end": 39745, "idx": 258}, {"begin": 39746, "end": 39919, "idx": 259}, {"begin": 39920, "end": 39957, "idx": 260}, {"begin": 39958, "end": 40179, "idx": 261}, {"begin": 40180, "end": 40229, "idx": 262}, {"begin": 40230, "end": 40267, "idx": 263}, {"begin": 40268, "end": 40395, "idx": 264}, {"begin": 40427, "end": 40475, "idx": 265}, {"begin": 40476, "end": 40679, "idx": 266}, {"begin": 40680, "end": 40773, "idx": 267}, {"begin": 40774, "end": 40815, "idx": 268}, {"begin": 40816, "end": 40921, "idx": 269}, {"begin": 41006, "end": 41220, "idx": 270}, {"begin": 41221, "end": 41363, "idx": 271}, {"begin": 41364, "end": 41486, "idx": 272}, {"begin": 41487, "end": 41707, "idx": 273}, {"begin": 41708, "end": 41884, "idx": 274}, {"begin": 41908, "end": 41966, "idx": 275}, {"begin": 41967, "end": 42009, "idx": 276}, {"begin": 42145, "end": 42186, "idx": 277}, {"begin": 42211, "end": 42233, "idx": 278}, {"begin": 42234, "end": 42255, "idx": 279}, {"begin": 42256, "end": 42301, "idx": 280}, {"begin": 42302, "end": 42360, "idx": 281}, {"begin": 42361, "end": 42444, "idx": 282}, {"begin": 42478, "end": 42592, "idx": 283}, {"begin": 42593, "end": 42670, "idx": 284}, {"begin": 42693, "end": 42750, "idx": 285}, {"begin": 42792, "end": 42914, "idx": 286}, {"begin": 42915, "end": 42981, "idx": 287}, {"begin": 43088, "end": 43142, "idx": 288}, {"begin": 43187, "end": 43252, "idx": 289}, {"begin": 43313, "end": 43421, "idx": 290}, {"begin": 43422, "end": 43540, "idx": 291}, {"begin": 43679, "end": 43899, "idx": 292}, {"begin": 43900, "end": 44054, "idx": 293}, {"begin": 44055, "end": 44170, "idx": 294}, {"begin": 44171, "end": 44301, "idx": 295}, {"begin": 44302, "end": 44334, "idx": 296}, {"begin": 44335, "end": 44395, "idx": 297}, {"begin": 44396, "end": 44401, "idx": 298}, {"begin": 44424, "end": 44467, "idx": 299}, {"begin": 44468, "end": 44489, "idx": 300}, {"begin": 44490, "end": 44514, "idx": 301}, {"begin": 44515, "end": 44520, "idx": 302}, {"begin": 44592, "end": 44629, "idx": 303}, {"begin": 44630, "end": 44645, "idx": 304}, {"begin": 44646, "end": 44687, "idx": 305}, {"begin": 44688, "end": 44747, "idx": 306}, {"begin": 44748, "end": 44770, "idx": 307}, {"begin": 44771, "end": 44776, "idx": 308}, {"begin": 44881, "end": 45001, "idx": 309}, {"begin": 45002, "end": 45057, "idx": 310}, {"begin": 45058, "end": 45085, "idx": 311}, {"begin": 45086, "end": 45091, "idx": 312}, {"begin": 45189, "end": 45308, "idx": 313}, {"begin": 45309, "end": 45445, "idx": 314}, {"begin": 45446, "end": 45503, "idx": 315}, {"begin": 45504, "end": 45646, "idx": 316}, {"begin": 46232, "end": 46349, "idx": 317}, {"begin": 46388, "end": 46549, "idx": 318}, {"begin": 46779, "end": 46830, "idx": 319}, {"begin": 46831, "end": 46910, "idx": 320}, {"begin": 47194, "end": 47281, "idx": 321}, {"begin": 47282, "end": 47335, "idx": 322}, {"begin": 47415, "end": 47477, "idx": 323}, {"begin": 47597, "end": 47638, "idx": 324}, {"begin": 47767, "end": 47870, "idx": 325}, {"begin": 48026, "end": 48066, "idx": 326}, {"begin": 48067, "end": 48132, "idx": 327}, {"begin": 48300, "end": 48423, "idx": 328}, {"begin": 48651, "end": 48737, "idx": 329}, {"begin": 48738, "end": 48822, "idx": 330}, {"begin": 48976, "end": 49049, "idx": 331}, {"begin": 49485, "end": 49525, "idx": 332}, {"begin": 49807, "end": 49925, "idx": 333}, {"begin": 49926, "end": 50045, "idx": 334}, {"begin": 50292, "end": 50440, "idx": 335}, {"begin": 50441, "end": 50589, "idx": 336}, {"begin": 50590, "end": 50744, "idx": 337}, {"begin": 50745, "end": 50780, "idx": 338}, {"begin": 50932, "end": 51077, "idx": 339}, {"begin": 51078, "end": 51083, "idx": 340}, {"begin": 51304, "end": 51412, "idx": 341}, {"begin": 51413, "end": 51479, "idx": 342}, {"begin": 51480, "end": 51606, "idx": 343}, {"begin": 51607, "end": 51789, "idx": 344}, {"begin": 51790, "end": 51850, "idx": 345}, {"begin": 52007, "end": 52074, "idx": 346}, {"begin": 52075, "end": 52197, "idx": 347}, {"begin": 52198, "end": 52256, "idx": 348}, {"begin": 52930, "end": 53156, "idx": 349}, {"begin": 53157, "end": 53256, "idx": 350}, {"begin": 53573, "end": 53613, "idx": 351}, {"begin": 53614, "end": 53846, "idx": 352}, {"begin": 53847, "end": 53912, "idx": 353}, {"begin": 53913, "end": 54006, "idx": 354}, {"begin": 54007, "end": 54192, "idx": 355}, {"begin": 54316, "end": 54377, "idx": 356}, {"begin": 54378, "end": 54452, "idx": 357}, {"begin": 54453, "end": 54463, "idx": 358}, {"begin": 54674, "end": 54760, "idx": 359}, {"begin": 54761, "end": 54970, "idx": 360}, {"begin": 54971, "end": 55041, "idx": 361}, {"begin": 55056, "end": 55181, "idx": 362}, {"begin": 55182, "end": 55282, "idx": 363}, {"begin": 55283, "end": 55404, "idx": 364}, {"begin": 55405, "end": 55452, "idx": 365}, {"begin": 55453, "end": 55674, "idx": 366}, {"begin": 55675, "end": 55681, "idx": 367}, {"begin": 55682, "end": 55796, "idx": 368}, {"begin": 55870, "end": 55909, "idx": 369}, {"begin": 55910, "end": 56001, "idx": 370}, {"begin": 56002, "end": 56179, "idx": 371}, {"begin": 56180, "end": 56377, "idx": 372}, {"begin": 56378, "end": 56416, "idx": 373}, {"begin": 56477, "end": 56562, "idx": 374}, {"begin": 56563, "end": 56676, "idx": 375}, {"begin": 56843, "end": 56889, "idx": 376}, {"begin": 56934, "end": 57005, "idx": 377}, {"begin": 57087, "end": 57099, "idx": 378}, {"begin": 57100, "end": 57147, "idx": 379}, {"begin": 57216, "end": 57291, "idx": 380}, {"begin": 57369, "end": 57445, "idx": 381}, {"begin": 57558, "end": 57584, "idx": 382}, {"begin": 57585, "end": 57697, "idx": 383}, {"begin": 57698, "end": 57827, "idx": 384}, {"begin": 57886, "end": 57999, "idx": 385}, {"begin": 58000, "end": 58184, "idx": 386}, {"begin": 58228, "end": 58269, "idx": 387}, {"begin": 58270, "end": 58354, "idx": 388}, {"begin": 58403, "end": 58542, "idx": 389}, {"begin": 58543, "end": 58548, "idx": 390}, {"begin": 58744, "end": 58750, "idx": 391}, {"begin": 58751, "end": 58841, "idx": 392}, {"begin": 58842, "end": 58931, "idx": 393}, {"begin": 59157, "end": 59236, "idx": 394}, {"begin": 59237, "end": 59256, "idx": 395}, {"begin": 59292, "end": 59494, "idx": 396}, {"begin": 59495, "end": 59652, "idx": 397}, {"begin": 59653, "end": 59681, "idx": 398}, {"begin": 59834, "end": 59918, "idx": 399}, {"begin": 59919, "end": 60012, "idx": 400}, {"begin": 60299, "end": 60354, "idx": 401}, {"begin": 60355, "end": 60369, "idx": 402}, {"begin": 60529, "end": 60640, "idx": 403}, {"begin": 60641, "end": 60669, "idx": 404}, {"begin": 60924, "end": 61072, "idx": 405}, {"begin": 61073, "end": 61216, "idx": 406}, {"begin": 61509, "end": 61562, "idx": 407}, {"begin": 61563, "end": 61660, "idx": 408}, {"begin": 61746, "end": 61771, "idx": 409}, {"begin": 61793, "end": 61867, "idx": 410}, {"begin": 61966, "end": 62040, "idx": 411}, {"begin": 62041, "end": 62119, "idx": 412}, {"begin": 62120, "end": 62156, "idx": 413}, {"begin": 62354, "end": 62391, "idx": 414}, {"begin": 62392, "end": 62476, "idx": 415}, {"begin": 62477, "end": 62535, "idx": 416}, {"begin": 62723, "end": 62800, "idx": 417}, {"begin": 62884, "end": 62945, "idx": 418}, {"begin": 63041, "end": 63120, "idx": 419}, {"begin": 63307, "end": 63394, "idx": 420}, {"begin": 63549, "end": 63641, "idx": 421}, {"begin": 63642, "end": 63691, "idx": 422}, {"begin": 63898, "end": 63997, "idx": 423}, {"begin": 64429, "end": 64471, "idx": 424}, {"begin": 64650, "end": 64755, "idx": 425}, {"begin": 64756, "end": 64821, "idx": 426}, {"begin": 64822, "end": 64861, "idx": 427}, {"begin": 64862, "end": 64901, "idx": 428}, {"begin": 64902, "end": 64986, "idx": 429}, {"begin": 65078, "end": 65084, "idx": 430}, {"begin": 65085, "end": 65132, "idx": 431}, {"begin": 65133, "end": 65190, "idx": 432}, {"begin": 65209, "end": 65325, "idx": 433}, {"begin": 65326, "end": 65363, "idx": 434}, {"begin": 65364, "end": 65398, "idx": 435}, {"begin": 65709, "end": 65965, "idx": 436}, {"begin": 65966, "end": 66021, "idx": 437}, {"begin": 66249, "end": 66328, "idx": 438}, {"begin": 66329, "end": 66350, "idx": 439}, {"begin": 66351, "end": 66388, "idx": 440}, {"begin": 66440, "end": 66503, "idx": 441}, {"begin": 66504, "end": 66509, "idx": 442}, {"begin": 66604, "end": 66610, "idx": 443}, {"begin": 66611, "end": 66664, "idx": 444}, {"begin": 66795, "end": 66899, "idx": 445}, {"begin": 66993, "end": 67113, "idx": 446}, {"begin": 67214, "end": 67247, "idx": 447}, {"begin": 67289, "end": 67340, "idx": 448}, {"begin": 67510, "end": 67536, "idx": 449}, {"begin": 67537, "end": 67596, "idx": 450}, {"begin": 67597, "end": 67633, "idx": 451}, {"begin": 67879, "end": 67885, "idx": 452}, {"begin": 67886, "end": 68038, "idx": 453}, {"begin": 68039, "end": 68157, "idx": 454}, {"begin": 68158, "end": 68179, "idx": 455}, {"begin": 68180, "end": 68318, "idx": 456}, {"begin": 68319, "end": 68417, "idx": 457}, {"begin": 68418, "end": 68453, "idx": 458}, {"begin": 68540, "end": 68586, "idx": 459}, {"begin": 68587, "end": 68691, "idx": 460}, {"begin": 68692, "end": 68772, "idx": 461}, {"begin": 68773, "end": 68801, "idx": 462}, {"begin": 68859, "end": 68921, "idx": 463}, {"begin": 69004, "end": 69047, "idx": 464}, {"begin": 69082, "end": 69182, "idx": 465}, {"begin": 69214, "end": 69254, "idx": 466}, {"begin": 69255, "end": 69276, "idx": 467}, {"begin": 69277, "end": 69404, "idx": 468}, {"begin": 69405, "end": 69474, "idx": 469}, {"begin": 69475, "end": 69553, "idx": 470}, {"begin": 69608, "end": 69665, "idx": 471}, {"begin": 69666, "end": 69728, "idx": 472}, {"begin": 69789, "end": 69846, "idx": 473}, {"begin": 69847, "end": 69895, "idx": 474}, {"begin": 69896, "end": 69927, "idx": 475}, {"begin": 70258, "end": 70280, "idx": 476}, {"begin": 70467, "end": 70509, "idx": 477}, {"begin": 70510, "end": 70565, "idx": 478}, {"begin": 70760, "end": 70827, "idx": 479}, {"begin": 70828, "end": 70872, "idx": 480}, {"begin": 70942, "end": 71052, "idx": 481}, {"begin": 71110, "end": 71175, "idx": 482}, {"begin": 71263, "end": 71320, "idx": 483}, {"begin": 71393, "end": 71460, "idx": 484}, {"begin": 71461, "end": 71582, "idx": 485}, {"begin": 71583, "end": 71756, "idx": 486}, {"begin": 71757, "end": 71765, "idx": 487}, {"begin": 72047, "end": 72072, "idx": 488}, {"begin": 72073, "end": 72101, "idx": 489}, {"begin": 72300, "end": 72365, "idx": 490}, {"begin": 72519, "end": 72598, "idx": 491}, {"begin": 72599, "end": 72627, "idx": 492}, {"begin": 72992, "end": 73039, "idx": 493}, {"begin": 73040, "end": 73106, "idx": 494}, {"begin": 73338, "end": 73364, "idx": 495}, {"begin": 73385, "end": 73544, "idx": 496}, {"begin": 73568, "end": 73655, "idx": 497}, {"begin": 73656, "end": 73795, "idx": 498}, {"begin": 73796, "end": 73852, "idx": 499}, {"begin": 73853, "end": 73917, "idx": 500}, {"begin": 73918, "end": 74054, "idx": 501}, {"begin": 74126, "end": 74318, "idx": 502}, {"begin": 74319, "end": 74325, "idx": 503}, {"begin": 74326, "end": 74375, "idx": 504}, {"begin": 74376, "end": 74412, "idx": 505}, {"begin": 74413, "end": 74535, "idx": 506}, {"begin": 74536, "end": 74610, "idx": 507}, {"begin": 74645, "end": 74692, "idx": 508}, {"begin": 74897, "end": 74995, "idx": 509}, {"begin": 74996, "end": 75005, "idx": 510}, {"begin": 75085, "end": 75103, "idx": 511}, {"begin": 75104, "end": 75109, "idx": 512}, {"begin": 75243, "end": 75336, "idx": 513}, {"begin": 75397, "end": 75464, "idx": 514}, {"begin": 75510, "end": 75517, "idx": 515}, {"begin": 75524, "end": 75544, "idx": 516}, {"begin": 75579, "end": 75638, "idx": 517}, {"begin": 75639, "end": 75647, "idx": 518}, {"begin": 75713, "end": 75737, "idx": 519}, {"begin": 75738, "end": 75748, "idx": 520}, {"begin": 75807, "end": 75830, "idx": 521}, {"begin": 75831, "end": 75839, "idx": 522}, {"begin": 76020, "end": 76046, "idx": 523}, {"begin": 76083, "end": 76209, "idx": 524}, {"begin": 76210, "end": 76414, "idx": 525}, {"begin": 76415, "end": 76591, "idx": 526}, {"begin": 76592, "end": 76677, "idx": 527}, {"begin": 76830, "end": 77003, "idx": 528}, {"begin": 77004, "end": 77136, "idx": 529}, {"begin": 77137, "end": 77240, "idx": 530}, {"begin": 77241, "end": 77262, "idx": 531}, {"begin": 77263, "end": 77379, "idx": 532}, {"begin": 77380, "end": 77488, "idx": 533}, {"begin": 77538, "end": 77589, "idx": 534}, {"begin": 77626, "end": 77693, "idx": 535}, {"begin": 77694, "end": 77701, "idx": 536}, {"begin": 77890, "end": 78005, "idx": 537}, {"begin": 78006, "end": 78030, "idx": 538}, {"begin": 78081, "end": 78084, "idx": 539}, {"begin": 78150, "end": 78155, "idx": 540}, {"begin": 78207, "end": 78233, "idx": 541}, {"begin": 78234, "end": 78340, "idx": 542}, {"begin": 78387, "end": 78410, "idx": 543}, {"begin": 78411, "end": 78549, "idx": 544}, {"begin": 78751, "end": 78856, "idx": 545}, {"begin": 78905, "end": 78978, "idx": 546}, {"begin": 79296, "end": 79347, "idx": 547}, {"begin": 79348, "end": 79427, "idx": 548}, {"begin": 79801, "end": 79901, "idx": 549}, {"begin": 79987, "end": 80049, "idx": 550}, {"begin": 80166, "end": 80207, "idx": 551}, {"begin": 80334, "end": 80437, "idx": 552}, {"begin": 80588, "end": 80628, "idx": 553}, {"begin": 80629, "end": 80694, "idx": 554}, {"begin": 80862, "end": 80985, "idx": 555}, {"begin": 81215, "end": 81301, "idx": 556}, {"begin": 81302, "end": 81385, "idx": 557}, {"begin": 81537, "end": 81610, "idx": 558}, {"begin": 81820, "end": 81858, "idx": 559}, {"begin": 82177, "end": 82336, "idx": 560}, {"begin": 82763, "end": 82911, "idx": 561}, {"begin": 82955, "end": 83059, "idx": 562}, {"begin": 83060, "end": 83193, "idx": 563}, {"begin": 83365, "end": 83371, "idx": 564}, {"begin": 83372, "end": 83462, "idx": 565}, {"begin": 83463, "end": 83518, "idx": 566}, {"begin": 83742, "end": 83952, "idx": 567}, {"begin": 83953, "end": 84048, "idx": 568}, {"begin": 84049, "end": 84077, "idx": 569}, {"begin": 84230, "end": 84314, "idx": 570}, {"begin": 84315, "end": 84408, "idx": 571}, {"begin": 84695, "end": 84750, "idx": 572}, {"begin": 84751, "end": 84810, "idx": 573}, {"begin": 84925, "end": 85036, "idx": 574}, {"begin": 85037, "end": 85065, "idx": 575}, {"begin": 85320, "end": 85468, "idx": 576}, {"begin": 85469, "end": 85612, "idx": 577}, {"begin": 85913, "end": 85995, "idx": 578}, {"begin": 85996, "end": 86055, "idx": 579}, {"begin": 86138, "end": 86142, "idx": 580}, {"begin": 86143, "end": 86202, "idx": 581}, {"begin": 86311, "end": 86408, "idx": 582}, {"begin": 86508, "end": 86512, "idx": 583}, {"begin": 86513, "end": 86587, "idx": 584}, {"begin": 86588, "end": 86666, "idx": 585}, {"begin": 86667, "end": 86703, "idx": 586}, {"begin": 86899, "end": 86936, "idx": 587}, {"begin": 86937, "end": 87021, "idx": 588}, {"begin": 87022, "end": 87080, "idx": 589}, {"begin": 87272, "end": 87324, "idx": 590}, {"begin": 87378, "end": 87459, "idx": 591}, {"begin": 87513, "end": 87574, "idx": 592}, {"begin": 87676, "end": 87760, "idx": 593}, {"begin": 87947, "end": 88034, "idx": 594}, {"begin": 88195, "end": 88287, "idx": 595}, {"begin": 88288, "end": 88337, "idx": 596}, {"begin": 88556, "end": 88602, "idx": 597}, {"begin": 88625, "end": 88694, "idx": 598}, {"begin": 88695, "end": 88821, "idx": 599}, {"begin": 88822, "end": 88851, "idx": 600}, {"begin": 88890, "end": 88924, "idx": 601}], "Abstract": [{"begin": 60, "end": 1432, "idx": 0}], "SectionFootnote": [{"begin": 88926, "end": 89124, "idx": 0}], "Footnote": [{"begin": 88937, "end": 89039, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 89040, "end": 89124, "id": "foot_1", "n": "2", "idx": 1}]}}