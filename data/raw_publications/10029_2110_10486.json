{"text": "A TinyML Platform for On-Device Continual Learning with Quantized Latent Replays\n\nAbstract:\nIn the last few years, research and development on Deep Learning models & techniques for ultra-low-power devices -in a word, TinyML -has mainly focused on a train-thendeploy assumption, with static models that cannot be adapted to newly collected data without cloud-based data collection and finetuning. Latent Replay-based Continual Learning (CL) techniques [1] enable online, serverless adaptation in principle, but so far they have still been too computation-and memory-hungry for ultra-low-power TinyML devices, which are typically based on microcontrollers. In this work, we introduce a HW/SW platform for end-to-end CL based on a 10-core FP32-enabled parallel ultra-low-power (PULP) processor. We rethink the baseline Latent Replay CL algorithm, leveraging quantization of the frozen stage of the model and Latent Replays (LRs) to reduce their memory cost with minimal impact on accuracy. In particular, 8-bit compression of the LR memory proves to be almost lossless (-0.26% with 3000LR) compared to the full-precision baseline implementation, but requires 4\u00d7 less memory, while 7-bit can also be used with an additional minimal accuracy degradation (up to 5%). We also introduce optimized primitives for forward and backward propagation on the PULP processor, together with data tiling strategies to fully exploit its memory hierarchy, while maximizing efficiency. Our results show that by combining these techniques, continual learning can be achieved in practice using less than 64MB of memory -an amount compatible with embedding in TinyML devices. On an advanced 22nm prototype of our platform, called VEGA, the proposed solution performs on average 65\u00d7 faster than a low-power STM32 L4 microcontroller, being 37\u00d7 more energy efficient -enough for a lifetime of 535h when learning a new mini-batch of data once every minute.\n\nMain:\n\n\n\nI. INTRODUCTION\nThe internet-of-Things ecosystem is made possible by miniaturized and smart end-node devices, which can sense the surrounding environment and take decisions based on the information inferred from sensor data. Because of their tiny form L. Ravaglia, M. Rusci, D. Nadalini, F. Conti, and L. Benini are with the Department of Electrical, Electronic and Information Engineering (DEI) of the University of Bologna, Viale del Risorgimento 2, 40136 Bologna, Italy (e-mail: {leonardo.ravaglia2, manuele.rusci, d.nadalini, f.conti, luca.benini}@unibo.it).\nA. Capotondi is with the Department of Physics, Informatics and Mathematics of the University of Modena and Reggio Emilia, Via Campi 213/A, 41125 Modena, Italy (e-mail: alessandro.capotondi@unibo.it).\nL. Benini is also with the integrated Systems Laboratory (IIS) of ETH Z\u00fcrich, ETZ, Gloriastrasse 35, 8092 Z\u00fcrich, Switzerland (e-mail: lbenini@iis.ee.ethz.ch).\nThis work was supported in part by the ECSEL Horizon 2020 project AI4DI (Artificial intelligence for Digital Industry, g.a. no. 826060); and by EU Horizon 2020 project BonsAPPs (g.a. no. 101015848). We also acknowledge CINECA for the availability of high-performance computing resources and support awarded under the ISCRA initiative through the NAS4NPC project.\nManuscript received May 15, 2021.\nfactor and the requirement for low cost and battery-operated nature, these smart networked devices are severely constrained in terms of memory capacity and maximum performance and use small Microcontroller Units (MCUs) as their main onboard computing device [2]. At the same time, there is an evergrowing interest in deploying more accurate and sophisticated data analytics pipelines, such as Deep Learning (DL) inference models, directly on IoT end-nodes. These competing needs have given rise in the last few years to a specific branch of machine learning (ML) and DL research called TinyML [3] focused on shrinking and compressing top-accurate DL models with respect to the target device characteristics. The primary limitation of the current generation of TinyML hardware and software is that it is mostly focused on inference. The inference task can be strongly optimized by quantizing [4] or pruning [5] the trained model. Many vendors of AI-oriented system-on-chips (SoCs) provide deployment frameworks to automatically translate DL inference graphs into humanreadable or machine code [6]. This train-then-deploy design process rigidly separates the learning phase from the runtime inference, resulting in a static intelligence model design flow, incapable of adapting to phenomena such as data distribution shift: a shift in the statistical properties of real incoming data vs the training set that often impacts applications, causing the smart sensors platform to be unreliable when deployed in the field [7].\nEven if the algorithms themselves are technically capable to learn and adapt to new incoming data, the update process can only be handled from a centralized service, running on the cloud or host servers [8]. In this regard, the original training dataset would have to be enriched with the newly collected dataset, and the model would have to be retrained from scratch on the enlarged dataset, adapting to the new data without forgetting the original information [8]. Such an adaptive mechanism belongs to the rehearsal category and requires the storage of the full training set, often amounting to gigabytes of data. Additionally, large amounts of data have to be collected in a centralized fashion by network communication, resulting in potential security and privacy concerns, as well as issues of radio power consumption and network reliability in non-urban areas.\nWe argue that a robust and privacy-aware solution to these challenges is enabling future smart IoT end-nodes to Lifelong Learning, also known as Continual Learning [9] (CL): the capability to autonomously adapt to the ever-changing surrounding environment by learning continually (only) from incoming data without forgetting the original knowledge -a phenomenon known as catastrophic forgetting [10]. Despite many approaches exists to learn from data [11], recently the focus has moved to improve the recognition accuracy of DL models because of their superior capabilities, accounting on new data belonging to known classes (domain-incremental CL) or a new classes (class-incremental CL) [12], [13]. The CL techniques recently proposed are grouped in three categories: architectural, regularization and memory (or rehearsal) strategies. The architectural approaches specialize a subset of parameters for every (new and old) task but require the task-ID information at inference time, indicating the nature of current task in a multi-head network, and therefore they are not suitable for class or domain incremental continual learning. Concerning these latter scenarios, memory-based approaches, which preserve samples from previous tasks for replaying, perform better than regularization techniques, which simply address catastrophic forgetting by imposing constraints on the network parameter update at low memory cost [13] - [15]. This finding was confirmed during the recent CL competition at CVPR2020 [16], where the best entry leveraged on rehearsal based strategies.\nThe main drawback of memory-based CL approaches concerns the high memory overhead for the storage of previous samples: the memory requirement can potentially grows over time preventing the applicability of these methods at the tiny scale, e.g.  [17]. To address this problem, Pellegrini et al. [1] have recently introduced Continual Learning based on Latent Replays (LRs). The idea behind this is to combine a few old data points taken from the original training set, but encoded intoa low-dimensional latent space to reduce the memory cost, with the new data for the incremental learning tasks. Hence, the previous knowledge is retained by means of Latent Replays samples, i.e. the intermediate feature maps of the DL model inference, selected so that they require less space with respect to the input data (up to 48\u00d7 smaller compared to raw images [1]). This strategy also leads to reduced computational cost: the Latent intermediate layer splits the network in a frozen stage at the front and an adaptive stage at the back, and only layers in the latter need to be updated. So far, LRbased Continual Learning has been successfully prototyped on high-performance embedded devices such as smartphones, including a Snapdragon-845 CPU running Android OS in the power envelope of a few Watts 1 . On the contrary, in this work, we focus on IoT applications and TinyML devices, with 100\u00d7 tighter power constraints and 1000\u00d7 smaller memories available.\nIn our preliminary work [18], we proposed the early design concept of a HW/SW platform for Continual Learning based on the Parallel Ultra Low Power (PULP) paradigm [19], and assessed the computational and memory costs to deploy Latent Replay-based CL algorithms.\nIn this paper, we complete and extend that effort by introducing several novel contributions from the software stack, system integration and algorithm viewpoint. To the best of our knowledge, we present the first TinyML processing platform 1 https://hothardware.com/reviews/qualcomm-snapdragon-845performance-benchmarks and framework capable of on-device CL, together with the design flow required to sustain learning tasks within a few tens of mW of power envelope (> 10\u00d7 lower than state-ofthe-art solutions). The proposed platform is based on VEGA, a recently introduced end-node System-on-Chip prototype fabricated in 22nm technology [20]. Unlike traditional lowpower and flexible MCUs design, VEGA exploits explicit data parallelism, by featuring a multi-core SW programmable RISC-V cluster with shared Floating Point Units (FPUs), DSPoriented ISA and optimized memory management to enable the learning paradigm on low-end IoT devices. Additionally, to gain minimum-cost on-device retention of Latent Replays and better enable deployment on an ultra-low-power platform, we extend the LR algorithm proposed by Pellegrini et al. [1] to work with a fully quantized frozen front-end and compress Latent Replays using quantization down to 7 bits, with a small accuracy drop (almost lossless for 8-bit) when compared to the single-precision floating-point datatype (FP32) on the Core50 CL classification benchmark.\nIn summary, the contributions of this work are:\n1) We extend the LR algorithm to work with an 8-bit quantized and frozen front-end without impact on the CL process and to support LR compression with quantization, reducing up to 4.5\u00d7 the memory needed for rehearsing. We call this extension Quantized Latent Replay-based Continual Learning or QLR-CL. 2) We propose a set of CL primitives including forward and backward propagation of common layers such as convolution, depthwise convolution, and fully connected layers, fine-tuned for optimized execution on VEGA, a TinyML platform for Deep Learning based on PULP [19], fabricated in 22nm technology. We also introduce a tiling scheme to manage data movement for the CL primitives. 3) We compare the performance of our CL primitives on VEGA with that on other devices that could in the future target on-chip at-edge learning, such as a state-of-the-art low-power STM32L4 microcontroller.\nOur results show that the Quantized Latent Replay based Continual Learning lead to a minimal accuracy loss on the Core50 dataset compared to the FP32 baseline, when compressing the Latent Replay memory by 4\u00d7 by means of 8-bit quantization. Compression to 7 bit can also be exploited but at the cost of a slightly lower accuracy, up to 5% wrt the baseline when retraining one of the intermediate layer. When testing the QLR-CL pipeline on the proposed VEGA platform, our CL primitives demonstrated to run up to 65\u00d7 faster with respect to the MCUs for TinyML that can be found currently on the market. Compared against edge devices with a power envelope of 4W our solution is about 6\u00d7 more energy-efficient, enough to operate 317h with a typical battery for embedded devices. The rest of the paper is organized as follows: Section II discusses related work in CL, inference and learning at the edge, and hardware architectures targeted at edge learning. Section III introduces the proposed methodology for Quantized Continual Learning. Section IV describes the HW/SW architecture of the proposed TinyML. Section V evaluates and discusses experimental results. Section VI concludes the paper.\n\nII. RELATED WORK\nIn this section, we first review the recent memory-efficient Continual Learning approaches before discussing the main solutions and methods for the TinyML ecosystem, including the first attempts for on-device learning on embedded systems.\n\nA. Memory-efficient Continual Learning\nDifferently from Transfer Learning [21], [22], which by design does not retain the knowledge of the primitive learned task when learning a new one, Continual Learning (CL) has recently emerged as a new technique to tackle the acquisition of new/extended capabilities without losing the original ones -a phenomenon known as catastrophic forgetting [12], [13]. One of the main causes of this phenomenon is that the newly acquired set breaks one of the main assumptions underlying supervised learning -i.e., that training data are statistically independent and identically distributed (IID). Instead, CL deals with training data that is organized in non-IID learning events. Maltoni et al. in [26] sort the main CL techniques intothree groups: rehearsal, which includes a periodic replay of the past information; architectural, relying on a specialized architecture, layers, and activation functions to mitigate forgetting; and regularization-based, where the loss term is extended to encourage retaining memory of pre-learned tasks.\nAmong these groups, rehearsal CL strategies have emerged as the most effective to deal with catastrophic forgetting, at the cost of an additional replay memory [1], [27], [28]. In the recent CL challenge at CVPR2020 on the Core50 image dataset, \u223c90% of the competitors used rehearsal strategies [16]. The best entry of the more challenging New Instances and Classes track (the same scenario considered in our work) [17], which is evaluated in terms of test accuracy but also memory and computation requirements, scores 91% by replaying image data. Unfortunately, this strategy results untractable for an IoT platform because of the expanding replay memory (up to 78k images) and the usage of a large DenseNet-161 model. Conversely, the Latent Replay-based approach [1] relies on a fixed, and relatively small, amount of compressed latent activations as replay data; it scores 71% if retraining only the last layer, which presents a peak of 52\u00d7 lower (compressed) data points than the winning solution. Additionally, the Jodelet entry -also employing LR-based CL -achieves 83% thanks to 3\u00d7 more replays and a more accurate pre-trained model (ResNet50) [16]. In our work, we focus on [1] because of the tunable accuracy-memory setting. Nevertheless, our proposed platform and compression methodology can be applied to any replay-based CL approach.\nAlso related to our work, ExStream [29] clusters in a streaming fashion the training samples before pushing them into the replay buffer while [30] uses discrete autoencoders to compress the input data for rehearsing. In contrast, we propose low-bitwidth quantization to compress the Latent Replay memory by >4\u00d7 and, at the same time, reduce the inference latency and the memory requirement of the inference task of the frozen stage if compared to a full-precision FP32 implementation.\n\nB. Deep Learning at the Extreme Edge\nTwo main trends can be identified for TinyML platforms targeting the extreme edge. On the one hand, Deep Learning applications are dominated by linear algebra which is an ideal target for application-specific HW acceleration [31], [32]. Most efforts in this direction employ a variety of inferenceonly acceleration techniques such as pruning [33] and byte and sub-byte integer quantization [4]; the use of large arrays of simple MAC units [34] or even mixed-signal techniques such as in-memory computing [35].\nOn the other hand, there are also many reasons for the alternative approach: running TinyML applications as software on top of commercial off-the-shelf (COTS) extremeedge platforms, such as MCUs. Extreme-edge TinyML devices need to be very cheap; they have to be flexible due both to economy of scale and to their need for integration within larger applications, composed of both neural and nonneural tasks [36]. For these reasons, there is a strong push towards squeezing the maximal performance out of platforms based on COTS ARM Cortex-M class microcontrollers and DSPs, such as STMicroelectronics STM32 microcontrollers 2, or on multi-core parallel ultra-low-power (PULP) end-nodes, like GreenWaves Technologies GAP-8 3. To cope with the severe constraints in terms of memory and maximum compute throughput of these platforms, a large number of deployment tools have been recently proposed. Examples of this trend include non-vendor-locked tools such as Google TFLite Micro [6], ARM CMSIS-NN [37], Apache TVM [38], as well as frameworks that only support specific families of devices, such as STMicroelectronics X-CUBE-AI 4, GreenWaves Technologies NNTOOL 5, and DORY [39]. Internally, these tools employ hardware-independent techniques, such as posttraining compression & quantization [40] - [42], as well as hardware-dependent ones such as data tiling [43] and loop unrolling to boost data reuse exploitation [37], coupled with automated generation of optimized backend code [44].\nAs previously discussed, all of these efforts are mostly targeted at extreme edge inference, with little hardware and/or software dedicated to training. Most of the techniques used to boost inference efficiency are not as effective for learning. For example, the vast majority of training is done in full precision floating-point (FP32) or, with some restrictions, using half-precision floats (FP16) [45] -whereas inference is commonly pushed to INT8 or even below [4], [40]. IBM has recently proposed a specialized 8-bit format for training called HFP8 [46], but its effectiveness is still under investigation.\nHardware-accelerated on-device learning has so far been limited to high-performance embedded platforms (e.g., NVIDIA TensorCores on Tegra Xavier 6 and mobile platforms such as Qualcomm Snapdragon 845 [1]) or very narrow in scope. For example, Shin et al. [47] claim to implement an online adaptable architecture, but this is done using a simple [51] enable partial gradient backpropagation by using selective and compressed weight updates, but they do not address the large memory footprint required by training. Finally, several online-learning devices using bioinspired algorithms such as Spiking Neural Networks [52] and High-Dimensional Computing [25] have been proposed [53] - [55]. Most of these approaches, however, have only been demonstrated on simple MNIST-like tasks.\nIn this work, we propose the first, to the best of our knowledge, MCU-class hardware-software system capable of continual learning based on gradient back-propagation with a LR approach. We achieve these results by leveraging on few key ideas in the state-of-the-art: INT8 inference, FP32 continual learning, and exploitation of linear algebra kernels, back-propagation, and aggressive parallelization by deploying them on a multi-core FPU-enhanced PULP cluster.\n\nC. On-Device Learning on low-end platforms\nTable I lists the main edge solutions featuring on-device learning capabilities. Every approach is evaluated by considering the memory and computational costs for the continual learning task and the suitability for deployment on highly resource-constrained (tiny) devices.\nA first group of works deals with on-device transfer learning. The Coral Edge TPU, which presents a power budget of several Watts, features SW support for on-device finetuning of the parameters of the last fully-connected layer [21]. TinyTL [22] demonstrated on a high-end CPU that the transfer learning task results more effective (+32% on the target Image Classification task) by retraining the bias terms and adding lite residual modules. TinyOL [23] brought the transfer learning task on a tiny devices, i.e. an Arduino Nano platform featuring a 64MHz ARM Cortex-M4, by adding a trainable layer on top of a frozen inference model. Because only the coefficients of the last layer are updated during the online training process, no backpropagation of error gradients applies. Compared to these works, we address a continual learning scenario and therefore we provide a more capable and optimized HW/SW solution to match the memory and computational requirements of the adopted CL method. Differently from the above works, de Prado et al. [8] proposed a Continual Learning framework for self-driving minicars. The embedded PULP-based MCU engine streams new data to a remote server, where the inference model is retrained from scratch on the enhanced dataset to improve the accuracy over time. This fully-rehearsal methodology cannot be migrated to low-end devices because of the unconstrained increase of the memory footprint. In contrast, Disabato et al. [24] presented an online adaptive scheme based on a kNN classifier placed on top of a frozen feature extraction CNN model. The final stage is updated by incrementally adding the labeled samples to the knowledge memory of the kNN classifier. This approach has been evaluated on a tiny STM32F76ZI device but unfortunately has proven the effectiveness only on limited 2-classes problems and presents an unbounded memory requirement, which scales linearly with the number of training samples. PULP-HD [25] showed few-shot continual learning capabilities on an ultra-low power prototype using Hyperdimensional Computing. During the training phase the new data are mapped intoa limited hyperdimensional space by making use of a complex encoding procedure; at inference time the incoming samples are compared to the computed class prototypes. The method has been demonstrated on a 10 gesture classification scenario based on EMG data but lacks of experimental evidences to be effective on complex image classification problems. In contrast to the these works, we demonstrate superior learning capabilities for a TinyML platform by i) running backpropagation on-device to update intermediate layers, and ii) supporting a memory-efficient Latent Replay-based strategy to address catastrophic forgetting on a more complex Continual Learning scenario. An initial CNNbased prototype of a Continual Learning system was presented in in [1] using Latent Replays. The authors demonstrated the on-device learning capabilities using a Qualcomm Snapdragon processor, which features a power envelope 100\u00d7 higher than our target and therefore it results not suitable for batteryoperated tiny devices. In contrast to them, we also extend the LR algorithm by leveraging on quantization to compress the LR memory requirements.\n\nIII. METHODS\nIn this section, we analyze the memory requirements of the Latent Replay-based Continual Learning method and present QLR-CL, our strategy to reduce the memory footprint of the LR vectors based on a quantization process.\n\nA. Background: Continual Learning with Latent Replays\nIn general, supervised learning aims at fitting an unknown function by using a set of known examples -the training dataset. In the case of Deep Neural Networks, the training procedure returns the values of the network parameters, such as weights and biases, that minimize a loss function. Among the used optimization strategies, the mini-batch Stochastic Gradient Descent (SGD), which is an iterative method applied over multiple learning step (i.e. the epochs), is widely adopted. In particular, The SGD algorithm computes the gradient of the parameters based on the loss function by back-propagating the error value through the network. This error function compares the model prediction, i.e. the output of the forward pass, with the expected outcome (the data label). Parameter gradients obtained after the backward pass are weighted over a minibatch of data before updating the model coefficients.\nAs introduced at the beginning of this work, the Latent Replay CL method [1] is a viable solution to gain TinyML adaptive systems with on-device learning capabilities based on the availability of new labeled data. In Fig. 1 we illustrate the CL process with Latent Replays. The new data are injected into the model to obtain the latent embeddings, which are the feature maps of a specific intermediate layer. We indicate such a layer with the index l, where l \u2208 [0, L), assuming the targeted model to be composed by L stacked layers. At runtime, the new latent vectors are combined with the precomputed N LR Latent Replays vectors to execute the learning algorithm on the last L \u2212 l \u2212 1 layers. More specifically, the coefficient parameters of the adaptive stage are updated by using a minibatch gradient descend algorithm. Every mini-batch includes both new data (in the latent embedding form) and LR vectors. The typical ratio of new data over the full mini-batch is 1/6 [1]. The coefficient gradients are computed through forward and backward passes over the adaptive (learned) layers. Multiple iterations, i.e. the epochs, of the learning algorithms take place within the training procedure.\n\nB. Memory Requirements\nWe model the Latent Replay-based Continual Learning task as operating on a set of new data coming from a sensor (e.g., a camera), which is interfaced with an embedded digital processing engine, namely the TinyML Platform, and its memory subsystem. Given the limited memory capacity of IoT endnodes, the quantification of the learning algorithm's memory requirements is essential. We distinguish between two different memory requirements: additional memory necessary for CL, e.g., the LR memory, and that required to save intermediate tensors during forward-prop to be used for back-prop -a requirement common to all algorithms based on gradient descent, not specific to CL.\nConcerning the LR memory, the system has to save a set of N LR LRs, each one of the size of the feature map computed at the l-th layer of the network. In our scenario, LR vectors are represented employing floating-point (FP32) datatype and typically determine the majority of the memory requirement [18]. Since LRs are part of the static long-term memory of the CL system, for their storage, we use nonvolatile memory, e.g., external Flash.\nOn the other hand, forward-and back-prop of the network model require to allocate the space for N P network parameters statically. In addition, forward-prop requires dynamically allocated buffers to store the activation feature maps for all layers. Up to the l-th layer, these buffers are temporary and can be released after their usage. Conversely, the system must keep in memory the feature maps after l to compute the gradients during back-prop. They can only be released after the corresponding layer has been back-propagated. Lastly, the system must also keep in memory the coefficients' gradients, demanding a second array of N P elements. To keep accuracy on the learning process, every tensor, i.e. coefficients, gradients, and activations, employ a FP32 format in our baseline scenario. Different from LRs, these tensors are kept intovolatile memories, except the frozen weights, which are stored in a non-volatile memory.\n\nC. Quantized Latent Replay-based Continual Learning\nQuantization techniques have been extensively used to reduce the data size of model parameters, and activation feature maps for the inference task, i.e. the forward pass. An effective quantization strategy reduces the data bitwidth from 32-bit (FP32) to low bit-precision, 8-bit or less (Q bits, in general) while paying an almost negligible accuracy loss.\nIn this paper, we introduce the Quantized Latent Replaybased Continual Learning method (QLR-CL) relying on lowbitwidth quantization to speed up the execution of the network  up to the l-th layer and at the same time reduce the memory requirement of the LR vectors from the baseline FP32 arrays. To do so, we split the deep model intotwo sub-networks, namely the frozen stage and the adaptive stage. The frozen stage includes the lower layers of the network, up to the Latent Replay layer l. The coefficients of this sub-network, including batch normalization statistics, are frozen during the incremental learning process. On the contrary, the parameters of the adaptive stage are updated based on the new data samples.\nIn QLR-CL, the Latent Replay vectors are generated by feeding the frozen stage sub-network with a random subset of training samples from the CL dataset, which we denote as X train . The frozen stage is initialized using pre-trained weights from a related problem -in the case of Core50, we use a network pre-trained on the ImageNet-1k dataset. Post-Training Quantization of the frozen stage is based on training samples X train . We apply a standard Post-Training Quantization process that works by i) determining the dynamic range of coefficient and activation tensors, ii) dividing the range intoequal steps, using a uniform affine quantization scheme [56]. While the statistics of the parameters can be drawn without relying on data, the dynamic range of the activation features maps is estimated using X train as a calibration set. If we denote the dynamic range of the weights at the i-th layer of the network as [w i,min , w i,max ], we can define the w i,quant INT-Q representation of parameters asw i,quant = w i S w,i , S w,i = w i,max \u2212 w i,min 2 Q \u2212 1 ()\nwhere Q is the number of bits, w i is the full-precision output of the frozen stage. The representation of activations is similar, but we further restrict (1) for activations a i by considering the effect of ReLU's: a i are always positive and a i,quant can be represented using an unsigned UINT-Q format:a i,quant = a i S a,i , S a,i = a i,max 2 Q \u2212 1\nwhere a i,max is obtained through calibration on X train . Quantized Latent Replays (QLRs) a l,replay are represented similarly to other quantized activations, setting the layer i to the LR l. Their value is initialized during the initial setup of the QLR-CL process using the latent quantized activations a l,quant over the X train set.\nDuring the QLR-CL process, the adaptive stage is fed by dequantized vectors obtained as S a,l \u2022 a l,replay , along with the dequantized latent representation of the new data sample S a,l \u2022 a l,quant . Hence, the single FP32 parameter S a,l is also stored in memory as part of the frozen stage. In our experiments, we set the bitwidth Q of all activations and coefficients to 8-bit, while the output of the frozen stage is compressed to 8-bit or less, as further explored in Section V.\n\nIV. HARDWARE/SOFTWARE PLATFORM\nIn this section, we describe the hardware architecture of the proposed platform for TinyML learning and the related software stack.\n\nA. Hardware architecture\nThe CL platform we propose is inspired and extends on our previous work [18]. We build it upon an advanced PULP-based SoC, called VEGA, which combines parallel programming for high-performance with ultra-low-power features. An advanced prototype of this platform has been taped out in Global-Foundries 22nm technology [20]. The system architecture, which is outlined in Fig. 2, is based on an I/O-rich MCU platform coupled with a multi-core cluster of RISC-V ISA digital signal processing cores which are used to accelerate data-parallel machine learning & linear algebra code. The MCU side features a single RISC-V core, namely the Fabric-Controller (FC), and a large set of peripherals. Besides the FC core, the MCU-side of the platform includes a large L2 SRAM, organized in an FC-private section of 64kB and a larger interleaved section of 1.5MB. The interleaved L2 is shared between the FC core and an autonomous I/O DMA controller, connected to a broad set of peripherals such as OctaSPI/HyperBus to access an external Flash or DRAM of up to 64MB, as well as camera interfaces (CPI, MIPI) and standard MCU interfaces (SPI, UART, I2C, I2S, and GPIO). The I/O DMA controller is connected to an on-chip magnetoresistive RAM (MRAM) of 4MB, which resides in its power and clock domain and can be accessed through the I/O DMA to move data to/from the L2 SRAM.\nThe multi-core cluster features nine processing elements (PE) that share data on a 128kB multi-banked L1 tightly coupled data memory (TCDM) through a 1-cycle latency logarithmic interconnect. All cores are identical, using an in-order 4-stage architecture implementing the RISC-V RV32IMCFXpulpv2 ISA. The cluster includes a set of four highly flexible FPUs shared between all nine cores, capable of FP32 and FP16 computation [57]. Eight cores are meant to execute primarily data-parallel code, and therefore they use a hierarchical Instruction cache (I$) with a small private part (512B) plus 4kB of shared I$ [58]. The ninth core is meant to be used as a cluster controller for control-heavy data tiling & marshaling operations; it has a private I$ of 1kB. The cluster also features a multi-channel DMA engine that autonomously handles data transfers between the shared L1 and the external memories through a 64-bit AXI4 cluster bus. The DMA can transfer up to 8B/cycle between L2 and L1 TCDM in both directions simultaneously and perform 2D FW BW error BW grad   strided access on the L2 side by generating multiple AXI4 bursts. The cluster can be switched on and off at runtime by the FC core employing clock-gating; it also resides on a separate power domain than the MCU, making it possible to completely turn it off and to tune its Vdd using an embedded DC-DC regulator.im2col transform K K Cin K \u00d7 K \u00d7 Cin K \u00d7 K \u00d7 Cin 1 \u00d7 1 \u00d7 Cout 1 \u00d7 1 \u00d7 Cout output weight input K \u00d7 K \u00d7 Cin K \u00d7 K \u00d7 Cin 1 \u00d7 1 \u00d7 Cout 1 \u00d7 1 \u00d7 Cout grad_weight L input grad_output L 1 \u00d7 1 \u00d7 Cout K \u00d7 K \u00d7 Cin 1 \u00d7 1 \u00d7 Cout K \u00d7 K \u00d7 Cin\n\nB. Software stack\nTo execute the CL algorithm, the workload is largely dominated by the execution of convolutional layers, such as pointwise, and depthwise, or fully connected layers (\u223c98% of operations in MobileNet-V1). Consequently, the main load on computations is due to variants of matrix multiplications during the forward and backward steps, which can be efficiently parallelized on the 8 compute PEs of the cluster, leaving one core out to manage tiling and program data transfers. Thus, to enable the learning paradigm on the PULP platform, we propose a SW stack composed of parallel layer-wise primitives that realize the forward step and the back-propagation. The latter concerns either the computation of the activation gradients (backward error step) and coefficient gradients (backward gradient step). Fig. 3 depicts the dataflow of the forward and backward for commonly used convolutional kernels such as pointwise (PW), depthwise (DW), and linear (L) layers. To reshape all convolution operations intomatrix multiplications, the im2col transformation is applied to the activation tensors to reshape them into2D matrix operands [37]. The FP32 matrix multiplication kernel is parallelized over the eight cores of the cluster according to a data-parallelism strategy, making use of fmadd.s (floating multiply-add) instructions made available by the shared FPU engines.\nThe cores must operate on data from arrays located in the low-latency L1 TCDM to maximize throughput and computational efficiency (i.e., IPC). However, the operands of a layer function may not entirely fit into the lower memory level because of the limited space (128kB). For instance, the tensors of the PW layer #22 of the used MobileNet-V1 occupy 1.25MB. Hence, the operands have to be sliced intoreducedsize blocks that can fit intothe available L1 memory and convolutional functions are applied on L1 tensor slices to increase the computational efficiency.\nThis approach is generally referred to as tiling [39], which is schematized in Fig. 4. By locating layer-wise data on the larger L2 memory (1.5MB), the DMA firstly copies individual slices of operand data, also referred to as tiles, intoL1 buffers, to be later fetched by the cores. Since the cluster DMA engine is capable of 2D-strided access on the L2 side, this operation can also be designed to perform im2col, without any manual data marshaling overhead on L1.\nTo increase the computation efficiency, we implement a software flow that interleaves DMA transfers between L2 and L1 and calls to parallel primitives, e.g. forward, backward error, or backward gradient steps, which operate on individual tiles of data. Hence, every layer is expected to load and process all the tiles of any operand tensor. To reduce the overhead due to the data copy, the DMA transfers take place in the background of the multi-core computation: the copy of the next tile is launched before invoking the computation on loaded tiles. On the other side, this optimization requires doubling the L1 memory requirement: while one L1 buffer is used for computation, an equally-sized buffer is used by the data movement task. From a different viewpoint, the maximum tile size must not exceed half of the available memory. At runtime, layer-wise tiled kernels are invoked sequentially to run the learning algorithm with respect to the input data. To this aim, LRs are loaded from external embedded memory, if not fitting the internal memory, and copied to the on-chip L2 memory thanks to the I/O DMA.\n\nV. EXPERIMENTAL RESULTS\nIn this section, we provide the experimental evidence about our proposed TinyML platform for on-device Continual Learning. First, we evaluate the impact of quantization of the frozen stage and the LR vectors upon the overall accuracy, and we analyze the memory-accuracy trade-off.\nSecondly, we study the efficiency of the proposed SW architecture with respect to multiple HW configurations, namely #cores, L1 size and DMA bandwidth, introducing the tiling requirements and evaluating the latency for each kernel of computation. Then, we measure performance on an advanced PULP prototype, VEGA, fabricated in GlobalFoundries 22nm technology with 4 FPUs shared among all cores. We analyze the latency results for individual layers forward and backward and estimate the overall energy consumption to perform a CL task on our platform. Finally, we compare the efficiency of our TinyML platform to other devices used for on-device learning.\n\nA. Experimental Setup\nWe benchmark the compression technique for the Latent Replay memory on the image-classification Core50 dataset, which includes 120k 128\u00d7128 RGB images of 50 objects for the training and about 40k images for the testing. On the Core50 dataset, the CL setting is regulated by the NICv2-391 protocol [59]. According to this protocol, 3000 images belonging to ten classes are made available during the initial phase to fine-tune the targeted deep model on the Core50 problem. Afterward, the remaining 40 classes are introduced at training time in 390 learning events. Each event, as described more in detail in Section III-A, comprises iterations over minibatches of 128 samples each: 21 coming from actual images, all from the same class and typically not independent (e.g., coming from a video), and 107 latent replays. After each learning event, the accuracy is measured on the test set, which includes samples from the complete set of classes.\nFollowing [1], we use a MobileNet-V1 model with an input resolution of 128\u00d7128 and width multiplier 1, pretrained on ImageNet; we start from their public released code 7 and use PyTorch 1.5. In our experiments, we replace BatchReNormalization with BatchNormalization layers and we freeze the statistics of the frozen stage after fine-tuning.\n\nB. QLR-CL memory usage and accuracy\nTo evaluate the proposed QLR-CL setting, we quantize the frozen stage of the model using the PyTorch-based NEMO library [60] after fine-tuning the MobileNet-V1 model with the initially available 3000 images. We set the activation and parameters bitwidth of the frozen stage to Q = 8 bit while we vary the bitwidth Q LR of the latent replay layer. The quantized frozen stage is used to generate a set of N LR Latent Replays, as sampled from the initial images.\nThe plots in Fig. 5 show the test accuracy on the Core50 that is achieved at the end of the NICv2-391 training protocol for a varying N LR = {375, 750, 1500, 3000} while sweeping the LR layer l. Depending on the selected layer type, the size of the LR vector varies as reported in Table III.\nEach subplot of Fig. 5 compares the baseline FP32 version with our 8-bit fully-quantized solutions with a varying Q LR = {8, 7, 6}, denoted in the figures, respectively, as UINT-8, UINT-7 and UINT-6. For a Q LR < 6, we observe the Continual Learning process to not converge on the Core50 dataset.\nFrom the obtained results, we can observe the UINT-8 compressed solution featuring a small accuracy drop with respect to the full-precision FP32 baseline. When increasing the number of latent replays N LR to 3000, the UINT-8 quantized version results almost lossless (-0.26%), if LR= 19. On the contrary, if the LR layer is moved towards the last layer (LR=27), the accuracy drop increases up to 3.4%. The same effect is observed when reducing N LR to 1500, 750 or 375. In particular, when N LR = 1500, the UINT-8 quantatized version presents an accuracy drop from 1.2% (LR= 19) to 2.9% (LR= 27). On the other hand, lowering the bit precision to UINT-7, the accuracy reduces on average of up to 5.2%, if compared to the FP32 baseline. Bringing this further down to UINT-6 largely degrades the accuracy by more than 10%.\nTo deeply investigate the impact of the quantization process on the overall accuracy, we perform an ablation study to distinguish the individual effects of i) the quantization of the front-end and ii) the quantization of the LRs. In case of N LR = 1500, Table II compares the accuracy on the Core50 dataset for different LR layers, if applying quantization to both the LR memory and the frozen stage or only to the LR memory. The accuracy statistics are averaged over 5 experiments; we report in the table the mean and the std deviation of the obtained results. In particular, we see that quantizing the LRs has a larger effect on the accuracy than quantizing the frozen graph. By quantizing only the LR memory to UINT-8, the accuracy drops by up to 1.2-2.6% (higher in case of larger adaptive stages) with respect to the FP32 baseline. On the contrary, the UINT-8 quantized frozen graph brings only an additional 0.5-1% of accuracy drop. With UINT-7 LRs, the accuracy drop is mainly due to the LR quantization: when compressing also the frozen stage to 8-bit the accuracy drop is up to -1%, which is small compared to the total 4-7% of accuracy degradation.\nTo facilitate the interpretation of the results, Fig. 6 reports the test accuracy for multiple quantization settings compared to the size (in MB) of the Latent Replay Memory. In red, we highlight a Pareto frontier of non-dominated points, to have a range of options to maximize accuracy and minimize the memory footprint. Among the best solutions, we detect two clusters of points on the frontier. The first cluster (A), corresponding to the low-memory side of the frontier, is  constituted by experiments that use l = 27 with 1500 or 3000 LRs and UINT-7 or UINT-8 representation. On the other hand, if we aim at the highest accuracy possible for our QLR-CL classification algorithm, we can follow the Pareto frontier to the right towards higher accuracies at steeper memory cost, reaching cluster B. All points in cluster B features l = 23 as Latent Replay layer, which is a bottleneck layer of the network and allows to store more compact tensors as LR (refer to Table III). Adopting LR layers within B leads accuracy to an average of 76%, gaining \u223c5% on average with respect to the layers within cluster A. A single point C1 is shown further to the right, but still below 128MB.\nFor a deeper analysis of the Pareto frontier, in Fig. 7, we detail the memory requirements when analyzing the points into the two clusters A and B, as well as C1. We make two observations: first, in all A points, it would be possible to fit entirely within the on-chip memory available on VEGA, exploiting the 4MB of non-volatile MRAM. This would allow avoiding any external memory access, increasing the energy efficiency of the algorithm by a factor of up to \u223c3\u00d7 [20]. Moreover, considering that the maximization of accuracy is often the primary objective in CL, we observe that accumulating features at l = 19 with 1500 UINT-8 LRs (point C1) enables accuracy to grow above 77%, almost 10% more than the compact solutions in A (Fig. 7). This analysis allows us to also speculate over possible future architectural explorations to design optimized bottleneck layers that could facilitate better memory accuracy trade-off for QLR-CL.\n\nC. Hardware/Software Efficiency\nTo assess the performance of the proposed solution, we study the efficiency of the CL Software primitives on the target platform and the sensitivity to some of the HW architectural parameters, namely the #cores, the L1 memory size and the cluster DMA Bandwidth.\nSingle-tile performance on L1 TCDM: Based on the tiling strategy described in Section IV-B, we run experiments concerning the CL primitives of the software stack that operates on individual tiles of data placed in the L1 memory. Figure 8 shows the latency performance, expressed as MAC/cyc, i.e. the ratio between Multiply-Accumulate operations (MAC) and elapsed clock cycles (cyc), for each of the main FP32 computation kernels in case of single-core (1-CORE) or multi-core (2-4-8-CORES) execution. We highlight that a higher value of MAC/cyc denotes a more efficient processing scheme, leading to lower latency for a given computation workload, i.e. fixed MAC. More specifically, in this plot, we evaluate the forward (FW), backward error (BW ERR), and backward gradient (BW GRAD) for each of the considered layer for a varying size of the L1 TCDM memory, i.e. 128, 256 or 512kB. The shapes of the tiles for PointWise (PW), DepthWise (DW), and Linear (Lin) layers used for the experiments are reported in the tables on the left of the figure. Such dimensions are defined to fit three different sizes of the TCDM, considering buffers of size 64kB, 128kB and 256kB.\nFocusing firstly on the PW layers (histograms at the top of the figure), we observe a peak performance in the 8-cores FW step, achieving up to 1.91 MAC/cyc for a L1 memory size of 512kB. We observe also a performance improvement of up to 11% by increasing the L1 size from 128kB to 512kB, which is is motivated by the higher computational density of the kernel: if L1= 512kB the inner loop features 4\u00d7 iterations than a scenario with 128kB of L1 size. Moreover, the parallel speedup scales almost linearly with respect to the number of cores and archives 7.2\u00d7 in case of 8 cores. With respect to the theoretical maximum of 8\u00d7, the parallel implementation presents some overheads mainly due to increased L1 TCDM contentions and cluster's cache misses.\nIf we look at DW convolutions, their performance is lower with respect to the others. The main reason is that it requires a software-based im2col data layout transformation, which increase the amount of data marshaling operations and adds an extra L1 buffer, thus reducing the size of matrices in the matrix  multiplication, leading to increased overheads. Specifically, we measure the workload of the im2col to achieve up to 70% of the FW kernel's latency. As mentioned in Section IV, the primitives we introduce also support performing the im2col directly when moving the data tile from L2 via DMA transferin that case, this source of performance loss is not present, and the MAC/cyc necessary for depthwise convolutions increases up to 1 MAC/cycles for depthwise forward-prop, depending also on the L1 size selected. The remaining overhead with respect to pointwise convolutions is justified by the fact that depthwise convolutions can only exploit filter reuse (of size 3\u00d73, for example, in MobileNet-V1 DW layers) and no input channel data-reuse, resulting in much shorter inner loops and more visible effect of overheads. This latter effect cannot be counteracted by efficient DMA usage; on the other hand, since depthwise convolutions account for less than 1.5% of the computation, their impact on the overall latency is limited, as we further explore in the following section.\nMoving our analysis towards the different performance between forward-and backward-prop layers (particularly BW grad), we observe that this effect is again due to different data re-use between the matrix multiplication kernels. The reduction in re-use in the backward-prop is due to the tiling strategy adopted (see Fig. 3) has a grad output vector which is shorter than the input in the forward matrix multiplication. Specifically, the input to the matrix multiplication has size 8x1x1 in backward, while the input shape in forward changes accordingly with the L1 memory: 512x1x1 for 128kB L1, 1024x1x1 for 256kB L1 and 2048 for 512kB L1. In this scenario, the inner loop of the matrix multiplication of a forward computation is 64\u00d7, 128\u00d7 or 256\u00d7 larger with respect to the backward kernels' cases. This fact motivates the lower MAC/cyc of the BW ERR step (22%) and BW GRAD step (-46%) if compared to the FW kernel.\nL2-L1 DMA Bandwidth effects on performance: Next we analyze the impact of L2-L1 DMA Bandwidth variations, due to the Cluster DMA, on the overall performance of the learning task. In particular, we monitor the latency and the MAC/cyc for multiple values of L2-L1 bandwidth ranging from 8 to 128 bits per clock cycle (bit/cyc) and different configurations of #cores and L1 size. We remark that a higher value of MAC/cyc indicates a better performing HW configuration. Our analysis assumes a single half-duplex DMA channel, hence the bandwidth value accounts for either read or write transfers. Fig. 9 reports the average MAC/cyc when running the forward and backward steps with respect to the L2-L1 cluster's DMA bandwidth. As a benchmark, we consider the adaptive stage of the MobileNetV1 model when the LR layer is set to the 19th layer. Hence, we adopt our tiling strategy and doublebuffering scheme to realize the training. When increasing the L1 size, the tensor tiles feature a larger size, therefore demanding a higher transfer time to copy data between the L1 memory (used for computation) and L2 memory (used for storage). Thanks to the adopted double-buffering technique, such transfer time can be hidden by the computation time because the DMA works in the background of CPU operation (compute-bound). On the contrary, if the transfer time results dominating, the computation becomes DMA transfer-bound, with lower benefits from the multi-core acceleration.\nIn case of single core execution, the measured MAC/cyc does not vary with respect to the L1 size (128kB, 256kB or 512kB) as can be seen from the plot. In this scenario, the CPU time results as the dominant contribution with respect to the transfer time: the execution is compute-bound and a higher L2-L1 bandwidth does not impact the overall performance. Differently, in a multi-core execution (2, 4 or 8 cores), the average MAC/cyc increases and therefore the ratio between transfer time and the computation time decreases: from the plot we can observe higher performance if the DMA bandwidth is increased. If featuring a L1 size of 128kB, the sweet spots between DMA and compute bound are observed when the L2-L1 DMA bandwidth is 16 (2 cores), 32 (4 cores) and 64 (8 cores) bit/cyc, respectively, as highlighted by the red circles in the plot. These configurations denote the sweet spots to tune the DMA requirements with respect to the chosen L1 memory size and #cores.\nIf focusing more on the impact of the L1 memory size to the multi-core performance, we observe up to 2\u00d7 efficiency gain with 8 cores with a larger L1 memory, increasing from 0.25 MAC/cyc for a 128kB L1 memory to 0.4MAC/cyc at L1=256kB and to 0.53MAC/cyc for 512kB of L1. At 64 bit/cyc of L2-L1 DMA bandwidth, the execution, which is dominated by the computation, reaches 0.52MAC/cyc, 2.12\u00d7 faster than the low-bandwidth configuration.\nFrom this analysis we can conclude that the best design point for the learning task on a low-end multi-core architecture can be pinpointed leveraging the L2-L1 DMA Bandwidth and the L1 memory size tuning: when using 8 cores, 128kB of L1 memory, which is typically the main expensive resource for the system, can lead already to the highest performance as long as the DMA features a bandwidth of 64 bit/cyc. On the contrary, if the DMA's bandwidth is as low as 8 bit/cyc, a 512 kB L1 memory is needed to gain maximum performance. The target chip VEGA includes a L1 memory of 128 kB; the DMA follows a full-duplex scheme and can provide up to 64 bit/cyc for read transactions and 64 bit/cyc for write transactions. Therefore the VEGA HW architecture can fully exploit the presented SW architecture and optimization schemes to reach the optimal utilization and performance for the learning task.\n\nD. Latency Evaluation on VEGA SoC\nWe run experiments on the VEGA SoC to assess the ondevice learning performance, in terms of latency and energy consumption, of the proposed QLR-CL framework. Specifically, we report the computation time, i.e. the latency, at the running frequency of 375MHz and the power consumption by measuring the current absorbed by the chip when powered at 1.8V. To measure the full layer latency, we profile forward and backward tiled kernels, which include DMA transfers of data, initially stored in L2, and calls to low-level kernel primitives, introduced above. On average, we observe a 7% of tiling overhead with respect to the single-tile execution on L1. This is not surprising, due to the large bandwidth availability between L1 and L2 and the presence of compute-bound matrix multiplication operations. Based on the implemented tiled functions, we report the layer-wise performance in Table IV for any of the layers of the MobileNet-V1 model. We consider as complete time for the execution of a layer the cumulated time for frozen stage and adaptive stage. The latency of the frozen stage is obtained using DORY [39] to deploy the network, as this operation is performed as pure 8-bit quantized inference. We compute the full latency of the adaptive stage as the time needed to execute the forward and backward phases of each layer. Since we have multiple configurations, latencies for retraining start growing from the last layer (#27) up to layer #20, where retraining comprises a total of eight layers.\nFirst of all, we note that frozen stage latencies are utterly dominated by the adaptive stage. Apart from the faster inference backend, which can rely on 8-bit SIMD vectorization, this is because only 21 images per mini-batch pass through the frozen stage, while the adaptive stage has to be executed on 128 latent inputs (107 LRs and the 21 dequantized outputs from the frozen stage), and it has to run for multiple epochs (by default, 4) in order to work.\nWhen l = 27, the adaptive stage is very fast thanks to its very small number of parameters (it produces just the 50 output classes). This is the only case in which the frozen stage is non-negligible (\u223c 1/6 of the overall time). Progressing upward in the table, the frozen stage becomes negligible. The cumulative impact of forward and backward passes through all the other layers we take intoaccount (l from #20 to #26) is in the range between 0.3h and 1.5h. In particular, l = 23 corresponds to \u223c14 min per learning event; this LR layer corresponds to high accuracy (>75% in Core50, see Fig. 6), which means that in this time the proposed system is capable of acquiring a very significant new capability (e.g., a new task/object to classify) while retaining previous knowledge to a high degree.\nHaving the basic mini-batch measurements, we can estimate any scenario, by considering that to train with 1500 LR and l = 27, we will need 300 new images, thus we need 14 minibatches (300/21), which leads to 3.30 seconds to learn a new set of images, with an accuracy of 69.2%. If we push back the LR layer l, this leads to an increase of accuracy 76.5%, at the expense of much larger latency, up to 42 minutes for layer #20 (see Table IV).\n\nE. Energy Evaluation on CL Use-Cases and Comparison with other Solutions\nTo understand the performance of our system and its realworld applicability, we study two use-cases: a single minibatch of the Core50 training we used, and the simplified scenario presented by Pellegrini et al. [1] in their demonstration video. We compare our results with another MCU targeting ultra-power consumption: a NUCLEO-64 board based on the STM32L476RG MCU, on which we ran a direct port of the same code we use on the PULP-based platforms. It has two onchip SRAMs with 1-cycle access time and an overall capacity of 96kB. Performance results, in terms of latency, are reported in Table IV, where we take intoaccount the cumulative latency values both for VEGA and STM32 implementations, along with the cumulative energy consumption. Cumulative latency is computed by adding from the linear layer of the network the latencies of the preceding layers.\nOn average, execution on VEGA's 8-cores on performs 65\u00d7 faster with respect to the STM32 solution thanks to three main factors. Firstly, the clock frequency of VEGA is 4.7\u00d7 higher than the max clock frequency of the STM32L4 (375MHz vs 80MHz), also thanks to the superior technology node. Secondly, VEGA presents a parallel speed-up of up to 7.2\u00d7. Lastly, thanks to the more optimized ISA and the core microarchitecture, VEGA performs less operations while executing the same learning task. For example, the inner loop of the matrix multiplication on VEGA requires only 4 instructions while the STM32L4 takes 9 instructions, resulting 2.25\u00d7 faster, mainly thanks to the HW loop extension and the fmadd.s instruction.\nThe latency speed up, leads to an energy gain of around 37\u00d7, because the average power consumption of VEGA is 2\u00d7 higher than the STM32L4 at full load.\nNotice that the latency measurement of the STM32L4 does not account for possible overheads due to the tiling data between the small on-chip SRAM banks and off-chip memory. Even then, our results show that fine-tuning from any layer above the last one results in too large a latency to be realistic on STM32L4 -in the order of a day per learning event with l = 23. On the contrary, CL on VEGA can be completed in \u223c 14 minutes if selecting l = 23 or as fast as 3.3 seconds if retraining only the last layer.\nGiven the reported energy consumption, we estimated the battery lifetime of our device when adapting the inference model by means of multiple learning events per hour; we assumed no extra energy consumption for the remaining time. In particular, Fig. 10 shows the battery lifetime (in hours) depending on the selected Latent Replay layer and the adaptation rate, expressed as the amount of learning events per hour. We considered a 3300 mAh battery as the only energy source for the device. By retraining only the last layer (LR= 27), an intelligent node featuring our device can perform more than 1080 continual learning events per hour, leading to a lifetime of about 175h. On the contrary, if retraining larger portions of the network, the training time increases and the maximum rate of the learning events reduces to less than 10/hour, with a lifetime in the range 200-1000h. In comparison, on a STM32L4, if retraining the coefficients of the last layer, the maximum learning rate per hour is limited to 750, with a lifetime of about 10h. This latter can be increased up to 10000h but retraining only once in one hour. At the same learning event rate, the battery lifetime of VEGA is 20x higher.\nLastly, we compare with the use-case presented by Pellegrini et al. [1], where they developed a mobile phone application that performs CL with LRs on a OnePlus6 with Snapdragon845. For this scenario, they consider only 500 LRs before the linear layer, these will be shuffled with 100 new images. Then, by construction the mini-batch is composed of 100 LRs and 20 new images, thus, for each of the 8 training epochs, the network will process 5 times over the 20 new images and the 100 LRs. This scenario leads them to obtain an average latency of 502 ms for a single learning event. On the other hand, considering our measurements on VEGA we obtain a forward latency of 1.25s and a training time of 2.07s for a whole learning event.\nConsidering the power envelope of a Snapdragon845 of about 4W, and the average power consumption of VEGA of 62mW, this implies that our solution is 9.7\u00d7 more efficient in terms of energy. We additionally assess the energy consumption and the duration of a battery in the mobile application scenario, provided the energy measurements on VEGA, when using a 3300mAh battery. Thus, if we consider performing learning over a mini-batch of images once every minute in the ultra-fast scenario (just retraining the linear layer) and to perform an inference each second, we obtain an energy consumption of 0.25J per minute. This leads the accuracy of the model to achieve an average of 69.2%, with an overall lifetime of about 108 days.\n\nVI. CONCLUSION\nIn this work, we presented what, to the best of our knowledge, is the first HW/SW platform for TinyML Continual Learning -together with the novel Quantized Latent Replaybased Continual Learning (QLR-CL) methodology. More specifically, we propose to use low-bitwidth quantization to reduce the high memory requirements of a Continual Learning strategy based on Latent Replay rehearsing. We show a small accuracy drop as small as 0.26% if using 8-bit quantized LR memory if compared to floating-point vectors and an average degradation of 5% if lowering the bit precision to 7-bit, depending on the LR layer selected. Our results demonstrate that sophisticated adaptive behavior based on CL is within reach for next-generation TinyML devices, such as PULP devices; we show the capability to learn a new Core50 class with accuracy up to 77%, using less than 64MB of memorya typical constraint to fit Flash memories. We show that our QLR-CL library based on VEGA achieves up to \u223c65\u00d7 better performance than a conventional STM32 microcontroller.\nThese results constitute an initial step towards moving the TinyML from a strict train-then-deploy approach to a more flexible and adaptive scenario, where low power devices are capable to learn and adapt to changing tasks and conditions directly in the field.\nDespite this work focused on a single CL method, we remark that, thanks to the flexibility of the proposed platform, other adaptation methods or models can be also supported, especially if relying on the back-propagation algorithm and CNN primitives, such as convolution operations.\n\nFootnotes:\n2: https://www.st.com/content/st com/en/ecosystems/stm32-ann.html\n3: https://greenwaves-technologies.com/gap8 gap9/\n4: https://www.st.com/en/embedded-software/x-cube-ai.html\n5: https://greenwaves-technologies.com/sdk-manuals/nn quick start guide\n6: https://www.nvidia.com/en-us/autonomous-machines/embeddedsystems/jetson-xavier-nx\n7: Available at https://github.com/vlomonaco/ar1-pytorch/. While Pellegrini et al. [1] report lower accuracies in their paper, our FP32 baseline results are aligned with their released code.\n\nReferences:\n\n- L. Pellegrini, G. Graffieti, V. Lomonaco, and D. Maltoni, \"Latent Replay for Real-Time Continual Learning,\" 2020 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS), pp. 10 203-10 209, 2020.- A. Kumar, S. Goyal, and M. Varma, \"Resource-efficient machine learn- ing in 2 kb ram for the internet of things,\" in International Conference on Machine Learning. PMLR, 2017, pp. 1935-1944.\n\n- C. R. Banbury, V. Janapa Reddi, M. Lam, W. Fu, A. Fazel, J. Holleman, X. Huang, R. Hurtado, D. Kanter, A. Lokhmotov et al., \"Benchmarking tinyml systems: Challenges and direction,\" arXiv e-prints, pp. arXiv- 2003, 2020.\n\n- J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and K. Gopalakrishnan, \"PACT: Parameterized Clipping Activation for Quantized Neural Networks,\" arXiv e-prints, pp. arXiv-1805, 2018.\n\n- D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, \"What is the state of neural network pruning?\" in Proceedings of Machine Learning and Systems, I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 129-146.\n\n- R. David, J. Duke, A. Jain, V. J. Reddi, N. Jeffries, J. Li, N. Kreeger, I. Nappier, M. Natraj, S. Regev, R. Rhodes, T. Wang, and P. Warden, \"TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems,\" arXiv e-prints, pp. arXiv-2010, 2020.\n\n- D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9, \"Concrete Problems in AI Safety,\" arXiv e-prints, pp. arXiv- 1606, 2016.\n\n- M. de Prado, M. Rusci, A. Capotondi, R. Donze, L. Benini, and N. Pa- zos, \"Robustifying the Deployment of tinyML Models for Autonomous mini-vehicles,\" Sensors, vol. 21, no. 4, p. 1339, 2021.\n\n- M. Song, K. Zhong, J. Zhang, Y. Hu, D. Liu, W. Zhang, J. Wang, and T. Li, \"In-situ ai: Towards autonomous and incremental deep learning for iot systems,\" in 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2018, pp. 92-103.\n\n- J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell, \"Overcoming catastrophic forgetting in neural networks,\" in Proceedings of the na- tional academy of sciences, N. A. Sciences, Ed., vol. 114, no. 13, 2017, pp. 3521-3526.\n\n- S. Dhar, J. Guo, J. Liu, S. Tripathi, U. Kurup, and M. Shah, \"A survey of on-device machine learning: An algorithms and learning theory perspective,\" ACM Transactions on Internet of Things, vol. 2, no. 3, pp. 1-49, 2021.\n\n- M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars, \"A continual learning survey: Defying forgetting in classification tasks,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\n- Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner, \"Online continual learning in image classification: An empirical survey,\" arXiv preprint arXiv:2101.10423, 2021.\n\n- G. M. Van de Ven and A. S. Tolias, \"Three scenarios for continual learning,\" arXiv preprint arXiv:1904.07734, 2019.\n\n- A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato, \"On tiny episodic memories in continual learning,\" arXiv preprint arXiv:1902.10486, 2019.\n\n- V. Lomonaco, L. Pellegrini, P. Rodriguez, M. Caccia, Q. She, Y. Chen, Q. Jodelet, R. Wang, Z. Mai, D. Vazquez, G. I. Parisi, N. Churamani, M. Pickett, I. Laradji, and D. Maltoni, \"CVPR 2020 Continual Learning in Computer Vision Competition: Approaches, Results, Current Chal- lenges and Future Directions,\" arXiv preprint arXiv:2009.09929, 2020.\n\n- Z. Mai, H. Kim, J. Jeong, and S. Sanner, \"Batch-level experience replay with review for continual learning,\" arXiv preprint arXiv:2007.05683, 2020.\n\n- L. Ravaglia, M. Rusci, A. Capotondi, F. Conti, L. Pellegrini, V. Lomonaco, D. Maltoni, and L. Benini, \"Memory-Latency-Accuracy Trade-offs for Continual Learning on a RISC-V Extreme-Edge Node,\" in 2020 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2020, pp. 1-6.\n\n- D. Rossi, F. Conti, A. Marongiu, A. Pullini, I. Loi, M. Gautschi, G. Tagliavini, A. Capotondi, P. Flatresse, and L. Benini, \"PULP: A parallel ultra low power platform for next generation IoT applications,\" in 2015 IEEE Hot Chips 27 Symposium (HCS). IEEE, 2015, pp. 1-39.\n\n- D. Rossi, F. Conti, M. Eggiman, S. Mach, A. D. Mauro, M. Guermandi, G. Tagliavini, A. Pullini, I. Loi, J. Chen, E. Flamand, and L. Benini, \"4.4 a 1.3tops/w @ 32gops fully integrated 10-core soc for iot end-nodes with 1.7uw cognitive wake-up from mram-based state-retentive sleep mode,\" in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64, 2021, pp. 60-62.\n\n- S. Cass, \"Taking ai to the edge: Google's tpu now comes in a maker- friendly package,\" IEEE Spectrum, vol. 56, no. 5, pp. 16-17, 2019.\n\n- H. Cai, C. Gan, L. Zhu, and S. Han, \"TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning,\" Advances in Neural Information Processing Systems, vol. 33, 2020.\n\n- H. Ren, D. Anicic, and T. Runkler, \"TinyOL: TinyML with Online- Learning on Microcontrollers,\" arXiv e-prints, pp. arXiv-2103, 2021.\n\n- S. Disabato and M. Roveri, \"Incremental On-Device Tiny Machine Learning,\" in Proceedings of the 2nd International Workshop on Chal- lenges in Artificial Intelligence and Machine Learning for Internet of Things, 2020, pp. 7-13.\n\n- S. Benatti, F. Montagna, V. Kartsch, A. Rahimi, D. Rossi, and L. Benini, \"Online learning and classification of emg-based gestures on a parallel ultra-low power platform using hyperdimensional computing,\" IEEE transactions on biomedical circuits and systems, vol. 13, no. 3, pp. 516- 528, 2019.\n\n- D. Maltoni and V. Lomonaco, \"Continuous learning in single- incremental-task scenarios,\" Neural Networks, vol. 116, pp. 56-73, 2019.\n\n- F. M. Castro, M. J. Mar\u00edn-Jim\u00e9nez, N. Guil, C. Schmid, and K. Alahari, \"End-to-end incremental learning,\" in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 233-248.\n\n- S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \"icarl: Incremental classifier and representation learning,\" in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2017, pp. 2001-2010.\n\n- T. L. Hayes, N. D. Cahill, and C. Kanan, \"Memory efficient experience replay for streaming learning,\" in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 9769-9776.\n\n- L. Caccia, E. Belilovsky, M. Caccia, and J. Pineau, \"Online learned con- tinual compression with adaptive quantization modules,\" in International Conference on Machine Learning. PMLR, 2020, pp. 1240-1250.\n\n- B. Moons, R. Uytterhoeven, W. Dehaene, and M. Verhelst, \"Envi- sion: A 0.26-to-10TOPS/W subword-parallel dynamic-voltage-accuracy- frequency-scalable Convolutional Neural Network processor in 28nm FDSOI,\" in 2017 IEEE International Solid-State Circuits Conference (ISSCC), Feb. 2017, pp. 246-247.\n\n- V. Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, \"Efficient Processing of Deep Neural Networks: A Tutorial and Survey,\" arXiv:1703.09039 [cs], Mar. 2017.\n\n- S. Han, H. Mao, and W. J. Dally, \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,\" arXiv:1510.00149 [cs], Feb. 2016.\n\n- Y. H. Chen, T. Krishna, J. S. Emer, and V. Sze, \"Eyeriss: An Energy- Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks,\" IEEE Journal of Solid-State Circuits, vol. 52, no. 1, pp. 127-138, Jan. 2017.\n\n- M. Le Gallo, A. Sebastian, R. Mathis, M. Manica, H. Giefers, T. Tuma, C. Bekas, A. Curioni, and E. Eleftheriou, \"Mixed-precision in-memory computing,\" Nature Electronics, vol. 1, no. 4, pp. 246-253, Apr. 2018.\n\n- M. Zemlyanikin, A. Smorkalov, T. Khanova, A. Petrovicheva, and G. Serebryakov, \"512KiB RAM Is Enough! Live Camera Face Recog- nition DNN on MCU,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0-0.\n\n- L. Lai, N. Suda, and V. Chandra, \"CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs,\" arXiv e-prints, p. arXiv:1801.06601, Jan. 2018.\n\n- T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy, \"TVM: An automated end-to-end optimizing compiler for deep learning,\" in 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CA: USENIX Association, Oct. 2018, pp. 578-594. [Online]. Available: https: //www.usenix.org/conference/osdi18/presentation/chen\n\n- A. Burrello, A. Garofalo, N. Bruschi, G. Tagliavini, D. Rossi, and F. Conti, \"Dory: Automatic end-to-end deployment of real-world dnns on low-cost iot mcus,\" IEEE Transactions on Computers, p. 1-1, 2021. [Online]. Available: http://dx.doi.org/10.1109/TC.2021.3066883\n\n- A. Capotondi, M. Rusci, M. Fariselli, and L. Benini, \"CMix-NN: Mixed low-precision CNN library for memory-constrained edge devices,\" IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 67, no. 5, pp. 871-875, 2020.\n\n- J. Cheng, J. Wu, C. Leng, Y. Wang, and Q. Hu, \"Quantized CNN: A unified approach to accelerate and compress convolutional networks,\" IEEE transactions on neural networks and learning systems, vol. 29, no. 10, pp. 4730-4743, 2017.\n\n- S. Ghamari, K. Ozcan, T. Dinh, A. Melnikov, J. Carvajal, J. Ernst, and S. Chai, \"Quantization-Guided Training for Compact TinyML Models,\" arXiv e-prints, pp. arXiv-2103, 2021.\n\n- L. Cecconi, S. Smets, L. Benini, and M. Verhelst, \"Optimal Tiling Strategy for Memory Bandwidth Reduction for CNNs,\" in Advanced Concepts for Intelligent Vision Systems, ser. Lecture Notes in Computer Science, J. Blanc-Talon, R. Penne, W. Philips, D. Popescu, and P. Sche- unders, Eds. Springer International Publishing, 2017, pp. 89-100.\n\n- T. Moreau, T. Chen, L. Vega, J. Roesch, E. Yan, L. Zheng, J. Fromm, Z. Jiang, L. Ceze, C. Guestrin, and A. Krishnamurthy, \"A hardware- software blueprint for flexible deep learning specialization,\" IEEE Micro, vol. 39, no. 5, pp. 8-16, 2019.\n\n- D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, J. Yang, J. Park, A. Heinecke, E. Georganas, S. Srinivasan, A. Kundu, M. Smelyanskiy, B. Kaul, and P. Dubey, \"A Study of BFLOAT16 for Deep Learning Training,\" arXiv e-prints, pp. arXiv-1905, 2019.\n\n- X. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan, \"Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks,\" Advances in neural information processing systems, vol. 32, pp. 4900- 4909, 2019.\n\n- D. Shin, J. Lee, J. Lee, and H.-J. Yoo, \"14.2 DNPU: An 8.1 TOPS/W reconfigurable CNN-RNN processor for general-purpose deep neural networks,\" in 2017 IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2017, pp. 240-241.\n\n- T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam, \"Diannao: A small-footprint high-throughput accelerator for ubiqui- tous machine-learning,\" ACM SIGARCH Computer Architecture News, vol. 42, no. 1, pp. 269-284, 2014.\n\n- J. Shin, S. Choi, Y. Choi, and L.-S. Kim, \"A pragmatic approach to on-device incremental learning system with selective weight updates,\" in 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 2020, pp. 1-6.\n\n- D. Han, D. Im, G. Park, Y. Kim, S. Song, J. Lee, and H.-J. Yoo, \"HNPU: An Adaptive DNN Training Processor Utilizing Stochastic Dynamic Fixed-Point and Active Bit-Precision Searching,\" IEEE Journal of Solid- State Circuits, pp. 1-1, 2021.\n\n- S. Kang, D. Han, J. Lee, D. Im, S. Kim, S. Kim, and H.-J. Yoo, \"7.4 GANPU: A 135TFLOPS/W Multi-DNN Training Processor for GANs with Speculative Dual-Sparsity Exploitation,\" in 2020 IEEE International Solid-State Circuits Conference -(ISSCC), 2020, pp. 140- 142.\n\n- J. L. Lobo, J. Del Ser, A. Bifet, and N. Kasabov, \"Spiking Neural Networks and online learning: An overview and perspectives,\" Neural Networks, vol. 121, pp. 88-100, Jan. 2020.\n\n- M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday, G. Dimou, P. Joshi, N. Imam, S. Jain, Y. Liao, C.-K. Lin, A. Lines, R. Liu, D. Mathaikutty, S. McCoy, A. Paul, J. Tse, G. Venkataramanan, Y.-H. Weng, A. Wild, Y. Yang, and H. Wang, \"Loihi: A Neuromorphic Manycore Processor with On-Chip Learning,\" IEEE Micro, vol. 38, no. 1, pp. 82-99, Jan. 2018.\n\n- J. Pei, L. Deng, S. Song, M. Zhao, Y. Zhang, S. Wu, G. Wang, Z. Zou, Z. Wu, W. He, F. Chen, N. Deng, S. Wu, Y. Wang, Y. Wu, Z. Yang, C. Ma, G. Li, W. Han, H. Li, H. Wu, R. Zhao, Y. Xie, and L. Shi, \"Towards artificial general intelligence with hybrid Tianjic chip architecture,\" Nature, vol. 572, no. 7767, pp. 106-111, Aug. 2019.\n\n- G. Karunaratne, M. Schmuck, M. Le Gallo, G. Cherubini, L. Benini, A. Sebastian, and A. Rahimi, \"Robust high-dimensional memory- augmented neural networks,\" Nature Communications, vol. 12, no. 1, p. 2468, Apr. 2021.\n\n- B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, \"Quantization and training of neural networks for efficient integer-arithmetic-only inference,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2704-2713.\n\n- S. Mach, D. Rossi, G. Tagliavini, A. Marongiu, and L. Benini, \"A transprecision floating-point architecture for energy-efficient embedded computing,\" in 2018 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 2018, pp. 1-5.\n\n- I. Loi, A. Capotondi, D. Rossi, A. Marongiu, and L. Benini, \"The quest for energy-efficient i$ design in ultra-low-power clustered many-cores,\" IEEE Transactions on Multi-Scale Computing Systems, vol. 4, no. 2, pp. 99-112, 2017.\n\n- V. Lomonaco, D. Maltoni, and L. Pellegrini, \"Rehearsal-free continual learning over small non-iid batches,\" in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE Computer Society, 2020, pp. 989-998.\n\n- F. Conti, \"Technical Report: NEMO DNN Quantization for Deployment Model,\" arXiv preprint arXiv:2004.05930, 2020.\n\n", "annotations": {"ReferenceToTable": [{"begin": 40658, "end": 40661, "target": "#tab_3", "idx": 0}, {"begin": 42040, "end": 42042, "target": "#tab_2", "idx": 1}, {"begin": 43910, "end": 43913, "target": "#tab_3", "idx": 2}, {"begin": 54260, "end": 54262, "target": "#tab_4", "idx": 3}, {"begin": 56565, "end": 56567, "target": "#tab_4", "idx": 4}, {"begin": 57241, "end": 57243, "target": "#tab_4", "idx": 5}], "ReferenceToFootnote": [{"begin": 16804, "end": 16805, "target": "#foot_0", "idx": 0}, {"begin": 16902, "end": 16903, "target": "#foot_1", "idx": 1}, {"begin": 17306, "end": 17307, "target": "#foot_2", "idx": 2}, {"begin": 17340, "end": 17341, "target": "#foot_3", "idx": 3}, {"begin": 18424, "end": 18425, "target": "#foot_4", "idx": 4}, {"begin": 39700, "end": 39701, "target": "#foot_5", "idx": 5}], "SectionMain": [{"begin": 1936, "end": 63139, "idx": 0}], "SectionReference": [{"begin": 63675, "end": 77757, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1936, "idx": 0}], "Div": [{"begin": 92, "end": 1928, "idx": 0}, {"begin": 1939, "end": 12472, "idx": 1}, {"begin": 12474, "end": 12729, "idx": 2}, {"begin": 12731, "end": 15631, "idx": 3}, {"begin": 15633, "end": 19519, "idx": 4}, {"begin": 19521, "end": 23096, "idx": 5}, {"begin": 23098, "end": 23330, "idx": 6}, {"begin": 23332, "end": 25483, "idx": 7}, {"begin": 25485, "end": 27554, "idx": 8}, {"begin": 27556, "end": 30926, "idx": 9}, {"begin": 30928, "end": 31090, "idx": 10}, {"begin": 31092, "end": 34081, "idx": 11}, {"begin": 34083, "end": 37603, "idx": 12}, {"begin": 37605, "end": 38564, "idx": 13}, {"begin": 38566, "end": 39873, "idx": 14}, {"begin": 39875, "end": 45054, "idx": 15}, {"begin": 45056, "end": 53336, "idx": 16}, {"begin": 53338, "end": 56569, "idx": 17}, {"begin": 56571, "end": 61538, "idx": 18}, {"begin": 61540, "end": 63139, "idx": 19}], "Head": [{"begin": 1939, "end": 1954, "idx": 0}, {"begin": 12474, "end": 12490, "idx": 1}, {"begin": 12731, "end": 12769, "idx": 2}, {"begin": 15633, "end": 15669, "idx": 3}, {"begin": 19521, "end": 19563, "idx": 4}, {"begin": 23098, "end": 23110, "idx": 5}, {"begin": 23332, "end": 23385, "idx": 6}, {"begin": 25485, "end": 25507, "idx": 7}, {"begin": 27556, "end": 27607, "idx": 8}, {"begin": 30928, "end": 30958, "idx": 9}, {"begin": 31092, "end": 31116, "idx": 10}, {"begin": 34083, "end": 34100, "idx": 11}, {"begin": 37605, "end": 37628, "idx": 12}, {"begin": 38566, "end": 38587, "idx": 13}, {"begin": 39875, "end": 39910, "idx": 14}, {"begin": 45056, "end": 45087, "idx": 15}, {"begin": 53338, "end": 53371, "idx": 16}, {"begin": 56571, "end": 56643, "idx": 17}, {"begin": 61540, "end": 61554, "idx": 18}], "Paragraph": [{"begin": 92, "end": 1928, "idx": 0}, {"begin": 1955, "end": 2501, "idx": 1}, {"begin": 2502, "end": 2702, "idx": 2}, {"begin": 2703, "end": 2862, "idx": 3}, {"begin": 2863, "end": 3225, "idx": 4}, {"begin": 3226, "end": 3259, "idx": 5}, {"begin": 3260, "end": 4778, "idx": 6}, {"begin": 4779, "end": 5646, "idx": 7}, {"begin": 5647, "end": 7220, "idx": 8}, {"begin": 7221, "end": 8668, "idx": 9}, {"begin": 8669, "end": 8931, "idx": 10}, {"begin": 8932, "end": 10345, "idx": 11}, {"begin": 10346, "end": 10393, "idx": 12}, {"begin": 10394, "end": 11282, "idx": 13}, {"begin": 11283, "end": 12472, "idx": 14}, {"begin": 12491, "end": 12729, "idx": 15}, {"begin": 12770, "end": 13800, "idx": 16}, {"begin": 13801, "end": 15146, "idx": 17}, {"begin": 15147, "end": 15631, "idx": 18}, {"begin": 15670, "end": 16179, "idx": 19}, {"begin": 16180, "end": 17666, "idx": 20}, {"begin": 17667, "end": 18278, "idx": 21}, {"begin": 18279, "end": 19057, "idx": 22}, {"begin": 19058, "end": 19519, "idx": 23}, {"begin": 19564, "end": 19836, "idx": 24}, {"begin": 19837, "end": 23096, "idx": 25}, {"begin": 23111, "end": 23330, "idx": 26}, {"begin": 23386, "end": 24287, "idx": 27}, {"begin": 24288, "end": 25483, "idx": 28}, {"begin": 25508, "end": 26181, "idx": 29}, {"begin": 26182, "end": 26622, "idx": 30}, {"begin": 26623, "end": 27554, "idx": 31}, {"begin": 27608, "end": 27964, "idx": 32}, {"begin": 27965, "end": 28684, "idx": 33}, {"begin": 28685, "end": 29690, "idx": 34}, {"begin": 29751, "end": 30056, "idx": 35}, {"begin": 30104, "end": 30441, "idx": 36}, {"begin": 30442, "end": 30926, "idx": 37}, {"begin": 30959, "end": 31090, "idx": 38}, {"begin": 31117, "end": 32476, "idx": 39}, {"begin": 32477, "end": 33853, "idx": 40}, {"begin": 34101, "end": 35464, "idx": 41}, {"begin": 35465, "end": 36026, "idx": 42}, {"begin": 36027, "end": 36492, "idx": 43}, {"begin": 36493, "end": 37603, "idx": 44}, {"begin": 37629, "end": 37909, "idx": 45}, {"begin": 37910, "end": 38564, "idx": 46}, {"begin": 38588, "end": 39531, "idx": 47}, {"begin": 39532, "end": 39873, "idx": 48}, {"begin": 39911, "end": 40370, "idx": 49}, {"begin": 40371, "end": 40662, "idx": 50}, {"begin": 40663, "end": 40959, "idx": 51}, {"begin": 40960, "end": 41779, "idx": 52}, {"begin": 41780, "end": 42938, "idx": 53}, {"begin": 42939, "end": 44120, "idx": 54}, {"begin": 44121, "end": 45054, "idx": 55}, {"begin": 45088, "end": 45349, "idx": 56}, {"begin": 45350, "end": 46515, "idx": 57}, {"begin": 46516, "end": 47266, "idx": 58}, {"begin": 47267, "end": 48651, "idx": 59}, {"begin": 48652, "end": 49568, "idx": 60}, {"begin": 49569, "end": 51035, "idx": 61}, {"begin": 51036, "end": 52008, "idx": 62}, {"begin": 52009, "end": 52443, "idx": 63}, {"begin": 52444, "end": 53336, "idx": 64}, {"begin": 53372, "end": 54874, "idx": 65}, {"begin": 54875, "end": 55332, "idx": 66}, {"begin": 55333, "end": 56128, "idx": 67}, {"begin": 56129, "end": 56569, "idx": 68}, {"begin": 56644, "end": 57504, "idx": 69}, {"begin": 57505, "end": 58220, "idx": 70}, {"begin": 58221, "end": 58371, "idx": 71}, {"begin": 58372, "end": 58877, "idx": 72}, {"begin": 58878, "end": 60078, "idx": 73}, {"begin": 60079, "end": 60810, "idx": 74}, {"begin": 60811, "end": 61538, "idx": 75}, {"begin": 61555, "end": 62595, "idx": 76}, {"begin": 62596, "end": 62856, "idx": 77}, {"begin": 62857, "end": 63139, "idx": 78}], "ReferenceToBib": [{"begin": 3518, "end": 3521, "target": "#b1", "idx": 0}, {"begin": 3853, "end": 3856, "target": "#b2", "idx": 1}, {"begin": 4151, "end": 4154, "target": "#b3", "idx": 2}, {"begin": 4166, "end": 4169, "target": "#b4", "idx": 3}, {"begin": 4352, "end": 4355, "target": "#b5", "idx": 4}, {"begin": 4774, "end": 4777, "target": "#b6", "idx": 5}, {"begin": 4982, "end": 4985, "target": "#b7", "idx": 6}, {"begin": 5241, "end": 5244, "target": "#b7", "idx": 7}, {"begin": 5811, "end": 5814, "target": "#b8", "idx": 8}, {"begin": 6042, "end": 6046, "target": "#b9", "idx": 9}, {"begin": 6098, "end": 6102, "target": "#b10", "idx": 10}, {"begin": 6336, "end": 6340, "target": "#b11", "idx": 11}, {"begin": 6342, "end": 6346, "target": "#b12", "idx": 12}, {"begin": 7068, "end": 7072, "target": "#b12", "idx": 13}, {"begin": 7075, "end": 7079, "target": "#b14", "idx": 14}, {"begin": 7153, "end": 7157, "target": "#b15", "idx": 15}, {"begin": 7466, "end": 7470, "target": "#b16", "idx": 16}, {"begin": 7515, "end": 7518, "target": "#b0", "idx": 17}, {"begin": 8071, "end": 8074, "target": "#b0", "idx": 18}, {"begin": 8693, "end": 8697, "target": "#b17", "idx": 19}, {"begin": 8833, "end": 8837, "target": "#b18", "idx": 20}, {"begin": 9570, "end": 9574, "target": "#b19", "idx": 21}, {"begin": 10064, "end": 10067, "target": "#b0", "idx": 22}, {"begin": 10959, "end": 10963, "target": "#b18", "idx": 23}, {"begin": 12805, "end": 12809, "target": "#b20", "idx": 24}, {"begin": 12811, "end": 12815, "target": "#b21", "idx": 25}, {"begin": 13117, "end": 13121, "target": "#b11", "idx": 26}, {"begin": 13123, "end": 13127, "target": "#b12", "idx": 27}, {"begin": 13460, "end": 13464, "target": "#b25", "idx": 28}, {"begin": 13961, "end": 13964, "target": "#b0", "idx": 29}, {"begin": 13966, "end": 13970, "target": "#b26", "idx": 30}, {"begin": 13972, "end": 13976, "target": "#b27", "idx": 31}, {"begin": 14096, "end": 14100, "target": "#b15", "idx": 32}, {"begin": 14216, "end": 14220, "target": "#b16", "idx": 33}, {"begin": 14566, "end": 14569, "target": "#b0", "idx": 34}, {"begin": 14952, "end": 14956, "target": "#b15", "idx": 35}, {"begin": 14983, "end": 14986, "target": "#b0", "idx": 36}, {"begin": 15182, "end": 15186, "target": "#b28", "idx": 37}, {"begin": 15289, "end": 15293, "target": "#b29", "idx": 38}, {"begin": 15895, "end": 15899, "target": "#b30", "idx": 39}, {"begin": 15901, "end": 15905, "target": "#b31", "idx": 40}, {"begin": 16012, "end": 16016, "target": "#b32", "idx": 41}, {"begin": 16060, "end": 16063, "target": "#b3", "idx": 42}, {"begin": 16109, "end": 16113, "target": "#b33", "idx": 43}, {"begin": 16174, "end": 16178, "target": "#b34", "idx": 44}, {"begin": 16587, "end": 16591, "target": "#b35", "idx": 45}, {"begin": 17158, "end": 17161, "target": "#b5", "idx": 46}, {"begin": 17176, "end": 17180, "target": "#b36", "idx": 47}, {"begin": 17193, "end": 17197, "target": "#b37", "idx": 48}, {"begin": 17352, "end": 17356, "target": "#b38", "idx": 49}, {"begin": 17470, "end": 17474, "target": "#b39", "idx": 50}, {"begin": 17477, "end": 17481, "target": "#b41", "idx": 51}, {"begin": 17538, "end": 17542, "target": "#b42", "idx": 52}, {"begin": 17595, "end": 17599, "target": "#b36", "idx": 53}, {"begin": 17661, "end": 17665, "target": "#b43", "idx": 54}, {"begin": 18067, "end": 18071, "target": "#b44", "idx": 55}, {"begin": 18132, "end": 18135, "target": "#b3", "idx": 56}, {"begin": 18137, "end": 18141, "target": "#b39", "idx": 57}, {"begin": 18221, "end": 18225, "target": "#b45", "idx": 58}, {"begin": 18479, "end": 18482, "target": "#b0", "idx": 59}, {"begin": 18534, "end": 18538, "target": "#b46", "idx": 60}, {"begin": 18624, "end": 18628, "target": "#b50", "idx": 61}, {"begin": 18894, "end": 18898, "target": "#b51", "idx": 62}, {"begin": 18930, "end": 18934, "target": "#b24", "idx": 63}, {"begin": 18954, "end": 18958, "target": "#b52", "idx": 64}, {"begin": 18961, "end": 18965, "target": "#b54", "idx": 65}, {"begin": 20065, "end": 20069, "target": "#b20", "idx": 66}, {"begin": 20078, "end": 20082, "target": "#b21", "idx": 67}, {"begin": 20286, "end": 20290, "target": "#b22", "idx": 68}, {"begin": 20877, "end": 20880, "target": "#b7", "idx": 69}, {"begin": 21294, "end": 21298, "target": "#b23", "idx": 70}, {"begin": 21791, "end": 21795, "target": "#b24", "idx": 71}, {"begin": 22716, "end": 22719, "target": "#b0", "idx": 72}, {"begin": 24361, "end": 24364, "target": "#b0", "idx": 73}, {"begin": 25261, "end": 25264, "target": "#b0", "idx": 74}, {"begin": 26481, "end": 26485, "target": "#b17", "idx": 75}, {"begin": 29339, "end": 29343, "target": "#b55", "idx": 76}, {"begin": 31189, "end": 31193, "target": "#b17", "idx": 77}, {"begin": 31435, "end": 31439, "target": "#b19", "idx": 78}, {"begin": 32902, "end": 32906, "target": "#b56", "idx": 79}, {"begin": 33087, "end": 33091, "target": "#b57", "idx": 80}, {"begin": 35226, "end": 35230, "target": "#b36", "idx": 81}, {"begin": 36076, "end": 36080, "target": "#b38", "idx": 82}, {"begin": 38885, "end": 38889, "target": "#b58", "idx": 83}, {"begin": 39542, "end": 39545, "target": "#b0", "idx": 84}, {"begin": 40031, "end": 40035, "target": "#b59", "idx": 85}, {"begin": 44586, "end": 44590, "target": "#b19", "idx": 86}, {"begin": 54481, "end": 54485, "target": "#b38", "idx": 87}, {"begin": 56855, "end": 56858, "target": "#b0", "idx": 88}, {"begin": 60147, "end": 60150, "target": "#b0", "idx": 89}, {"begin": 63566, "end": 63569, "target": "#b0", "idx": 90}], "ReferenceString": [{"begin": 63690, "end": 63905, "id": "b0", "idx": 0}, {"begin": 63907, "end": 64096, "id": "b1", "idx": 1}, {"begin": 64100, "end": 64319, "id": "b2", "idx": 2}, {"begin": 64323, "end": 64524, "id": "b3", "idx": 3}, {"begin": 64528, "end": 64759, "id": "b4", "idx": 4}, {"begin": 64763, "end": 65011, "id": "b5", "idx": 5}, {"begin": 65015, "end": 65163, "id": "b6", "idx": 6}, {"begin": 65167, "end": 65357, "id": "b7", "idx": 7}, {"begin": 65361, "end": 65625, "id": "b8", "idx": 8}, {"begin": 65629, "end": 65986, "id": "b9", "idx": 9}, {"begin": 65990, "end": 66210, "id": "b10", "idx": 10}, {"begin": 66214, "end": 66459, "id": "b11", "idx": 11}, {"begin": 66463, "end": 66634, "id": "b12", "idx": 12}, {"begin": 66638, "end": 66753, "id": "b13", "idx": 13}, {"begin": 66757, "end": 66942, "id": "b14", "idx": 14}, {"begin": 66946, "end": 67291, "id": "b15", "idx": 15}, {"begin": 67295, "end": 67442, "id": "b16", "idx": 16}, {"begin": 67446, "end": 67718, "id": "b17", "idx": 17}, {"begin": 67722, "end": 67992, "id": "b18", "idx": 18}, {"begin": 67996, "end": 68375, "id": "b19", "idx": 19}, {"begin": 68379, "end": 68513, "id": "b20", "idx": 20}, {"begin": 68517, "end": 68692, "id": "b21", "idx": 21}, {"begin": 68696, "end": 68828, "id": "b22", "idx": 22}, {"begin": 68832, "end": 69058, "id": "b23", "idx": 23}, {"begin": 69062, "end": 69356, "id": "b24", "idx": 24}, {"begin": 69360, "end": 69492, "id": "b25", "idx": 25}, {"begin": 69496, "end": 69689, "id": "b26", "idx": 26}, {"begin": 69693, "end": 69915, "id": "b27", "idx": 27}, {"begin": 69919, "end": 70115, "id": "b28", "idx": 28}, {"begin": 70119, "end": 70323, "id": "b29", "idx": 29}, {"begin": 70327, "end": 70623, "id": "b30", "idx": 30}, {"begin": 70627, "end": 70776, "id": "b31", "idx": 31}, {"begin": 70780, "end": 70954, "id": "b32", "idx": 32}, {"begin": 70958, "end": 71182, "id": "b33", "idx": 33}, {"begin": 71186, "end": 71395, "id": "b34", "idx": 34}, {"begin": 71399, "end": 71645, "id": "b35", "idx": 35}, {"begin": 71649, "end": 71797, "id": "b36", "idx": 36}, {"begin": 71801, "end": 72219, "id": "b37", "idx": 37}, {"begin": 72223, "end": 72489, "id": "b38", "idx": 38}, {"begin": 72493, "end": 72722, "id": "b39", "idx": 39}, {"begin": 72726, "end": 72955, "id": "b40", "idx": 40}, {"begin": 72959, "end": 73134, "id": "b41", "idx": 41}, {"begin": 73138, "end": 73476, "id": "b42", "idx": 42}, {"begin": 73480, "end": 73721, "id": "b43", "idx": 43}, {"begin": 73725, "end": 74047, "id": "b44", "idx": 44}, {"begin": 74051, "end": 74333, "id": "b45", "idx": 45}, {"begin": 74337, "end": 74571, "id": "b46", "idx": 46}, {"begin": 74575, "end": 74804, "id": "b47", "idx": 47}, {"begin": 74808, "end": 75023, "id": "b48", "idx": 48}, {"begin": 75027, "end": 75264, "id": "b49", "idx": 49}, {"begin": 75268, "end": 75529, "id": "b50", "idx": 50}, {"begin": 75533, "end": 75709, "id": "b51", "idx": 51}, {"begin": 75713, "end": 76078, "id": "b52", "idx": 52}, {"begin": 76082, "end": 76412, "id": "b53", "idx": 53}, {"begin": 76416, "end": 76630, "id": "b54", "idx": 54}, {"begin": 76634, "end": 76920, "id": "b55", "idx": 55}, {"begin": 76924, "end": 77164, "id": "b56", "idx": 56}, {"begin": 77168, "end": 77396, "id": "b57", "idx": 57}, {"begin": 77400, "end": 77639, "id": "b58", "idx": 58}, {"begin": 77643, "end": 77755, "id": "b59", "idx": 59}], "Sentence": [{"begin": 92, "end": 395, "idx": 0}, {"begin": 396, "end": 654, "idx": 1}, {"begin": 655, "end": 791, "idx": 2}, {"begin": 792, "end": 986, "idx": 3}, {"begin": 987, "end": 1260, "idx": 4}, {"begin": 1261, "end": 1464, "idx": 5}, {"begin": 1465, "end": 1651, "idx": 6}, {"begin": 1652, "end": 1928, "idx": 7}, {"begin": 1955, "end": 2163, "idx": 8}, {"begin": 2164, "end": 2441, "idx": 9}, {"begin": 2442, "end": 2456, "idx": 10}, {"begin": 2457, "end": 2501, "idx": 11}, {"begin": 2502, "end": 2702, "idx": 12}, {"begin": 2703, "end": 2862, "idx": 13}, {"begin": 2863, "end": 2986, "idx": 14}, {"begin": 2987, "end": 2990, "idx": 15}, {"begin": 2991, "end": 3045, "idx": 16}, {"begin": 3046, "end": 3049, "idx": 17}, {"begin": 3050, "end": 3061, "idx": 18}, {"begin": 3062, "end": 3225, "idx": 19}, {"begin": 3226, "end": 3259, "idx": 20}, {"begin": 3260, "end": 3522, "idx": 21}, {"begin": 3523, "end": 3716, "idx": 22}, {"begin": 3717, "end": 3967, "idx": 23}, {"begin": 3968, "end": 4091, "idx": 24}, {"begin": 4092, "end": 4188, "idx": 25}, {"begin": 4189, "end": 4356, "idx": 26}, {"begin": 4357, "end": 4778, "idx": 27}, {"begin": 4779, "end": 4986, "idx": 28}, {"begin": 4987, "end": 5245, "idx": 29}, {"begin": 5246, "end": 5395, "idx": 30}, {"begin": 5396, "end": 5646, "idx": 31}, {"begin": 5647, "end": 6047, "idx": 32}, {"begin": 6048, "end": 6347, "idx": 33}, {"begin": 6348, "end": 6484, "idx": 34}, {"begin": 6485, "end": 6782, "idx": 35}, {"begin": 6783, "end": 7080, "idx": 36}, {"begin": 7081, "end": 7220, "idx": 37}, {"begin": 7221, "end": 7464, "idx": 38}, {"begin": 7465, "end": 7471, "idx": 39}, {"begin": 7472, "end": 7593, "idx": 40}, {"begin": 7594, "end": 7816, "idx": 41}, {"begin": 7817, "end": 8076, "idx": 42}, {"begin": 8077, "end": 8297, "idx": 43}, {"begin": 8298, "end": 8514, "idx": 44}, {"begin": 8515, "end": 8668, "idx": 45}, {"begin": 8669, "end": 8931, "idx": 46}, {"begin": 8932, "end": 9093, "idx": 47}, {"begin": 9094, "end": 9443, "idx": 48}, {"begin": 9444, "end": 9575, "idx": 49}, {"begin": 9576, "end": 9872, "idx": 50}, {"begin": 9873, "end": 10345, "idx": 51}, {"begin": 10346, "end": 10393, "idx": 52}, {"begin": 10394, "end": 10612, "idx": 53}, {"begin": 10613, "end": 10695, "idx": 54}, {"begin": 10696, "end": 10995, "idx": 55}, {"begin": 10996, "end": 11076, "idx": 56}, {"begin": 11077, "end": 11282, "idx": 57}, {"begin": 11283, "end": 11522, "idx": 58}, {"begin": 11523, "end": 11684, "idx": 59}, {"begin": 11685, "end": 11882, "idx": 60}, {"begin": 11883, "end": 12056, "idx": 61}, {"begin": 12057, "end": 12234, "idx": 62}, {"begin": 12235, "end": 12316, "idx": 63}, {"begin": 12317, "end": 12384, "idx": 64}, {"begin": 12385, "end": 12440, "idx": 65}, {"begin": 12441, "end": 12472, "idx": 66}, {"begin": 12491, "end": 12729, "idx": 67}, {"begin": 12770, "end": 13128, "idx": 68}, {"begin": 13129, "end": 13358, "idx": 69}, {"begin": 13359, "end": 13441, "idx": 70}, {"begin": 13442, "end": 13800, "idx": 71}, {"begin": 13801, "end": 13977, "idx": 72}, {"begin": 13978, "end": 14101, "idx": 73}, {"begin": 14102, "end": 14348, "idx": 74}, {"begin": 14349, "end": 14520, "idx": 75}, {"begin": 14521, "end": 14802, "idx": 76}, {"begin": 14803, "end": 14957, "idx": 77}, {"begin": 14958, "end": 15034, "idx": 78}, {"begin": 15035, "end": 15146, "idx": 79}, {"begin": 15147, "end": 15363, "idx": 80}, {"begin": 15364, "end": 15631, "idx": 81}, {"begin": 15670, "end": 15752, "idx": 82}, {"begin": 15753, "end": 15906, "idx": 83}, {"begin": 15907, "end": 16179, "idx": 84}, {"begin": 16180, "end": 16375, "idx": 85}, {"begin": 16376, "end": 16592, "idx": 86}, {"begin": 16593, "end": 16904, "idx": 87}, {"begin": 16905, "end": 17074, "idx": 88}, {"begin": 17075, "end": 17357, "idx": 89}, {"begin": 17358, "end": 17666, "idx": 90}, {"begin": 17667, "end": 17819, "idx": 91}, {"begin": 17820, "end": 17912, "idx": 92}, {"begin": 17913, "end": 18142, "idx": 93}, {"begin": 18143, "end": 18278, "idx": 94}, {"begin": 18279, "end": 18508, "idx": 95}, {"begin": 18509, "end": 18791, "idx": 96}, {"begin": 18792, "end": 18966, "idx": 97}, {"begin": 18967, "end": 19057, "idx": 98}, {"begin": 19058, "end": 19243, "idx": 99}, {"begin": 19244, "end": 19519, "idx": 100}, {"begin": 19564, "end": 19644, "idx": 101}, {"begin": 19645, "end": 19836, "idx": 102}, {"begin": 19837, "end": 19899, "idx": 103}, {"begin": 19900, "end": 20070, "idx": 104}, {"begin": 20071, "end": 20278, "idx": 105}, {"begin": 20279, "end": 20471, "idx": 106}, {"begin": 20472, "end": 20614, "idx": 107}, {"begin": 20615, "end": 20826, "idx": 108}, {"begin": 20827, "end": 20947, "idx": 109}, {"begin": 20948, "end": 21130, "idx": 110}, {"begin": 21131, "end": 21264, "idx": 111}, {"begin": 21265, "end": 21416, "idx": 112}, {"begin": 21417, "end": 21534, "idx": 113}, {"begin": 21535, "end": 21782, "idx": 114}, {"begin": 21783, "end": 21909, "idx": 115}, {"begin": 21910, "end": 22129, "idx": 116}, {"begin": 22130, "end": 22314, "idx": 117}, {"begin": 22315, "end": 22634, "idx": 118}, {"begin": 22635, "end": 22741, "idx": 119}, {"begin": 22742, "end": 22973, "idx": 120}, {"begin": 22974, "end": 23096, "idx": 121}, {"begin": 23111, "end": 23330, "idx": 122}, {"begin": 23386, "end": 23509, "idx": 123}, {"begin": 23510, "end": 23674, "idx": 124}, {"begin": 23675, "end": 23867, "idx": 125}, {"begin": 23868, "end": 24024, "idx": 126}, {"begin": 24025, "end": 24156, "idx": 127}, {"begin": 24157, "end": 24287, "idx": 128}, {"begin": 24288, "end": 24501, "idx": 129}, {"begin": 24502, "end": 24561, "idx": 130}, {"begin": 24562, "end": 24696, "idx": 131}, {"begin": 24697, "end": 24821, "idx": 132}, {"begin": 24822, "end": 24982, "idx": 133}, {"begin": 24983, "end": 25111, "idx": 134}, {"begin": 25112, "end": 25198, "idx": 135}, {"begin": 25199, "end": 25265, "idx": 136}, {"begin": 25266, "end": 25376, "idx": 137}, {"begin": 25377, "end": 25483, "idx": 138}, {"begin": 25508, "end": 25755, "idx": 139}, {"begin": 25756, "end": 25887, "idx": 140}, {"begin": 25888, "end": 26181, "idx": 141}, {"begin": 26182, "end": 26332, "idx": 142}, {"begin": 26333, "end": 26486, "idx": 143}, {"begin": 26487, "end": 26622, "idx": 144}, {"begin": 26623, "end": 26753, "idx": 145}, {"begin": 26754, "end": 26871, "idx": 146}, {"begin": 26872, "end": 26960, "idx": 147}, {"begin": 26961, "end": 27071, "idx": 148}, {"begin": 27072, "end": 27153, "idx": 149}, {"begin": 27154, "end": 27268, "idx": 150}, {"begin": 27269, "end": 27418, "idx": 151}, {"begin": 27419, "end": 27554, "idx": 152}, {"begin": 27608, "end": 27778, "idx": 153}, {"begin": 27779, "end": 27964, "idx": 154}, {"begin": 27965, "end": 28259, "idx": 155}, {"begin": 28260, "end": 28363, "idx": 156}, {"begin": 28364, "end": 28455, "idx": 157}, {"begin": 28456, "end": 28587, "idx": 158}, {"begin": 28588, "end": 28684, "idx": 159}, {"begin": 28685, "end": 28866, "idx": 160}, {"begin": 28867, "end": 29028, "idx": 161}, {"begin": 29029, "end": 29114, "idx": 162}, {"begin": 29115, "end": 29344, "idx": 163}, {"begin": 29345, "end": 29520, "idx": 164}, {"begin": 29521, "end": 29690, "idx": 165}, {"begin": 29751, "end": 29835, "idx": 166}, {"begin": 29836, "end": 30056, "idx": 167}, {"begin": 30104, "end": 30162, "idx": 168}, {"begin": 30163, "end": 30296, "idx": 169}, {"begin": 30297, "end": 30441, "idx": 170}, {"begin": 30442, "end": 30642, "idx": 171}, {"begin": 30643, "end": 30735, "idx": 172}, {"begin": 30736, "end": 30926, "idx": 173}, {"begin": 30959, "end": 31090, "idx": 174}, {"begin": 31117, "end": 31194, "idx": 175}, {"begin": 31195, "end": 31340, "idx": 176}, {"begin": 31341, "end": 31440, "idx": 177}, {"begin": 31441, "end": 31694, "idx": 178}, {"begin": 31695, "end": 31805, "idx": 179}, {"begin": 31806, "end": 31967, "idx": 180}, {"begin": 31968, "end": 32272, "idx": 181}, {"begin": 32273, "end": 32476, "idx": 182}, {"begin": 32477, "end": 32668, "idx": 183}, {"begin": 32669, "end": 32777, "idx": 184}, {"begin": 32778, "end": 32907, "idx": 185}, {"begin": 32908, "end": 33092, "idx": 186}, {"begin": 33093, "end": 33234, "idx": 187}, {"begin": 33235, "end": 33411, "idx": 188}, {"begin": 33412, "end": 33607, "idx": 189}, {"begin": 33608, "end": 33853, "idx": 190}, {"begin": 34101, "end": 34303, "idx": 191}, {"begin": 34304, "end": 34572, "idx": 192}, {"begin": 34573, "end": 34753, "idx": 193}, {"begin": 34754, "end": 34898, "idx": 194}, {"begin": 34899, "end": 35057, "idx": 195}, {"begin": 35058, "end": 35231, "idx": 196}, {"begin": 35232, "end": 35385, "idx": 197}, {"begin": 35386, "end": 35464, "idx": 198}, {"begin": 35465, "end": 35607, "idx": 199}, {"begin": 35608, "end": 35736, "idx": 200}, {"begin": 35737, "end": 35822, "idx": 201}, {"begin": 35823, "end": 36026, "idx": 202}, {"begin": 36027, "end": 36309, "idx": 203}, {"begin": 36310, "end": 36492, "idx": 204}, {"begin": 36493, "end": 36649, "idx": 205}, {"begin": 36650, "end": 36745, "idx": 206}, {"begin": 36746, "end": 36833, "idx": 207}, {"begin": 36834, "end": 37043, "idx": 208}, {"begin": 37044, "end": 37229, "idx": 209}, {"begin": 37230, "end": 37325, "idx": 210}, {"begin": 37326, "end": 37449, "idx": 211}, {"begin": 37450, "end": 37603, "idx": 212}, {"begin": 37629, "end": 37751, "idx": 213}, {"begin": 37752, "end": 37909, "idx": 214}, {"begin": 37910, "end": 38156, "idx": 215}, {"begin": 38157, "end": 38304, "idx": 216}, {"begin": 38305, "end": 38460, "idx": 217}, {"begin": 38461, "end": 38564, "idx": 218}, {"begin": 38588, "end": 38807, "idx": 219}, {"begin": 38808, "end": 38890, "idx": 220}, {"begin": 38891, "end": 39059, "idx": 221}, {"begin": 39060, "end": 39151, "idx": 222}, {"begin": 39152, "end": 39405, "idx": 223}, {"begin": 39406, "end": 39531, "idx": 224}, {"begin": 39532, "end": 39722, "idx": 225}, {"begin": 39723, "end": 39873, "idx": 226}, {"begin": 39911, "end": 40118, "idx": 227}, {"begin": 40119, "end": 40257, "idx": 228}, {"begin": 40258, "end": 40370, "idx": 229}, {"begin": 40371, "end": 40565, "idx": 230}, {"begin": 40566, "end": 40662, "idx": 231}, {"begin": 40663, "end": 40862, "idx": 232}, {"begin": 40863, "end": 40959, "idx": 233}, {"begin": 40960, "end": 41114, "idx": 234}, {"begin": 41115, "end": 41247, "idx": 235}, {"begin": 41248, "end": 41361, "idx": 236}, {"begin": 41362, "end": 41429, "idx": 237}, {"begin": 41430, "end": 41556, "idx": 238}, {"begin": 41557, "end": 41694, "idx": 239}, {"begin": 41695, "end": 41779, "idx": 240}, {"begin": 41780, "end": 42009, "idx": 241}, {"begin": 42010, "end": 42205, "idx": 242}, {"begin": 42206, "end": 42341, "idx": 243}, {"begin": 42342, "end": 42457, "idx": 244}, {"begin": 42458, "end": 42538, "idx": 245}, {"begin": 42539, "end": 42616, "idx": 246}, {"begin": 42617, "end": 42718, "idx": 247}, {"begin": 42719, "end": 42938, "idx": 248}, {"begin": 42939, "end": 43113, "idx": 249}, {"begin": 43114, "end": 43260, "idx": 250}, {"begin": 43261, "end": 43336, "idx": 251}, {"begin": 43337, "end": 43519, "idx": 252}, {"begin": 43520, "end": 43915, "idx": 253}, {"begin": 43916, "end": 44120, "idx": 254}, {"begin": 44121, "end": 44283, "idx": 255}, {"begin": 44284, "end": 44456, "idx": 256}, {"begin": 44457, "end": 44591, "idx": 257}, {"begin": 44592, "end": 44859, "idx": 258}, {"begin": 44860, "end": 45054, "idx": 259}, {"begin": 45088, "end": 45349, "idx": 260}, {"begin": 45350, "end": 45578, "idx": 261}, {"begin": 45579, "end": 45849, "idx": 262}, {"begin": 45850, "end": 46012, "idx": 263}, {"begin": 46013, "end": 46231, "idx": 264}, {"begin": 46232, "end": 46394, "idx": 265}, {"begin": 46395, "end": 46515, "idx": 266}, {"begin": 46516, "end": 46702, "idx": 267}, {"begin": 46703, "end": 46967, "idx": 268}, {"begin": 46968, "end": 47095, "idx": 269}, {"begin": 47096, "end": 47266, "idx": 270}, {"begin": 47267, "end": 47352, "idx": 271}, {"begin": 47353, "end": 47623, "idx": 272}, {"begin": 47624, "end": 47724, "idx": 273}, {"begin": 47725, "end": 48086, "idx": 274}, {"begin": 48087, "end": 48394, "idx": 275}, {"begin": 48395, "end": 48651, "idx": 276}, {"begin": 48652, "end": 48879, "idx": 277}, {"begin": 48880, "end": 49070, "idx": 278}, {"begin": 49071, "end": 49291, "idx": 279}, {"begin": 49292, "end": 49451, "idx": 280}, {"begin": 49452, "end": 49568, "idx": 281}, {"begin": 49569, "end": 49747, "idx": 282}, {"begin": 49748, "end": 49945, "idx": 283}, {"begin": 49946, "end": 50034, "idx": 284}, {"begin": 50035, "end": 50160, "idx": 285}, {"begin": 50161, "end": 50290, "idx": 286}, {"begin": 50291, "end": 50406, "idx": 287}, {"begin": 50407, "end": 50494, "idx": 288}, {"begin": 50495, "end": 50698, "idx": 289}, {"begin": 50699, "end": 50879, "idx": 290}, {"begin": 50880, "end": 51035, "idx": 291}, {"begin": 51036, "end": 51186, "idx": 292}, {"begin": 51187, "end": 51390, "idx": 293}, {"begin": 51391, "end": 51643, "idx": 294}, {"begin": 51644, "end": 51881, "idx": 295}, {"begin": 51882, "end": 52008, "idx": 296}, {"begin": 52009, "end": 52279, "idx": 297}, {"begin": 52280, "end": 52443, "idx": 298}, {"begin": 52444, "end": 52850, "idx": 299}, {"begin": 52851, "end": 52972, "idx": 300}, {"begin": 52973, "end": 53156, "idx": 301}, {"begin": 53157, "end": 53336, "idx": 302}, {"begin": 53372, "end": 53529, "idx": 303}, {"begin": 53530, "end": 53722, "idx": 304}, {"begin": 53723, "end": 53925, "idx": 305}, {"begin": 53926, "end": 54021, "idx": 306}, {"begin": 54022, "end": 54171, "idx": 307}, {"begin": 54172, "end": 54311, "idx": 308}, {"begin": 54312, "end": 54425, "idx": 309}, {"begin": 54426, "end": 54574, "idx": 310}, {"begin": 54575, "end": 54701, "idx": 311}, {"begin": 54702, "end": 54874, "idx": 312}, {"begin": 54875, "end": 54969, "idx": 313}, {"begin": 54970, "end": 55332, "idx": 314}, {"begin": 55333, "end": 55465, "idx": 315}, {"begin": 55466, "end": 55560, "idx": 316}, {"begin": 55561, "end": 55630, "idx": 317}, {"begin": 55631, "end": 55791, "idx": 318}, {"begin": 55792, "end": 56128, "idx": 319}, {"begin": 56129, "end": 56406, "idx": 320}, {"begin": 56407, "end": 56569, "idx": 321}, {"begin": 56644, "end": 56888, "idx": 322}, {"begin": 56889, "end": 57094, "idx": 323}, {"begin": 57095, "end": 57176, "idx": 324}, {"begin": 57177, "end": 57387, "idx": 325}, {"begin": 57388, "end": 57504, "idx": 326}, {"begin": 57505, "end": 57632, "idx": 327}, {"begin": 57633, "end": 57792, "idx": 328}, {"begin": 57793, "end": 57851, "idx": 329}, {"begin": 57852, "end": 57994, "idx": 330}, {"begin": 57995, "end": 58207, "idx": 331}, {"begin": 58208, "end": 58220, "idx": 332}, {"begin": 58221, "end": 58371, "idx": 333}, {"begin": 58372, "end": 58543, "idx": 334}, {"begin": 58544, "end": 58735, "idx": 335}, {"begin": 58736, "end": 58877, "idx": 336}, {"begin": 58878, "end": 59108, "idx": 337}, {"begin": 59109, "end": 59293, "idx": 338}, {"begin": 59294, "end": 59368, "idx": 339}, {"begin": 59369, "end": 59553, "idx": 340}, {"begin": 59554, "end": 59758, "idx": 341}, {"begin": 59759, "end": 59921, "idx": 342}, {"begin": 59922, "end": 60001, "idx": 343}, {"begin": 60002, "end": 60078, "idx": 344}, {"begin": 60079, "end": 60259, "idx": 345}, {"begin": 60260, "end": 60374, "idx": 346}, {"begin": 60375, "end": 60567, "idx": 347}, {"begin": 60568, "end": 60660, "idx": 348}, {"begin": 60661, "end": 60810, "idx": 349}, {"begin": 60811, "end": 60998, "idx": 350}, {"begin": 60999, "end": 61182, "idx": 351}, {"begin": 61183, "end": 61425, "idx": 352}, {"begin": 61426, "end": 61538, "idx": 353}, {"begin": 61555, "end": 61770, "idx": 354}, {"begin": 61771, "end": 61940, "idx": 355}, {"begin": 61941, "end": 62170, "idx": 356}, {"begin": 62171, "end": 62467, "idx": 357}, {"begin": 62468, "end": 62595, "idx": 358}, {"begin": 62596, "end": 62856, "idx": 359}, {"begin": 62857, "end": 63139, "idx": 360}], "ReferenceToFigure": [{"begin": 24510, "end": 24511, "target": "#fig_0", "idx": 0}, {"begin": 31492, "end": 31493, "target": "#fig_1", "idx": 1}, {"begin": 34904, "end": 34905, "target": "#fig_3", "idx": 2}, {"begin": 36111, "end": 36112, "target": "#fig_4", "idx": 3}, {"begin": 40389, "end": 40390, "target": "#fig_5", "idx": 4}, {"begin": 40684, "end": 40685, "target": "#fig_5", "idx": 5}, {"begin": 42993, "end": 42994, "target": "#fig_6", "idx": 6}, {"begin": 44175, "end": 44176, "target": "#fig_7", "idx": 7}, {"begin": 44856, "end": 44857, "target": "#fig_7", "idx": 8}, {"begin": 45586, "end": 45587, "target": "#fig_8", "idx": 9}, {"begin": 48973, "end": 48974, "target": "#fig_3", "idx": 10}, {"begin": 50166, "end": 50167, "target": "#fig_9", "idx": 11}, {"begin": 55926, "end": 55927, "target": "#fig_6", "idx": 12}, {"begin": 59129, "end": 59131, "target": "#fig_10", "idx": 13}], "Abstract": [{"begin": 82, "end": 1928, "idx": 0}], "SectionFootnote": [{"begin": 63141, "end": 63673, "idx": 0}], "Footnote": [{"begin": 63152, "end": 63217, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 63218, "end": 63267, "id": "foot_1", "n": "3", "idx": 1}, {"begin": 63268, "end": 63325, "id": "foot_2", "n": "4", "idx": 2}, {"begin": 63326, "end": 63397, "id": "foot_3", "n": "5", "idx": 3}, {"begin": 63398, "end": 63482, "id": "foot_4", "n": "6", "idx": 4}, {"begin": 63483, "end": 63673, "id": "foot_5", "n": "7", "idx": 5}]}}