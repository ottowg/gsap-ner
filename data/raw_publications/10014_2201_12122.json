{"text": "Can Wikipedia Help Offline Reinforcement Learning?\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains. 1\n\nMain:\n\n\n\n1. Introduction\nLarge pre-trained language models have shown impressive performance in natural language (Devlin et al., 2019; Radford et al., 2018) and vision (Dosovitskiy et al., 2021) tasks. Furthermore, Transformer-based autoregressive language models (Vaswani et al., 2017; Baevski & Auli, 2019; Radford et al., 2019) have shown to be powerful sources of zeroshot and few-shot performance (Brown et al., 2020), with notable rapid adaptation in low resource settings, demonstrating their easy adaptability and transferability to a number of tasks in their respective domains. Adapting autoregressive language models has also been extended to the multimodal setting (Tsimpoukelli et al., 2021) for tasks such as visual question answering.\nConcurrently, offline reinforcement learning (RL) has been seen as analogous to sequence modeling (Chen et al., 2021; Janner et al., 2021; Furuta et al., 2021), framed as simply supervised learning to fit return-augmented trajectories in an offline dataset. This relaxation, doing away with many of the complexities commonly associated with reinforcement learning (Watkins & Dayan, 1992; Kakade, 2001), allows us to take advantage of techniques popularized in sequence modeling tasks for RL.\nPre-training, particularly, is an essential technique for alleviating higher compute costs from using more expressive models such as Transformers. However, such concept is still relatively fresh in RL (Singh et al., 2020; Tirumala et al., 2020), due to the difficulty in parameterizing different scenes and tasks through a single network (Wang et al., 2018b; Jiang et al., 2019; Zeng et al., 2020) as well as the lack of large off-the-shelf datasets for pre-training (Cobbe et al., 2020; Zhu et al., 2020; Yu et al., 2020). Adopting pre-training as a default option for recent Transformer-based methods (Chen et al., 2021; Janner et al., 2021; Furuta et al., 2021) appears far away -if we only look within RL.\nUnified under the umbrella of sequence modeling, we look at whether Transformer-based pre-trained language models are able to be adapted to standard offline reinforcement learning tasks that have no relations to language. Given the setting of having a single model pre-trained on natural language to finetune on each offline RL task individually, we demonstrate drastic improvements in convergence speeds and final policy performances. We also consider further techniques (e.g. extension of positional embeddings, embedding similarity encouragement) in order to better take advantage of the features learned by the pre-trained language model and demonstrate greater improvements. We demonstrate that pre-training on autoregressively modeling natural language provides consistent performance gains when compared to the Decision Transformer (Chen et al., 2021) on both the popular OpenAI Gym (Brockman et al., 2016) and Atari (Bellemare et al., 2013) offline RL benchmarks. We also note a significantly faster convergence speed, with a 3-6x improvement over a vanilla Decision Transformer turning hours of training to tens of minutes, indicating long-term computational efficiency benefits on language pre-training.\nOur findings allude to the potential impact of large scale pre-training for reinforcement learning, given its surprising efficacy when transferring from a distant sequence modeling domain such as natural language. Notably, unlike other work on multi-task offline RL, our model provides consistent results in terms of both reward and convergence regardless of environment and setting, indicating a forseeable future where everyone should use a pre-trained language model for offline RL.\n\n2. Background\nOffline Reinforcement Learning We consider a standard Markov Decision Process (MDP) with state space s \u2208 S and action space a \u2208 A, specified by a initial state distribution p(s 1 ), a dynamics distribution p(s t+1 |s t , a t ), and a scalar reward function r(s, a). The goal of reinforcement learning (RL) is to find the optimal policy \u03c0 * (a|s) which maximizes the \u03b3-discounted expected return as the agent interacts in the environment,max \u03c0 E s1:\u221e,a1:\u221e\u223cp,\u03c0 \u221e t=1 \u03b3 t r(s t , a t )\nIn offline RL, the objective remains the same, but has to be optimized with no interactive data collection on a fixed set of trajectories \u03c4 i , each of the form below with horizon N ,\u03c4 = (r 1 , s 1 , a 1 , r 2 , s 2 , a 2 , . . . , r N , s N , a N ).\nCommon approaches include value-based or model-based objectives with regularization (Fujimoto et al., 2019; Levine et al., 2020), and more recently, direct generative modeling of these trajectories conditioned on hindsight returns (Chen et al., 2021; Janner et al., 2021; Furuta et al., 2021).\nTransformer model In this subsection, we briefly review the Transformer architecture (Vaswani et al., 2017) used to model sequences. The Transformer is comprised of stacks of identical Transformer layers. Each of these layers takes in a set of n-dimensional vectors that are fed through the two main building blocks: a multi-head self-attention sublayer and a feedfoward MLP as shown below:Attention(x) = softmax Q(x)K(x) \u221a n V (x) (3) Feedforward(x) = L 2 (g(L 1 (x)))\nwhere Q, K and V represent linear projections that parameterize the projection of input x into the query, key and value spaces; while L 1 , L 2 and g represent the first linear projection, second linear projection, and activation function that comprise the feedforward MLP. This is followed by a residual connection (He et al., 2015) and layer normalization (Ba et al., 2016).\nAutoregressive Language Model Pre-training Although there are now multiple techniques for language model pre-training (e.g. masked language modeling; Devlin et al., 2019), we will review autoregressive language modeling given its correspondence with the sequence modeling objective we employ for our offline reinforcement learning tasks.\nGiven a sequence x = [x 1 , x 2 , . . . x N ] comprised of tokens x i , we look to model the likelihood of the sequence P (x) by way of modeling the probability of predicting each token x i in a step-by-step, or autoregressive, fashion (commonly left-to-right). Naturally, it follows that each tokens prediction will be conditioned on all the previous elements in the sequence x <i as shown below (Bengio et al., 2001) :P (x) = N i=1 p(x i |x i\u22121 , x i\u22122 , . . . , x 1 )\n\n3. Methodology\nIn this section we discuss our proposed methodology and techniques to better adapt pre-trained language models to model trajectories, as in the case of offline RL tasks with minimal modification to architecture and objectives shown in Figure .\n\n3.1. Modeling\nFollowing Chen et al. (2021), we model trajectories autoregressively by representing them in the following manner:t = ( R1 , s 1 , a 1 , R2 , s 2 , a 2 , . . . , RN , s N , a N ) (6)\nwhere trajectory t is modeled analogously to sequence x as shown in in Equation 5, and Ri = N t=i r t , s i , a i represent the returns-to-go, state and action for each timestep i given N timesteps, respectively.\n\n3.2. Techniques\nEncouraging similarity between language representations and offline RL input representations We find the issue of lack of alignment between state, action and reward input representations and language representationspartially holding back further extraction of the capabilities of the language model. To this end, we use a similaritybased objective in order to maximize the similarity between the set of language embeddings E = [E 1 , . . . , E V ] with vocabulary size V and the set of input representations I = I 1 , . . . , I 3N . The input representations are parameterized by linear projections L r , L a , L s corresponding to the target reward projection, action projection and state projection, respectively.\nGiven the following cosine similarity function:C(z 1 , z 2 ) = z 1 z 1 2 \u2022 z 2 z 2 2 (7)\nwe compute the negative (as we use gradient descent to optimize this objective) of the sum of the maximum similarity value for each embedding E 1 , . . . , E j , . . . , E V and each input representation I 0 , . . . , I i , . . . , I N as follows: 2L cos = \u2212 3N i=0 max j C(I i , E j )\nThis allows us to encourage the input embeddings to become more similar to their language counterparts. However, due to computational cost of computing this loss for large values of V , we propose to use K-means clustering over the embeddings to reduce the size of V to number of clusters K. We then treat the cluster centers akin to the original embeddings in order to compute our loss. Furthermore, we optimize this computation with vectorization.\n\nLanguage model co-training\nWe also experiment with continuing to train jointly on language modeling and trajectory modeling. This allows us to encouraging the model's transformer backbone to be able to handle both language and trajectories simultaneously.\n\n3.3. Final Objective\nWe now combine the objectives into the final objective below:L = L MSE + \u03bb 1 L cos + \u03bb 2 L LM (9)\nwhere L MSE represents the mean squared error loss used for the primary trajectory modeling objective, L LM represents the negative log likelihood-based language modeling objective, and \u03bb 1 , \u03bb 2 represent hyperparameters to control the weight of the cosine similarity loss and language modeling loss, respectively.\n\n4. Experiments\n\n\n4.1. Models\nPre-trained Models We use the popular GPT2-small model to benchmark the impact of language-only pretraining. For direct comparison with the Decision Transformer (Chen et al., 2021), we also pre-train a language model with the same parameter count on the popular language modeling Wikitext-103 dataset (Merity et al., 2016), consisting of over 100 million tokens from full Wikipedia articles. We refer to this model as ChibiT. 5\nTo explore the effect of pre-training on vision datasets, we also study CLIP (Radford et al., 2021) and ImageGPT (Chen et al., 2020). CLIP is comprised of an image encoder and a text encoder, and trained to predict which caption matches with which image. While the text encoder is an autoregressive Transformer, the image encoder is a Vision Transformer, which is not autoregressive. Therefore, for the autoregressive setup of offline reinforcement learning, we use the pre-trained text encoder as our initializer, while discarding the image encoder part. ImageGPT is based on the same Transformer architecture as GPT2, but instead of language, it is trained on images unrolled into long sequences of pixels in an autoregressive manner.\n\nRL Baselines\nIn addition to benchmarking our pre-trained language models, we compare to popular state-of-the-art offline RL algorithms as follows: Decision Transformer (DT) (Chen et al., 2021), CQL (Kumar et al., 2020), TD3+BC (Fujimoto & Gu, 2021), BRAC (Wu et al., 2019), and AWR baselines (Peng et al., 2019).\nHyperparameters We use the following hyperparameters for our language model pre-training: the architecture is the same as that of Chen et al. ( 2021) (128 model dim, 1 attention head, 3 layers), learning rate of 3e-4, a batch size 65536 tokens, for 6 hours (80000 steps), using a warmup schedule over the first 10000. We the same byte-pair encoding (BPE; Sennrich et al., 2016; Kudo & Richardson, 2018) as that used by GPT-2 (Radford et al., 2019). For our offline RL tasks, we follow the hyperparameters used by (Chen et al., 2021). For our additional objectives, we decay \u03bb 1 , \u03bb 2 , to reach 0.0 each after 5000 steps. We tune initial values of \u03bb 1 for values of {0.1, 0.2} and \u03bb 2 for values of {0.0, 0.2, 0.4}. We include additional details in the appendix.\nWe benchmark our models against the D4RL offline RL benchmark datasets (Fu et al., 2020) for the OpenAI Gym MuJoCo (Brockman et al., 2016) and Atari (Bellemare et al., 2013) tasks.\n\n4.2. Atari\nWe run our ChibiT and GPT2 models on the challenging Atari dataset (Bellemare et al., 2013). We use the four Atari tasks evaluated in Agarwal et al. (2020) We show results in Table 1. It can be seen that ChibiT and GPT2 results consistently improve over/match a strong vanilla Decision Transformer baseline. Our models are competitive with the Decision Transformer on all four games and competitive with CQL on 3/4 games.\n\n4.3. Gym\nIn this section, we consider results on the OpenAI Gym tasks (HalfCheetah, Walker2d, and Hopper) from the D4RL benchmark (Fu et al., 2020).\nWe train our models for a total of 100k timesteps and evaluate every 5000 timesteps, with each evaluation consisting of 10 episodes. Note that we perform early stopping. Base- 2. Pretraining improves the Decision Transformer by large margins in an overwhelming majority of tasks, clearly demonstrating that language pre-training improves over random initialization using sequence modeling techniques in terms of reward. We also take note of the minimal difference between ChibiT, CLIP, and GPT2, showing that that at this scale, improvements on offline RL are not necessarily strongly correlated with model size as has been shown on both large-scale vision and language tasks. We note that CLIP, while improving over a vanilla DT model, is often slightly less competitive that our pure language modeling objectives. Our ChibiT and GPT2 models achieve and average performance of 78.3 and 80.1, respectively, showing strong competitiveness on all settings with all baselines. These pre-trained language models acheive state-of-the-art results by outperforming the strong Decision Transformer and TD3+BC baselines by a significant 3.0-5.4 points.\n\n5. Analysis\nIn this section, we look at more fine-grained details and properties of various aspects of adapting pre-trained language models to offline RL tasks with ablations on OpenAI Gym.\n\n5.1. Convergence Speed\nWe evaluate time-to-convergence of GPT2, ChibiT and DT using the our implementations of the former two and the author-provided implementation of the latter. Results are\n\nModel\nWalker2d HalfCheetah Hopper DT (GitHub) 3h14m 3h23m 2h47m ChibiT (ours) 43m 48m 36m GPT2 (ours) 1h27m 1h32m 1h2m\nTable 3. Training time comparison (measured in hours and minutes on a single V100 GPU on the medium-expert setting) between the Decision Transformer and two pre-trained models: ChibiT and GPT2 on OpenAI gym tasks. Note that GPT2 is 144x larger than the other models with 84M model parameters.\nreported in Table 3. We find that pre-training on language allows us to speed up the training process of Transformerbased offline RL models, measured in wall-clock time. Convergence is defined as the point where average performance attains a score within 2 (normalized score) of the best score. Interestingly, we also find that GPT2, despite its larger model size at 84M model parameters, still manages to train faster than DT. This points towards potential benefits of pretraining at scale and increased efficiency during finetuning.\nWe run experiments on a single NVIDIA V100 16GB GPU and an Intel Xeon Gold 6148 Processor.\n\n5.2. Language initialization versus vision initialization\nAs we establish that Transformers pre-trained on language data are surprisingly effective for accelerating training convergence time on offline reinforcement learning tasks, it is tempting to ask if this phenomenon is inherent to language pre-training or does it extend to vision pre-training as well. To answer this question, we compare two GPT models, ImageGPT-small (iGPT) and GPT2-small (GPT2), pre-trained on language and vision data, respectively. Since Transformer architectures are domain-agnostic, these models can be trained on 1D sequences of any form. Hence, we can compare GPT2, which was pre-trained on many sequences of discrete language tokens, and iGPT, which was pre-trained on autoregressive image generation at the pixel level (note that both models were trained on \u223c 10 10 tokens). Given the results in Table 2 for iGPT, we found that the model had extremely low returns, and did not reach convergence. Notably, on some seeds, the model even performed worse than a random score after training on Walker medium, with a normalized score of \u22120.1, in contrast with GPT-2 pre-training which gives us an average increase of 5.1 points (measured in terms of normalized reward) over the Decision Transformer.\nFurthermore, when we turn our attention to the difference between GPT2 and CLIP, we see that GPT2, which is based on pure-language based pre-training, performs better. While the text encoder of CLIP is also an autoregressive Transformer pre-trained on text data, the objective of CLIP is different from GPT2 in that the former attempts to match the text description with their corresponding image, while the latter is pre-trained on pure autoregressive language modeling. Given this, we hypothesize that generative (versus discriminative) training objective is more useful for transfer to a generative task.\nWe believe that this alludes to underlying similarities between language modeling and trajectory modeling, whereas a large difference between image modeling and trajectory modeling. Perhaps this can be attributed to the \"natural\" sequential nature of language and trajectories, versus the forced 2D\u21921D nature that was used to pre-train iGPT.\nAttention Analysis To further understand the discrepancy between language-based and vision-based pre-training, we visualize attention weights, extracted from GPT2 and iGPT after fine-tuning on Hopper medium, as an example offline RL task. As a reference, we also extract attention weights from randomly initialized networks of Decision Transformers. In Figure 4.2, we plot the attention weights averaged over all attention heads in each model, and present the visualizations for early, middle, and last layers, respectively. Due to the autoregressive nature of our task, attention weights in the upper right triangle are masked out, so that the model can only attend to past sequences.\nAs a general trend, we see that in earlier layers GPT2 and the randomly initialized model tend to attend to positions with multiples of 3 timesteps behind the current position. This indicates that actions attend to previous actions, states attend to previous states, and returns-to-go attend to previous returns-to-go. Constrasted with this, iGPT's attention is less interpretable, however showing a notably stronger recency bias. In the middle layers, DT continues the trends of its early layers, whereas iGPT tends to fixate on a single state (given the overwhelming brightness of timestep 2), GPT2 starts showing a stronger preference for previous returns to go (given that lighter colors are consistently timestep 1, 4, etc...). Finally, in the models' last layer, while iGPT and random initialization tend to exhibit a behaviour closer to mean pooling over all previous inputs, GPT's final prediction seems to be heavily reliant on the initial returns-to-go. This perhaps indicates that goal conditioning is stronger in GPT2.\n\n5.3. How important is the model size of Transformer?\nWe explore how pre-training changes the impact on model size for these offline RL tasks. We train randomly initialized models with various parameter counts (approx. 600K, 3M, 18M, 84M) as well as language-pre-trained models on WikiText-103 with the same parameter counts We visualize the average (over Hopper, Walker2d, and HalfCheetah) of Medium-Expert results in Figure 3. Unsurprisingly, we observe that a randomly initialized Decision Transformer, tends to have lower relative returns as parameter sizes increase likely due to overfitting on finite data. Interestingly, however, pre-trained language models tend to increase performance as parameter count increases, despite diminishing returns with increasing parameter count. Nonetheless, this is exciting as it demonstrates that even language pre-training may be beneficial at scale, especially for larger and more diverse offline RL datasets in the future.  4. It can be seen that additional context does not seem to help even when pretraining on long range language modeling, perhaps alluding to the limited utility of long-range context for the OpenAI Gym tasks.\n\n5.5. Can we freeze model parameters?\nWe also look at how ChibiT performs when model weights (transformer blocks: self-attention and feedforward) are frozen with only action, state and return projections L a , L s , L r being trained. Previous work (Tsimpoukelli et al., 2021; Lu et al., 2021) has demonstrated how frozen language models have the capability to extend to the vision domain with respectable performance, which we aim to test with this experiment. We show results on Table 5 on the D4RL medium setting in OpenAI Gym. When freezing model weights, performance is underwhelming with performance drastically reducing as much as \u223c40%. We conjecture this is due to our tasks being complex generative modeling as opposed to discriminative classification (Lu et al., 2021), where the output distribution is of a higher dimension -hence the need for more intensive finetuning.\n\n5.6. Ablation of proposed techniques\nWe perform an ablation study of our proposed auxiliary techniques and compare the impact of including and not including pre-trained positional embeddings. Results are shown in Table 6. It can be seen that the combination of our objectives are able to increase performance consistently. We also note that the removal of pre-trained positional embeddings results in the largest average decrease in performance over ChibiT, alluding to the fact that this positional information is important and transferable to offline RL. (Vaswani et al., 2017) was initially proposed by Radford et al. (2018) with their Generative Pretrained Transformer (GPT). They performed autoregressive language modeling on a relatively large dataset, showing promising initial success not only on its ability to scale to large models sizes, but also for its impressive performance when fine-tuning on task-specific natural language understanding (NLU; Wang et al., 2018a) datasets. BERT (Devlin et al., 2019), extended this pre-train\u2192finetune paradigm with their masked language modeling objective for pretraining which allowed the model to take advantage of its bidirectional attention capabilities for NLU tasks. Furthermore, recently this paradigm has extended to computer vision with the Vision Transformer (ViT; Dosovitskiy et al., 2021). SwinTransformer (Liu et al., 2021) extends ViT by introducing hierarchical multi-resolution feature maps. By pre-training SwinTransformer on ImageNet-22k, and finetuning on downstream tasks such as detection and semantic segmentation, SwinTransfomer outperform previous state-of-the-arts models based on Convolutional Neural Networks (CNN) (Su et al., 2020).\nSequence Modeling for Offline RL Offline RL became popular starting from a simple observation that many performant off-policy algorithms (Mnih et al., 2015; Lillicrap et al., 2015; Gu et al., 2016; Haarnoja et al., 2018; Fujimoto et al., 2018) fail to learn in a fully off-policy, i.e. offline, batch setting (Fujimoto et al., 2019). Numerous algorithmic work ensued (Wu et al., 2019; Jaques et al., 2020; Ghasemipour et al., 2021; Kumar et al., 2020; Fujimoto & Gu, 2021) with various applications (Jaques et al., 2020; Chebotar et al., 2021). Building on reward-conditioned imitation learning (Srivastava et al., 2019; Kumar et al., 2019), Transformer architecture has been recently adopted for replacing offline RL with sequence modeling (Chen et al., 2021; Janner et al., 2021; Furuta et al., 2021). Despite initial successes, many techniques popular in language modeling have yet to be experimented in these offline RL benchmarks, and our work constitutes an initial step toward bridging the two communities.\nPre-training for RL Contrary to language or vision (Devlin et al., 2019; Dosovitskiy et al., 2021), major successes in deep RL have largely focused on isolated tasks or domains (Mnih et al., 2015; Silver et al., 2016; Gu et al., 2017; Kalashnikov et al., 2018; Vinyals et al., 2019). Pretraining results are often limited to vision or language processing (Yen- Chen et al., 2020; Lynch & Sermanet, 2021) or specially-crafted domains (Singh et al., 2020; Tirumala et al., 2020). Arguably, a fundamental bottleneck for pretraining in RL is the difficulty in reusing a single network across vastly different tasks, of distinct observation spaces, action spaces, rewards, scenes, and agent morphologies. Preliminary work explored various aspects of this problem through graph neural networks for morphology generalization (Wang et al., 2018b; Pathak et al., 2019; Chen et al., 2018; Kurin et al., 2020), language for universal reward specification (Jiang et al., 2019; Lynch & Sermanet, 2021; Shridhar et al., 2022), and object-centric action spaces (Zeng et al., 2020; Shridhar et al., 2022; Noguchi et al., 2021). Our work is orthogonal to these as we essentially amortize RL algorithm itself, expressed as sequence modeling with Transformer, instead of specific RL domain information, and can be combined with domain-specific pre-training techniques (Yen- Chen et al., 2020; Lynch & Sermanet, 2021) effortlessly.\nAdapting language models to new modalities and domains Within language modeling recently there has been interest in domain adaptation of pre-trained language models (Gururangan et al., 2020), where it has been shown that continued modeling on a domain-specific datasets tends to lead to greater performance on domain-related downstream tasks. Furthermore, Tsimpoukelli et al. (2021) looked at adapting frozen autoregressive language models for fewshot question answering by adding an auxiliary vision encoder. More related to our work is that of Lu et al. (2021), where they look at adapting frozen language models to various tasks such as image classification. Our work extends on the spirit of these works by adapting language models to a new domain of RL, however, as far was we know, we are the first to propose leveraging a generative model (in language) for generation in another domain (RL) as opposed to a discriminatory task such as classification.\n\n7. Conclusion\nWe investigate how pre-trained models can improve generic offline RL problems, recently casted as sequence modeling. To our surprise, we discover that fine-tuning from a Wikipedia-trained small transformer (ChibiT) or a GPT2 model outperforms the basic Decision Transformer (DT) and other RL-based offline baselines by a large margin in terms of policy performance and convergence, establishing state-of-the-art scores on the competitive D4RL benchmark in both Gym and Atari and cutting down the DT training time by 3-6x. We perform extensive ablation studies and analyses, and found how language pre-training (as opposed to vision pre-training), model size, and fine-tuning (as opposed to freezing parameters) play critical roles in the final performances. We hope our work can accelerate the adoption of pre-training in RL and leads to more interest in applying other sequence modeling techniques from language and vision into RL.\nBeyond RL, our work constitutes the first successful transfer, to the best of our knowledge, of a pre-trained generative model in one domain (language) to a generative modeling task in a completely different domain (RL on continuous control and games). This hints at some underlying universal structure across sequence modeling domains, and could perhaps lead to unified generative modeling pre-training for better transferability among them. In future work, we look to investigate in more depth which properties of language structure are useful for reinforcement learning and sequence modeling in other domains, and whether previous work studying language structure (Hupkes et al., 2019) does indeed relate to compositional generalization of neural networks.\n\nFootnotes:\n2: We looked at using mean pooling instead of max pooling for this objective and found that models with the mean pooling objective did not converge.5 \"Chibi\" means \"small\" or \"mini\" in Japanese.\n6: Note that when pre-training language models with 600K, 3M, and 18M parameters, we control that our pre-training takes exactly 6 hours on 4 V100 GPUs.\n\nReferences:\n\n- Agarwal, R., Schuurmans, D., and Norouzi, M. An op- timistic perspective on offline reinforcement learning, 2020.- Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 29304- 29320, 2021. URL https://proceedings. neurips.cc/paper/2021/hash/ f514cec81cb148559cf475e7426eed5e-Abstract. html.\n\n- Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv: Arxiv-1607.06450, 2016.\n\n- Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https:// openreview.net/forum?id=ByxZX20qFQ.\n\n- Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artificial Intelligence Research, 47:253-279, jun 2013.\n\n- Bengio, Y., Ducharme, R., and Vincent, P. A neural probabilistic language model. In Leen, T., Diet- terich, T., and Tresp, V. (eds.), Advances in Neural Information Processing Systems, volume 13. MIT Press, 2001. URL https://proceedings. neurips.cc/paper/2000/file/ 728f206c2a01bf572b5940d7d9a8fa4c-Paper. pdf. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym, 2016.\n\n- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020.\n\n- Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., et al. Actionable models: Unsupervised offline re- inforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n\n- Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De- cision transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv: Arxiv-2106.01345, 2021.\n\n- Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1691-1703. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/ v119/chen20s.html.\n\n- Chen, T., Murali, A., and Gupta, A. Hardware conditioned policies for multi-robot transfer learning. arXiv preprint arXiv:1811.09864, 2018.\n\n- Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Lever- aging procedural generation to benchmark reinforcement learning. In International conference on machine learn- ing, pp. 2048-2056. PMLR, 2020.\n\n- Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding, 2019.\n\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv: Arxiv-2004.07219, 2020.\n\n- Fujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. arXiv preprint arXiv:2106.06860, 2021.\n\n- Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.\n\n- Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In Interna- tional Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.\n\n- Furuta, H., Matsuo, Y., and Gu, S. S. Generalized decision transformer for offline hindsight information matching. arXiv preprint arXiv:2111.10364, 2021.\n\n- Ghasemipour, S. K. S., Schuurmans, D., and Gu, S. S. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In International Conference on Machine Learning, pp. 3682-3691. PMLR, 2021.\n\n- Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. Continu- ous deep q-learning with model-based acceleration. In International conference on machine learning, pp. 2829- 2838. PMLR, 2016.\n\n- Gu, S., Holly, E., Lillicrap, T., and Levine, S. Deep rein- forcement learning for robotic manipulation with asyn- chronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3389- 3396. IEEE, 2017.\n\n- Gururangan, S., Marasovi\u0107, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don't stop pretraining: Adapt language models to domains and tasks, 2020.\n\n- Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. In International conference on machine learning, pp. 1861-1870. PMLR, 2018.\n\n- Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mastering atari with discrete world models, 2021.\n\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. arXiv preprint arXiv: Arxiv-1512.03385, 2015.\n\n- Hupkes, D., Dankers, V., Mul, M., and Bruni, E. Compo- sitionality decomposed: how do neural networks gener- alise? arXiv preprint arXiv: Arxiv-1908.08351, 2019.\n\n- Janner, M., Li, Q., and Levine, S. Reinforcement learning as one big sequence modeling problem. arXiv preprint arXiv:2106.02039, 2021.\n\n- Jaques, N., Shen, J. H., Ghandeharioun, A., Ferguson, C., Lapedriza, A., Jones, N., Gu, S. S., and Picard, R. Human- centric dialog training via offline reinforcement learning. arXiv preprint arXiv:2010.05848, 2020.\n\n- Jiang, Y., Gu, S., Murphy, K., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. arXiv preprint arXiv:1906.07343, 2019.\n\n- Kakade, S. M. A natural policy gradient. Advances in neural information processing systems, 14, 2001.\n\n- Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.\n\n- Kudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018.\n\n- Kumar, A., Peng, X. B., and Levine, S. Reward-conditioned policies. arXiv preprint arXiv:1912.13465, 2019.\n\n- Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conserva- tive q-learning for offline reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\n\n- Kurin, V., Igl, M., Rockt\u00e4schel, T., Boehmer, W., and White- son, S. My body is a cage: the role of morphology in graph-based incompatible control. arXiv preprint arXiv:2010.01856, 2020.\n\n- Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline rein- forcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\n- Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\n- Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.\n\n- Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Pretrained transformers as universal computation engines, 2021.\n\n- Lynch, C. and Sermanet, P. Language conditioned imitation learning over unstructured data. Proceedings of Robotics: Science and Systems. doi, 10, 2021.\n\n- Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016.\n\n- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540): 529-533, 2015.\n\n- Noguchi, Y., Matsushima, T., Matsuo, Y., and Gu, S. S. Tool as embodiment for recursive manipulation. arXiv preprint arXiv:2112.00359, 2021.\n\n- Pathak, D., Lu, C., Darrell, T., Isola, P., and Efros, A. A. Learning to control self-assembling morphologies: a study of generalization via modularity. arXiv preprint arXiv:1902.05546, 2019.\n\n- Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv: Arxiv-1910.00177, 2019.\n\n- Radford, A., Narasimhan, K., Salimans, T., and Suskeveter, I. Improving language understanding by generative pre- training. 2018.\n\n- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.\n\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\n- Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Pro- ceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016. Associ- ation for Computational Linguistics. doi: 10.18653/v1/ P16-1162. URL https://aclanthology.org/ P16-1162.\n\n- Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.\n\n- Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.\n\n- Singh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., and Levine, S. Parrot: Data-driven behavioral priors for re- inforcement learning. arXiv preprint arXiv:2011.10024, 2020.\n\n- Srivastava, R. K., Shyam, P., Mutz, F., Ja\u015bkowski, W., and Schmidhuber, J. Training agents using upside-down re- inforcement learning. arXiv preprint arXiv:1912.02877, 2019.\n\n- Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J. Vl-bert: Pre-training of generic visual-linguistic represen- tations, 2020.\n\n- Tirumala, D., Galashov, A., Noh, H., Hasenclever, L., Pas- canu, R., Schwarz, J., Desjardins, G., Czarnecki, W. M., Ahuja, A., Teh, Y. W., et al. Behavior priors for efficient reinforcement learning. arXiv preprint arXiv:2010.14274, 2020.\n\n- Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models, 2021.\n\n- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.\n\n- Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350-354, 2019.\n\n- Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. arXiv preprint arXiv: Arxiv-1804.07461, 2018a.\n\n- Wang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning structured policy with graph neural networks. In Interna- tional Conference on Learning Representations, 2018b.\n\n- Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 8(3-4):279-292, 1992.\n\n- Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingface's transformers: State-of-the-art natural language processing, 2020.\n\n- Wu, Y., Tucker, G., and Nachum, O. Behavior regu- larized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.\n\n- Yen-Chen, L., Zeng, A., Song, S., Isola, P., and Lin, T.-Y. Learning to see before learning to act: Visual pre-training for manipulation. In 2020 IEEE International Confer- ence on Robotics and Automation (ICRA), pp. 7286-7293. IEEE, 2020.\n\n- Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and evalua- tion for multi-task and meta reinforcement learning. In Conference on Robot Learning, pp. 1094-1100. PMLR, 2020.\n\n- Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et al. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv:2010.14406, 2020.\n\n- Zhu, Y., Wong, J., Mandlekar, A., and Mart\u00edn-Mart\u00edn, R. ro- bosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\n\n", "annotations": {"ReferenceToTable": [{"begin": 13321, "end": 13322, "target": "#tab_0", "idx": 0}, {"begin": 13888, "end": 13889, "target": "#tab_1", "idx": 1}, {"begin": 15366, "end": 15367, "idx": 2}, {"begin": 15671, "end": 15672, "idx": 3}, {"begin": 17168, "end": 17169, "target": "#tab_1", "idx": 4}, {"begin": 21196, "end": 21197, "target": "#tab_3", "idx": 5}, {"begin": 21890, "end": 21891, "target": "#tab_4", "idx": 6}, {"begin": 22505, "end": 22506, "idx": 7}], "ReferenceToFootnote": [{"begin": 9494, "end": 9495, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 1408, "end": 29088, "idx": 0}], "ReferenceToFormula": [{"begin": 8291, "end": 8292, "target": "#formula_3", "idx": 0}, {"begin": 12328, "end": 12332, "idx": 1}], "SectionReference": [{"begin": 29450, "end": 42992, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1408, "idx": 0}], "Div": [{"begin": 62, "end": 1400, "idx": 0}, {"begin": 1411, "end": 5053, "idx": 1}, {"begin": 5055, "end": 7752, "idx": 2}, {"begin": 7754, "end": 8012, "idx": 3}, {"begin": 8014, "end": 8423, "idx": 4}, {"begin": 8425, "end": 9981, "idx": 5}, {"begin": 9983, "end": 10238, "idx": 6}, {"begin": 10240, "end": 10674, "idx": 7}, {"begin": 10676, "end": 10691, "idx": 8}, {"begin": 10693, "end": 11869, "idx": 9}, {"begin": 11871, "end": 13127, "idx": 10}, {"begin": 13129, "end": 13561, "idx": 11}, {"begin": 13563, "end": 14855, "idx": 12}, {"begin": 14857, "end": 15046, "idx": 13}, {"begin": 15048, "end": 15239, "idx": 14}, {"begin": 15241, "end": 16278, "idx": 15}, {"begin": 16280, "end": 20226, "idx": 16}, {"begin": 20228, "end": 21402, "idx": 17}, {"begin": 21404, "end": 22284, "idx": 18}, {"begin": 22286, "end": 27380, "idx": 19}, {"begin": 27382, "end": 29088, "idx": 20}], "Head": [{"begin": 1411, "end": 1426, "n": "1.", "idx": 0}, {"begin": 5055, "end": 5068, "n": "2.", "idx": 1}, {"begin": 7754, "end": 7768, "n": "3.", "idx": 2}, {"begin": 8014, "end": 8027, "n": "3.1.", "idx": 3}, {"begin": 8425, "end": 8440, "n": "3.2.", "idx": 4}, {"begin": 9983, "end": 10009, "idx": 5}, {"begin": 10240, "end": 10260, "n": "3.3.", "idx": 6}, {"begin": 10676, "end": 10690, "n": "4.", "idx": 7}, {"begin": 10693, "end": 10704, "n": "4.1.", "idx": 8}, {"begin": 11871, "end": 11883, "idx": 9}, {"begin": 13129, "end": 13139, "n": "4.2.", "idx": 10}, {"begin": 13563, "end": 13571, "n": "4.3.", "idx": 11}, {"begin": 14857, "end": 14868, "n": "5.", "idx": 12}, {"begin": 15048, "end": 15070, "n": "5.1.", "idx": 13}, {"begin": 15241, "end": 15246, "idx": 14}, {"begin": 16280, "end": 16337, "n": "5.2.", "idx": 15}, {"begin": 20228, "end": 20280, "n": "5.3.", "idx": 16}, {"begin": 21404, "end": 21440, "n": "5.5.", "idx": 17}, {"begin": 22286, "end": 22322, "n": "5.6.", "idx": 18}, {"begin": 27382, "end": 27395, "n": "7.", "idx": 19}], "Paragraph": [{"begin": 62, "end": 1400, "idx": 0}, {"begin": 1427, "end": 2151, "idx": 1}, {"begin": 2152, "end": 2643, "idx": 2}, {"begin": 2644, "end": 3353, "idx": 3}, {"begin": 3354, "end": 4567, "idx": 4}, {"begin": 4568, "end": 5053, "idx": 5}, {"begin": 5069, "end": 5506, "idx": 6}, {"begin": 5552, "end": 5735, "idx": 7}, {"begin": 5803, "end": 6096, "idx": 8}, {"begin": 6097, "end": 6487, "idx": 9}, {"begin": 6567, "end": 6943, "idx": 10}, {"begin": 6944, "end": 7281, "idx": 11}, {"begin": 7282, "end": 7702, "idx": 12}, {"begin": 7769, "end": 8012, "idx": 13}, {"begin": 8028, "end": 8142, "idx": 14}, {"begin": 8211, "end": 8423, "idx": 15}, {"begin": 8441, "end": 9156, "idx": 16}, {"begin": 9157, "end": 9204, "idx": 17}, {"begin": 9246, "end": 9495, "idx": 18}, {"begin": 9532, "end": 9981, "idx": 19}, {"begin": 10010, "end": 10238, "idx": 20}, {"begin": 10261, "end": 10322, "idx": 21}, {"begin": 10359, "end": 10674, "idx": 22}, {"begin": 10705, "end": 11132, "idx": 23}, {"begin": 11133, "end": 11869, "idx": 24}, {"begin": 11884, "end": 12183, "idx": 25}, {"begin": 12184, "end": 12946, "idx": 26}, {"begin": 12947, "end": 13127, "idx": 27}, {"begin": 13140, "end": 13561, "idx": 28}, {"begin": 13572, "end": 13711, "idx": 29}, {"begin": 13712, "end": 14855, "idx": 30}, {"begin": 14869, "end": 15046, "idx": 31}, {"begin": 15071, "end": 15239, "idx": 32}, {"begin": 15247, "end": 15359, "idx": 33}, {"begin": 15360, "end": 15652, "idx": 34}, {"begin": 15653, "end": 16187, "idx": 35}, {"begin": 16188, "end": 16278, "idx": 36}, {"begin": 16338, "end": 17559, "idx": 37}, {"begin": 17560, "end": 18167, "idx": 38}, {"begin": 18168, "end": 18509, "idx": 39}, {"begin": 18510, "end": 19195, "idx": 40}, {"begin": 19196, "end": 20226, "idx": 41}, {"begin": 20281, "end": 21402, "idx": 42}, {"begin": 21441, "end": 22284, "idx": 43}, {"begin": 22323, "end": 23996, "idx": 44}, {"begin": 23997, "end": 25010, "idx": 45}, {"begin": 25011, "end": 26422, "idx": 46}, {"begin": 26423, "end": 27380, "idx": 47}, {"begin": 27396, "end": 28328, "idx": 48}, {"begin": 28329, "end": 29088, "idx": 49}], "ReferenceToBib": [{"begin": 1515, "end": 1536, "target": "#b12", "idx": 0}, {"begin": 1537, "end": 1558, "target": "#b46", "idx": 1}, {"begin": 1570, "end": 1596, "target": "#b13", "idx": 2}, {"begin": 1666, "end": 1688, "target": "#b57", "idx": 3}, {"begin": 1689, "end": 1710, "target": "#b3", "idx": 4}, {"begin": 1711, "end": 1732, "target": "#b47", "idx": 5}, {"begin": 1804, "end": 1824, "target": "#b6", "idx": 6}, {"begin": 2079, "end": 2106, "target": "#b56", "idx": 7}, {"begin": 2250, "end": 2269, "target": "#b8", "idx": 8}, {"begin": 2270, "end": 2290, "target": "#b27", "idx": 9}, {"begin": 2291, "end": 2311, "target": "#b18", "idx": 10}, {"begin": 2516, "end": 2539, "target": "#b61", "idx": 11}, {"begin": 2540, "end": 2553, "target": "#b30", "idx": 12}, {"begin": 2845, "end": 2865, "target": "#b52", "idx": 13}, {"begin": 2866, "end": 2888, "target": "#b55", "idx": 14}, {"begin": 2982, "end": 3002, "idx": 15}, {"begin": 3003, "end": 3022, "target": "#b29", "idx": 16}, {"begin": 3023, "end": 3041, "target": "#b66", "idx": 17}, {"begin": 3111, "end": 3131, "target": "#b11", "idx": 18}, {"begin": 3132, "end": 3149, "target": "#b67", "idx": 19}, {"begin": 3150, "end": 3166, "target": "#b65", "idx": 20}, {"begin": 3247, "end": 3266, "target": "#b8", "idx": 21}, {"begin": 3267, "end": 3287, "target": "#b27", "idx": 22}, {"begin": 3288, "end": 3308, "target": "#b18", "idx": 23}, {"begin": 4193, "end": 4212, "target": "#b8", "idx": 24}, {"begin": 4244, "end": 4267, "target": "#b5", "idx": 25}, {"begin": 4278, "end": 4302, "target": "#b4", "idx": 26}, {"begin": 5887, "end": 5910, "target": "#b17", "idx": 27}, {"begin": 5911, "end": 5931, "target": "#b36", "idx": 28}, {"begin": 6034, "end": 6053, "target": "#b8", "idx": 29}, {"begin": 6054, "end": 6074, "target": "#b27", "idx": 30}, {"begin": 6075, "end": 6095, "target": "#b18", "idx": 31}, {"begin": 6182, "end": 6204, "target": "#b57", "idx": 32}, {"begin": 6883, "end": 6900, "target": "#b25", "idx": 33}, {"begin": 6925, "end": 6942, "target": "#b2", "idx": 34}, {"begin": 7679, "end": 7700, "target": "#b5", "idx": 35}, {"begin": 8038, "end": 8056, "target": "#b8", "idx": 36}, {"begin": 10866, "end": 10885, "target": "#b8", "idx": 37}, {"begin": 11006, "end": 11027, "target": "#b41", "idx": 38}, {"begin": 11210, "end": 11232, "target": "#b48", "idx": 39}, {"begin": 11246, "end": 11265, "target": "#b9", "idx": 40}, {"begin": 12044, "end": 12063, "target": "#b8", "idx": 41}, {"begin": 12069, "end": 12089, "target": "#b34", "idx": 42}, {"begin": 12098, "end": 12119, "target": "#b15", "idx": 43}, {"begin": 12126, "end": 12143, "target": "#b63", "idx": 44}, {"begin": 12163, "end": 12182, "target": "#b45", "idx": 45}, {"begin": 12539, "end": 12561, "target": "#b49", "idx": 46}, {"begin": 12562, "end": 12586, "target": "#b32", "idx": 47}, {"begin": 12609, "end": 12631, "target": "#b47", "idx": 48}, {"begin": 12697, "end": 12716, "target": "#b8", "idx": 49}, {"begin": 13018, "end": 13035, "target": "#b14", "idx": 50}, {"begin": 13062, "end": 13085, "target": "#b5", "idx": 51}, {"begin": 13096, "end": 13120, "target": "#b4", "idx": 52}, {"begin": 13207, "end": 13231, "target": "#b4", "idx": 53}, {"begin": 13274, "end": 13295, "target": "#b0", "idx": 54}, {"begin": 13693, "end": 13710, "target": "#b14", "idx": 55}, {"begin": 21652, "end": 21679, "target": "#b56", "idx": 56}, {"begin": 21680, "end": 21696, "target": "#b39", "idx": 57}, {"begin": 22164, "end": 22181, "target": "#b39", "idx": 58}, {"begin": 22843, "end": 22865, "target": "#b57", "idx": 59}, {"begin": 22892, "end": 22913, "target": "#b46", "idx": 60}, {"begin": 23246, "end": 23265, "idx": 61}, {"begin": 23281, "end": 23302, "target": "#b12", "idx": 62}, {"begin": 23611, "end": 23636, "target": "#b13", "idx": 63}, {"begin": 23654, "end": 23672, "target": "#b38", "idx": 64}, {"begin": 23978, "end": 23995, "target": "#b54", "idx": 65}, {"begin": 24134, "end": 24153, "target": "#b42", "idx": 66}, {"begin": 24154, "end": 24177, "target": "#b37", "idx": 67}, {"begin": 24178, "end": 24194, "target": "#b20", "idx": 68}, {"begin": 24195, "end": 24217, "target": "#b23", "idx": 69}, {"begin": 24218, "end": 24240, "target": "#b16", "idx": 70}, {"begin": 24306, "end": 24329, "target": "#b17", "idx": 71}, {"begin": 24364, "end": 24381, "target": "#b63", "idx": 72}, {"begin": 24382, "end": 24402, "target": "#b28", "idx": 73}, {"begin": 24403, "end": 24428, "target": "#b19", "idx": 74}, {"begin": 24429, "end": 24448, "target": "#b34", "idx": 75}, {"begin": 24449, "end": 24469, "target": "#b15", "idx": 76}, {"begin": 24496, "end": 24517, "target": "#b28", "idx": 77}, {"begin": 24518, "end": 24540, "target": "#b7", "idx": 78}, {"begin": 24592, "end": 24617, "target": "#b53", "idx": 79}, {"begin": 24618, "end": 24637, "target": "#b33", "idx": 80}, {"begin": 24738, "end": 24757, "target": "#b8", "idx": 81}, {"begin": 24758, "end": 24778, "target": "#b27", "idx": 82}, {"begin": 24779, "end": 24799, "target": "#b18", "idx": 83}, {"begin": 25062, "end": 25083, "target": "#b12", "idx": 84}, {"begin": 25084, "end": 25109, "target": "#b13", "idx": 85}, {"begin": 25188, "end": 25207, "target": "#b42", "idx": 86}, {"begin": 25208, "end": 25228, "target": "#b51", "idx": 87}, {"begin": 25229, "end": 25245, "target": "#b21", "idx": 88}, {"begin": 25246, "end": 25271, "target": "#b31", "idx": 89}, {"begin": 25272, "end": 25293, "target": "#b58", "idx": 90}, {"begin": 25372, "end": 25390, "target": "#b9", "idx": 91}, {"begin": 25391, "end": 25414, "target": "#b40", "idx": 92}, {"begin": 25444, "end": 25464, "target": "#b52", "idx": 93}, {"begin": 25465, "end": 25487, "target": "#b55", "idx": 94}, {"begin": 25829, "end": 25849, "idx": 95}, {"begin": 25850, "end": 25870, "target": "#b44", "idx": 96}, {"begin": 25871, "end": 25889, "target": "#b10", "idx": 97}, {"begin": 25890, "end": 25909, "target": "#b35", "idx": 98}, {"begin": 25955, "end": 25975, "target": "#b29", "idx": 99}, {"begin": 25976, "end": 25999, "target": "#b40", "idx": 100}, {"begin": 26000, "end": 26022, "target": "#b50", "idx": 101}, {"begin": 26057, "end": 26076, "target": "#b66", "idx": 102}, {"begin": 26077, "end": 26099, "target": "#b50", "idx": 103}, {"begin": 26100, "end": 26121, "target": "#b43", "idx": 104}, {"begin": 26366, "end": 26384, "target": "#b9", "idx": 105}, {"begin": 26385, "end": 26408, "target": "#b40", "idx": 106}, {"begin": 26588, "end": 26613, "target": "#b22", "idx": 107}, {"begin": 26779, "end": 26805, "target": "#b56", "idx": 108}, {"begin": 26969, "end": 26985, "target": "#b39", "idx": 109}, {"begin": 28996, "end": 29017, "target": "#b26", "idx": 110}], "ReferenceString": [{"begin": 29465, "end": 29578, "id": "b0", "idx": 0}, {"begin": 29580, "end": 30107, "id": "b1", "idx": 1}, {"begin": 30111, "end": 30220, "id": "b2", "idx": 2}, {"begin": 30224, "end": 30421, "id": "b3", "idx": 3}, {"begin": 30425, "end": 30627, "id": "b4", "idx": 4}, {"begin": 30631, "end": 31056, "id": "b5", "idx": 5}, {"begin": 31060, "end": 31495, "id": "b6", "idx": 6}, {"begin": 31499, "end": 31750, "id": "b7", "idx": 7}, {"begin": 31754, "end": 31979, "id": "b8", "idx": 8}, {"begin": 31983, "end": 32354, "id": "b9", "idx": 9}, {"begin": 32358, "end": 32497, "id": "b10", "idx": 10}, {"begin": 32501, "end": 32701, "id": "b11", "idx": 11}, {"begin": 32705, "end": 32847, "id": "b12", "idx": 12}, {"begin": 32851, "end": 33103, "id": "b13", "idx": 13}, {"begin": 33107, "end": 33270, "id": "b14", "idx": 14}, {"begin": 33274, "end": 33396, "id": "b15", "idx": 15}, {"begin": 33400, "end": 33580, "id": "b16", "idx": 16}, {"begin": 33584, "end": 33761, "id": "b17", "idx": 17}, {"begin": 33765, "end": 33918, "id": "b18", "idx": 18}, {"begin": 33922, "end": 34137, "id": "b19", "idx": 19}, {"begin": 34141, "end": 34330, "id": "b20", "idx": 20}, {"begin": 34334, "end": 34578, "id": "b21", "idx": 21}, {"begin": 34582, "end": 34753, "id": "b22", "idx": 22}, {"begin": 34757, "end": 34984, "id": "b23", "idx": 23}, {"begin": 34988, "end": 35088, "id": "b24", "idx": 24}, {"begin": 35092, "end": 35223, "id": "b25", "idx": 25}, {"begin": 35227, "end": 35388, "id": "b26", "idx": 26}, {"begin": 35392, "end": 35526, "id": "b27", "idx": 27}, {"begin": 35530, "end": 35745, "id": "b28", "idx": 28}, {"begin": 35749, "end": 35904, "id": "b29", "idx": 29}, {"begin": 35908, "end": 36009, "id": "b30", "idx": 30}, {"begin": 36013, "end": 36272, "id": "b31", "idx": 31}, {"begin": 36276, "end": 36420, "id": "b32", "idx": 32}, {"begin": 36424, "end": 36530, "id": "b33", "idx": 33}, {"begin": 36534, "end": 36682, "id": "b34", "idx": 34}, {"begin": 36686, "end": 36872, "id": "b35", "idx": 35}, {"begin": 36876, "end": 37047, "id": "b36", "idx": 36}, {"begin": 37051, "end": 37247, "id": "b37", "idx": 37}, {"begin": 37251, "end": 37405, "id": "b38", "idx": 38}, {"begin": 37409, "end": 37521, "id": "b39", "idx": 39}, {"begin": 37525, "end": 37676, "id": "b40", "idx": 40}, {"begin": 37680, "end": 37770, "id": "b41", "idx": 41}, {"begin": 37774, "end": 38016, "id": "b42", "idx": 42}, {"begin": 38020, "end": 38160, "id": "b43", "idx": 43}, {"begin": 38164, "end": 38355, "id": "b44", "idx": 44}, {"begin": 38359, "end": 38540, "id": "b45", "idx": 45}, {"begin": 38544, "end": 38673, "id": "b46", "idx": 46}, {"begin": 38677, "end": 38807, "id": "b47", "idx": 47}, {"begin": 38811, "end": 39047, "id": "b48", "idx": 48}, {"begin": 39051, "end": 39427, "id": "b49", "idx": 49}, {"begin": 39431, "end": 39587, "id": "b50", "idx": 50}, {"begin": 39591, "end": 39850, "id": "b51", "idx": 51}, {"begin": 39854, "end": 40028, "id": "b52", "idx": 52}, {"begin": 40032, "end": 40205, "id": "b53", "idx": 53}, {"begin": 40209, "end": 40347, "id": "b54", "idx": 54}, {"begin": 40351, "end": 40589, "id": "b55", "idx": 55}, {"begin": 40593, "end": 40740, "id": "b56", "idx": 56}, {"begin": 40744, "end": 40885, "id": "b57", "idx": 57}, {"begin": 40889, "end": 41139, "id": "b58", "idx": 58}, {"begin": 41143, "end": 41350, "id": "b59", "idx": 59}, {"begin": 41354, "end": 41527, "id": "b60", "idx": 60}, {"begin": 41531, "end": 41611, "id": "b61", "idx": 61}, {"begin": 41615, "end": 41955, "id": "b62", "idx": 62}, {"begin": 41959, "end": 42087, "id": "b63", "idx": 63}, {"begin": 42091, "end": 42330, "id": "b64", "idx": 64}, {"begin": 42334, "end": 42561, "id": "b65", "idx": 65}, {"begin": 42565, "end": 42814, "id": "b66", "idx": 66}, {"begin": 42818, "end": 42990, "id": "b67", "idx": 67}], "Sentence": [{"begin": 62, "end": 259, "idx": 0}, {"begin": 260, "end": 432, "idx": 1}, {"begin": 433, "end": 522, "idx": 2}, {"begin": 523, "end": 786, "idx": 3}, {"begin": 787, "end": 869, "idx": 4}, {"begin": 870, "end": 1135, "idx": 5}, {"begin": 1136, "end": 1400, "idx": 6}, {"begin": 1427, "end": 1603, "idx": 7}, {"begin": 1604, "end": 1989, "idx": 8}, {"begin": 1990, "end": 2151, "idx": 9}, {"begin": 2152, "end": 2409, "idx": 10}, {"begin": 2410, "end": 2643, "idx": 11}, {"begin": 2644, "end": 2790, "idx": 12}, {"begin": 2791, "end": 3167, "idx": 13}, {"begin": 3168, "end": 3353, "idx": 14}, {"begin": 3354, "end": 3575, "idx": 15}, {"begin": 3576, "end": 3789, "idx": 16}, {"begin": 3790, "end": 3831, "idx": 17}, {"begin": 3832, "end": 4033, "idx": 18}, {"begin": 4034, "end": 4325, "idx": 19}, {"begin": 4326, "end": 4567, "idx": 20}, {"begin": 4568, "end": 4781, "idx": 21}, {"begin": 4782, "end": 5053, "idx": 22}, {"begin": 5069, "end": 5334, "idx": 23}, {"begin": 5335, "end": 5506, "idx": 24}, {"begin": 5552, "end": 5735, "idx": 25}, {"begin": 5803, "end": 6096, "idx": 26}, {"begin": 6097, "end": 6229, "idx": 27}, {"begin": 6230, "end": 6301, "idx": 28}, {"begin": 6302, "end": 6487, "idx": 29}, {"begin": 6567, "end": 6840, "idx": 30}, {"begin": 6841, "end": 6943, "idx": 31}, {"begin": 6944, "end": 7067, "idx": 32}, {"begin": 7068, "end": 7281, "idx": 33}, {"begin": 7282, "end": 7321, "idx": 34}, {"begin": 7322, "end": 7543, "idx": 35}, {"begin": 7544, "end": 7702, "idx": 36}, {"begin": 7769, "end": 8012, "idx": 37}, {"begin": 8028, "end": 8142, "idx": 38}, {"begin": 8211, "end": 8423, "idx": 39}, {"begin": 8441, "end": 8740, "idx": 40}, {"begin": 8741, "end": 8880, "idx": 41}, {"begin": 8881, "end": 8964, "idx": 42}, {"begin": 8965, "end": 8973, "idx": 43}, {"begin": 8974, "end": 9156, "idx": 44}, {"begin": 9157, "end": 9204, "idx": 45}, {"begin": 9246, "end": 9399, "idx": 46}, {"begin": 9400, "end": 9413, "idx": 47}, {"begin": 9414, "end": 9461, "idx": 48}, {"begin": 9462, "end": 9475, "idx": 49}, {"begin": 9476, "end": 9495, "idx": 50}, {"begin": 9532, "end": 9635, "idx": 51}, {"begin": 9636, "end": 9823, "idx": 52}, {"begin": 9824, "end": 9919, "idx": 53}, {"begin": 9920, "end": 9981, "idx": 54}, {"begin": 10010, "end": 10107, "idx": 55}, {"begin": 10108, "end": 10238, "idx": 56}, {"begin": 10261, "end": 10322, "idx": 57}, {"begin": 10359, "end": 10674, "idx": 58}, {"begin": 10705, "end": 10813, "idx": 59}, {"begin": 10814, "end": 11096, "idx": 60}, {"begin": 11097, "end": 11132, "idx": 61}, {"begin": 11133, "end": 11266, "idx": 62}, {"begin": 11267, "end": 11387, "idx": 63}, {"begin": 11388, "end": 11516, "idx": 64}, {"begin": 11517, "end": 11688, "idx": 65}, {"begin": 11689, "end": 11869, "idx": 66}, {"begin": 11884, "end": 12183, "idx": 67}, {"begin": 12184, "end": 12501, "idx": 68}, {"begin": 12502, "end": 12632, "idx": 69}, {"begin": 12633, "end": 12717, "idx": 70}, {"begin": 12718, "end": 12805, "idx": 71}, {"begin": 12806, "end": 12899, "idx": 72}, {"begin": 12900, "end": 12946, "idx": 73}, {"begin": 12947, "end": 13127, "idx": 74}, {"begin": 13140, "end": 13232, "idx": 75}, {"begin": 13233, "end": 13323, "idx": 76}, {"begin": 13324, "end": 13447, "idx": 77}, {"begin": 13448, "end": 13561, "idx": 78}, {"begin": 13572, "end": 13711, "idx": 79}, {"begin": 13712, "end": 13844, "idx": 80}, {"begin": 13845, "end": 13881, "idx": 81}, {"begin": 13882, "end": 14131, "idx": 82}, {"begin": 14132, "end": 14388, "idx": 83}, {"begin": 14389, "end": 14527, "idx": 84}, {"begin": 14528, "end": 14685, "idx": 85}, {"begin": 14686, "end": 14847, "idx": 86}, {"begin": 14848, "end": 14855, "idx": 87}, {"begin": 14869, "end": 15046, "idx": 88}, {"begin": 15071, "end": 15227, "idx": 89}, {"begin": 15228, "end": 15239, "idx": 90}, {"begin": 15247, "end": 15359, "idx": 91}, {"begin": 15360, "end": 15573, "idx": 92}, {"begin": 15574, "end": 15652, "idx": 93}, {"begin": 15653, "end": 15673, "idx": 94}, {"begin": 15674, "end": 15822, "idx": 95}, {"begin": 15823, "end": 15947, "idx": 96}, {"begin": 15948, "end": 16080, "idx": 97}, {"begin": 16081, "end": 16187, "idx": 98}, {"begin": 16188, "end": 16278, "idx": 99}, {"begin": 16338, "end": 16639, "idx": 100}, {"begin": 16640, "end": 16791, "idx": 101}, {"begin": 16792, "end": 16901, "idx": 102}, {"begin": 16902, "end": 17140, "idx": 103}, {"begin": 17141, "end": 17261, "idx": 104}, {"begin": 17262, "end": 17559, "idx": 105}, {"begin": 17560, "end": 17727, "idx": 106}, {"begin": 17728, "end": 18031, "idx": 107}, {"begin": 18032, "end": 18167, "idx": 108}, {"begin": 18168, "end": 18349, "idx": 109}, {"begin": 18350, "end": 18509, "idx": 110}, {"begin": 18510, "end": 18748, "idx": 111}, {"begin": 18749, "end": 18859, "idx": 112}, {"begin": 18860, "end": 19034, "idx": 113}, {"begin": 19035, "end": 19195, "idx": 114}, {"begin": 19196, "end": 19372, "idx": 115}, {"begin": 19373, "end": 19514, "idx": 116}, {"begin": 19515, "end": 19626, "idx": 117}, {"begin": 19627, "end": 19928, "idx": 118}, {"begin": 19929, "end": 20159, "idx": 119}, {"begin": 20160, "end": 20226, "idx": 120}, {"begin": 20281, "end": 20369, "idx": 121}, {"begin": 20370, "end": 20445, "idx": 122}, {"begin": 20446, "end": 20839, "idx": 123}, {"begin": 20840, "end": 21011, "idx": 124}, {"begin": 21012, "end": 21194, "idx": 125}, {"begin": 21195, "end": 21402, "idx": 126}, {"begin": 21441, "end": 21637, "idx": 127}, {"begin": 21638, "end": 21864, "idx": 128}, {"begin": 21865, "end": 21933, "idx": 129}, {"begin": 21934, "end": 22046, "idx": 130}, {"begin": 22047, "end": 22284, "idx": 131}, {"begin": 22323, "end": 22477, "idx": 132}, {"begin": 22478, "end": 22507, "idx": 133}, {"begin": 22508, "end": 22608, "idx": 134}, {"begin": 22609, "end": 22965, "idx": 135}, {"begin": 22966, "end": 23275, "idx": 136}, {"begin": 23276, "end": 23508, "idx": 137}, {"begin": 23509, "end": 23637, "idx": 138}, {"begin": 23638, "end": 23743, "idx": 139}, {"begin": 23744, "end": 23996, "idx": 140}, {"begin": 23997, "end": 24330, "idx": 141}, {"begin": 24331, "end": 24541, "idx": 142}, {"begin": 24542, "end": 24800, "idx": 143}, {"begin": 24801, "end": 25010, "idx": 144}, {"begin": 25011, "end": 25294, "idx": 145}, {"begin": 25295, "end": 25488, "idx": 146}, {"begin": 25489, "end": 25710, "idx": 147}, {"begin": 25711, "end": 26122, "idx": 148}, {"begin": 26123, "end": 26422, "idx": 149}, {"begin": 26423, "end": 26765, "idx": 150}, {"begin": 26766, "end": 26932, "idx": 151}, {"begin": 26933, "end": 27084, "idx": 152}, {"begin": 27085, "end": 27380, "idx": 153}, {"begin": 27396, "end": 27512, "idx": 154}, {"begin": 27513, "end": 27917, "idx": 155}, {"begin": 27918, "end": 28153, "idx": 156}, {"begin": 28154, "end": 28328, "idx": 157}, {"begin": 28329, "end": 28581, "idx": 158}, {"begin": 28582, "end": 28771, "idx": 159}, {"begin": 28772, "end": 29088, "idx": 160}], "ReferenceToFigure": [{"begin": 18870, "end": 18871, "target": "#fig_3", "idx": 0}, {"begin": 20653, "end": 20654, "target": "#fig_2", "idx": 1}], "Abstract": [{"begin": 52, "end": 1400, "idx": 0}], "SectionFootnote": [{"begin": 29090, "end": 29448, "idx": 0}], "Footnote": [{"begin": 29101, "end": 29295, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 29296, "end": 29448, "id": "foot_1", "n": "6", "idx": 1}]}}