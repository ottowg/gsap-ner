{"text": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search\n\nAbstract:\nWhile pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a carefully designed search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.\n\nMain:\n\n\n\n1 INTRODUCTION\nPre-trained Transformer [45] -based language models like BERT [10], XLNet [54] and RoBERTa [30] have achieved impressive performance on a variety of downstream natural language processing tasks. These models are pre-trained on massive language corpus via self-supervised tasks to learn language representation and finetuned on specific downstream tasks. Despite their effectiveness, these models are quite expensive in terms of computation and memory cost, which makes them difficult for the deployment on different downstream tasks and various resource-restricted scenarios such as online servers, mobile phones, and embedded devices. Therefore, it is crucial to compress pre-trained models for practical deployment.\nRecently, a variety of compression techniques have been adopted to compress pre-trained models, such as pruning [14, 33], weight factorization [24], quantization [38, 56], and knowledge distillation [5, 16, 18, 37, 40, 41]. Several existing works [5, 14, 24, 33, 37, 41, 43, 56] compress a large pre-trained model into a small or fast model with fixed size on the pre-training or fine-tuning stage and have achieved good compression efficiency and accuracy. However, from the perspective of practical deployment, a fixed-size model cannot be deployed in devices with different memory and latency constraints. For example, smaller models are preferred in embedded devices than in online servers, and faster inference speed is more critical in online services than in offline services. On the other hand, some previous methods [5, 16] compress the models on the fine-tuning stage for each specific downstream task. This can achieve good accuracy due to the dedicated design in each task. However, compressing the model for each task can be laborious and a compressed model for one task may not generalize well on another downstream task.\nIn this paper, we study the BERT compression in a different setting: the compressed models need to cover different sizes and latencies, in order to support devices with different kinds of memory and latency constraints; the compression is conducted on the pre-training stage so as to be downstream task-agnostic 1. To this end, we propose NAS-BERT, which leverages neural architecture search (NAS) to automatically compress BERT models. We carefully design a search space that contains multi-head attention [45], separable convolution [20], feed-forward network and identity operations with different hidden sizes to find efficient models. In order to search models with adaptive sizes that satisfy diverse requirements of memory and latency constraints in different devices, we train a big supernet that contains all the candidate operations and architectures with weight sharing [1] [2] [3] 55]. In order to reduce the laborious compressing on each downstream task, we directly train the big supernet and get the compressed model on the pre-training task to make it applicable across different downstream tasks.\nHowever, it is extremely expensive to directly perform architecture search in a big supernet on the heavy pre-training task. To improve the search efficiency and accuracy, we employ several techniques including block-wise search, search space pruning and performance approximation during the search process: (1) We adopt block-wise search [25] to divide the supernet into blocks so that the size of the search space can be reduced exponentially. To train each block, we leverage a pre-trained teacher model, divide the teacher model into blocks similarly, and use the input and output hidden states of the corresponding teacher block as paired data for training.  (2) To further reduce the search cost of each block (even if block-wise search has greatly reduced the search space) due to the heavy burden of the pre-training task, we propose progressive shrinking to dynamically prune the search space according to the validation loss during training. To ensure that architectures with different sizes and latencies can be retained during the pruning process, we further divide all the architectures in each block into several bins according to their model sizes and perform progressive shrinking in each bin.  (3) We obtain the compressed models under specific constraints of memory and latency by assembling the architectures in every block using performance approximation, which can reduce the cost in model selection.\nWe evaluate the models compressed by NAS-BERT on the GLUE [46] and SQuAD [35, 36] benchmark datasets. The results show that NAS-BERT can find lightweight models with various sizes from 5M to 60M with better accuracy than that achieved by previous work. Our contributions of NAS-BERT can be summarized as follows:\n\u2022 We carefully design a search space that contains various architectures and different sizes, and apply NAS on the pre-training task to search for efficient lightweight models, which is able to deliver adaptive model sizes given different requirements of memory or latency and apply for different downstream tasks.\n\u2022 We further apply block-wise search, progressive shrinking and performance approximation to reduce the huge search cost and improve the search accuracy. \u2022 Experiments on the GLUE and SQuAD benchmark datasets demonstrate the effectiveness of NAS-BERT for model compression.\n\n2 RELATED WORK 2.1 BERT Model Compression\nRecently, compressing pre-trained models such as BERT [10] has been studied extensively and several techniques have been proposed such as knowledge distillation, pruning, weight factorization, quantization and so on. Existing works [5, 18, 24, 24, 37, 38, 40-43, 56, 56] aim to compress the pre-trained model into a fixed size of the model and have achieved a trade-off between the small parameter size (usually no more than 66M) and the good performance. However, these compressed models cannot be deployed in devices with different memory and latency constraints. Recent works [16] can deliver adaptive models for each specific downstream task and demonstrate the effectiveness of the task-oriented compression. For practical deployment, it can be laborious to compress models from each task. Other works [13] can produce compressed models on the pre-training stage that can directly generalize to downstream tasks, and allow for efficient pruning at inference time. However, they do not explore the potential of different architectures as in our work. Different from existing works, NAS-BERT aims for task-agnostic compression on the pre-training stage which eliminates the laborious compression for each specific downstream task, and carefully designs the search space which is capable to explore the potential of different architectures and deliver various models given diverse memory and latency requirements.\n\n2.2 Neural Architecture Search for Efficient Models\nMany works have leveraged NAS to search efficient models [2, 3, 17, 19, 26, 28, 29, 43, 47, 55]. Most of them focus on computer vision tasks and rely on specific designs on the convolutional layers (e.g., inverted bottleneck convolution [17] or elastic kernel size [2, 55]). Among them, once-for-all [2] and BigNAS [55] train a big supernet that contains all the candidate architectures and can get a specialized sub-network by selecting from the supernet to support various requirements (e.g., model size and latency). HAT [47] also trains a supernet with the adaptive widths and depths for machine translation tasks. Our proposed NAS-BERT also trains a big supernet. However, different from these methods, we target model compression for BERT at the pre-training stage, which is a more challenging task due to the large model size, search space and huge pre-training cost. In our preliminary experiments, directly applying the commonly used single path optimization [1, 2, 7, 15] in such a big supernet cannot even converge. Therefore, we introduce several techniques including block-wise search, progressive shrinking, and performance approximation to reduce the training cost, search space and improve search efficiency. Tsai et al. [43] apply one-shot NAS to search a faster Transformer but they cannot deliver multiple architectures to meet various constraints for deployment. Different from Tsai et al. [43], NAS-BERT 1) progressively shrinks the search space to allocate more resources to promising architectures and thus can deliver various architectures without adding much computation; 2) designs bins in the shrinking algorithm to guarantee that we can search architectures to meet diverse memory and latency constraints. 3) explores novel architectures with convolution layer, multi-head attention, and feed-forward layer, and achieves better performance than previous works for BERT compression.\n\n3 METHOD\nIn this section, we describe NAS-BERT, which conducts neural architecture search to find small, novel and accurate BERT models. To meet the requirements of deployment for different memory and latency constraints and across different downstream tasks, we 1) train a supernet with a novel search space that contains different sizes of models for various resource-restricted devices, and 2) directly search the models on the pre-training task to make them generalizable on different downstream tasks. The method can be divided into three steps: 1) search space design (Section 3.1); 2) supernet training (Section 3.2); 3) model selection (Section 3.3). Due to the huge cost to train the big supernet on the heavy pre-training task and select compressed models under specific constraints, we introduce several techniques including block-wise search, search space pruning and performance approximation in Section 3.2 and 3.3 to reduce the search space and improve the search efficiency.\n\n3.1 Search Space Design\nA novel search space allows the potential of combinations of different operations, instead of simply stacking basic Transformer block (multi-head attention and feed-forward network) as in the original BERT model. We adopt the chain-structured search space [12], and construct an over-parameterized supernet A with  layers and each layer contains all candidate operations in O = { 1 , \u2022 \u2022 \u2022 ,   }, where  is the number of predefined candidate operations. Residual connection is applied to each layer by adding the input to the output. There are   possible paths (architectures) in the supernet, and a specific architecture  = ( 1 , \u2022 \u2022 \u2022 ,   ) is a sub-net (path) in the supernet, where   \u2208 O is the operation in the -th layer, as shown in Fig. 2 (a). We adopt weight sharing mechanism that is widely used in NAS [1, 2] for efficient training, where each architecture (path) shares the same set of operations in each layer. We further describe each operation in O as follows: 1) Multihead attention (MHA) and feed-forward network (FFN), which are the two basic operations in Transformer and are popular in pre-training models (in this way we can cover BERT model as a subnet in our supernet). 2) Separable convolution (SepConv), whose effectiveness and efficiency in natural language processing tasks have been demonstrated by previous work [20, 21]. 3) Identity operation, which can support architectures with the number of layers less than . Identity operation is regarded as a placeholder in a candidate architecture and can be removed to obtain a shallower network. Apart from different types of operations, to allow adaptive model sizes, each operation can have different hidden sizes: {128, 192, 256, 384, 512}. In this way, architectures in the search space can be of different depths and widths. The complete candidate operation set O contains (1 + 1 + 3) * 5 + 1 = 26 operations, where the first product term represents the number of types of operations and  3 represents the SepConv with different kernel size {3, 5, 7}, the second product term represents that there are 5 different hidden sizes. We present 26 operations in Table 1 and structure of separable convolution in Fig. 1.\n\n3.2 Supernet Training\n\n\n3.2.1 Block-Wise\nTraining with Knowledge Distillation. Directly training the whole supernet causes huge cost due to its large model size and huge search space. With limited computational resources (total training time, steps, etc.), the amortized training time of each architecture from the huge search space is insufficient for accurate evaluation [7, 27, 31]. Inspired by Li et al. [25], we adopt blockwise search to uniformly divide the supernet A into  blocks (A 1 , A 2 , \u2022 \u2022 \u2022 , A  ) to reduce the search space and improve the efficiency. To train each block independently and effectively, knowledge distillation is applied with a pre-trained BERT model. The pre-trained BERT model (teacher) is divided into corresponding  blocks as in Fig. 2. The input and output hidden states of the corresponding block in the teacher model are used as the paired data to train the block in the supernet (student). Specifically, the -th student block receives the output of the ( \u2212 1)-th teacher block as the input and is optimized to predict the output of the -th teacher block with mean square lossL \ud835\udc5b = ||\ud835\udc53 (Y \ud835\udc5b\u22121 ; A \ud835\udc5b ) \u2212 Y \ud835\udc5b || 2 2 ,\nwhere  (\u2022; A  ) is the mapping function of -th block A  , Y  is the output of the -th block of the teacher model (Y 0 is the output of the embedding layer of the teacher model). At each training step, we randomly sample an architecture from the search space following Bender et al. [1], Cai et al. [2], Guo et al. [15], which is memory-efficient due to the single path optimization. Different from Li et al. [25], we allow different hidden sizes and incorporate identity layer in each block to support elastic width and depth to derive models that meet various requirements. Besides, the search space within each block in our work is larger compared to Li et al. [25] (100x larger) which is much more sample in-efficient and requires more techniques (described in Section 3.2.2) to improve the training efficiency. Since the hidden sizes of the student block may be different from that in the teacher block, we cannot directly leverage the input and output hidden of the teacher block as the training data of the student block. To solve this problem, we use a learnable linear transformation layer at the input and output of the student block to transform each hidden size to match that of the teacher block, as shown in Fig. 2.3\ud835\udc59 \ud835\udc4f > \ud835\udc59 (\ud835\udc4e) > \ud835\udc59 \ud835\udc4f\u22121 .\nArchitectures that cannot satisfy the constraint of latency are removed.\nThen we conduct the progressive shrinking algorithm in each bin at the end of each training epoch as follows: 1) Sample  architectures in each bin and get the validation losses on the dev set; 2) Rank the  architectures according to their validation losses in descending order; 3) Remove  architectures with the largest losses. The shrinking algorithm terminates when there are only  architectures left in the search space to avoid all the architectures being deleted. The design of bins ensures the diversity of models when shrinking the search space, which makes it possible to select a model given diverse constraints at the model selection stage.\n\n3.3 Model Selection\nAfter the supernet training with progressive shrinking, each block contains  *  possible architectures and the whole supernet ( blocks) contains ( * )  complete architectures. The model selection procedure is as follows: 1) We build a large lookup table  \u210e with ( * )  items, where each item contains the metainformation of a complete architecture: (architecture, parameter, latency, loss). Since it is extremely time-consuming to measure the exact latency and loss for ( * )  (e.g., 10 8 in our experiments) architectures, we use performance approximation to obtain the two values as described in the next paragraph. 2) For a given constraint of model size and inference latency, we select the top  architectures with low loss from  \u210e that meet the parameter and latency constraint. 3) We evaluate the validation loss of the top  complete architectures on the dev set and select the best one as the final compressed model.\nNext we introduce the performance approximation of the latency and loss when building the lookup table  \u210e . We measure the latency of each candidate operation (just 26 in our design) on the target device and store in a lookup table   in advance, and then approximate the latency of an architecture  () by  () =  =1  (  ) following Cai et al. [3], where  (  ) is from   . To approximate the loss of an architectures in  \u210e , we add up the block-wise distillation loss of the sub-architecture in each block on the dev set. Obtaining the dev loss of all sub-architectures in all blocks only involves  *  *  evaluations.\n\n4 EXPERIMENT 4.1 Experimental Setup\n4.1.1 Supernet Training Setup. The supernet consists of  = 24 layers, which is consistent with BERT base [10] (BERT base has 12 Transformer layers with 24 sub-layers in total, since each Transformer In progressive shrinking, to reduce the time of evaluating all the candidates, we only evaluate on 5 batches rather than the whole dev set, which is accurate enough for the pruning according to our preliminary study.\n\n4.1.2 Evaluation\nSetup on Downstream Tasks. We evaluate the effectiveness of NAS-BERT by pre-training the compressed models from scratch on the pre-training task and fine-tuning on the GLUE [46] and SQuAD benchmarks [35, 36]. The GLUE benchmark includes CoLA [50], SST-2 [39], MRPC [11], STS-B [4], QQP [6], MNLI [51], QNLI [36] and RTE [9]. Similar to previous methods [16, 37, 44, 49],\nwe also apply knowledge distillation and conduct it on two stages (i.e., pre-training and fine-tuning) as the default setting for evaluation. Considering the focus of our work is to compress pre-trained models with novel architectures instead of knowledge distillation, we only use prediction layer distillation and leave the various distillation techniques like layer-by-layer distillation, embedding layer distillation and attention matrix distillation [16, 18, 37, 41, 42, 49] that can further improve the performance as to future work. During fine-tuning on the GLUE benchmark, RTE, MRPC and STS-B are started from the model fine-tuned on MNLI following Liu et al. [30].\n\n4.2 Results on the GLUE Datasets\nWhile our NAS-BERT can compress models with adaptive sizes, we only show the results of compressed models with 60M, 30M, 10M and 5M parameter sizes (denoted as NAS-BERT 60 , NAS-BERT 30 , NAS-BERT 10 and NAS-BERT 5 respectively) on the GLUE benchmark due to the large evaluation cost, and list the model structures with different sizes in Table 10. We compare our NAS-BERT models with hand-designed BERT models under the same parameter size and latency (denoted as BERT 60 , BERT 30 , BERT 10 and BERT 5 We mainly compare our NAS-BERT with methods whose teacher model is BERT base for a fair comparison including 1) DistilBERT [37], which uses knowledge distillation on the pre-training stage; 2) BERT-PKD [41], which distills the knowledge from the intermediate layers and the final output logits on the pre-training stage; 3) BERT-of-Theseus [53], which uses module replacement for compression; 4) MiniLM [49], which transfers the knowledge from the self-attention module; 5) PD-BERT [44], which distills the knowledge from the target domain in BERT training; 6) DynaBERT [16], which uses network rewiring to adjust width and depth of BERT for each downstream task. 7) TinyBERT [18], which leverage embedding layer, hidden layer, and attention matrix distillation to mimic the teacher model at both the pre-training and fine-tuning stages.\nThe comparison of their teacher models is presented in Table 9. We do not compare with MobileBERT [42] since it uses a teacher model with much higher accuracy than the commonly used BERT base (nearly matches to the accuracy of BERT large ), and the pre-training computations of its student model (4096 batch size * 740,000 steps) is significantly larger than that of other method [10, 37, 41] and NAS-BERT (2048 batch size * 125,000 steps). Nevertheless, we will compare with MobileBERT in future work. To make comparison with DynaBERT and TinyBERT, we also use their data augmentation [16, 18] on downstream tasks. Table 3 reports the results on the dev and test set of the GLUE benchmark. Without data augmentation, NAS-BERT achieves better results on nearly all the tasks compared to previous work. Further, with data augmentation, NAS-BERT outperforms DynaBERT and TinyBERT. Different from these methods that leverage advanced knowledge distillation techniques in pre-training and/or fine-tuning, NAS-BERT mainly takes advantage of architectures and achieves better accuracy, which demonstrates the advantages of NAS-BERT in model compression.\n\n4.3.2\nComparison in extremely small model size. Compressing the pre-training model into an extremely small model with good accuracy is challenging. Previous works usually compress the model at the fine-tuning stage for each specific task with sophisticated techniques. For example, AdaBERT [5] searches a task-specific architecture (6M -10M) for each task, and introduces the special distillation techniques (e.g., using probe classifiers to hierarchically decompose the task-useful knowledge from the teacher model) and data augmentations to boost the performance. To show the effectiveness of our searched architecture in the extremely small size itself, we compare NAS-BERT with AdaBERT without sophisticated techniques. Specifically, we do not use two-stage distillation and data augmentation for NAS-BERT 5 , and do not use data augmentation and probe classifiers for AdaBERT. As shown in Table 4, we can find that NAS-BERT 5 (with an only 5M model size) can greatly improve upon AdaBERT (with slightly larger model size) by 1.9%, 2.8% and 10.3% (absolute differences) on QNLI, MRPC, and RTE. 5. It can be seen that NAS-BERT with progressive shrinking searches better architectures, with less total search time. We further show the training loss curve in Fig. 3. It can be seen that the superset without progressive shrinking suffers from slow convergence. The huge number of architectures in the supernet need long time for sufficient training. Given a fixed budget of training time, progressive shrinking can ensure the promising architectures to be trained with more resources and result in more accurate evaluation, and thus better architectures can be selected. On the contrary, without progressive shrinking, the amortized training time of each architecture is insufficient, resulting in inaccurate evaluation and model selection.\n\n4.4.2 Different\nProgressive Shrinking Approaches. Instead of pruning architectures (paths) from the search space, we can also prune operations (nodes) from the supernet [32, 34] in progressive shrinking. From the perspective of supernet, the former is to remove paths and the latter is to remove nodes from the supernet. To evaluate the performance of operations (nodes) in supernet, at the end of each training epoch, we evaluate the architectures on the dev set and prune the search space according to the performance (validation loss) of operations. The validation loss of the operation   in -layer is estimated by the mean validation losses of all the architectures whose operation in the -th layer   =   . The shrinking algorithm proceeds as follows:\n\u2022 Sample  architectures and get the validation losses on the dev set. \u2022 Rank operations according to their mean validation losses in descending order. \u2022 Prune operations with the largest losses from the supernet repeatedly until removing  ( is a hyper-parameter to control the speed of pruning) of the architectures in the search space.\nThe shrinking algorithm performs at the end of each training epoch, and terminates when only  = 10 architectures are left in each bin in each block, and the training also ends. For the fair comparison, we set  = 10 and  = 1000, which are same as settings in Section 4.1. At the end of each epoch, we perform this algorithm to remove  = 30% architectures for each bin. In this way, the algorithm terminates at the same epoch as that in Section 3.2.2. At the end of each training epoch, we evaluate the architectures on the dev set and prune the search space according to the performance of operations. As shown in Table 6, pruning architectures in progressive shrinking achieves better results.\n\n4.4.3\nAblation study of two-stage distillation. We further study the effectiveness of NAS-BERT by removing the distillation on pretraining or fine-tuning stage: 1) only using the distillation on the pre-training stage; 2) only using the distillation on the fine-tuning stage. The results are presented in Table 7. We can find that 1) NAS-BERT consistently outperforms the BERT baseline in different settings; 2) distillation on either pre-training or fine-tuning stage can improve the accuracy, and two-stage distillation can further get better accuracy; 3) when removing the distillation at the fine-tuning stage, this setting becomes fully task-agnostic (the model is compressed in pre-training stage, and only needs fine-tuning on each downstream task). We can find that NAS-BERT can still achieve a score of 83.5 on average, which outperforms the previous works in Table 3. These results demonstrate that our searched architectures are effective regardless of the specific distillation settings. NAS-BERT can still achieve good results when removing distillation on the fine-tuning stage.\n\n4.5 Results on the SQuAD Datasets\nWe further evaluate our method on the SQuAD v1.1 [36] and SQuAD v2.0 [35], which require to predict the answer span based on the given question and passage. For SQuAD v1.1, each question has the corresponding answer based on the given passage. And for SQuAD v2.0, some questions do not have the corresponding answer. Following previous practices [30], we add an additional binary classification layer to predict whether the answer exists for SQuAD v2.0. The results are shown in Table 8. We can find that our NAS-BERT outperforms previous works on both SQuAD v1.1 and v2.0.\n\n5 CONCLUSION\nIn this paper, we propose NAS-BERT, which leverages neural architecture search (NAS) to compress BERT models. We carefully design a search space with different operations associated with different hidden sizes, to explore the potential of diverse architectures and derive models with adaptive sizes according to the memory and latency requirements of different devices. The compression is conducted on the pre-training stage and is downstream task agnostic, where the compressed models can be applicable for different downstream tasks. Experiments on the GLUE and SQuAD benchmark datasets demonstrate the effectiveness of our proposed NAS-BERT compared with both hand-designed BERT baselines and previous works on BERT compression. For future work, we will explore more advanced search space and NAS methods to achieve better performance. of 1e-4,  1 = 0.9 and  2 = 0.999. The learning rate is warmed up to a peak value of 5e-4 for the first 10,000 steps, and then linearly decays. The weight decay is 0.01 and the dropout rate is 0.1. We apply the best practices proposed in Liu et al. [30] to train the BERT base on 16 NVIDIA V100 GPUs with large batches leveraging gradient accumulation (2048 samples per batch) and 125000 training steps.\n\nA.3 Comparison of Teacher Models\nWe present the performance of the teacher model for reproducibility and compare it with teacher models used in other works in Table 9.\nThe teacher model (IB-BERT) of MobileBERT [42] achieves the performance close to the BERT large , which is much better than our teacher model and those used in most related works such as TinyBERT, DynaBERT, DistilBERT. Thus we do not compare NAS-BERT with MobileBERT in\n\nA.5 Architectures of NAS-BERT\nOur NAS-BERT can generate different compressed models given specific constraints, as described in Section 3.3. Besides the several NAS-BERT models we evaluate in Table 2, we further select architectures with various sizes, from 5M to 60M with 5M intervals, yielding a total of 12 different architectures with different sizes. We present the architectures in Table 10.\n\nFootnotes:\n1: Here \"task-agnostic\" only refers to that we do not need to compress the model specifically for each downstream tasks, but we can still use a pre-trained model to help training the downstream tasks, such as knowledge distillation. Actually, we study the full \"task-agnostic\" setting that we do not use the pre-trained model for knowledge distillation in downstream tasks (seeSection 4.4.3). The results still demonstrate the effectiveness of our proposed method.\n2: The next sentence tasks are not considered since previous work [30] have achieved good results without it.\n3: We do not use a hidden size smaller than 128 since it cannot yield models with enough accuracy.\n\nReferences:\n\n- Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. 2018. Understanding and simplifying one-shot architecture search. In Inter- national Conference on Machine Learning. 550-559.- Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2019. Once- for-All: Train One Network and Specialize it for Efficient Deployment. In Inter- national Conference on Learning Representations.\n\n- Han Cai, Ligeng Zhu, and Song Han. 2018. ProxylessNAS: Direct Neural Ar- chitecture Search on Target Task and Hardware. In International Conference on Learning Representations.\n\n- Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). 1-14.\n\n- Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. 2020. AdaBERT: Task- Adaptive BERT Compression with Differentiable Neural Architecture Search. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, Christian Bessiere (Ed.). International Joint Conferences on Artificial Intelligence Organization, 2463-2469. https://doi.org/10.24963/ijcai. 2020/341 Main track.\n\n- Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. 2018. Quora question pairs.\n\n- Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. 2019. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search. arXiv preprint arXiv:1907.01845 (2019).\n\n- Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning Representations.\n\n- Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognis- ing Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entail- ment. 177-190.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171-4186. https://doi.org/10.18653/v1/N19-1423\n\n- William B. Dolan and Chris Brockett. 2005. Automatically Constructing a Corpus of Sentential Paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\n\n- Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2018. Neural architecture search: A survey. arXiv preprint arXiv:1808.05377 (2018).\n\n- Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing Transformer Depth on Demand with Structured Dropout. In International Conference on Learn- ing Representations.\n\n- Mitchell Gordon, Kevin Duh, and Nicholas Andrews. 2020. Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning. In Proceedings of the 5th Workshop on Representation Learning for NLP. 143-155.\n\n- Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. 2020. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision. Springer, 544-560.\n\n- Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020. DynaBERT: Dynamic BERT with Adaptive Width and Depth. Advances in Neural Information Processing Systems 33 (2020).\n\n- Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingx- ing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. 2019. Searching for mobilenetv3. In Proceedings of the IEEE International Conference on Computer Vision. 1314-1324.\n\n- Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language Understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 4163-4174.\n\n- Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-keras: An efficient neural architecture search system. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1946-1956.\n\n- Lukasz Kaiser, Aidan N Gomez, and Francois Chollet. 2018. Depthwise Separable Convolutions for Neural Machine Translation. In International Conference on Learning Representations.\n\n- Antonios Karatzoglou, Nikolai Schnell, and Michael Beigl. 2020. Applying depth- wise separable and multi-channel convolutional neural networks of varied kernel size on semantic trajectories. Neural Computing and Applications 32, 11 (2020), 6685-6698.\n\n- Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti- mization. In ICLR (Poster).\n\n- Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic De- noyer, and Herv\u00e9 J\u00e9gou. 2019. Large memory layers with product keys. In Advances in Neural Information Processing Systems. 8548-8559.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations.\n\n- Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang Lin, and Xiaojun Chang. 2020. Block-wisely Supervised Neural Architecture Search with Knowledge Distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1989-1998.\n\n- Ting Li, Junbo Zhang, Kainan Bao, Yuxuan Liang, Yexin Li, and Yu Zheng. 2020. Autost: Efficient neural architecture search for spatio-temporal prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 794-802.\n\n- Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie Yan, and Wanli Ouyang. 2020. Improving one-shot nas by suppressing the posterior fading. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13836-13845.\n\n- Bill Yuchen Lin, Ying Sheng, Nguyen Vo, and Sandeep Tata. 2020. FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1092-1102.\n\n- Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: Differentiable Architecture Search. In International Conference on Learning Representations.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\n\n- Renqian Luo, Tao Qin, and Enhong Chen. 2019. Balanced One-shot Neural Architecture Optimization. arXiv:1909.10815 [cs.LG]\n\n- Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2020. Neural architecture search with gbdt. arXiv preprint arXiv:2007.04785 (2020).\n\n- J Scott McCarley. 2019. Pruning a bert-based question answering model. arXiv preprint arXiv:1910.06360 (2019).\n\n- Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. 2020. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10428-10436.\n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don't Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 784-789.\n\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In EMNLP. 2383-2392.\n\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis- tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n\n- Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.. In AAAI. 8815-8821.\n\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In EMNLP. 1631-1642.\n\n- Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. 2020. LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning. arXiv preprint arXiv:2004.12817 (2020).\n\n- Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Dis- tillation for BERT Model Compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 4314-4323.\n\n- Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics. 2158-2170.\n\n- Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, and Jason Riesa. 2020. Finding Fast Transformers: One-Shot Neural Architecture Search by Com- ponent Composition. arXiv preprint arXiv:2008.06808 (2020).\n\n- Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962 (2019).\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu- ral Language Understanding. In International Conference on Learning Representa- tions.\n\n- Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. 2020. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7675-7688.\n\n- Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. 2019. Sample-efficient neural architecture search by learning action space. arXiv preprint arXiv:1906.06832 (2019).\n\n- Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings. neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n\n- Alex Warstadt, Amanpreet Singh, and Samuel Bowman. 2019. Neural Network Acceptability Judgments. Transactions of the Association for Computational Lin- guistics 7 (2019), 625-641.\n\n- Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In NAACL. 1112-1122.\n\n- Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2018. Pay Less Attention with Lightweight and Dynamic Convolutions. In International Conference on Learning Representations.\n\n- Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. BERT- of-Theseus: Compressing BERT by Progressive Module Replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 7859-7869.\n\n- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems. 5753-5763.\n\n- Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. 2020. Bignas: Scaling up neural architecture search with big single-stage models. In European Conference on Computer Vision. Springer, 702-717.\n\n- Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\n\n", "annotations": {"ReferenceToTable": [{"begin": 13920, "end": 13921, "target": "#tab_0", "idx": 0}, {"begin": 20561, "end": 20563, "target": "#tab_14", "idx": 1}, {"begin": 21619, "end": 21620, "target": "#tab_13", "idx": 2}, {"begin": 22180, "end": 22181, "target": "#tab_4", "idx": 3}, {"begin": 23607, "end": 23608, "target": "#tab_7", "idx": 4}, {"begin": 23805, "end": 23806, "target": "#tab_8", "idx": 5}, {"begin": 26262, "end": 26263, "target": "#tab_9", "idx": 6}, {"begin": 26649, "end": 26650, "target": "#tab_10", "idx": 7}, {"begin": 27213, "end": 27214, "target": "#tab_4", "idx": 8}, {"begin": 27951, "end": 27952, "target": "#tab_11", "idx": 9}, {"begin": 29462, "end": 29463, "target": "#tab_13", "idx": 10}, {"begin": 29934, "end": 29935, "target": "#tab_2", "idx": 11}, {"begin": 30130, "end": 30132, "target": "#tab_14", "idx": 12}], "ReferenceToFootnote": [{"begin": 4215, "end": 4216, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 2031, "end": 30133, "idx": 0}], "SectionReference": [{"begin": 30821, "end": 43548, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 2031, "idx": 0}], "Div": [{"begin": 102, "end": 2023, "idx": 0}, {"begin": 2034, "end": 7340, "idx": 1}, {"begin": 7342, "end": 8799, "idx": 2}, {"begin": 8801, "end": 10762, "idx": 3}, {"begin": 10764, "end": 11754, "idx": 4}, {"begin": 11756, "end": 13971, "idx": 5}, {"begin": 13973, "end": 13995, "idx": 6}, {"begin": 13997, "end": 17103, "idx": 7}, {"begin": 17105, "end": 18664, "idx": 8}, {"begin": 18666, "end": 19117, "idx": 9}, {"begin": 19119, "end": 20181, "idx": 10}, {"begin": 20183, "end": 22705, "idx": 11}, {"begin": 22707, "end": 24548, "idx": 12}, {"begin": 24550, "end": 26336, "idx": 13}, {"begin": 26338, "end": 27430, "idx": 14}, {"begin": 27432, "end": 28039, "idx": 15}, {"begin": 28041, "end": 29295, "idx": 16}, {"begin": 29297, "end": 29734, "idx": 17}, {"begin": 29736, "end": 30133, "idx": 18}], "Head": [{"begin": 2034, "end": 2048, "n": "1", "idx": 0}, {"begin": 7342, "end": 7383, "n": "2", "idx": 1}, {"begin": 8801, "end": 8852, "n": "2.2", "idx": 2}, {"begin": 10764, "end": 10772, "n": "3", "idx": 3}, {"begin": 11756, "end": 11779, "n": "3.1", "idx": 4}, {"begin": 13973, "end": 13994, "n": "3.2", "idx": 5}, {"begin": 13997, "end": 14013, "n": "3.2.1", "idx": 6}, {"begin": 17105, "end": 17124, "n": "3.3", "idx": 7}, {"begin": 18666, "end": 18701, "n": "4", "idx": 8}, {"begin": 19119, "end": 19135, "n": "4.1.2", "idx": 9}, {"begin": 20183, "end": 20215, "n": "4.2", "idx": 10}, {"begin": 22707, "end": 22712, "idx": 11}, {"begin": 24550, "end": 24565, "n": "4.4.2", "idx": 12}, {"begin": 26338, "end": 26343, "idx": 13}, {"begin": 27432, "end": 27465, "n": "4.5", "idx": 14}, {"begin": 28041, "end": 28053, "n": "5", "idx": 15}, {"begin": 29297, "end": 29329, "idx": 16}, {"begin": 29736, "end": 29765, "idx": 17}], "Paragraph": [{"begin": 102, "end": 2023, "idx": 0}, {"begin": 2049, "end": 2766, "idx": 1}, {"begin": 2767, "end": 3902, "idx": 2}, {"begin": 3903, "end": 5016, "idx": 3}, {"begin": 5017, "end": 6438, "idx": 4}, {"begin": 6439, "end": 6751, "idx": 5}, {"begin": 6752, "end": 7066, "idx": 6}, {"begin": 7067, "end": 7340, "idx": 7}, {"begin": 7384, "end": 8799, "idx": 8}, {"begin": 8853, "end": 10762, "idx": 9}, {"begin": 10773, "end": 11754, "idx": 10}, {"begin": 11780, "end": 13971, "idx": 11}, {"begin": 14014, "end": 15089, "idx": 12}, {"begin": 15129, "end": 16357, "idx": 13}, {"begin": 16380, "end": 16452, "idx": 14}, {"begin": 16453, "end": 17103, "idx": 15}, {"begin": 17125, "end": 18048, "idx": 16}, {"begin": 18049, "end": 18664, "idx": 17}, {"begin": 18702, "end": 19117, "idx": 18}, {"begin": 19136, "end": 19506, "idx": 19}, {"begin": 19507, "end": 20181, "idx": 20}, {"begin": 20216, "end": 21557, "idx": 21}, {"begin": 21558, "end": 22705, "idx": 22}, {"begin": 22713, "end": 24548, "idx": 23}, {"begin": 24566, "end": 25305, "idx": 24}, {"begin": 25306, "end": 25642, "idx": 25}, {"begin": 25643, "end": 26336, "idx": 26}, {"begin": 26344, "end": 27430, "idx": 27}, {"begin": 27466, "end": 28039, "idx": 28}, {"begin": 28054, "end": 29295, "idx": 29}, {"begin": 29330, "end": 29464, "idx": 30}, {"begin": 29465, "end": 29734, "idx": 31}, {"begin": 29766, "end": 30133, "idx": 32}], "ReferenceToBib": [{"begin": 2073, "end": 2077, "target": "#b44", "idx": 0}, {"begin": 2111, "end": 2115, "target": "#b9", "idx": 1}, {"begin": 2123, "end": 2127, "target": "#b53", "idx": 2}, {"begin": 2140, "end": 2144, "target": "#b29", "idx": 3}, {"begin": 2879, "end": 2883, "target": "#b13", "idx": 4}, {"begin": 2884, "end": 2887, "target": "#b32", "idx": 5}, {"begin": 2910, "end": 2914, "target": "#b23", "idx": 6}, {"begin": 2929, "end": 2933, "target": "#b37", "idx": 7}, {"begin": 2934, "end": 2937, "target": "#b55", "idx": 8}, {"begin": 2966, "end": 2969, "target": "#b4", "idx": 9}, {"begin": 2970, "end": 2973, "target": "#b15", "idx": 10}, {"begin": 2974, "end": 2977, "target": "#b17", "idx": 11}, {"begin": 2978, "end": 2981, "target": "#b36", "idx": 12}, {"begin": 2982, "end": 2985, "target": "#b39", "idx": 13}, {"begin": 2986, "end": 2989, "target": "#b40", "idx": 14}, {"begin": 3014, "end": 3017, "target": "#b4", "idx": 15}, {"begin": 3018, "end": 3021, "target": "#b13", "idx": 16}, {"begin": 3022, "end": 3025, "target": "#b23", "idx": 17}, {"begin": 3026, "end": 3029, "target": "#b32", "idx": 18}, {"begin": 3030, "end": 3033, "target": "#b36", "idx": 19}, {"begin": 3034, "end": 3037, "target": "#b40", "idx": 20}, {"begin": 3038, "end": 3041, "target": "#b42", "idx": 21}, {"begin": 3042, "end": 3045, "target": "#b55", "idx": 22}, {"begin": 3592, "end": 3595, "target": "#b4", "idx": 23}, {"begin": 3596, "end": 3599, "target": "#b15", "idx": 24}, {"begin": 4410, "end": 4414, "target": "#b44", "idx": 25}, {"begin": 4438, "end": 4442, "target": "#b19", "idx": 26}, {"begin": 4784, "end": 4787, "target": "#b0", "idx": 27}, {"begin": 4788, "end": 4791, "target": "#b1", "idx": 28}, {"begin": 4792, "end": 4795, "target": "#b2", "idx": 29}, {"begin": 4796, "end": 4799, "target": "#b54", "idx": 30}, {"begin": 5325, "end": 5328, "target": "#b0", "idx": 31}, {"begin": 5356, "end": 5360, "target": "#b24", "idx": 32}, {"begin": 5681, "end": 5684, "target": "#b1", "idx": 33}, {"begin": 6228, "end": 6231, "target": "#b2", "idx": 34}, {"begin": 6497, "end": 6501, "target": "#b45", "idx": 35}, {"begin": 6512, "end": 6516, "target": "#b34", "idx": 36}, {"begin": 6517, "end": 6520, "target": "#b35", "idx": 37}, {"begin": 7438, "end": 7442, "target": "#b9", "idx": 38}, {"begin": 7616, "end": 7654, "idx": 39}, {"begin": 7963, "end": 7967, "target": "#b15", "idx": 40}, {"begin": 8191, "end": 8195, "target": "#b12", "idx": 41}, {"begin": 8910, "end": 8913, "target": "#b1", "idx": 42}, {"begin": 8914, "end": 8916, "target": "#b2", "idx": 43}, {"begin": 8917, "end": 8920, "target": "#b16", "idx": 44}, {"begin": 8921, "end": 8924, "target": "#b18", "idx": 45}, {"begin": 8925, "end": 8928, "target": "#b25", "idx": 46}, {"begin": 8929, "end": 8932, "target": "#b27", "idx": 47}, {"begin": 8933, "end": 8936, "target": "#b28", "idx": 48}, {"begin": 8937, "end": 8940, "target": "#b42", "idx": 49}, {"begin": 8941, "end": 8944, "target": "#b46", "idx": 50}, {"begin": 8945, "end": 8948, "target": "#b54", "idx": 51}, {"begin": 9090, "end": 9094, "target": "#b16", "idx": 52}, {"begin": 9118, "end": 9121, "target": "#b1", "idx": 53}, {"begin": 9122, "end": 9125, "target": "#b54", "idx": 54}, {"begin": 9153, "end": 9156, "target": "#b1", "idx": 55}, {"begin": 9168, "end": 9172, "target": "#b54", "idx": 56}, {"begin": 9377, "end": 9381, "target": "#b46", "idx": 57}, {"begin": 9821, "end": 9824, "target": "#b0", "idx": 58}, {"begin": 9825, "end": 9827, "target": "#b1", "idx": 59}, {"begin": 9828, "end": 9830, "target": "#b6", "idx": 60}, {"begin": 9831, "end": 9834, "target": "#b14", "idx": 61}, {"begin": 10090, "end": 10094, "target": "#b42", "idx": 62}, {"begin": 10263, "end": 10267, "target": "#b42", "idx": 63}, {"begin": 12036, "end": 12040, "target": "#b11", "idx": 64}, {"begin": 12592, "end": 12595, "target": "#b0", "idx": 65}, {"begin": 12596, "end": 12598, "target": "#b1", "idx": 66}, {"begin": 13120, "end": 13124, "target": "#b19", "idx": 67}, {"begin": 13125, "end": 13128, "target": "#b20", "idx": 68}, {"begin": 14346, "end": 14349, "target": "#b6", "idx": 69}, {"begin": 14350, "end": 14353, "target": "#b26", "idx": 70}, {"begin": 14354, "end": 14357, "target": "#b30", "idx": 71}, {"begin": 14381, "end": 14385, "target": "#b24", "idx": 72}, {"begin": 15411, "end": 15414, "target": "#b0", "idx": 73}, {"begin": 15427, "end": 15430, "target": "#b1", "idx": 74}, {"begin": 15443, "end": 15447, "target": "#b14", "idx": 75}, {"begin": 15537, "end": 15541, "target": "#b24", "idx": 76}, {"begin": 18391, "end": 18394, "target": "#b2", "idx": 77}, {"begin": 18807, "end": 18811, "target": "#b9", "idx": 78}, {"begin": 19309, "end": 19313, "target": "#b45", "idx": 79}, {"begin": 19335, "end": 19339, "target": "#b34", "idx": 80}, {"begin": 19340, "end": 19343, "target": "#b35", "idx": 81}, {"begin": 19378, "end": 19382, "target": "#b49", "idx": 82}, {"begin": 19390, "end": 19394, "target": "#b38", "idx": 83}, {"begin": 19401, "end": 19405, "target": "#b10", "idx": 84}, {"begin": 19413, "end": 19416, "target": "#b3", "idx": 85}, {"begin": 19422, "end": 19425, "target": "#b5", "idx": 86}, {"begin": 19432, "end": 19436, "target": "#b50", "idx": 87}, {"begin": 19443, "end": 19447, "target": "#b35", "idx": 88}, {"begin": 19456, "end": 19459, "target": "#b8", "idx": 89}, {"begin": 19489, "end": 19493, "target": "#b15", "idx": 90}, {"begin": 19494, "end": 19497, "target": "#b36", "idx": 91}, {"begin": 19498, "end": 19501, "target": "#b43", "idx": 92}, {"begin": 19502, "end": 19505, "target": "#b48", "idx": 93}, {"begin": 19962, "end": 19966, "target": "#b15", "idx": 94}, {"begin": 19967, "end": 19970, "target": "#b17", "idx": 95}, {"begin": 19971, "end": 19974, "target": "#b36", "idx": 96}, {"begin": 19975, "end": 19978, "target": "#b40", "idx": 97}, {"begin": 19979, "end": 19982, "target": "#b41", "idx": 98}, {"begin": 19983, "end": 19986, "target": "#b48", "idx": 99}, {"begin": 20176, "end": 20180, "target": "#b29", "idx": 100}, {"begin": 20843, "end": 20847, "target": "#b36", "idx": 101}, {"begin": 20922, "end": 20926, "target": "#b40", "idx": 102}, {"begin": 21060, "end": 21064, "target": "#b52", "idx": 103}, {"begin": 21123, "end": 21127, "target": "#b48", "idx": 104}, {"begin": 21202, "end": 21206, "target": "#b43", "idx": 105}, {"begin": 21290, "end": 21294, "target": "#b15", "idx": 106}, {"begin": 21396, "end": 21400, "target": "#b17", "idx": 107}, {"begin": 21656, "end": 21660, "target": "#b41", "idx": 108}, {"begin": 21938, "end": 21942, "target": "#b9", "idx": 109}, {"begin": 21943, "end": 21946, "target": "#b36", "idx": 110}, {"begin": 21947, "end": 21950, "target": "#b40", "idx": 111}, {"begin": 22144, "end": 22148, "target": "#b15", "idx": 112}, {"begin": 22149, "end": 22152, "target": "#b17", "idx": 113}, {"begin": 22997, "end": 23000, "target": "#b4", "idx": 114}, {"begin": 24719, "end": 24723, "target": "#b31", "idx": 115}, {"begin": 24724, "end": 24727, "target": "#b33", "idx": 116}, {"begin": 27515, "end": 27519, "target": "#b35", "idx": 117}, {"begin": 27535, "end": 27539, "target": "#b34", "idx": 118}, {"begin": 27812, "end": 27816, "target": "#b29", "idx": 119}, {"begin": 29141, "end": 29145, "target": "#b29", "idx": 120}, {"begin": 29507, "end": 29511, "target": "#b41", "idx": 121}, {"begin": 30677, "end": 30681, "target": "#b29", "idx": 122}], "ReferenceString": [{"begin": 30836, "end": 31043, "id": "b0", "idx": 0}, {"begin": 31045, "end": 31249, "id": "b1", "idx": 1}, {"begin": 31253, "end": 31429, "id": "b2", "idx": 2}, {"begin": 31433, "end": 31707, "id": "b3", "idx": 3}, {"begin": 31711, "end": 32196, "id": "b4", "idx": 4}, {"begin": 32200, "end": 32282, "id": "b5", "idx": 5}, {"begin": 32286, "end": 32470, "id": "b6", "idx": 6}, {"begin": 32474, "end": 32683, "id": "b7", "idx": 7}, {"begin": 32687, "end": 32939, "id": "b8", "idx": 8}, {"begin": 32943, "end": 33392, "id": "b9", "idx": 9}, {"begin": 33396, "end": 33579, "id": "b10", "idx": 10}, {"begin": 33583, "end": 33719, "id": "b11", "idx": 11}, {"begin": 33723, "end": 33895, "id": "b12", "idx": 12}, {"begin": 33899, "end": 34113, "id": "b13", "idx": 13}, {"begin": 34117, "end": 34342, "id": "b14", "idx": 14}, {"begin": 34346, "end": 34536, "id": "b15", "idx": 15}, {"begin": 34540, "end": 34800, "id": "b16", "idx": 16}, {"begin": 34804, "end": 35079, "id": "b17", "idx": 17}, {"begin": 35083, "end": 35300, "id": "b18", "idx": 18}, {"begin": 35304, "end": 35483, "id": "b19", "idx": 19}, {"begin": 35487, "end": 35737, "id": "b20", "idx": 20}, {"begin": 35741, "end": 35842, "id": "b21", "idx": 21}, {"begin": 35846, "end": 36055, "id": "b22", "idx": 22}, {"begin": 36059, "end": 36293, "id": "b23", "idx": 23}, {"begin": 36297, "end": 36580, "id": "b24", "idx": 24}, {"begin": 36584, "end": 36848, "id": "b25", "idx": 25}, {"begin": 36852, "end": 37096, "id": "b26", "idx": 26}, {"begin": 37100, "end": 37375, "id": "b27", "idx": 27}, {"begin": 37379, "end": 37530, "id": "b28", "idx": 28}, {"begin": 37534, "end": 37772, "id": "b29", "idx": 29}, {"begin": 37776, "end": 37897, "id": "b30", "idx": 30}, {"begin": 37901, "end": 38054, "id": "b31", "idx": 31}, {"begin": 38058, "end": 38168, "id": "b32", "idx": 32}, {"begin": 38172, "end": 38395, "id": "b33", "idx": 33}, {"begin": 38399, "end": 38636, "id": "b34", "idx": 34}, {"begin": 38640, "end": 38794, "id": "b35", "idx": 35}, {"begin": 38798, "end": 38987, "id": "b36", "idx": 36}, {"begin": 38991, "end": 39189, "id": "b37", "idx": 37}, {"begin": 39193, "end": 39411, "id": "b38", "idx": 38}, {"begin": 39415, "end": 39622, "id": "b39", "idx": 39}, {"begin": 39626, "end": 39930, "id": "b40", "idx": 40}, {"begin": 39934, "end": 40195, "id": "b41", "idx": 41}, {"begin": 40199, "end": 40410, "id": "b42", "idx": 42}, {"begin": 40414, "end": 40606, "id": "b43", "idx": 43}, {"begin": 40610, "end": 40832, "id": "b44", "idx": 44}, {"begin": 40836, "end": 41077, "id": "b45", "idx": 45}, {"begin": 41081, "end": 41351, "id": "b46", "idx": 46}, {"begin": 41355, "end": 41541, "id": "b47", "idx": 47}, {"begin": 41545, "end": 42082, "id": "b48", "idx": 48}, {"begin": 42086, "end": 42265, "id": "b49", "idx": 49}, {"begin": 42269, "end": 42425, "id": "b50", "idx": 50}, {"begin": 42429, "end": 42623, "id": "b51", "idx": 51}, {"begin": 42627, "end": 42872, "id": "b52", "idx": 52}, {"begin": 42876, "end": 43112, "id": "b53", "idx": 53}, {"begin": 43116, "end": 43406, "id": "b54", "idx": 54}, {"begin": 43410, "end": 43546, "id": "b55", "idx": 55}], "Sentence": [{"begin": 102, "end": 372, "idx": 0}, {"begin": 373, "end": 479, "idx": 1}, {"begin": 480, "end": 911, "idx": 2}, {"begin": 912, "end": 1034, "idx": 3}, {"begin": 1035, "end": 1212, "idx": 4}, {"begin": 1213, "end": 1390, "idx": 5}, {"begin": 1391, "end": 1463, "idx": 6}, {"begin": 1464, "end": 1577, "idx": 7}, {"begin": 1578, "end": 1730, "idx": 8}, {"begin": 1731, "end": 2023, "idx": 9}, {"begin": 2049, "end": 2243, "idx": 10}, {"begin": 2244, "end": 2402, "idx": 11}, {"begin": 2403, "end": 2684, "idx": 12}, {"begin": 2685, "end": 2766, "idx": 13}, {"begin": 2767, "end": 2990, "idx": 14}, {"begin": 2991, "end": 3224, "idx": 15}, {"begin": 3225, "end": 3375, "idx": 16}, {"begin": 3376, "end": 3550, "idx": 17}, {"begin": 3551, "end": 3679, "idx": 18}, {"begin": 3680, "end": 3752, "idx": 19}, {"begin": 3753, "end": 3902, "idx": 20}, {"begin": 3903, "end": 4217, "idx": 21}, {"begin": 4218, "end": 4339, "idx": 22}, {"begin": 4340, "end": 4542, "idx": 23}, {"begin": 4543, "end": 4800, "idx": 24}, {"begin": 4801, "end": 5016, "idx": 25}, {"begin": 5017, "end": 5141, "idx": 26}, {"begin": 5142, "end": 5462, "idx": 27}, {"begin": 5463, "end": 5679, "idx": 28}, {"begin": 5680, "end": 5968, "idx": 29}, {"begin": 5969, "end": 6226, "idx": 30}, {"begin": 6227, "end": 6438, "idx": 31}, {"begin": 6439, "end": 6540, "idx": 32}, {"begin": 6541, "end": 6691, "idx": 33}, {"begin": 6692, "end": 6751, "idx": 34}, {"begin": 6752, "end": 7066, "idx": 35}, {"begin": 7067, "end": 7220, "idx": 36}, {"begin": 7221, "end": 7340, "idx": 37}, {"begin": 7384, "end": 7600, "idx": 38}, {"begin": 7601, "end": 7839, "idx": 39}, {"begin": 7840, "end": 7949, "idx": 40}, {"begin": 7950, "end": 8097, "idx": 41}, {"begin": 8098, "end": 8178, "idx": 42}, {"begin": 8179, "end": 8352, "idx": 43}, {"begin": 8353, "end": 8438, "idx": 44}, {"begin": 8439, "end": 8799, "idx": 45}, {"begin": 8853, "end": 8949, "idx": 46}, {"begin": 8950, "end": 9127, "idx": 47}, {"begin": 9128, "end": 9372, "idx": 48}, {"begin": 9373, "end": 9471, "idx": 49}, {"begin": 9472, "end": 9521, "idx": 50}, {"begin": 9522, "end": 9727, "idx": 51}, {"begin": 9728, "end": 9879, "idx": 52}, {"begin": 9880, "end": 10077, "idx": 53}, {"begin": 10078, "end": 10235, "idx": 54}, {"begin": 10236, "end": 10586, "idx": 55}, {"begin": 10587, "end": 10762, "idx": 56}, {"begin": 10773, "end": 10900, "idx": 57}, {"begin": 10901, "end": 11270, "idx": 58}, {"begin": 11271, "end": 11422, "idx": 59}, {"begin": 11423, "end": 11754, "idx": 60}, {"begin": 11780, "end": 11992, "idx": 61}, {"begin": 11993, "end": 12233, "idx": 62}, {"begin": 12234, "end": 12313, "idx": 63}, {"begin": 12314, "end": 12530, "idx": 64}, {"begin": 12531, "end": 12702, "idx": 65}, {"begin": 12703, "end": 12971, "idx": 66}, {"begin": 12972, "end": 13129, "idx": 67}, {"begin": 13130, "end": 13222, "idx": 68}, {"begin": 13223, "end": 13348, "idx": 69}, {"begin": 13349, "end": 13496, "idx": 70}, {"begin": 13497, "end": 13582, "idx": 71}, {"begin": 13583, "end": 13885, "idx": 72}, {"begin": 13886, "end": 13971, "idx": 73}, {"begin": 14014, "end": 14051, "idx": 74}, {"begin": 14052, "end": 14156, "idx": 75}, {"begin": 14157, "end": 14358, "idx": 76}, {"begin": 14359, "end": 14541, "idx": 77}, {"begin": 14542, "end": 14657, "idx": 78}, {"begin": 14658, "end": 14903, "idx": 79}, {"begin": 14904, "end": 15089, "idx": 80}, {"begin": 15129, "end": 15306, "idx": 81}, {"begin": 15307, "end": 15511, "idx": 82}, {"begin": 15512, "end": 15703, "idx": 83}, {"begin": 15704, "end": 15943, "idx": 84}, {"begin": 15944, "end": 16156, "idx": 85}, {"begin": 16157, "end": 16357, "idx": 86}, {"begin": 16380, "end": 16452, "idx": 87}, {"begin": 16453, "end": 16780, "idx": 88}, {"begin": 16781, "end": 16921, "idx": 89}, {"begin": 16922, "end": 17103, "idx": 90}, {"begin": 17125, "end": 17300, "idx": 91}, {"begin": 17301, "end": 17515, "idx": 92}, {"begin": 17516, "end": 17742, "idx": 93}, {"begin": 17743, "end": 17908, "idx": 94}, {"begin": 17909, "end": 18048, "idx": 95}, {"begin": 18049, "end": 18156, "idx": 96}, {"begin": 18157, "end": 18419, "idx": 97}, {"begin": 18420, "end": 18568, "idx": 98}, {"begin": 18569, "end": 18664, "idx": 99}, {"begin": 18702, "end": 18707, "idx": 100}, {"begin": 18708, "end": 18732, "idx": 101}, {"begin": 18733, "end": 19117, "idx": 102}, {"begin": 19136, "end": 19162, "idx": 103}, {"begin": 19163, "end": 19344, "idx": 104}, {"begin": 19345, "end": 19460, "idx": 105}, {"begin": 19461, "end": 19506, "idx": 106}, {"begin": 19507, "end": 19648, "idx": 107}, {"begin": 19649, "end": 20046, "idx": 108}, {"begin": 20047, "end": 20181, "idx": 109}, {"begin": 20216, "end": 20564, "idx": 110}, {"begin": 20565, "end": 21383, "idx": 111}, {"begin": 21384, "end": 21557, "idx": 112}, {"begin": 21558, "end": 21621, "idx": 113}, {"begin": 21622, "end": 21998, "idx": 114}, {"begin": 21999, "end": 22060, "idx": 115}, {"begin": 22061, "end": 22173, "idx": 116}, {"begin": 22174, "end": 22248, "idx": 117}, {"begin": 22249, "end": 22359, "idx": 118}, {"begin": 22360, "end": 22436, "idx": 119}, {"begin": 22437, "end": 22705, "idx": 120}, {"begin": 22713, "end": 22754, "idx": 121}, {"begin": 22755, "end": 22854, "idx": 122}, {"begin": 22855, "end": 22975, "idx": 123}, {"begin": 22976, "end": 23272, "idx": 124}, {"begin": 23273, "end": 23430, "idx": 125}, {"begin": 23431, "end": 23588, "idx": 126}, {"begin": 23589, "end": 23807, "idx": 127}, {"begin": 23808, "end": 23923, "idx": 128}, {"begin": 23924, "end": 23974, "idx": 129}, {"begin": 23975, "end": 24068, "idx": 130}, {"begin": 24069, "end": 24157, "idx": 131}, {"begin": 24158, "end": 24378, "idx": 132}, {"begin": 24379, "end": 24548, "idx": 133}, {"begin": 24566, "end": 24599, "idx": 134}, {"begin": 24600, "end": 24753, "idx": 135}, {"begin": 24754, "end": 24870, "idx": 136}, {"begin": 24871, "end": 25102, "idx": 137}, {"begin": 25103, "end": 25260, "idx": 138}, {"begin": 25261, "end": 25305, "idx": 139}, {"begin": 25306, "end": 25375, "idx": 140}, {"begin": 25376, "end": 25456, "idx": 141}, {"begin": 25457, "end": 25642, "idx": 142}, {"begin": 25643, "end": 25819, "idx": 143}, {"begin": 25820, "end": 25913, "idx": 144}, {"begin": 25914, "end": 26010, "idx": 145}, {"begin": 26011, "end": 26092, "idx": 146}, {"begin": 26093, "end": 26243, "idx": 147}, {"begin": 26244, "end": 26336, "idx": 148}, {"begin": 26344, "end": 26385, "idx": 149}, {"begin": 26386, "end": 26613, "idx": 150}, {"begin": 26614, "end": 26651, "idx": 151}, {"begin": 26652, "end": 27094, "idx": 152}, {"begin": 27095, "end": 27215, "idx": 153}, {"begin": 27216, "end": 27337, "idx": 154}, {"begin": 27338, "end": 27430, "idx": 155}, {"begin": 27466, "end": 27622, "idx": 156}, {"begin": 27623, "end": 27709, "idx": 157}, {"begin": 27710, "end": 27782, "idx": 158}, {"begin": 27783, "end": 27919, "idx": 159}, {"begin": 27920, "end": 27953, "idx": 160}, {"begin": 27954, "end": 28039, "idx": 161}, {"begin": 28054, "end": 28163, "idx": 162}, {"begin": 28164, "end": 28423, "idx": 163}, {"begin": 28424, "end": 28589, "idx": 164}, {"begin": 28590, "end": 28785, "idx": 165}, {"begin": 28786, "end": 28926, "idx": 166}, {"begin": 28927, "end": 29035, "idx": 167}, {"begin": 29036, "end": 29089, "idx": 168}, {"begin": 29090, "end": 29295, "idx": 169}, {"begin": 29330, "end": 29464, "idx": 170}, {"begin": 29465, "end": 29683, "idx": 171}, {"begin": 29684, "end": 29734, "idx": 172}, {"begin": 29766, "end": 29876, "idx": 173}, {"begin": 29877, "end": 30091, "idx": 174}, {"begin": 30092, "end": 30133, "idx": 175}], "ReferenceToFigure": [{"begin": 12524, "end": 12525, "target": "#fig_1", "idx": 0}, {"begin": 13969, "end": 13970, "target": "#fig_0", "idx": 1}, {"begin": 14744, "end": 14745, "target": "#fig_1", "idx": 2}, {"begin": 16355, "end": 16356, "target": "#fig_1", "idx": 3}, {"begin": 23972, "end": 23973, "target": "#fig_3", "idx": 4}], "Abstract": [{"begin": 92, "end": 2023, "idx": 0}], "SectionFootnote": [{"begin": 30135, "end": 30819, "idx": 0}], "Footnote": [{"begin": 30146, "end": 30610, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 30611, "end": 30720, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 30721, "end": 30819, "id": "foot_2", "n": "3", "idx": 2}]}}