{"text": "Multi-class Classification from Multiple Unlabeled Datasets with Partial Risk Regularization\n\nAbstract:\nRecent years have witnessed a great success of supervised deep learning, where predictive models were trained from a large amount of fully labeled data. However, in practice, labeling such big data can be very costly and may not even be possible for privacy reasons. Therefore, in this paper, we aim to learn an accurate classifier without any class labels. More specifically, we consider the case where multiple sets of unlabeled data and only their class priors, i.e., the proportions of each class, are available. Under this problem setup, we first derive an unbiased estimator of the classification risk that can be estimated from the given unlabeled sets and theoretically analyze the generalization error of the learned classifier. We then find that the classifier obtained as such tends to cause overfitting as its empirical risks go negative during training. To prevent overfitting, we further propose a partial risk regularization that maintains the partial risks with respect to unlabeled datasets and classes to certain levels. Experiments demonstrate that our method effectively mitigates overfitting and outperforms state-of-the-art methods for learning from multiple unlabeled sets.\n\nMain:\n\n\n\n1. Introduction\nSupervised deep learning approaches are very successful but data-hungry (Goodfellow et al., 2016). As it is very expensive and time-consuming to collect full labels for big data, it is desirable for machine learning techniques to work with weaker forms of supervision (Zhou, 2018; Sugiyama et al., 2022).\nIn this paper, we consider a challenging weakly supervised multi-class classification problem: instead of fully labeled data, we are given multiple sets of only unlabeled (U) data for classifier training. We assume that each available unlabeled dataset varies only in class-prior probabilities, and these class priors are known (Lu et al., 2019 (Lu et al., , 2020)). Our problem setting traces back to a classical problem of learning with label proportions (LLP) (Quadrianto et al., 2009); however, there are two key differences between our work and previous LLP studies. First, most of the existing LLP papers consider U sets sampled from the identical distribution (Liu et al., 2019; Tsai and Lin, 2020); while in our case, the U sets are generated from distributions with different class priors. Second, for the solution, the majority of deep LLP methods are based on the empirical proportion risk minimization (EPRM) (Yu et al., 2013 (Yu et al., , 2015) ) principle, which aims at predicting accurate label proportions for the U sets; while our method is based on the ordinary empirical risk minimization (ERM) (Vapnik, 1998) principle, which is superior to EPRM as the consistency of learning can be guaranteed.\nSuch a framework of learning from multiple unlabeled datasets can find applications in various real-world scenarios. For example, in politics, predicting the demographic information of users in social networks is critical to policy making (Culotta et al., 2015); however, labeling individual users according to their demographic information may violate data privacy, and thus the traditional supervised learning methods cannot be applied. Fortunately, multiple unlabeled datasets can be obtained from different time points or geographic locations, and their class priors can be collected from pre-existing census data, which suffices for our learning framework.\nUnder this problem setup, the first challenge is to estimate the classification risk from only unlabeled datasets and their class priors, since the typical ERM approach used in supervised learning cannot be directly applied to this unlabeled setting. A promising direction to solve the problem is risk rewriting (Sugiyama et al., 2022), i.e., rewrite the classification risk into an equivalent form that can be estimated from the given data. Existing risk rewriting results on learning from multiple U sets focused on binary classification (Lu et al., 2019 (Lu et al., , 2020)). In this paper, we extend them and derive an unbiased risk estimator for multi-class classification from multiple U sets. In this way, ERM is enabled and statistical consistency (Mohri et al., 2018) is guaranteed, i.e., the risk of the learned classifier converges to the risk of the optimal classifier in the model, as the amount of training data approaches infinity.\nHowever, in practice, with only finite training data, we found that the risk rewriting method suffers from severe overfitting, which is another challenge we are facing. For tackling overfitting, a common practice is to use regularization techniques to limit the flexibility of the model. Unfortunately, we also observed that these general-propose regularization techniques cannot mitigate this overfitting. Based on another observation that the empirical training risks can go negative during training, we conjecture that the overfitting is due to the negative terms included in the rewritten risk function: when unbounded loss functions, e.g., the cross-entropy loss, are used, the empirical training risk can even diverge to negative infinity. Therefore, it is necessary to fix this negative risk issue.\nRecently, Ishida et al. (2020) proposed the flooding regularization to combat overfitting in supervised learning, which intentionally prevents further reduction of the empirical training risk when it reaches a reasonably small value called the flood level. We could naively apply the flooding regularization in our unlabeled setting and observed that overfitting can be mitigated to some extent. However, its effect was still on the same level as naive early stopping (Morgan and Bourlard, 1989).\nTo further improve the performance, we propose a fine-grained partial risk regularization inspired by the flooding method. Specifically, we decompose the rewritten risk into partial risks, i.e., the risks regarding all data in each U set being in same class, and then maintain each partial risk to a certain level as flooding does. We find that under some idealistic conditions, the optimal levels can be determined by the class priors, which serves as a guidance for selecting the hyper-parameters of these levels. Experimental results show that our proposed partial risk regularization approach can successfully mitigate overfitting and outperform existing methods for learning from multiple U sets.\nThe rest of the paper is organized as follows. In Section 2, we formulate the problem of learning from multiple U sets, and in Section 3, we propose an ERM method and analyze its estimation error. The partial risk regularizer is proposed in Section 4, and experimental results are discussed in Section 5. Finally, conclusions are given in Section 6.\n\n2. From Supervised Learning to Learning from Multiple Unlabeled Datasets\nTo begin with, let us consider a multi-class classification task with the input feature space X \u2282 R d and the output label space Y = {1, 2, . . . , K} =: [K], where d is the input dimension and K is the number of classes. Let x \u2208 X and y \u2208 Y be the input and output random variables following an underlying joint distribution with density p(x, y), which can be decomposed using the class priors {\u03c0 k = Pr(y = k)} K k=1 and the class-conditional densities{p k (x) = p(x | y = k)} K k=1 as p(x, y) = K k=1 \u03c0 k p k (x)\n. The goal of multi-class classification is to learn a classifier g : X \u2192 R K that minimizes the following classification error, also known as the risk :R(g) := E (x,y)\u223cp(x,y) [ (g(x), y)] ,\nwhere E denotes the expectation, and : R K \u00d7 Y \u2192 R + denotes a real-valued loss function that measures the discrepancy between the true label y and its prediction g(x). Typically, the predicted label is given by y = arg max k\u2208[K] (g(x)) k , where (g(x)) k is the k-th element of g(x). Note that when (1) is used for evaluation, is often chosen as the zero-one loss 01 (g(x), y) = I( y = y), where I is the indicator function; when (1) is used for training, 01 is replaced by a surrogate loss due to its difficulty for optimization, e.g., by the softmax cross-entropy loss.\nAs the density p(x, y) in (1) remains unknown, in traditional supervised learning, we normally require a vast amount of fully labeled training data D := {(x i , y i )} n i=1 (where n is the training sample size and is assumed to be a sufficiently large number) drawn independently from p(x, y) to learn an accurate classifier (Wahba, 1990; Vapnik, 1998; Hastie et al., 2009; Sugiyama, 2015). With such supervised data, empirical risk minimization (ERM) (Vapnik, 1998) is a common practice that approximates (1) by replacing the expectation with the average over the training data: R(g) = 1 n n i=1 [ (g(x i ), y i )]; then minimizing the empirical risk R(g) over a class of functions, also known as the model G: g = arg min g\u2208G R(g).\nHowever, in practice, collecting massive fully labeled data may be difficult due to the high labeling cost. In this paper, we consider a challenging setting of learning from multiple unlabeled datasets where no explicit labels are given. More specifically, assume that we have access to M sets of unlabeled samples D U := M m=1 X m , and each unlabeled setX m = {x m,i } nm i=1 is a collection of n m data points drawn from a mixture of class-conditional densities {p k = p(x | y = k)} K k=1 : x m,i i.i.d. \u223c q m (x) = K k=1 \u03b8 m,k p k ,\nwhere \u03b8 m,k \u2265 0 denotes the k-th class prior of the m-th unlabeled set. The class priors are assumed to form a full column rank matrix \u0398 := (\u03b8 m,k ) \u2208 R M \u00d7K with the constraint that K k=1 \u03b8 m,k = 1. Through the paper, we assume that the class priors of each unlabeled set are given, including training class priors \u0398 and test class priors {\u03c0 k } K k=1 . 1 Our goal is the same as standard supervised learning where we wish to learn a classifier g that generalizes well with respect to p(x, y), despite the fact that it is unobserved and we can only access unlabeled training sets {X m } M m=1 .\n\n3. Learning from Multiple Unlabeled Datasets\nIn this section, we derive an unbiased estimator of the classification risk in learning from multiple unlabeled datasets and analyze its estimation error.\n\n3.1. Estimation of the Multi-Class Risk\nUnbiased risk estimator based approaches have shown promising results in many weaklysupervised learning scenarios (du Plessis et al., 2014; Du Plessis et al., 2015; Ishida et al., 2017; Lu et al., 2019; Van Rooyen and Williamson, 2017), most of which mainly focused on binary classification problems. In what follows, we extend the results of unbiased risk estimation in their papers to multi-class classification from multiple unlabeled datasets.\nThe key technique for deriving unbiased risk estimators is risk rewriting, which enables the risk to be calculated only from observable distributions. In the context of our setting, risk rewriting means to evaluate the risk using {q m } M m=1 instead of unknown {p k } K k=1 . Formally, we extend the definition of rewritability of the risk introduced in Lu et al. (2019) to our setting and show that R(g) is rewritable as follows.\nDefinition 1 We say that R(g) is rewritable given {q m } M m=1 , if and only if there exist constants {w m,k } m\u2208[M ],k\u2208 [K] such that for any model g it holds thatR(g) = M m=1 E x\u223cqm \u00af m (g(x)) ,\nwhere\u00af m (z) = K k=1 w m,k (z, k).\nTheorem 2 Assume that \u0398 has full column rank. Then R(g) is rewritable by lettingW = (\u03a0\u0398 \u2020 ) ,\nwhereW := (w m,k ) \u2208 R M \u00d7K , \u03a0 := diag{\u03c0 1 , \u03c0 2 , .\n. . , \u03c0 K }, and \u2020 denotes the Moore-Penrose generalized inverse.\nBased on Theorem 2, we can immediately obtain a rewritten risk function asR U (g) = M m=1 K k=1 w m,k E qm [ (g(x), k)] ,\nand its unbiased risk estimatorR U (g) = M m=1 K k=1 w m,k n m nm i=1 (g(x m,i ), k),\nwhere w m,k 's are given by (4). Then, any off-the-shelf optimization algorithms, such as stochastic gradient descent (Robbins and Monro, 1951) and Adam (Kingma and Ba, 2015), can be applied for minimizing the empirical risk given by (6).\n\n3.2. Theoretical Analysis\nHere, we establish an estimation error bound for the unbiased risk estimator.Theorem 3 Let R nm (H k ) be the Rademacher complexity of H k over q m (x) (Mohri et al., 2018), i.e., R nm (H k ) = E qm(x) E \u03c3 sup h\u2208H k 1 nm nm i=1 \u03c3 i h(x i )\n, where \u03c3 i 's are independent uniform random variables taking values in {+1, \u22121} andH k = {h : x \u2192 (g(x)) k |g \u2208 G}.\nAssume the loss function (g(x), y) is L-Lipschitz with respect to g(x) for all y \u2208 Y and upper-bounded by C , i.e., C = sup g\u2208G,x\u2208X ,y\u2208Y (g(x), y). g U := arg min g\u2208G R U (g) is the risk of the learned classifier and g * := arg min g\u2208G R(g) is the risk of the optimal classifier in the given model class. Then, for any \u03b4 > 0, with probability at least1 \u2212 \u03b4, R( g U ) \u2212 R(g * ) \u2264 4 \u221a 2LKC w M m=1 K k=1 R nm (H k ) + 2C w KC M ln 2 \u03b4 2N min ,\nwhereC w = max m\u2208[M ],k\u2208[K] |w m,k | and N min = min m\u2208[M ] n m .\nThe proof of Theorem 3 is provided in Appendix B. Generally, R nm (H k ) can be bounded by C G / \u221a N min for a positive constant C G (Golowich et al., 2018; Xia et al., 2019). The Theorem 3 shows that as N min \u2192 \u221e, g U converges to g * .\n\n4. Partial Risk Regularization\nIn this section, we empirically analyze the unbiased risk estimator in a finite-sample case, and then propose partial risk regularization for further boosting its performance.\n\n4.1. Overfitting of the Unbiased Risk Estimator\nSo far, we have obtained an unbiased risk estimator for learning from unlabeled datasets and derived an estimation error bound which guarantees the consistency of learning. However, we find that in practice, with only finite training data, the unbiased method tends to overfit especially when flexible models are used (see Figure 1 (a)).\nFor reducing overfitting, a common method is to use regularization techniques to limit the flexibility of the model. Popular choices of general-propose regularization techniques are dropout and weight decay (Goodfellow et al., 2016). In Figure 1 the results of applying them on MNIST. For the dropout experiment, we fixed the weight decay parameter to be 1 \u00d7 10 \u22125 and changed the dropout probability from 0 to 0.8. For the weight decay experiment, we fixed the dropout probability to be 0 and changed the weight decay parameter from 1 \u00d7 10 \u22125 to 1 \u00d7 10 1 . The results show that general-propose regularization techniques cannot mitigate the overfitting of the unbiased risk estimation method in learning from multiple unlabeled datasets.\nTo further analyze this phenomenon, 3 let us take a close look at the unbiased risk estimator (6). We find that the coefficients w m,k 's given by Theorem 2 are not necessarily non-negative, some of which are exactly negative numbers. These negative terms may lead the empirical training risk to go negative during training (see Figure 1 (a)). When overparameterized models, e.g., deep neural networks, and unbounded loss functions, e.g., the cross-entropy loss, are used, the empirical training risk may even diverge to negative infinity; the model can memorize all training data and become overly confident. This may be the reasons why severe overfitting occurs here and why general-propose regularizations cannot solve it.\n\n4.2. Partial Risk Regularization\nBased on the analysis in Section 4.1, the issue that the empirical training risk goes negative and may even diverge should be fixed. Flooding (Ishida et al., 2020) is a regularization method dedicated to maintaining a fixed-level empirical training risk, which has been shown to mitigate overfitting effectively in the supervised setting. This method seems promising to solve our problem, however, it is based on supervised learning and needs full labels to calculate the risk. Thanks to the unbiased risk estimator derived in Section 3.1, we can straightforwardly apply the flooding method to it as follows:R U-flood (g) = |R U (g) \u2212 b| + b,\n3. This phenomenon is also observed in other weakly supervised learning settings using the unbiased risk estimator method, for example, in learning from positive and unlabeled data (Kiryo et al., 2017) and learning from two unlabeled datasets (Lu et al., 2020).\nwhere b \u2265 0 is a hyperparameter called the flood level. The experimental results in Section 5 show this simple method relieves overfitting to some extent, but only performs on the same level of early stopping.\nTo further boost performance, we decompose the total risk R U (g) into partial risks:R qm,k (g) := E qm [ (g(x), k)] ,\nthat is, the risk of considering all data in one unlabeled dataset to belong to the same class.\nThen we propose a fine-grained partial risk regularization (PRR) approach that maintains each partial risk in the rewritten risk function ( 5) to a certain level:R U-PRR (g) = \u03b1R U (g) + (1 \u2212 \u03b1)R reg (g),\nwhereR reg (g) = M m=1 K k=1 \u03bb m,k |R qm,k (g) \u2212 b m,k | ,\n0 \u2264 \u03b1 \u2264 1 and \u03bb m,k 's are trade-off parameters, and b m,k 's are the flood levels for the corresponding partial risks. To select the flood levels, we provide the following lemma which studies the relationship between them and the Bayes optimal classifier g where R(g) = inf g R(g). Lemma 4 Let R p,k (g) = E p [ (g(x), k)]\nwhere p is any probability density function, k = 1, . . . , K, and m = 1, . . . , M . We haveR qm,k (g ) = K i=1 \u03b8 m,i R p i ,k (g ). When all classes are separable, we have R 01 qm,k (g ) = 1 \u2212 \u03b8 m,k\n, where R 01 qm,k (g) denotes R qm,k (g) calculated by the zero-one loss.\nGuided by Lemma 4, in order to push g towards the optimal classifier g , we propose to choose b m,k = 1 \u2212 \u03b8 m,k as the flood level of R 01 qm,k (g), and replace R qm,k (g) by R 01 qm,k (g) in equation ( 11). Therefore, the empirical learning objective of PRR approach isR U-PRR (g) = \u03b1 R U (g) + (1 \u2212 \u03b1) R reg (g),\nwhereR reg (g) = M m=1 K k=1 \u03bb m,k R 01 qm,k (g) \u2212 (1 \u2212 \u03b8 m,k ) ,\nandR 01 qm,k (g) = 1 n m nm i=1 01 (g(x m,i ), k).\nIn the implementation (see Algorithm 1), we use the zero-one loss for determining the sign of the term in the absolute function and use a surrogate loss in calculating the gradients due to the difficulty of optimizing the zero-one loss. For dealing with absolute functions, when the term in it is greater than 0, we perform gradient descent as usual; when the term is less than 0, we perform gradient ascent instead and add a hyperparameter s GA in the gradient ascent process.\nIn addition, we have to determine hyperparameters \u03bb m,k in the regularization part. Here, we set them as \u03bb m,k = |w m,k | since choosing such a large number of hyperparameters separately is not straightforward. This choice means that the term that receives more attention in the unbiased risk estimator also receives more attention in partial risk regularization. Compute R U (g) followed Eq. ( 6)8:\nCompute R reg (g):9: if E X m [ 01 (g(x), k)] \u2265 (1 \u2212 \u03b8 m,k ) then 10: E X m [ (g(x), k)] \u2212 (1 \u2212 \u03b8 m,k ) \u2190 E X m [ (g(x), k)] \u2212 (1 \u2212 \u03b8 m,k ) 11: else 12: E X m [ (g(x), k)] \u2212 (1 \u2212 \u03b8 m,k ) \u2190 \u2212s GA (E X m [ (g(x), k)] \u2212 (1 \u2212 \u03b8 m,k )) 13:\nend if\n\n14:\nCompute R U-PRR (g) followed Eq. ( 12)\n15:\nCompute gradient \u03b3 R U-PRR (g)\n\n16:\nUpdate \u03b3 by A 17:\nend for 18: end for\n\n5. Experiments\nIn this section, we verify the effectiveness of the proposed method 4 on various datasets. We also show the robustness against noisy class priors of the proposed method.\n\n5.1. Experimental Setup\nWe describe the details of the experimental setup as follows.\nDatasets We used widely adopted benchmarks: Pendigits (Alimoglu and Alpaydin, 1996), USPS (Hull, 1994), MNIST (LeCun et al., 1998), Fashion-MNIST (Xiao et al., 2017), Kuzushiji-MNIST (Clanuwat et al., 2018) and, CIFAR-10 ( Krizhevsky et al., 2009). Table 1 briefly summaries the benchmark datasets. The m-th U set contains n m samples where its proportions \u03b8 m,k were randomly chosen from class k for k = 1, . . . , K. The size of each unlabeled dataset n m is set to 1/M of the total number of training datasets in all these settings.\nBaselines To better analyze the performance of the proposed unbiased risk estimator (6) (Unbiased ) and partial risk regularization approach (12) (U-PRR), we compared them with following baseline methods:\n\u2022 Biased proportion (Biased ): We consider the label of the largest category in each unlabeled dataset as the label of all samples in it, and perform supervised learning from such pseudo-label samples.\n\u2022 Proportion loss (Prop/Prop-CR): Prop uses class priors of each unlabeled dataset as weak supervision and aims to minimize the difference between true class priors and the predicted class priors (Yu et al., 2015). Prop-CR builds upon the proportion loss baseline by adding a consistency regularization term (Tsai and Lin, 2020).\n\u2022 Corrected unbiased risk estimator (U-correct): We apply a corrected risk estimation method (Lu et al., 2020), which aims to correct partial risks corresponding to each class to be non-negative, to the unbiased risk estimator. The learning objective isR U-correct (g) = K k=1 M m=1 w m,k 1 nm nm i=1 (g(x m,i ), k) .\n\u2022 Unbiased risk estimator with early stopping (U-stop): We apply the early stopping on the proposed unbiased risk estimator, and stop the training when the empirical risk goes negative since Lu et al. (2020) showed a high co-occurrence between overfitting and negative empirical risk.\n\u2022 Unbiased risk estimator with flooding (U-flood ) (Ishida et al., 2020) : We directly apply the flooding method to the unbiased risk estimator. The learning objective isR(g) = | R U (g) \u2212 b| + b.\nWe choose b from {0, 0.05, 0.1} and report the best test error.\nTraining Details The models are described in Table 1, where FC refers to the fully connected neural network and ResNet refers to the residual network (He et al., 2016). As a common practice, Adam (Kingma and Ba, 2015) with the cross-entropy loss was used for optimization. We ran each experiment five times, and we trained the model for 500 epochs on all datasets except that CIFAR-10 only used 200 epochs since 200 epochs were sufficient for convergence. The classification error rate was used for evaluating the test performance. The learning rates were chosen from {5 \u00d7 10 \u22125 , 1 \u00d7 10 \u22124 , 2 \u00d7 10 \u22124 , 5 \u00d7 10 \u22124 , 1 \u00d7 10 \u22123 } and the batch sizes were chosen from { n 500 , n 200 , n 100 , n 50 , n 20 , n 10 }, where n is the size of the training dataset. In the PRR method, \u03b1's were chosen from {0.1, 0.3, 0.5, 0.7, 0.9} and s GA 's were chosen from {0.1, 0.2, 0.5, 1, 2, 5, 10}. Note that for fair comparison, we used the same model and the same hyperparameters for the implementation of all methods in each benchmark. The implementation in our experiments was based on PyTorch 5 , and the experiments were conducted on an NVIDIA Tesla P100 GPU.\n\n5.2. Comparison with Baseline Methods\nHere, we report the final test error (Err) and the test error drop (\u2206 E ), which is the difference between the smallest test error among all training epochs and the test error at the end of training, for the baseline methods and proposed methods. We designed different \u0398 for evaluating the proposed methods in various scenarios:  \u2022 Symmetric square class-prior matrix. It is defined as\u03b8 m,k = a + b m = k, b m = k,\nwhere a > 0 and b > 0 are constants satisfying a + Kb = 1. We tested our proposed methods under two different training class-prior settings: (a, b) was chosen as (0.5, 0.05) and (0.1, 0.09). The experimental results are reported in Table 2.\n\u2022 Asymmetric diagonal-dominated square class-prior matrix. In this setting, the values on the diagonals of the matrix \u0398 are larger than the other values, and they can be different from each other. We randomly and uniformly generated the class priors \u03b8 m,k from range [0, 1/M ] when m = k, and set \u03b8 k,k = 1 \u2212 m =k \u03b8 m,k . The experimental results are shown in Table 3.\n\u2022 Non-square class-prior matrix. We also conduct experiments when the number of unlabeled datasets M is more than the number of classes K. Here, we set M = 2K. The class-prior matrix \u0398 is a concatenation of two asymmetric diagonal-dominated square matrices. Compared with the previous experiments, this setting is more in line with real-world situations since it is rare that the number of unlabeled datasets is exactly equal to the number of classes. The experimental results are reported in Table 4.\nThe experimental results show: the unbiased risk estimator suffers severe overfitting and this issue is significantly alleviated by correction methods (U-correct, U-stop, U-flood  and U-PRR) across various scenarios and datasets; the performance of directly applying flooding on the unbiased risk estimator can only reach the same level as early stopping; the partial risk regularization method outperformed all baseline methods and achieved the best performance for all the datasets and class prior settings.\n\n5.3. Robustness against Noisy Class Priors\nHitherto, we have assumed that the values of the class priors were accurately accessible, which may not be true in practice. We designed experiments where random noise was added to each class prior in order to simulate real-world situations. In the experiment, we added different levels (1%, 3%, and 5%) of perturbation to each element of \u0398 to obtain \u0398 , and the noisy \u0398 was treated as the true \u0398 during the whole learning process.\nThe results on the MNIST dataset with different class priors are shown in Fig. 2, where 0% noise rate means that we used the true class priors. This figure shows that the proposed method is not strongly affected by noise in the class priors and thus it may be safely applied in the real world.\n\n6. Conclusion\nIn this work, we focused on the problem of multi-class classification from multiple unlabeled datasets. We established an unbiased risk estimator and provided a generalization error bound for it. Based on our empirical observations, the negative empirical training risk was a potential reason why the unbiased risk estimation suffers severe overfitting. To overcome overfitting, we proposed a partial risk regularization approach customized for learning from multiple unlabeled datasets. Experiments demonstrated the superiority of our proposed partial risk regularization approach.\nor equivalently, with probability at least 1 \u2212 \u03b4/2, By symmetrization (Vapnik, 1998) and using the same trick in Maurer (2016), it is a routine work to show thatE[sup g\u2208G R U (g) \u2212 R(g)] \u2264 2 \u221a 2L M m=1 K k=1 |w m,k | K k =1 R nm (H k ) \u2264 2 \u221a 2LKC w M m=1 K k=1 R nm (H k ).\nThe one-side uniform deviation R(g) \u2212 sup g\u2208G R U (g) can be bounded similarly.\nBased on Lemma 5, the estimation error bound ( 7) is proven throughR( g U ) \u2212 R(g * ) = R U ( g U ) \u2212 R U (g * ) + R( g U ) \u2212 R U ( g U ) + R U (g * ) \u2212 R(g * ) \u2264 0 + 2 sup g\u2208G R U (g) \u2212 R(g) \u2264 4 \u221a 2LKC w M m=1 K k=1 R nm (H k ) + 2C w KC M ln 2 \u03b4 2N min .\nwhere R U ( g U ) \u2264 R U (g * ) by the definition of g U .\n\nAppendix C. Proof of Lemma 4\nBy the relationship of distributions p k and q m , we haveb m,k = K i=1 \u03b8 m,i R 01 p i ,k (g ) = R 01 K i=1 \u03b8 m,i p i ,k (g ) = R 01 qm,k (g ).\nWhen g is the Bayes optimal classifier and all classes are separable, we haveR 01 p i ,k (g ) = 0 if i = k, 1 otherwise. Then b m,k = K i=1 \u03b8 m,i R 01 p i ,k (g ) = i =k \u03b8 m,i = 1 \u2212 \u03b8 m,k\nholds.\n\nFootnotes:\n1: . Later in Section 5.3, we will experimentally show that the use of approximate class-priors is sufficient to obtain an accurate classifier.\n2: . The class priors \u0398 were set to be an asymmetric diagonal dominated square matrix, see the detailed experimental setup in Section 5.2.\n4: . Our implementation is available at https://github.com/Tang-Yuting/U-PRR.\n\nReferences:\n\n- Fevzi Alimoglu and Ethem Alpaydin. Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition. In Proceedings of the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN) 96. Citeseer, 1996.- Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002. Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.\n\n- Aron Culotta, Nirmal Ravi Kumar, and Jennifer Cutler. Predicting the demographics of twitter users from website traffic data. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n\n- Marthinus Du Plessis, Gang Niu, and Masashi Sugiyama. Convex formulation for learning from positive and unlabeled data. In International conference on machine learning, pages 1386-1394. PMLR, 2015.\n\n- Marthinus C du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from positive and unlabeled data. Advances in neural information processing systems, 27: 703-711, 2014.\n\n- Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Conference On Learning Theory, pages 297-299. PMLR, 2018.\n\n- Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n\n- Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learn- ing: Data mining, Inference, and Prediction. Springer Science & Business Media, 2009.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\n- J.J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5):550-554, 1994. doi: 10.1109/34.291440.\n\n- Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positive- confidence data. arXiv preprint arXiv:1710.07138, 2017.\n\n- Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero training loss after achieving zero training error? In International Conference on Machine Learning, pages 4604-4614. PMLR, 2020.\n\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.\n\n- Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positive- unlabeled learning with non-negative risk estimator. Advances in neural information processing systems, 2017.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Jiabin Liu, Bo Wang, Zhiquan Qi, YingJie Tian, and Yong Shi. Learning from label propor- tions with generative adversarial networks. Advances in Neural Information Processing Systems, 32, 2019.\n\n- Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n\n- Nan Lu, Tianyi Zhang, Gang Niu, and Masashi Sugiyama. Mitigating overfitting in super- vised classification from two unlabeled datasets: A consistent risk correction approach. In International Conference on Artificial Intelligence and Statistics, pages 1115-1125. 2020.\n\n- Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Interna- tional Conference on Algorithmic Learning Theory, pages 3-17. Springer, 2016.\n\n- Colin McDiarmid et al. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188, 1989.\n\n- Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.\n\n- Nelson Morgan and Herv\u00e9 Bourlard. Generalization and parameter estimation in feedfor- ward nets: Some experiments. Advances in neural information processing systems, 2, 1989.\n\n- Novi Quadrianto, Alex J Smola, Tiberio S Caetano, and Quoc V Le. Estimating labels from label proportions. Journal of Machine Learning Research, 10(10), 2009.\n\n- Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400-407, 1951.\n\n- Masashi Sugiyama. Introduction to statistical machine learning. Morgan Kaufmann, 2015.\n\n- Masashi Sugiyama, Han Bao, Takashi Ishida, Nan Lu, and Tomoya Sakai. Machine Learning from Weak Supervision: An Empirical Risk Minimization Approach. MIT Press, 2022.\n\n- Kuen-Han Tsai and Hsuan-Tien Lin. Learning from label proportions with consistency regularization. In Sinno Jialin Pan and Masashi Sugiyama, editors, Proceedings of The 12th Asian Conference on Machine Learning, volume 129 of Proceedings of Machine Learning Research, pages 513-528. PMLR, 18-20 Nov 2020.\n\n- Brendan Van Rooyen and Robert C Williamson. A theory of learning with corrupted labels. Journal of Machine Learning Research, 18(1):8501-8550, 2017.\n\n- Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\n\n- Grace Wahba. Spline Models for Observational Data, volume 59. SIAM, 1990.\n\n- Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in Neural Information Processing Systems, 32, 2019.\n\n- Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\n- Felix Yu, Dong Liu, Sanjiv Kumar, Jebara Tony, and Shih-Fu Chang. \\proptosvm for learn- ing with label proportions. In Proceedings of the International Conference on Machine Learning (ICML), pages 504-512. PMLR, 2013.\n\n- Felix X. Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, and Shih-Fu Chang. On learning from label proportions, 2015.\n\n- Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):44-53, 2018.\n\n", "annotations": {"ReferenceToTable": [{"begin": 19669, "end": 19670, "target": "#tab_0", "idx": 0}, {"begin": 21602, "end": 21603, "target": "#tab_0", "idx": 1}, {"begin": 23394, "end": 23395, "target": "#tab_1", "idx": 2}, {"begin": 23763, "end": 23764, "target": "#tab_2", "idx": 3}, {"begin": 24265, "end": 24266, "target": "#tab_3", "idx": 4}], "ReferenceToFootnote": [{"begin": 9793, "end": 9794, "target": "#foot_0", "idx": 0}, {"begin": 19225, "end": 19226, "target": "#foot_2", "idx": 1}], "SectionMain": [{"begin": 1308, "end": 27183, "idx": 0}], "ReferenceToFormula": [{"begin": 16744, "end": 16745, "target": "#formula_7", "idx": 0}, {"begin": 17670, "end": 17672, "target": "#formula_16", "idx": 1}, {"begin": 18772, "end": 18773, "target": "#formula_8", "idx": 2}, {"begin": 19059, "end": 19061, "target": "#formula_19", "idx": 3}, {"begin": 26547, "end": 26548, "target": "#formula_11", "idx": 4}], "SectionReference": [{"begin": 27558, "end": 33933, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1308, "idx": 0}], "Div": [{"begin": 104, "end": 1300, "idx": 0}, {"begin": 1311, "end": 6812, "idx": 1}, {"begin": 6814, "end": 10033, "idx": 2}, {"begin": 10035, "end": 10234, "idx": 3}, {"begin": 10236, "end": 12048, "idx": 4}, {"begin": 12050, "end": 13179, "idx": 5}, {"begin": 13181, "end": 13387, "idx": 6}, {"begin": 13389, "end": 15239, "idx": 7}, {"begin": 15241, "end": 19018, "idx": 8}, {"begin": 19020, "end": 19097, "idx": 9}, {"begin": 19099, "end": 19140, "idx": 10}, {"begin": 19142, "end": 19326, "idx": 11}, {"begin": 19328, "end": 22701, "idx": 12}, {"begin": 22703, "end": 24777, "idx": 13}, {"begin": 24779, "end": 25547, "idx": 14}, {"begin": 25549, "end": 26814, "idx": 15}, {"begin": 26816, "end": 27183, "idx": 16}], "Head": [{"begin": 1311, "end": 1326, "n": "1.", "idx": 0}, {"begin": 6814, "end": 6886, "n": "2.", "idx": 1}, {"begin": 10035, "end": 10079, "n": "3.", "idx": 2}, {"begin": 10236, "end": 10275, "n": "3.1.", "idx": 3}, {"begin": 12050, "end": 12075, "n": "3.2.", "idx": 4}, {"begin": 13181, "end": 13211, "n": "4.", "idx": 5}, {"begin": 13389, "end": 13436, "n": "4.1.", "idx": 6}, {"begin": 15241, "end": 15273, "n": "4.2.", "idx": 7}, {"begin": 19020, "end": 19023, "idx": 8}, {"begin": 19099, "end": 19102, "idx": 9}, {"begin": 19142, "end": 19156, "n": "5.", "idx": 10}, {"begin": 19328, "end": 19351, "n": "5.1.", "idx": 11}, {"begin": 22703, "end": 22740, "n": "5.2.", "idx": 12}, {"begin": 24779, "end": 24821, "n": "5.3.", "idx": 13}, {"begin": 25549, "end": 25562, "n": "6.", "idx": 14}, {"begin": 26816, "end": 26844, "idx": 15}], "Paragraph": [{"begin": 104, "end": 1300, "idx": 0}, {"begin": 1327, "end": 1631, "idx": 1}, {"begin": 1632, "end": 2848, "idx": 2}, {"begin": 2849, "end": 3510, "idx": 3}, {"begin": 3511, "end": 4457, "idx": 4}, {"begin": 4458, "end": 5263, "idx": 5}, {"begin": 5264, "end": 5760, "idx": 6}, {"begin": 5761, "end": 6462, "idx": 7}, {"begin": 6463, "end": 6812, "idx": 8}, {"begin": 6887, "end": 7341, "idx": 9}, {"begin": 7403, "end": 7556, "idx": 10}, {"begin": 7594, "end": 8166, "idx": 11}, {"begin": 8167, "end": 8900, "idx": 12}, {"begin": 8901, "end": 9257, "idx": 13}, {"begin": 9438, "end": 10033, "idx": 14}, {"begin": 10080, "end": 10234, "idx": 15}, {"begin": 10276, "end": 10723, "idx": 16}, {"begin": 10724, "end": 11155, "idx": 17}, {"begin": 11156, "end": 11320, "idx": 18}, {"begin": 11353, "end": 11358, "idx": 19}, {"begin": 11388, "end": 11468, "idx": 20}, {"begin": 11482, "end": 11487, "idx": 21}, {"begin": 11536, "end": 11601, "idx": 22}, {"begin": 11602, "end": 11676, "idx": 23}, {"begin": 11724, "end": 11755, "idx": 24}, {"begin": 11810, "end": 12048, "idx": 25}, {"begin": 12076, "end": 12153, "idx": 26}, {"begin": 12316, "end": 12401, "idx": 27}, {"begin": 12434, "end": 12785, "idx": 28}, {"begin": 12876, "end": 12881, "idx": 29}, {"begin": 12942, "end": 13179, "idx": 30}, {"begin": 13212, "end": 13387, "idx": 31}, {"begin": 13437, "end": 13774, "idx": 32}, {"begin": 13775, "end": 14513, "idx": 33}, {"begin": 14514, "end": 15239, "idx": 34}, {"begin": 15274, "end": 15882, "idx": 35}, {"begin": 15917, "end": 16178, "idx": 36}, {"begin": 16179, "end": 16388, "idx": 37}, {"begin": 16389, "end": 16474, "idx": 38}, {"begin": 16508, "end": 16603, "idx": 39}, {"begin": 16604, "end": 16766, "idx": 40}, {"begin": 16809, "end": 16814, "idx": 41}, {"begin": 16868, "end": 17135, "idx": 42}, {"begin": 17192, "end": 17285, "idx": 43}, {"begin": 17393, "end": 17466, "idx": 44}, {"begin": 17467, "end": 17737, "idx": 45}, {"begin": 17782, "end": 17787, "idx": 46}, {"begin": 17848, "end": 17851, "idx": 47}, {"begin": 17899, "end": 18376, "idx": 48}, {"begin": 18377, "end": 18774, "idx": 49}, {"begin": 18777, "end": 18795, "idx": 50}, {"begin": 19012, "end": 19018, "idx": 51}, {"begin": 19024, "end": 19062, "idx": 52}, {"begin": 19063, "end": 19066, "idx": 53}, {"begin": 19067, "end": 19097, "idx": 54}, {"begin": 19103, "end": 19120, "idx": 55}, {"begin": 19121, "end": 19140, "idx": 56}, {"begin": 19157, "end": 19326, "idx": 57}, {"begin": 19352, "end": 19413, "idx": 58}, {"begin": 19414, "end": 19949, "idx": 59}, {"begin": 19950, "end": 20154, "idx": 60}, {"begin": 20155, "end": 20356, "idx": 61}, {"begin": 20357, "end": 20686, "idx": 62}, {"begin": 20687, "end": 20940, "idx": 63}, {"begin": 21005, "end": 21289, "idx": 64}, {"begin": 21290, "end": 21460, "idx": 65}, {"begin": 21487, "end": 21550, "idx": 66}, {"begin": 21551, "end": 22701, "idx": 67}, {"begin": 22741, "end": 23126, "idx": 68}, {"begin": 23156, "end": 23396, "idx": 69}, {"begin": 23397, "end": 23765, "idx": 70}, {"begin": 23766, "end": 24267, "idx": 71}, {"begin": 24268, "end": 24777, "idx": 72}, {"begin": 24822, "end": 25253, "idx": 73}, {"begin": 25254, "end": 25547, "idx": 74}, {"begin": 25563, "end": 26145, "idx": 75}, {"begin": 26146, "end": 26307, "idx": 76}, {"begin": 26420, "end": 26499, "idx": 77}, {"begin": 26500, "end": 26567, "idx": 78}, {"begin": 26757, "end": 26814, "idx": 79}, {"begin": 26845, "end": 26903, "idx": 80}, {"begin": 26989, "end": 27066, "idx": 81}, {"begin": 27177, "end": 27183, "idx": 82}], "ReferenceToBib": [{"begin": 1399, "end": 1424, "target": "#b6", "idx": 0}, {"begin": 1595, "end": 1607, "target": "#b35", "idx": 1}, {"begin": 1608, "end": 1630, "target": "#b26", "idx": 2}, {"begin": 1960, "end": 1976, "target": "#b17", "idx": 3}, {"begin": 1977, "end": 1997, "target": "#b18", "idx": 4}, {"begin": 2095, "end": 2120, "target": "#b23", "idx": 5}, {"begin": 2299, "end": 2317, "target": "#b16", "idx": 6}, {"begin": 2318, "end": 2337, "target": "#b27", "idx": 7}, {"begin": 2553, "end": 2569, "target": "#b33", "idx": 8}, {"begin": 2570, "end": 2591, "target": "#b34", "idx": 9}, {"begin": 2747, "end": 2761, "target": "#b29", "idx": 10}, {"begin": 3088, "end": 3110, "target": "#b2", "idx": 11}, {"begin": 3823, "end": 3846, "target": "#b26", "idx": 12}, {"begin": 4051, "end": 4067, "target": "#b17", "idx": 13}, {"begin": 4068, "end": 4088, "target": "#b18", "idx": 14}, {"begin": 4267, "end": 4287, "target": "#b21", "idx": 15}, {"begin": 5274, "end": 5294, "target": "#b11", "idx": 16}, {"begin": 5732, "end": 5759, "target": "#b22", "idx": 17}, {"begin": 8493, "end": 8506, "target": "#b30", "idx": 18}, {"begin": 8507, "end": 8520, "target": "#b29", "idx": 19}, {"begin": 8521, "end": 8541, "target": "#b7", "idx": 20}, {"begin": 8542, "end": 8557, "target": "#b25", "idx": 21}, {"begin": 8620, "end": 8634, "target": "#b29", "idx": 22}, {"begin": 10390, "end": 10415, "target": "#b4", "idx": 23}, {"begin": 10416, "end": 10440, "target": "#b3", "idx": 24}, {"begin": 10441, "end": 10461, "target": "#b10", "idx": 25}, {"begin": 10462, "end": 10478, "target": "#b17", "idx": 26}, {"begin": 10479, "end": 10511, "target": "#b28", "idx": 27}, {"begin": 11079, "end": 11095, "target": "#b17", "idx": 28}, {"begin": 11277, "end": 11280, "idx": 29}, {"begin": 11928, "end": 11953, "target": "#b24", "idx": 30}, {"begin": 11958, "end": 11984, "idx": 31}, {"begin": 13075, "end": 13098, "target": "#b5", "idx": 32}, {"begin": 13099, "end": 13116, "target": "#b31", "idx": 33}, {"begin": 13982, "end": 14007, "target": "#b6", "idx": 34}, {"begin": 15416, "end": 15437, "target": "#b11", "idx": 35}, {"begin": 16098, "end": 16118, "target": "#b13", "idx": 36}, {"begin": 16160, "end": 16177, "target": "#b18", "idx": 37}, {"begin": 19468, "end": 19497, "target": "#b0", "idx": 38}, {"begin": 19504, "end": 19516, "target": "#b9", "idx": 39}, {"begin": 19518, "end": 19544, "idx": 40}, {"begin": 19560, "end": 19579, "target": "#b32", "idx": 41}, {"begin": 19581, "end": 19620, "idx": 42}, {"begin": 19637, "end": 19661, "target": "#b14", "idx": 43}, {"begin": 20553, "end": 20570, "target": "#b34", "idx": 44}, {"begin": 20665, "end": 20685, "target": "#b27", "idx": 45}, {"begin": 20780, "end": 20797, "target": "#b18", "idx": 46}, {"begin": 21196, "end": 21212, "target": "#b18", "idx": 47}, {"begin": 21341, "end": 21362, "target": "#b11", "idx": 48}, {"begin": 21701, "end": 21718, "target": "#b8", "idx": 49}, {"begin": 26216, "end": 26230, "target": "#b29", "idx": 50}, {"begin": 26259, "end": 26272, "target": "#b19", "idx": 51}], "ReferenceString": [{"begin": 27573, "end": 27858, "id": "b0", "idx": 0}, {"begin": 27860, "end": 28217, "id": "b1", "idx": 1}, {"begin": 28221, "end": 28412, "id": "b2", "idx": 2}, {"begin": 28416, "end": 28613, "id": "b3", "idx": 3}, {"begin": 28617, "end": 28797, "id": "b4", "idx": 4}, {"begin": 28801, "end": 28967, "id": "b5", "idx": 5}, {"begin": 28971, "end": 29054, "id": "b6", "idx": 6}, {"begin": 29058, "end": 29233, "id": "b7", "idx": 7}, {"begin": 29237, "end": 29440, "id": "b8", "idx": 8}, {"begin": 29444, "end": 29617, "id": "b9", "idx": 9}, {"begin": 29621, "end": 29761, "id": "b10", "idx": 10}, {"begin": 29765, "end": 29984, "id": "b11", "idx": 11}, {"begin": 29988, "end": 30152, "id": "b12", "idx": 12}, {"begin": 30156, "end": 30356, "id": "b13", "idx": 13}, {"begin": 30360, "end": 30461, "id": "b14", "idx": 14}, {"begin": 30465, "end": 30631, "id": "b15", "idx": 15}, {"begin": 30635, "end": 30828, "id": "b16", "idx": 16}, {"begin": 30832, "end": 31070, "id": "b17", "idx": 17}, {"begin": 31074, "end": 31343, "id": "b18", "idx": 18}, {"begin": 31347, "end": 31513, "id": "b19", "idx": 19}, {"begin": 31517, "end": 31625, "id": "b20", "idx": 20}, {"begin": 31629, "end": 31735, "id": "b21", "idx": 21}, {"begin": 31739, "end": 31913, "id": "b22", "idx": 22}, {"begin": 31917, "end": 32075, "id": "b23", "idx": 23}, {"begin": 32079, "end": 32207, "id": "b24", "idx": 24}, {"begin": 32211, "end": 32297, "id": "b25", "idx": 25}, {"begin": 32301, "end": 32467, "id": "b26", "idx": 26}, {"begin": 32471, "end": 32775, "id": "b27", "idx": 27}, {"begin": 32779, "end": 32927, "id": "b28", "idx": 28}, {"begin": 32931, "end": 33005, "id": "b29", "idx": 29}, {"begin": 33009, "end": 33082, "id": "b30", "idx": 30}, {"begin": 33086, "end": 33301, "id": "b31", "idx": 31}, {"begin": 33305, "end": 33471, "id": "b32", "idx": 32}, {"begin": 33475, "end": 33692, "id": "b33", "idx": 33}, {"begin": 33696, "end": 33819, "id": "b34", "idx": 34}, {"begin": 33823, "end": 33931, "id": "b35", "idx": 35}], "Sentence": [{"begin": 104, "end": 256, "idx": 0}, {"begin": 257, "end": 370, "idx": 1}, {"begin": 371, "end": 461, "idx": 2}, {"begin": 462, "end": 620, "idx": 3}, {"begin": 621, "end": 841, "idx": 4}, {"begin": 842, "end": 970, "idx": 5}, {"begin": 971, "end": 1142, "idx": 6}, {"begin": 1143, "end": 1300, "idx": 7}, {"begin": 1327, "end": 1425, "idx": 8}, {"begin": 1426, "end": 1631, "idx": 9}, {"begin": 1632, "end": 1836, "idx": 10}, {"begin": 1837, "end": 1998, "idx": 11}, {"begin": 1999, "end": 2203, "idx": 12}, {"begin": 2204, "end": 2430, "idx": 13}, {"begin": 2431, "end": 2848, "idx": 14}, {"begin": 2849, "end": 2965, "idx": 15}, {"begin": 2966, "end": 3287, "idx": 16}, {"begin": 3288, "end": 3510, "idx": 17}, {"begin": 3511, "end": 3761, "idx": 18}, {"begin": 3762, "end": 3952, "idx": 19}, {"begin": 3953, "end": 4089, "idx": 20}, {"begin": 4090, "end": 4210, "idx": 21}, {"begin": 4211, "end": 4457, "idx": 22}, {"begin": 4458, "end": 4626, "idx": 23}, {"begin": 4627, "end": 4745, "idx": 24}, {"begin": 4746, "end": 4864, "idx": 25}, {"begin": 4865, "end": 5203, "idx": 26}, {"begin": 5204, "end": 5263, "idx": 27}, {"begin": 5264, "end": 5520, "idx": 28}, {"begin": 5521, "end": 5659, "idx": 29}, {"begin": 5660, "end": 5760, "idx": 30}, {"begin": 5761, "end": 5883, "idx": 31}, {"begin": 5884, "end": 6092, "idx": 32}, {"begin": 6093, "end": 6276, "idx": 33}, {"begin": 6277, "end": 6462, "idx": 34}, {"begin": 6463, "end": 6509, "idx": 35}, {"begin": 6510, "end": 6659, "idx": 36}, {"begin": 6660, "end": 6812, "idx": 37}, {"begin": 6887, "end": 7032, "idx": 38}, {"begin": 7033, "end": 7108, "idx": 39}, {"begin": 7109, "end": 7341, "idx": 40}, {"begin": 7403, "end": 7556, "idx": 41}, {"begin": 7594, "end": 7762, "idx": 42}, {"begin": 7763, "end": 7878, "idx": 43}, {"begin": 7879, "end": 8166, "idx": 44}, {"begin": 8167, "end": 8558, "idx": 45}, {"begin": 8559, "end": 8900, "idx": 46}, {"begin": 8901, "end": 9008, "idx": 47}, {"begin": 9009, "end": 9138, "idx": 48}, {"begin": 9139, "end": 9257, "idx": 49}, {"begin": 9438, "end": 9509, "idx": 50}, {"begin": 9510, "end": 9637, "idx": 51}, {"begin": 9638, "end": 10033, "idx": 52}, {"begin": 10080, "end": 10234, "idx": 53}, {"begin": 10276, "end": 10576, "idx": 54}, {"begin": 10577, "end": 10723, "idx": 55}, {"begin": 10724, "end": 10874, "idx": 56}, {"begin": 10875, "end": 11000, "idx": 57}, {"begin": 11001, "end": 11155, "idx": 58}, {"begin": 11156, "end": 11320, "idx": 59}, {"begin": 11353, "end": 11358, "idx": 60}, {"begin": 11388, "end": 11433, "idx": 61}, {"begin": 11434, "end": 11468, "idx": 62}, {"begin": 11482, "end": 11487, "idx": 63}, {"begin": 11536, "end": 11539, "idx": 64}, {"begin": 11540, "end": 11601, "idx": 65}, {"begin": 11602, "end": 11676, "idx": 66}, {"begin": 11724, "end": 11755, "idx": 67}, {"begin": 11810, "end": 11842, "idx": 68}, {"begin": 11843, "end": 12048, "idx": 69}, {"begin": 12076, "end": 12153, "idx": 70}, {"begin": 12316, "end": 12401, "idx": 71}, {"begin": 12434, "end": 12581, "idx": 72}, {"begin": 12582, "end": 12738, "idx": 73}, {"begin": 12739, "end": 12785, "idx": 74}, {"begin": 12876, "end": 12881, "idx": 75}, {"begin": 12942, "end": 13117, "idx": 76}, {"begin": 13118, "end": 13179, "idx": 77}, {"begin": 13212, "end": 13387, "idx": 78}, {"begin": 13437, "end": 13609, "idx": 79}, {"begin": 13610, "end": 13774, "idx": 80}, {"begin": 13775, "end": 13891, "idx": 81}, {"begin": 13892, "end": 14008, "idx": 82}, {"begin": 14009, "end": 14059, "idx": 83}, {"begin": 14060, "end": 14190, "idx": 84}, {"begin": 14191, "end": 14332, "idx": 85}, {"begin": 14333, "end": 14513, "idx": 86}, {"begin": 14514, "end": 14612, "idx": 87}, {"begin": 14613, "end": 14748, "idx": 88}, {"begin": 14749, "end": 14857, "idx": 89}, {"begin": 14858, "end": 15123, "idx": 90}, {"begin": 15124, "end": 15239, "idx": 91}, {"begin": 15274, "end": 15406, "idx": 92}, {"begin": 15407, "end": 15612, "idx": 93}, {"begin": 15613, "end": 15751, "idx": 94}, {"begin": 15752, "end": 15882, "idx": 95}, {"begin": 15917, "end": 16178, "idx": 96}, {"begin": 16179, "end": 16234, "idx": 97}, {"begin": 16235, "end": 16388, "idx": 98}, {"begin": 16389, "end": 16474, "idx": 99}, {"begin": 16508, "end": 16603, "idx": 100}, {"begin": 16604, "end": 16766, "idx": 101}, {"begin": 16809, "end": 16814, "idx": 102}, {"begin": 16868, "end": 16987, "idx": 103}, {"begin": 16988, "end": 17135, "idx": 104}, {"begin": 17192, "end": 17249, "idx": 105}, {"begin": 17250, "end": 17271, "idx": 106}, {"begin": 17272, "end": 17277, "idx": 107}, {"begin": 17278, "end": 17285, "idx": 108}, {"begin": 17393, "end": 17466, "idx": 109}, {"begin": 17467, "end": 17674, "idx": 110}, {"begin": 17675, "end": 17737, "idx": 111}, {"begin": 17782, "end": 17787, "idx": 112}, {"begin": 17848, "end": 17851, "idx": 113}, {"begin": 17899, "end": 18135, "idx": 114}, {"begin": 18136, "end": 18376, "idx": 115}, {"begin": 18377, "end": 18460, "idx": 116}, {"begin": 18461, "end": 18587, "idx": 117}, {"begin": 18588, "end": 18740, "idx": 118}, {"begin": 18741, "end": 18774, "idx": 119}, {"begin": 18777, "end": 18795, "idx": 120}, {"begin": 19012, "end": 19018, "idx": 121}, {"begin": 19024, "end": 19062, "idx": 122}, {"begin": 19063, "end": 19066, "idx": 123}, {"begin": 19067, "end": 19097, "idx": 124}, {"begin": 19103, "end": 19120, "idx": 125}, {"begin": 19121, "end": 19140, "idx": 126}, {"begin": 19157, "end": 19247, "idx": 127}, {"begin": 19248, "end": 19326, "idx": 128}, {"begin": 19352, "end": 19413, "idx": 129}, {"begin": 19414, "end": 19662, "idx": 130}, {"begin": 19663, "end": 19712, "idx": 131}, {"begin": 19713, "end": 19827, "idx": 132}, {"begin": 19828, "end": 19949, "idx": 133}, {"begin": 19950, "end": 20154, "idx": 134}, {"begin": 20155, "end": 20356, "idx": 135}, {"begin": 20357, "end": 20571, "idx": 136}, {"begin": 20572, "end": 20686, "idx": 137}, {"begin": 20687, "end": 20914, "idx": 138}, {"begin": 20915, "end": 20940, "idx": 139}, {"begin": 21005, "end": 21289, "idx": 140}, {"begin": 21290, "end": 21434, "idx": 141}, {"begin": 21435, "end": 21460, "idx": 142}, {"begin": 21487, "end": 21550, "idx": 143}, {"begin": 21551, "end": 21719, "idx": 144}, {"begin": 21720, "end": 21823, "idx": 145}, {"begin": 21824, "end": 22006, "idx": 146}, {"begin": 22007, "end": 22082, "idx": 147}, {"begin": 22083, "end": 22309, "idx": 148}, {"begin": 22310, "end": 22434, "idx": 149}, {"begin": 22435, "end": 22574, "idx": 150}, {"begin": 22575, "end": 22701, "idx": 151}, {"begin": 22741, "end": 22987, "idx": 152}, {"begin": 22988, "end": 23109, "idx": 153}, {"begin": 23110, "end": 23126, "idx": 154}, {"begin": 23156, "end": 23214, "idx": 155}, {"begin": 23215, "end": 23346, "idx": 156}, {"begin": 23347, "end": 23396, "idx": 157}, {"begin": 23397, "end": 23455, "idx": 158}, {"begin": 23456, "end": 23593, "idx": 159}, {"begin": 23594, "end": 23718, "idx": 160}, {"begin": 23719, "end": 23765, "idx": 161}, {"begin": 23766, "end": 23798, "idx": 162}, {"begin": 23799, "end": 23925, "idx": 163}, {"begin": 23926, "end": 24023, "idx": 164}, {"begin": 24024, "end": 24217, "idx": 165}, {"begin": 24218, "end": 24267, "idx": 166}, {"begin": 24268, "end": 24777, "idx": 167}, {"begin": 24822, "end": 24946, "idx": 168}, {"begin": 24947, "end": 25063, "idx": 169}, {"begin": 25064, "end": 25253, "idx": 170}, {"begin": 25254, "end": 25397, "idx": 171}, {"begin": 25398, "end": 25547, "idx": 172}, {"begin": 25563, "end": 25666, "idx": 173}, {"begin": 25667, "end": 25758, "idx": 174}, {"begin": 25759, "end": 25916, "idx": 175}, {"begin": 25917, "end": 26050, "idx": 176}, {"begin": 26051, "end": 26145, "idx": 177}, {"begin": 26146, "end": 26307, "idx": 178}, {"begin": 26420, "end": 26499, "idx": 179}, {"begin": 26500, "end": 26567, "idx": 180}, {"begin": 26757, "end": 26814, "idx": 181}, {"begin": 26845, "end": 26903, "idx": 182}, {"begin": 26989, "end": 27066, "idx": 183}, {"begin": 27177, "end": 27183, "idx": 184}], "ReferenceToFigure": [{"begin": 13767, "end": 13772, "target": "#fig_1", "idx": 0}, {"begin": 14019, "end": 14020, "target": "#fig_1", "idx": 1}, {"begin": 14850, "end": 14855, "target": "#fig_1", "idx": 2}, {"begin": 25333, "end": 25334, "target": "#fig_3", "idx": 3}], "Abstract": [{"begin": 94, "end": 1300, "idx": 0}], "SectionFootnote": [{"begin": 27185, "end": 27556, "idx": 0}], "Footnote": [{"begin": 27196, "end": 27339, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 27340, "end": 27478, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 27479, "end": 27556, "id": "foot_2", "n": "4", "idx": 2}]}}