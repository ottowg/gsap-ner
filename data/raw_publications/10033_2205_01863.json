{"text": "Provably Confidential Language Modelling\n\nAbstract:\nLarge language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter all privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and show that our method is able to provably prevent unintended memorization by randomizing parts of the training process. Moreover, we show that redaction with an approximately correct screening policy amplifies the confidentiality guarantee. We implement the method for both LSTM and GPT language models. Our experimental results show that the models trained by CRT obtain almost the same perplexity while preserving strong confidentiality 1 . SYS: Hello, I am the customer support bot. What do you need? USR: Hello robot. Where is my package? SYS: May I have your full name? USR: Yes, James Bing. SYS: We will need the shipping address as well. USR: Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398. SYS: The tracking number is VD98ID6CXJ. What else can I do? USR: I have all I need. SYS: Hello, I am the customer support bot. What do you need? USR: Hello robot. Where is my package? SYS: May I have your full name? USR: Yes, James Bing. SYS: We will need the shipping address as well. USR: Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398. SYS: The tracking number is VD98ID6CXJ. What else can I do? USR: I have all I need. false negative SYS: Hello, I am the customer support bot. What do you need? USR: Hello robot. Where is my package? SYS: May I have your full name? USR: Yes, James Bing. SYS: We will need the shipping address as well. USR: Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398. SYS: The tracking number is VD98ID6CXJ. What else can I do? USR: I have all I need. SYS: Hello, I am the customer support bot. What do you need? USR: Hello robot. Where is my package? SYS: May I have your full name? USR: Yes, James Bing. SYS: We will need the shipping address as well. USR: Ok, it is 81171 Nguyen Ford North Crystalbury, MO 52398. SYS: The tracking number is VD98ID6CXJ. What else can I do? USR: I have all I need.\n\nMain:\n\n\n\n1 Introduction\nLanguage models (LM) have rich real-world applications in, among others, machine translation (Bahdanau et al., 2015), AI chatbots (Hosseini-Asl et al., 2020), question answering (Kwiatkowski et al., 2019), and information retrieval (Ganguly et al., 2015). The advent of transformers (Vaswani et al., 2017) has fostered a dramatic advancement in the capabilities of generative neural language models, yet they come at a cost to privacy, as the amount of excess parameters in the LM enables it to memorize certain training samples. Recent works show that sensitive user information from the training dataset, such as address and name, can be extracted verbatim from text generation models by querying the LM as an API (Carlini et al., 2019 (Carlini et al., , 2021;; Lee et al., 2022). How to train a highperforming language model without memorizing sensitive text has become a major research challenge.\nExisting solutions to this problem primarily leverage differential privacy (DP) (Dwork et al., 2006).\nDifferentially private learning algorithms ensure that an attacker could not infer whether a data point is used for training, let alone extracting the sensitive information within that data point.\nHowever, there are several mismatches between the problem of privacy that DP addresses, and our problem of preventing the memorization of sensitive text (henceforth referred to as confidentiality). First, confidential information in a natural language dataset is sparse (e.g., the bulk of an email might not carry confidential information). DP's undiscriminating protection for all sentences could be unnecessarily conservative which limits the utility of the trained model. Second, what needs to be protected is the content of the sensitive text, rather than the data context. For example, in the sentence \"My SSN is 123-45-6789.\", it is the actual SSN that we hope to conceal rather than the general information that someone entered her SSN in a chatbot dialogue. Thirdly, the same sensitive content could appear in many data points, which makes the protection of the content more challenging than protecting one data sample. These differences motivate us to treat the problem of confidentiality protection in LM separately with new definitions.\nBesides DP, we also consider classical techniques of redaction and deduplication. Redaction refers to the process of removing sensitive or classified information from a document prior to its publication in governmental and legal contexts. Deduplication is the procedure of detecting and removing identical and nearly identical texts from a corpus. The main challenge of applying these techniques is that it is hard to manually redact a gigantic dataset and automated tools are far from being perfect.\nThe contribution of this paper is fivefold.\n1. We show that in the absence of a perfect screening policy, the risk of a language model memorizing sensitive content is real and can be efficiently exploited with only blackbox access to the model even if the learning algorithm satisfies the recently proposed notion of selective differential privacy (Shi et al., 2021).\n2. Inspired by differential privacy, we introduce a new definition of confidentiality which precisely quantifies the risk of leaking sensitive text.\n3. We propose CRT to train language generation models while protecting confidential text. The method with deduplication and redaction operations work even under imperfect confidential text labeling policies.\n4. We theoretically prove that CRT, combined with differentially private stochastic gradient descent (DP-SGD), provides strong confidentiality guarantees.\n\n5. Our experiments on both MultiWOZ 2.2 and\nCustomerSim datasets show that different models trained by CRT can achieve the same or better perplexity than existing solutions (against the attacks of Carlini et al. (2019 Carlini et al. ( , 2021))).\nTo the best of our knowledge, we are the first that rigorously establish the role of deduplication and redaction in achieving provably stronger confidentiality (or the related differential privacy) guarantees; and the first that achieve provably confidentiality in transformer models with only a mild utility loss.\n\n2 Background & Related Work\nNext, we briefly introduce the relevant background and discuss the related work to put our work in context.\nLanguage modeling is a fundamental problem in natural language processing (Devlin et al., 2019; Howard and Ruder, 2018; Raffel et al., 2020). Consider a text sequence that consists of multiple tokens from a vocabulary V, i.e., w = (w 1 , w 2 , . . . , w n ), where w i is the i-th token. The goal of language modeling is to construct a generative model of the distribution Pr(w), by applying the chain rule Pr(w) = n i=1 Pr (w i | w <i ) . We let f \u03b8 (w i |w <i ) denote the likelihood of token w i when evaluating the neural network f with parameters \u03b8. A language model is trained to maximize the probability of the data in a training set W, by minimizing the negative log-likelihood over each training example with the loss function L(\u03b8) = \u2212 log n i=1 f \u03b8 (w i | w <i ) . Recurrent neural networks (RNNs) used to be a common choice for the neural network architecture to estimate the probability distribution Pr(w).  (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010). More recently, large-scale Transformer-based language models have replaced RNNs in state-of-the-art models for all sorts of NLP tasks (Vaswani et al., 2017; Radford et al., 2019). Nevertheless, common language models are vulnerable to privacy attacks and possibly expose information about their sensitive training data (Carlini et al., 2019 (Carlini et al., , 2021)).\nDifferentially private (DP) learning methods (see, e.g., Abadi et al., 2016) has been applied to language models as a blanket solution for a number of privacy and security risks.  McMahan et al. (2018) trained an RNN language model with DP guarantees in a federated learning setup.  Anil et al. (2021) pre-trained BERT under DP on datasets with hundreds of millions of examples. These paper also demonstrated that DP can effectively prevent data-extraction attacks in practice even for algorithms with DP guarantees that are considered too weak from a theoretical-perspective (e.g., = 8 or 16). However, the strong protection of DP often results in a substantial drop in the utility of the trained model, which makes them less desirable in practice. In fact, it was recently shown that it is necessary for deep learning models to memorize certain training data to achieve high accuracy (Feldman, 2020), which suggests that DP or any other techniques that require the model to not memorize any training data will perform poorly in the highdimensional, power-law distributed real datasets. This motivates us to consider weakened models that only prevent memorizing the sensitive part of the text.\nRecent works (Lee et al., 2022; Kandpal et al., 2022) show that deduplication enables language models to emit memorized text less frequently with same or better accuracy. However, deduplicating training datasets can not prevent all unintended Redaction with a policy with recall 0.9 and high precision compromises confidentiality.\nRedaction with a policy with recall 1.0 but poor precision results in useless data.\n\nfalse positives\nOur results: 1. Provable confidentiality ensures that these two are indistinguishable! 2. Approximate redaction policy amplifies the confidentiality guarantee.\nRaw sensitive text Perfectly redacted text Figure 1 : An example from simulated dialog dataset CustomerSim. The yellow highlights are confidential content (middle). Left shows the text after Redaction by a sequence labeling policy \u03c0. However, if the policy is not perfect, there exists false negative or false positive samples as shown on the right. memorization. We combine deduplication and redaction and then apply both techniques to the training process of LM to achieve confidentiality with provable guarantee.\nThe closest to us is perhaps the work of Shi et al. (2021), who proposed selective differential privacy (S-DP), which requires indistinguishability between two datasets that differ only on a sensitive message. Correspondingly, they propose an algorithm (Selective DP-SGD) for training RNN that adds noise only to the part of computation that involves sensitive tokens. To define S-DP and to run Selective DP-SGD, one needs to have access to a policy function F which determines which token is sensitive. This requirement limits the applicability of their approach to those applications where such perfect F is known. We note that even for name-entity recognition the state-of-the-art model is far from being perfect, and which part of the text is considered sensitive is often ambiguous even for human annotators. We will see that naively running Selective DP-SGD with an approximate policy function does not provide a meaningful confidentiality guarantee and is vulnerable to practical data extraction attacks. Finally, we note that in the case when a perfect policy function is available, we can simply use it for redaction, which provides a perfect S-DP with = 0. A big part of our contribution is to refine S-DP to a (slightly different) definition called \"confidentiality\" and to demonstrate that we use an approximate screening policy to amplify the confidentiality parameter.\n\n3 The CRT Method and Theory\nIn this section, we develop our method with provable confidentiality.\n\n3.1 Formally defining confidentiality\nLet the dataset be a collection of n data pointseach being a sequence of tokens. A \"secret\" x is a contiguous subsequence of tokens within a data point that is considered sensitive or confidential. The goal of our research is to allow us to train language models on such datasets that could contain secrets while provably prevent the model from remembering that these secrets were. We start by defining a formal definition of confidentiality, which uses the following idea of indistinguishability from the DP literature.\nDefinition 1 (Indistinguishability). We say that a pair of distributions P, Q defined on the same probability space are ( , \u03b4)-indistinguishable if for any measurable set S,Pr P [S] \u2264 e Pr Q [S] + \u03b4.\nDefinition 2 (Confidentiality). We say that A ensures that a secret x is ( (x), \u03b4)-confidential, if for any dataset D that contains x in one of its data points, and an alternative dataset D that replaces x in D with a generic <MASK>, it holds that (A(D), A(D )) are ( (x), \u03b4)-indistinguishable. In addition, we simply say that A ensures ( , \u03b4)confidentiality if (x) \u2264 for all secret x. This definition ensures that an attacker cannot distinguish from the output of A (the trained language model) whether it was x or <MASK> that was used for training, thus formalizing the idea of confidentiality. The protection should be viewed as relative, rather than absolute. The definition bounds the risk of any bad event by an multiplicative factor of e and an additive factor of \u03b4, which implies that anything that could happen when we run A on the sensitive data could've happened with with similar probability even if A runs on an alternative world where these sensitive information are perfectly masked.\nConnections to differential privacy. Our definition of confidentiality is related to (and inspired by) ( , \u03b4)-differential privacy (DP) but is different in several ways. DP is stronger (and implies confidentiality!) requires A to ensure ( , \u03b4)indistinguishability for all D, D that can be modified from each other by adding or removing one individual person / data point (or tokens, depending on the desired granularity); but for A to ensure ( , \u03b4)-confidentiality, it only requires ( , \u03b4)indistinguishability for specific D, D where D replaces x in D with <MASK>. Moreover, it is more informative to define as a function of each specific x, which is different from DP (it resembles personalized DP (Ghosh and Roth, 2015)).\nThe confidentiality definition makes sense for our problem because it protects the content of the sensitive text x rather than its existence. Specifically, a pre-processing algorithm that masks all sensitive text ensures (0, 0)-confidentiality but does not satisfy any non-trivial DP guarantees.\nSometimes, it is useful to consider the confidentiality of multiple secret texts. For example, a secret key x could appear multiple times in multiple data points. Also, there might be multiple secret texts that are correlated to each other such that the knowledge of one would reveal other secrets.\nDefinition 3 (Group Confidentiality). We say that A ensures that a list of sensitive texts S := [x 1 , ..., x k ] is ( (S), \u03b4)-(group) confidential, if for any dataset D that contains [x 1 , ..., x k ] in up to k data points, and D being the version that replaces each element in S with <MASK>, it holds that (A(D), A(D )) are ( (S), \u03b4)-indistinguishable.\nA special case of such group confidentiality is when S collects the all secret text in D, which protects all secret texts uniformly. We call this uniform-confidentiality. Note that the standard definition of confidentiality also protect every secret x, except that it protects each secret x individually, rather than together.\nInspired by the recent development of Bayesian DP (Triastcyn and Faltings, 2020), we also define Bayesian confidentiality as follows.\nDefinition 4 (Bayesian Confidentiality). Let D be a dataset that is fixed except a random secret x \u223c \u00b5 drawn from some distribution \u00b5. Let D be obtained by replacing x with <MASK> 2. Then A ensures ( , \u03b4)-Bayesian Confidentiality if for any D , (A(D), A(D )) is ( , \u03b4)-indistinguishable, where A(D) is jointly distributed over x \u223c \u00b5 and A.\nThe Bayesian confidentiality measures how much information an attacker could gain if he/she's prior knowledge about this secret x is described by the distribution \u00b5. This is a strict generalization because when \u00b5 is a single point mass at x, it recovers Definition 2. The additional generality allows us to quantify the stronger confidentiality guarantee against weaker adversaries without complete information.\n\n3.2 Confidentially redacted training\nIn this section we describe the CRT method to train language models with provable confidentiality guarantee. It includes two pre-processing operations (deduplication and redaction) and a switching optimization procedure. The overall idea is to screen the corpus into two separate sets, one public set including sentences with no confidential information, and one private set including sentences containing confidential content. We then use normal optimization algorithms (e.g. SGD) on the public set and differential privacy optimizer (e.g. DP-SGD) on the private set.\n\nDeduplication. The deduplication procedure\nDedup detects all sentences that appear multiple times in the training data and replace them into a single <MASK> from the second occurrence onwards (<MASK> is for proving purpose).\nRedaction. The redaction procedure Redact \u03c0 takes applies a sequence labelling policy \u03c0 to screen confidential content in the training corpus D. \u03c0(s, x) = 1 if a token x in a sentence s should be confidential. The labeled span in each detected sentence is replaced with a special token <MASK>. Note that we do not assume the policy is perfect. It Redact and Dedup could be implemented manually, but with the large text corpus nowadays it is more common that these procedures are implemented using automated tools. For example, Dedup could be implemented efficiently with just one pass of data using a bloom filter (Bloom, 1970) (or other hashing tricks that also catches nearduplicates). Bloom filter in particular, enjoys the nice property that it could have false positives but never any false negatives. Redact \u03c0 could be realized by a named entity recognition (NER) model or a personal-identifiable information (PII) detector.\nFinally, CRT combines the two pre-processing steps with normal optimizer and DP-SGD, the standard algorithm for deep learning with differential privacy. A pseudo-code of the algorithm is given in Algorithm 1.\nBesides using a sequence labeling policy \u03c0 with balanced precision/recall as part of the redaction process. The algorithm uses another, more conservative, policy \u03c0 c with nearly perfect recall to decide on the data points that do not contain sensitive text. In the situation when such \u03c0 c isn't available, we simply choose \u03c0 c (s, x) = 1 for all tokens x in a sentence s and the second part becomes the vanila DP-SGD. It is also important that every data point that contains a <MASK> requires protection.\n\n3.3 Theoretical analysis\nWe analyze the theoretical properties of the above method and show that they result in provable improvements in the (regular, group and Bayesian) confidentiality parameters for any algorithms that are provably ( (x), \u03b4)-confidential as defined in Section 3.1.\nThe following theorem captures the benefit of redaction in improving confidentiality.\nProposition 5 (Confidentiality under redaction). If A ensures ( (x), \u03b4)-Confidentiality for each token x of sentence s \u2208 S (S is a corpus), then A \u2022 Redact \u03c0 ensures (\u02dc (x), \u03b4)-confidentiality with\u02dc (x) = (x) if \u03c0(s, x) = 0 0 otherwise.\nIn addition, A \u2022 Redact \u03c0 also satisfies (\u02dc (S), \u03b4(S))-group confidentiality with\u02dc (S) = x\u2208s&s\u2208S (x)1(\u03c0(s, x) = 0), \u03b4(S) = ke \u02dc (S) \u03b4\nwhere k := x\u2208S 1(\u03c0(s, x) = 0).\nAs an application of the above, if A ensures ( , \u03b4)-confidentiality, and that the empirical recall rates of the redaction policy on D is 1 \u2212 \u03b3, then the above proposition suggests that A \u2022 Redact \u03c0 improves the uniform-confidentiality over applying A without redaction by a factor of \u03b3. The proof is in the appendix.\nRedaction also improves Bayesian confidentiality in a way that mirrors the privacy amplification by sampling from the DP literature.\nProposition 6 (Bayesian Confidentiality under Redaction). If A ensures ( , \u03b4)-Bayesian Confidentiality with respect to \u00b5[x|\u03c0(s, x) = 0] for a token x in a sentence s, then A \u2022 Redact \u03c0 ensures (log(1 + \u03b3(e \u2212 1)), \u03b3\u03b4)-Bayesian Confidentiality under \u00b5 if \u03c0 has a false negative rate (i.e., 1\u2212\"Recall\") of \u03b3 under \u00b5.\nThe proposition says that if the redaction policy is accurate for secrets x \u223c \u00b5, then we can have a stronger confidentiality parameter that scales roughly at \u02dc = O(\u03b3 ). The idea behind the proof is that over the distribution of x \u223c \u00b5, with prob-ability 1 \u2212 \u03b3, Redact \u03c0 (D) = Redact \u03c0 (D ), thus A \u2022 Redact \u03c0 (D) \u2261 A \u2022 Redact \u03c0 (D ).\nWith probability \u03b3, Redact \u03c0 (D), Redact \u03c0 (D ) are different and conditioning on the fact that Redact \u03c0 fails to detect x. Note that \u03c0 is also applied to other text that are not sensitive, and could result in false positives, but they do not matter as the modification of Redact \u03c0 to D and D will be identical. A full proof is given in the appendix.\nNext we turn to deduplication.\nProposition 7 (Group confidentiality under deduplication.). If A ensures ( (S), \u03b4(S))-Group Confidentiality, then A \u2022 Dedup ensures ( (Unique(S)), \u03b4(Unique(S)))-Group Confidentiality.\nDeduplication provides a stronger protection for those cases where some secret x could appear multiple times in the dataset.\nTheorem 8. Let DP-SGD from Algorithm 1 satisfies ( , \u03b4)-differential privacy.\n1. Assume \u03c0 c (s, x) = 1 for all secret tokens x in a sentence s such that \u03c0(s, x) = 0, then Algorithm 1 satisfies ( 1(\u03c0(s, x) = 0), \u03b4)confidentiality.\n2. Let S be a group containing m unique secrets such that \u03c0 c (s, x) = 1\u2200x \u2208 s and s \u2208 S and that \u03c0 detects \u03b3-proportion of the unique secrets in S. Then Algorithm 1 satisfies (\u03b3m , \u03b3me \u03b3m \u03b4)-group confidentiality for S.\n3. Let \u03c0, \u03c0 c has a a recall of 1 \u2212 \u03b3 and 1 \u2212 \u03b4 2 respectively on \u00b5, then Algorithm 1 satisfies (log(1 + \u03b3(e \u2212 1)), \u03b3\u03b4 + \u03b4 2 )-Bayesian Confidentiality for \u00b5.\nThe theorem demonstrates that our CRT algorithm enjoys a full suite of confidentiality guarantees and they all benefit from the deduplication and redaction, particularly if \u03c0 has high recall.\nNote that the CRT algorithm achieves the worstcase confidentiality guarantee if we have a nontrivial conservative screening policy that outputs \u03c0 c (x) = 1 for all secret x that \u03c0 misses, or we simply run vanilla DP-SGD after deduplication and redaction. On the other hand, CRT still satisfies Bayesian confidentiality for each \u00b5 depending on the recall rate of \u03c0 c under \u00b5.\n\n4 Experiments\nWe evaluate CRT by training two types of language model, LSTM and GPT-2, on two datasets: 1) Mul-tiWOZ 2.2, a well-known human-written dialogue dataset and 2) CustomerSim, a simulated dialogue dataset for conversation generation.\nMultiWOZ 2.2 is an already-public dialogue dataset written by crowd-workers, which collects over 10,000 annotated dialogues spanning 8 domains (Zang et al., 2020). We use this dataset to show how CRT works in real-world applications. Following US Department of Labor's guidance 4 on personal-identifiable information (PII), we treat all confidential information (e.g. email address, reference number, telephone number, etc.) as secrets. For the sequence labeling policy \u03c0 and conservative policy \u03c0 c , we build upon an NER model to do redaction. See Appendix A.4 for more details.\nCustomerSim. Following S-DP Shi et al. (2021), we simulate a dialog dataset called CustomerSim with synthetic user information. The dialog flow is simulated based on a fixed agenda and the language generation is template-based (Zhao and Esk\u00e9nazi, 2018). CustomerSim consists of 10 thousand examples and over one million tokens. We treat user name, address, phone number, order, and tracking number as secrets, and use a regular expression tester (regex) to detect them for the redaction process.\nExperiment details. For LSTM model, we follow the setting in S-DP to choose a one-layer LSTM. Because S-DP requires hidden states of the sensitive input to be protected, it doesn't support more layers nor Bidirectional LSTM. Since the advent of Transformers (Vaswani et al., 2017) significantly improves the capabilities of generative language models, we also test transformer-based language model GPT-2 (Radford et al., 2019) from HuggingFace (Wolf et al., 2019). As for deduplication, we use SHA-1 (Jarvinen, 2004) hash function to encode sequences to SHA-1 hash code and then remove identical sequences based on the same hash code. For Bayesian Confidentiality, we treat the uniform distribution over the secret sequences as the distribution \u00b5. More experiment details can be found in Appendix A.3.\nBaselines. For LSTM model, we compare four different training approaches: (1) vanilla SGD (denoted by \"Non-private-LSTM\"), (2) Selective DPSGD (denoted by \"S-DP-LSTM\") (3) DPSGD (denoted by \"DPSGD-LSTM\") and (4) confidentially redacted training (denoted by \"CRT-LSTM\"). While for GPT-2 model, we compare three different training approaches: (1) vanilla SGD (denoted by \"Non-private-GPT\"), ( 2) DPSGD (de- We fix \u03b4 = 8e \u2212 5 for all models. Since Selective DP-SGD with approximate policy gives = +\u221e, we show its result with a perfect screen policy. But when a perfect policy is available, Redaction only gives = 0 and achieves the PPL of vanilla training with no noise added (Non-private-GPT/LSTM). For other models we set \u03b3 = 0.1 to show the result under approximate policy.\nnoted by \"DPSGD-GPT\") and ( 3) CRT (denoted by \"CRT-GPT\"). Our implementation of S-DP-LSTM model is built upon Shi et al. (2021) 5. We run the experiment for the GPT-2 model following Li  et al. (2021) 6 , in which they propose ghost clipping method to alleviate the computational challenge of running DP-SGD with large Transformers.\nAll the models are trained five times to reduce randomness, and the parameters are tuned based on the validation set performances.\n\n5 Experimental Results\n\n\n5.1 Evaluation procedure\nWe need to evaluate both model utilities and privacy guarantees of the language models. We measure predictive perplexity (PPL) for the quality of LM.\nWe also analyze the theoretical privacy budget ( , \u03b4) and test whether language models are private under attacks detailed below.\nCanary insertion attack. Canary insertion is proposed as a testing methodology for quantitatively assessing the risk of unintended memorization (Carlini et al., 2019). It inserts random sequences called canaries into the training dataset, then trains the model, and finally calculates the following exposure for the inserted canaries to measure a model's potential for privacy risks. In our experiment, we randomly generate 10 canaries in the form of \"My ID is: <random 6-digit number here>\". Each canary is inserted into the training dataset 20 times to generate more salient differences between models.\nDefinition 9 (Canary Exposure). Given a canary s[r], a model with parameters \u03b8, and the random-ness space R, the exposure of s[r] isexposure \u03b8 = log 2 |R| \u2212 log 2 rank \u03b8 (s[r])\nAfter training, we calculate empirical model perplexity for all possibly-instantiated canaries and list them in sorted order. Then we can get the canary exposure based on the rank of a specific canary sequence rank \u03b8 (s[r]) and the number of all possible candidates |R|. In our setting, we show the highest canary exposure in 10 canaries. For example, if a canary ranks 1st among 1M candidates, the canary exposure is 19.93.\nMembership inference attack. Membership Inference is a widely used privacy attack method. Given a non-privately trained model, an adversary can predict whether or not a particular example was used to train the model. We adopt the membership inference attack in Carlini et al. (2021). The general idea is to calculate the given samples' perplexities under the model, rank them and choose the ones with the lowest perplexities, i.e., highest likelihood by the model. We can think of this process as training a binary classifier based on the perplexity feature. We also implement the group membership inference attack to show the group confidentiality. More details about the implementation can be found in the Appendix A.5.\n\n5.2 Overall performance\nFigure 2 presents the results of model utilities and confidentiality guarantees across our models of interest on MultiWOZ 2.2 and CustomerSim datasets. Each point denotes a model for different epochs in a training process. Since the X-axis is in Bayesian Confidentiality (the lower the better) and the Y-axis is perplexity (the lower the better), a perfect model will lie in the bottom-left corner. CRT-GPT and DPSGD-GPT in general, perform better than S-DP-LSTM, CRT-LSTM and, DPSGD-LSTM on the test sets. Our model CRT-GPT's performance is close to Non-private-GPT in terms of PPL while preserving strong confidentiality. Besides, CRT-GPT is better than DPSGD-GPT manifested by a much lower , which demonstrates that approximately correct screening policy amplifies the confidentiality guarantee.\nDifferences can be witnessed in the results from two different datasets: the models trained on Cus-tomerSim achieve overall better performances than those trained on MultiWOZ. We think it's due to the fact that CustomerSim contains simple dialogs from template-based simulations.\n\n5.3 Attack results\nFigure 3, 4, and 5 present the results from canary insertion attack and individual/group membership inference attack on MultiWOZ 2.2 and Customer-Sim datasets. The X-axis is the false negative rate \u03b3 of screening policy \u03c0, ranging from 0.0 to 0.5; the Y-axis is the canary exposure (in Figure 3) and membership inference accuracy (in Figure 4 and 5), which measures the effectiveness of the attacks. The lower the canary exposure or inference accuracy, the better protection the model provides against the attacks. For canary insertion attack, it can be seen from Figure 3 that the canary exposures for CRT-LSTM and CRT-GPT are both close to 0 which thus guarantee excellent confidentiality. Non-private-LSTM and Non-private-GPT with mask can also attain great protection at perfect screening policy accuracy (\u03b3 = 0), nonetheless a rise in \u03b3 results in a sharp increase in the exposure. It should be noticed that S-DP-LSTM also has high exposure, similar to Non-private models, given any \u03b3 above 0. This is because that many sensitive data has been falsely identified as non-sensitive by the approximate policy, S-DPSGD does not protect these false negative samples and hence a privacy leakage. For membership inference attack, we compare the inference accuracy with the benchmark value of 0.5, which equals the random guess performance. In Figure 4 and 5, we see that CRT-LSTM and CRT-GPT align well with the 0.5 horizontal line, suggesting that they are rather safe to the attack. The inference accuracy for Non-private-LSTM/Non-private-GPT/S-DP-LSTM, in contrast, surges above 0.5 as the false negative rate \u03b3 deviates from 0.0, indicating that these models become vulnerable to the attack under non-perfect screen policy. In addition, Non-private and S-DP models show even worse protection under the group attack than the individual one in view of a higher inference accuracy at certain \u03b3.\n\n5.4 CRT amplifies Bayesian Confidentiality guarantees\nFigure 6 shows that confidentially redacted training can help to amplify the confidentiality guarantees. We set the in DP-SGD fixed and show the corresponding in Bayesian Confidentiality with different screen policy \u03c0. Both and are for \u03b4 = 8e \u2212 5. If the approximately screening policy \u03c0 has a high recall (\u03b3 is small), we will achieve much improvement in the Bayesian Confidentiality parameter by deduplication and redaction. For example, with ( = 1.0, \u03b3 = 0.1), we reduce the to 0.12.\n\n6 Conclusion\nIn this paper, we propose confidentially redacted training (CRT), a method to train language models while protecting the secret texts. We introduce a new definition of confidentiality which quantifies the risk of leaking sensitive content. We prove the effectiveness of CRT both theoretically and empirically on multiple datasets and language models.\n\n7 Broader Impact\nThis work will alleviate ethical concerns of largescale pre-trained language models. This paper provides one promising solution to an important aspect of NLP: training high quality language models for text generation without compromising confidential information. The current use cases of language models involve pretraining on public web corpus and fine-tuning on individual application data. However, the private application specific data often contains user-generated sensitive information.\nThe proposed method in this paper aims to use as much individual fine-tuning data as possible, while does not leak or memorize any confidential information with provable guarantees. Without the method, one has to either use the general pretraining LM without fine-tuning or manually filter sensitive information and fine-tuning on the remaining. It can be applied in broader applications that need language models or text generation models.\nIn our experiments, we use a simulation scheme to mimic confidential content in a real corpus. We did not compromise any real user's confidential information.\n\nA.5 Membership inference attack details\nIn our experiments, we manually construct a dataset with 2000 sequences. We select 1000 sequences from the protected secrets used in the training data. And we randomly generate 1000 samples of similar format which are not used in the training data. In this way, a random guess generates an accuracy of 50%. For MultiWoz 2.2, we use sentences with reference numbers as the secrets. For CustomerSim, we choose customer addresses as the secrets.\nIn order to show group confidentiality guarantees, we also conduct group membership inference attack. In this setting, we construct a dataset with 2000 groups, each of which includes 20 sentences. One half of the groups are \"sensitive groups\" with all 20 sentences drawn from protected secrets and the other half are \"insensitive groups\" with all 20 sentences being random. We build the classifier based on the sum of the perplexities in one group.\nA.6 \"The devil is in the details\" -how things could go wrong with seemingly inocuous changes to the algorithm.\nIn this section, we highlight various aspects of our algorithms and why certain choices in the pre-processing steps need to be done in the specific way we recommend for our results to hold for them.\n1. It is important that the definition of confidentiality is defined with respect to a perfectly redacted version of the dataset. If we define it as in selective differential privacy, then there will not be an amplification effect from redaction. This is because if we replace a secret x that can be detected by \u03c0 with another x that cannot be detected by \u03c0, then even if x is replaced with <MASK>, x will not be and the two datasets are still different after redaction. In addition, the S-DP definition will not be useful for us we do not know how to define a confidentiality parameter specific for each x or Bayesian confidentiality parameter for each \u00b5 2. Tokenization and splitting into individual \"sentences\" (data points) should go before redaction / deduplication. Otherwise redaction with an approximate screening policy and with an ideal screening policy, or deduplication may cause misalignments, resulting in almost all data points being different in the preprocessed version of D and D .\n3. Each data point should contain only \"whole\" natural sentences, otherwise the sensitive part of a natural sentence could split into two data points.\n4. Deduplication steps should replace duplicate text with the same <MASK>, otherwise <MASK_Dedup> and <MASK_Redact> are not the same so even if all secrets are masked, there will be a difference between the pre-processed versions of D and its neighbor, while in our approach there are no differences and we achieve perfect confidentility (with = 0).\n5. Any data point containing <MASK> needs to be put in D pri . This is because otherwise our algorithm that works on D will be a deterministic algorithm that is perfectly distinguishable from the alternative world where the algorithm is random because the approximate policy \u03c0 fails to redact certain secrets x.\n6. In the DP-SGD algorithm, the sampled minibatches should contain the whole minibatch from D pri or the whole minibatch from D pub . Otherwise the noise always need to be added and the algorithm is identical to the vanilla DP-SGD, and there is no benefit of having a portion of the data being public comparing to all of the data are private.\n\nFootnotes:\n1: Our code is available at https://github.com/ XuandongZhao/CRT\n2: Notice that D is fixed even though x is random.\n3: DP-SGD uses Poisson-sampled Gaussian mechanisms (with a random batchsize), thus cannot ensure all data points are seen and some data points might be seen many times. One epoch means the number of iterations that in expectation covers |D pri | data points.\n4: https://www.dol.gov/general/ppii\n5: https://github.com/wyshi/lm_privacy\n6: https://github.com/lxuechen/private-transformers\n\nReferences:\n\n- Mart\u00edn Abadi, Andy Chu, Ian J. Goodfellow, H. Bren- dan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential pri- vacy. Proceedings of the 2016 ACM SIGSAC Con- ference on Computer and Communications Security.- Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2021. Large-scale differen- tially private bert. ArXiv, abs/2108.01624.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.\n\n- Borja Balle, Gilles Barthe, and Marco Gaboardi. 2018. Privacy amplification by subsampling: tight anal- yses via couplings and divergences. In Advances in Neural Information Processing Systems, pages 6280-6290.\n\n- Gilles Barthe and Federico Olmedo. 2013. Beyond differential privacy: Composition theorems and re- lational logic for f-divergences between probabilis- tic programs. In International Colloquium on Au- tomata, Languages, and Programming, pages 49-60. Springer.\n\n- Burton H Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7):422-426.\n\n- Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Xiaodong Song. 2019. The secret sharer: Evaluating and testing unintended memoriza- tion in neural networks. In USENIX Security Sympo- sium.\n\n- Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xi- aodong Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models. In USENIX Security Sympo- sium.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. ArXiv, abs/1810.04805.\n\n- Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer.\n\n- Vitaly Feldman. 2020. Does learning require memo- rization? a short tale about a long tail. In Proceed- ings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954-959.\n\n- Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth J.F. Jones. 2015. Word embedding based generalized language model for information retrieval. Proceedings of the 38th International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval.\n\n- Arpita Ghosh and Aaron Roth. 2015. Selling privacy at auction. Games and Economic Behavior, 91:334- 346.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:1735- 1780.\n\n- Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. ArXiv, abs/2005.00796.\n\n- Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL.\n\n- Kimmo Jarvinen. 2004. Design and implementation of a sha-1 hash module on fpgas. Helsinki University of Technology Signal Processing Laboratory.\n\n- Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates pri- vacy risks in language models. arXiv preprint arXiv:2202.06539.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur P. Parikh, Chris Al- berti, Danielle Epstein, Illia Polosukhin, Jacob De- vlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, and Slav Petrov. 2019. Natural questions: A benchmark for question an- swering research. Transactions of the Association for Computational Linguistics, 7:453-466.\n\n- Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Association for Compu- tational Linguistics.\n\n- Xuechen Li, Florian Tram\u00e8r, Percy Liang, and Tat- sunori Hashimoto. 2021. Large language models can be strong differentially private learners. ArXiv, abs/2110.05679.\n\n- H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learning differentially private recurrent language models. In ICLR. Tomas Mikolov, Martin Karafi\u00e1t, Luk\u00e1s Burget, Jan Honza Cernock\u00fd, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\n- Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex- ploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.\n\n- Weiyan Shi, Aiqi Cui, Evan Li, R. Jia, and Zhou Yu. 2021. Selective differential privacy for language modeling. ArXiv, abs/2108.12944.\n\n- Aleksei Triastcyn and Boi Faltings. 2020. Bayesian differential privacy for machine learning. In Inter- national Conference on Machine Learning, pages 9583-9592. PMLR.\n\n- Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Atten- tion is all you need. ArXiv, abs/1706.03762.\n\n- Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni- anwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, and et al. 2013. Ontonotes release 5.0. Linguistic Data Consortium, Philadelphia, PA, 23.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771.\n\n- Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with addi- tional annotation corrections and state tracking base- lines. In Proceedings of the 2nd Workshop on Nat- ural Language Processing for Conversational AI, ACL 2020, pages 109-117.\n\n- Tiancheng Zhao and Maxine Esk\u00e9nazi. 2018. Zero- shot dialog generation with cross-domain latent ac- tions. In SIGDIAL Conference.\n\n", "annotations": {"ReferenceToFootnote": [{"begin": 15958, "end": 15959, "target": "#foot_1", "idx": 0}, {"begin": 23270, "end": 23271, "target": "#foot_3", "idx": 1}, {"begin": 25774, "end": 25775, "target": "#foot_4", "idx": 2}], "SectionMain": [{"begin": 2406, "end": 36804, "idx": 0}], "ReferenceToFormula": [{"begin": 25262, "end": 25263, "idx": 0}, {"begin": 25673, "end": 25674, "idx": 1}], "SectionReference": [{"begin": 37320, "end": 43743, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 2406, "idx": 0}], "Div": [{"begin": 52, "end": 2398, "idx": 0}, {"begin": 2409, "end": 6052, "idx": 1}, {"begin": 6054, "end": 6614, "idx": 2}, {"begin": 6616, "end": 9707, "idx": 3}, {"begin": 9709, "end": 11783, "idx": 4}, {"begin": 11785, "end": 11882, "idx": 5}, {"begin": 11884, "end": 16529, "idx": 6}, {"begin": 16531, "end": 17136, "idx": 7}, {"begin": 17138, "end": 19007, "idx": 8}, {"begin": 19009, "end": 22746, "idx": 9}, {"begin": 22748, "end": 26109, "idx": 10}, {"begin": 26111, "end": 26134, "idx": 11}, {"begin": 26136, "end": 28368, "idx": 12}, {"begin": 28370, "end": 29472, "idx": 13}, {"begin": 29474, "end": 31386, "idx": 14}, {"begin": 31388, "end": 31928, "idx": 15}, {"begin": 31930, "end": 32293, "idx": 16}, {"begin": 32295, "end": 33405, "idx": 17}, {"begin": 33407, "end": 36804, "idx": 18}], "Head": [{"begin": 2409, "end": 2423, "n": "1", "idx": 0}, {"begin": 6054, "end": 6097, "n": "5.", "idx": 1}, {"begin": 6616, "end": 6643, "n": "2", "idx": 2}, {"begin": 9709, "end": 9724, "idx": 3}, {"begin": 11785, "end": 11812, "n": "3", "idx": 4}, {"begin": 11884, "end": 11921, "n": "3.1", "idx": 5}, {"begin": 16531, "end": 16567, "n": "3.2", "idx": 6}, {"begin": 17138, "end": 17180, "idx": 7}, {"begin": 19009, "end": 19033, "n": "3.3", "idx": 8}, {"begin": 22748, "end": 22761, "n": "4", "idx": 9}, {"begin": 26111, "end": 26133, "n": "5", "idx": 10}, {"begin": 26136, "end": 26160, "n": "5.1", "idx": 11}, {"begin": 28370, "end": 28393, "n": "5.2", "idx": 12}, {"begin": 29474, "end": 29492, "n": "5.3", "idx": 13}, {"begin": 31388, "end": 31441, "n": "5.4", "idx": 14}, {"begin": 31930, "end": 31942, "n": "6", "idx": 15}, {"begin": 32295, "end": 32311, "n": "7", "idx": 16}, {"begin": 33407, "end": 33446, "idx": 17}], "Paragraph": [{"begin": 52, "end": 2398, "idx": 0}, {"begin": 2424, "end": 3324, "idx": 1}, {"begin": 3325, "end": 3426, "idx": 2}, {"begin": 3427, "end": 3623, "idx": 3}, {"begin": 3624, "end": 4671, "idx": 4}, {"begin": 4672, "end": 5172, "idx": 5}, {"begin": 5173, "end": 5216, "idx": 6}, {"begin": 5217, "end": 5540, "idx": 7}, {"begin": 5541, "end": 5689, "idx": 8}, {"begin": 5690, "end": 5897, "idx": 9}, {"begin": 5898, "end": 6052, "idx": 10}, {"begin": 6098, "end": 6299, "idx": 11}, {"begin": 6300, "end": 6614, "idx": 12}, {"begin": 6644, "end": 6751, "idx": 13}, {"begin": 6752, "end": 8097, "idx": 14}, {"begin": 8098, "end": 9292, "idx": 15}, {"begin": 9293, "end": 9623, "idx": 16}, {"begin": 9624, "end": 9707, "idx": 17}, {"begin": 9725, "end": 9884, "idx": 18}, {"begin": 9885, "end": 10400, "idx": 19}, {"begin": 10401, "end": 11783, "idx": 20}, {"begin": 11813, "end": 11882, "idx": 21}, {"begin": 11922, "end": 12442, "idx": 22}, {"begin": 12443, "end": 12616, "idx": 23}, {"begin": 12643, "end": 13641, "idx": 24}, {"begin": 13642, "end": 14365, "idx": 25}, {"begin": 14366, "end": 14661, "idx": 26}, {"begin": 14662, "end": 14960, "idx": 27}, {"begin": 14961, "end": 15316, "idx": 28}, {"begin": 15317, "end": 15643, "idx": 29}, {"begin": 15644, "end": 15777, "idx": 30}, {"begin": 15778, "end": 16117, "idx": 31}, {"begin": 16118, "end": 16529, "idx": 32}, {"begin": 16568, "end": 17136, "idx": 33}, {"begin": 17181, "end": 17362, "idx": 34}, {"begin": 17363, "end": 18293, "idx": 35}, {"begin": 18294, "end": 18502, "idx": 36}, {"begin": 18503, "end": 19007, "idx": 37}, {"begin": 19034, "end": 19293, "idx": 38}, {"begin": 19294, "end": 19379, "idx": 39}, {"begin": 19380, "end": 19577, "idx": 40}, {"begin": 19617, "end": 19698, "idx": 41}, {"begin": 19751, "end": 19781, "idx": 42}, {"begin": 19782, "end": 20098, "idx": 43}, {"begin": 20099, "end": 20231, "idx": 44}, {"begin": 20232, "end": 20545, "idx": 45}, {"begin": 20546, "end": 20791, "idx": 46}, {"begin": 20879, "end": 21229, "idx": 47}, {"begin": 21230, "end": 21260, "idx": 48}, {"begin": 21261, "end": 21444, "idx": 49}, {"begin": 21445, "end": 21569, "idx": 50}, {"begin": 21570, "end": 21647, "idx": 51}, {"begin": 21648, "end": 21799, "idx": 52}, {"begin": 21800, "end": 22020, "idx": 53}, {"begin": 22021, "end": 22179, "idx": 54}, {"begin": 22180, "end": 22371, "idx": 55}, {"begin": 22372, "end": 22746, "idx": 56}, {"begin": 22762, "end": 22991, "idx": 57}, {"begin": 22992, "end": 23572, "idx": 58}, {"begin": 23573, "end": 24068, "idx": 59}, {"begin": 24069, "end": 24870, "idx": 60}, {"begin": 24871, "end": 25644, "idx": 61}, {"begin": 25645, "end": 25978, "idx": 62}, {"begin": 25979, "end": 26109, "idx": 63}, {"begin": 26161, "end": 26310, "idx": 64}, {"begin": 26311, "end": 26439, "idx": 65}, {"begin": 26440, "end": 27044, "idx": 66}, {"begin": 27045, "end": 27177, "idx": 67}, {"begin": 27222, "end": 27646, "idx": 68}, {"begin": 27647, "end": 28368, "idx": 69}, {"begin": 28394, "end": 29192, "idx": 70}, {"begin": 29193, "end": 29472, "idx": 71}, {"begin": 29493, "end": 31386, "idx": 72}, {"begin": 31442, "end": 31928, "idx": 73}, {"begin": 31943, "end": 32293, "idx": 74}, {"begin": 32312, "end": 32805, "idx": 75}, {"begin": 32806, "end": 33246, "idx": 76}, {"begin": 33247, "end": 33405, "idx": 77}, {"begin": 33447, "end": 33889, "idx": 78}, {"begin": 33890, "end": 34338, "idx": 79}, {"begin": 34339, "end": 34449, "idx": 80}, {"begin": 34450, "end": 34648, "idx": 81}, {"begin": 34649, "end": 35648, "idx": 82}, {"begin": 35649, "end": 35799, "idx": 83}, {"begin": 35800, "end": 36149, "idx": 84}, {"begin": 36150, "end": 36461, "idx": 85}, {"begin": 36462, "end": 36804, "idx": 86}], "ReferenceToBib": [{"begin": 2517, "end": 2540, "target": "#b2", "idx": 0}, {"begin": 2554, "end": 2581, "target": "#b14", "idx": 1}, {"begin": 2602, "end": 2628, "target": "#b18", "idx": 2}, {"begin": 2656, "end": 2678, "target": "#b11", "idx": 3}, {"begin": 2707, "end": 2729, "target": "#b26", "idx": 4}, {"begin": 3140, "end": 3161, "target": "#b6", "idx": 5}, {"begin": 3162, "end": 3187, "target": "#b7", "idx": 6}, {"begin": 3188, "end": 3205, "target": "#b19", "idx": 7}, {"begin": 3405, "end": 3425, "target": "#b9", "idx": 8}, {"begin": 5521, "end": 5539, "target": "#b24", "idx": 9}, {"begin": 6251, "end": 6271, "target": "#b6", "idx": 10}, {"begin": 6272, "end": 6297, "target": "#b7", "idx": 11}, {"begin": 6826, "end": 6847, "target": "#b8", "idx": 12}, {"begin": 6848, "end": 6871, "target": "#b15", "idx": 13}, {"begin": 6872, "end": 6892, "target": "#b23", "idx": 14}, {"begin": 7672, "end": 7706, "target": "#b13", "idx": 15}, {"begin": 7707, "end": 7728, "target": "#b21", "idx": 16}, {"begin": 7864, "end": 7886, "target": "#b26", "idx": 17}, {"begin": 7887, "end": 7908, "target": "#b22", "idx": 18}, {"begin": 8049, "end": 8070, "target": "#b6", "idx": 19}, {"begin": 8071, "end": 8096, "target": "#b7", "idx": 20}, {"begin": 8155, "end": 8174, "target": "#b0", "idx": 21}, {"begin": 8278, "end": 8299, "target": "#b21", "idx": 22}, {"begin": 8381, "end": 8399, "target": "#b1", "idx": 23}, {"begin": 9306, "end": 9324, "target": "#b19", "idx": 24}, {"begin": 9325, "end": 9346, "target": "#b17", "idx": 25}, {"begin": 10442, "end": 10459, "target": "#b24", "idx": 26}, {"begin": 14341, "end": 14363, "target": "#b12", "idx": 27}, {"begin": 15694, "end": 15724, "target": "#b25", "idx": 28}, {"begin": 23135, "end": 23154, "target": "#b29", "idx": 29}, {"begin": 23601, "end": 23618, "target": "#b24", "idx": 30}, {"begin": 23800, "end": 23825, "target": "#b30", "idx": 31}, {"begin": 24327, "end": 24349, "target": "#b26", "idx": 32}, {"begin": 24473, "end": 24495, "target": "#b22", "idx": 33}, {"begin": 24513, "end": 24532, "target": "#b28", "idx": 34}, {"begin": 24569, "end": 24585, "target": "#b16", "idx": 35}, {"begin": 25756, "end": 25773, "target": "#b24", "idx": 36}, {"begin": 26584, "end": 26606, "target": "#b6", "idx": 37}, {"begin": 27908, "end": 27929, "target": "#b7", "idx": 38}], "ReferenceString": [{"begin": 37335, "end": 37579, "id": "b0", "idx": 0}, {"begin": 37581, "end": 37725, "id": "b1", "idx": 1}, {"begin": 37729, "end": 37878, "id": "b2", "idx": 2}, {"begin": 37882, "end": 38092, "id": "b3", "idx": 3}, {"begin": 38096, "end": 38355, "id": "b4", "idx": 4}, {"begin": 38359, "end": 38482, "id": "b5", "idx": 5}, {"begin": 38486, "end": 38694, "id": "b6", "idx": 6}, {"begin": 38698, "end": 38989, "id": "b7", "idx": 7}, {"begin": 38993, "end": 39171, "id": "b8", "idx": 8}, {"begin": 39175, "end": 39363, "id": "b9", "idx": 9}, {"begin": 39367, "end": 39554, "id": "b10", "idx": 10}, {"begin": 39558, "end": 39822, "id": "b11", "idx": 11}, {"begin": 39826, "end": 39930, "id": "b12", "idx": 12}, {"begin": 39934, "end": 40037, "id": "b13", "idx": 13}, {"begin": 40041, "end": 40204, "id": "b14", "idx": 14}, {"begin": 40208, "end": 40318, "id": "b15", "idx": 15}, {"begin": 40322, "end": 40466, "id": "b16", "idx": 16}, {"begin": 40470, "end": 40630, "id": "b17", "idx": 17}, {"begin": 40634, "end": 41077, "id": "b18", "idx": 18}, {"begin": 41081, "end": 41401, "id": "b19", "idx": 19}, {"begin": 41405, "end": 41570, "id": "b20", "idx": 20}, {"begin": 41574, "end": 41868, "id": "b21", "idx": 21}, {"begin": 41872, "end": 42012, "id": "b22", "idx": 22}, {"begin": 42016, "end": 42258, "id": "b23", "idx": 23}, {"begin": 42262, "end": 42396, "id": "b24", "idx": 24}, {"begin": 42400, "end": 42567, "id": "b25", "idx": 25}, {"begin": 42571, "end": 42757, "id": "b26", "idx": 26}, {"begin": 42761, "end": 43005, "id": "b27", "idx": 27}, {"begin": 43009, "end": 43281, "id": "b28", "idx": 28}, {"begin": 43285, "end": 43608, "id": "b29", "idx": 29}, {"begin": 43612, "end": 43741, "id": "b30", "idx": 30}], "Sentence": [{"begin": 52, "end": 165, "idx": 0}, {"begin": 166, "end": 302, "idx": 1}, {"begin": 303, "end": 457, "idx": 2}, {"begin": 458, "end": 668, "idx": 3}, {"begin": 669, "end": 789, "idx": 4}, {"begin": 790, "end": 852, "idx": 5}, {"begin": 853, "end": 991, "idx": 6}, {"begin": 992, "end": 1034, "idx": 7}, {"begin": 1035, "end": 1052, "idx": 8}, {"begin": 1053, "end": 1070, "idx": 9}, {"begin": 1071, "end": 1091, "idx": 10}, {"begin": 1092, "end": 1123, "idx": 11}, {"begin": 1124, "end": 1145, "idx": 12}, {"begin": 1146, "end": 1193, "idx": 13}, {"begin": 1194, "end": 1255, "idx": 14}, {"begin": 1256, "end": 1295, "idx": 15}, {"begin": 1296, "end": 1315, "idx": 16}, {"begin": 1316, "end": 1339, "idx": 17}, {"begin": 1340, "end": 1382, "idx": 18}, {"begin": 1383, "end": 1400, "idx": 19}, {"begin": 1401, "end": 1418, "idx": 20}, {"begin": 1419, "end": 1439, "idx": 21}, {"begin": 1440, "end": 1471, "idx": 22}, {"begin": 1472, "end": 1493, "idx": 23}, {"begin": 1494, "end": 1541, "idx": 24}, {"begin": 1542, "end": 1603, "idx": 25}, {"begin": 1604, "end": 1643, "idx": 26}, {"begin": 1644, "end": 1663, "idx": 27}, {"begin": 1664, "end": 1687, "idx": 28}, {"begin": 1688, "end": 1745, "idx": 29}, {"begin": 1746, "end": 1763, "idx": 30}, {"begin": 1764, "end": 1781, "idx": 31}, {"begin": 1782, "end": 1802, "idx": 32}, {"begin": 1803, "end": 1834, "idx": 33}, {"begin": 1835, "end": 1856, "idx": 34}, {"begin": 1857, "end": 1904, "idx": 35}, {"begin": 1905, "end": 1966, "idx": 36}, {"begin": 1967, "end": 2006, "idx": 37}, {"begin": 2007, "end": 2026, "idx": 38}, {"begin": 2027, "end": 2050, "idx": 39}, {"begin": 2051, "end": 2093, "idx": 40}, {"begin": 2094, "end": 2111, "idx": 41}, {"begin": 2112, "end": 2129, "idx": 42}, {"begin": 2130, "end": 2150, "idx": 43}, {"begin": 2151, "end": 2182, "idx": 44}, {"begin": 2183, "end": 2204, "idx": 45}, {"begin": 2205, "end": 2252, "idx": 46}, {"begin": 2253, "end": 2314, "idx": 47}, {"begin": 2315, "end": 2354, "idx": 48}, {"begin": 2355, "end": 2374, "idx": 49}, {"begin": 2375, "end": 2398, "idx": 50}, {"begin": 2424, "end": 2679, "idx": 51}, {"begin": 2680, "end": 2953, "idx": 52}, {"begin": 2954, "end": 3206, "idx": 53}, {"begin": 3207, "end": 3324, "idx": 54}, {"begin": 3325, "end": 3426, "idx": 55}, {"begin": 3427, "end": 3623, "idx": 56}, {"begin": 3624, "end": 3821, "idx": 57}, {"begin": 3822, "end": 3964, "idx": 58}, {"begin": 3965, "end": 4098, "idx": 59}, {"begin": 4099, "end": 4201, "idx": 60}, {"begin": 4202, "end": 4256, "idx": 61}, {"begin": 4257, "end": 4389, "idx": 62}, {"begin": 4390, "end": 4551, "idx": 63}, {"begin": 4552, "end": 4671, "idx": 64}, {"begin": 4672, "end": 4753, "idx": 65}, {"begin": 4754, "end": 4910, "idx": 66}, {"begin": 4911, "end": 5019, "idx": 67}, {"begin": 5020, "end": 5172, "idx": 68}, {"begin": 5173, "end": 5216, "idx": 69}, {"begin": 5217, "end": 5540, "idx": 70}, {"begin": 5541, "end": 5689, "idx": 71}, {"begin": 5690, "end": 5779, "idx": 72}, {"begin": 5780, "end": 5897, "idx": 73}, {"begin": 5898, "end": 6052, "idx": 74}, {"begin": 6098, "end": 6299, "idx": 75}, {"begin": 6300, "end": 6614, "idx": 76}, {"begin": 6644, "end": 6751, "idx": 77}, {"begin": 6752, "end": 6893, "idx": 78}, {"begin": 6894, "end": 7001, "idx": 79}, {"begin": 7002, "end": 7039, "idx": 80}, {"begin": 7040, "end": 7191, "idx": 81}, {"begin": 7192, "end": 7306, "idx": 82}, {"begin": 7307, "end": 7526, "idx": 83}, {"begin": 7527, "end": 7670, "idx": 84}, {"begin": 7671, "end": 7729, "idx": 85}, {"begin": 7730, "end": 7909, "idx": 86}, {"begin": 7910, "end": 8097, "idx": 87}, {"begin": 8098, "end": 8276, "idx": 88}, {"begin": 8277, "end": 8379, "idx": 89}, {"begin": 8380, "end": 8476, "idx": 90}, {"begin": 8477, "end": 8692, "idx": 91}, {"begin": 8693, "end": 8847, "idx": 92}, {"begin": 8848, "end": 9185, "idx": 93}, {"begin": 9186, "end": 9292, "idx": 94}, {"begin": 9293, "end": 9463, "idx": 95}, {"begin": 9464, "end": 9623, "idx": 96}, {"begin": 9624, "end": 9707, "idx": 97}, {"begin": 9725, "end": 9811, "idx": 98}, {"begin": 9812, "end": 9884, "idx": 99}, {"begin": 9885, "end": 9992, "idx": 100}, {"begin": 9993, "end": 10049, "idx": 101}, {"begin": 10050, "end": 10118, "idx": 102}, {"begin": 10119, "end": 10234, "idx": 103}, {"begin": 10235, "end": 10248, "idx": 104}, {"begin": 10249, "end": 10400, "idx": 105}, {"begin": 10401, "end": 10610, "idx": 106}, {"begin": 10611, "end": 10769, "idx": 107}, {"begin": 10770, "end": 10904, "idx": 108}, {"begin": 10905, "end": 11017, "idx": 109}, {"begin": 11018, "end": 11214, "idx": 110}, {"begin": 11215, "end": 11412, "idx": 111}, {"begin": 11413, "end": 11567, "idx": 112}, {"begin": 11568, "end": 11783, "idx": 113}, {"begin": 11813, "end": 11882, "idx": 114}, {"begin": 11922, "end": 12002, "idx": 115}, {"begin": 12003, "end": 12119, "idx": 116}, {"begin": 12120, "end": 12303, "idx": 117}, {"begin": 12304, "end": 12442, "idx": 118}, {"begin": 12443, "end": 12479, "idx": 119}, {"begin": 12480, "end": 12616, "idx": 120}, {"begin": 12643, "end": 12674, "idx": 121}, {"begin": 12675, "end": 12937, "idx": 122}, {"begin": 12938, "end": 13028, "idx": 123}, {"begin": 13029, "end": 13239, "idx": 124}, {"begin": 13240, "end": 13306, "idx": 125}, {"begin": 13307, "end": 13641, "idx": 126}, {"begin": 13642, "end": 13678, "idx": 127}, {"begin": 13679, "end": 13811, "idx": 128}, {"begin": 13812, "end": 14206, "idx": 129}, {"begin": 14207, "end": 14365, "idx": 130}, {"begin": 14366, "end": 14507, "idx": 131}, {"begin": 14508, "end": 14661, "idx": 132}, {"begin": 14662, "end": 14743, "idx": 133}, {"begin": 14744, "end": 14824, "idx": 134}, {"begin": 14825, "end": 14960, "idx": 135}, {"begin": 14961, "end": 14998, "idx": 136}, {"begin": 14999, "end": 15316, "idx": 137}, {"begin": 15317, "end": 15449, "idx": 138}, {"begin": 15450, "end": 15487, "idx": 139}, {"begin": 15488, "end": 15643, "idx": 140}, {"begin": 15644, "end": 15777, "idx": 141}, {"begin": 15778, "end": 15818, "idx": 142}, {"begin": 15819, "end": 15912, "idx": 143}, {"begin": 15913, "end": 15960, "idx": 144}, {"begin": 15961, "end": 16117, "idx": 145}, {"begin": 16118, "end": 16283, "idx": 146}, {"begin": 16284, "end": 16529, "idx": 147}, {"begin": 16568, "end": 16676, "idx": 148}, {"begin": 16677, "end": 16788, "idx": 149}, {"begin": 16789, "end": 16995, "idx": 150}, {"begin": 16996, "end": 17044, "idx": 151}, {"begin": 17045, "end": 17108, "idx": 152}, {"begin": 17109, "end": 17136, "idx": 153}, {"begin": 17181, "end": 17362, "idx": 154}, {"begin": 17363, "end": 17373, "idx": 155}, {"begin": 17374, "end": 17572, "idx": 156}, {"begin": 17573, "end": 17656, "idx": 157}, {"begin": 17657, "end": 17706, "idx": 158}, {"begin": 17707, "end": 17876, "idx": 159}, {"begin": 17877, "end": 18050, "idx": 160}, {"begin": 18051, "end": 18169, "idx": 161}, {"begin": 18170, "end": 18293, "idx": 162}, {"begin": 18294, "end": 18446, "idx": 163}, {"begin": 18447, "end": 18502, "idx": 164}, {"begin": 18503, "end": 18610, "idx": 165}, {"begin": 18611, "end": 18760, "idx": 166}, {"begin": 18761, "end": 18920, "idx": 167}, {"begin": 18921, "end": 19007, "idx": 168}, {"begin": 19034, "end": 19293, "idx": 169}, {"begin": 19294, "end": 19379, "idx": 170}, {"begin": 19380, "end": 19428, "idx": 171}, {"begin": 19429, "end": 19577, "idx": 172}, {"begin": 19617, "end": 19698, "idx": 173}, {"begin": 19751, "end": 19781, "idx": 174}, {"begin": 19782, "end": 20068, "idx": 175}, {"begin": 20069, "end": 20098, "idx": 176}, {"begin": 20099, "end": 20231, "idx": 177}, {"begin": 20232, "end": 20289, "idx": 178}, {"begin": 20290, "end": 20545, "idx": 179}, {"begin": 20546, "end": 20714, "idx": 180}, {"begin": 20715, "end": 20791, "idx": 181}, {"begin": 20879, "end": 21002, "idx": 182}, {"begin": 21003, "end": 21190, "idx": 183}, {"begin": 21191, "end": 21229, "idx": 184}, {"begin": 21230, "end": 21260, "idx": 185}, {"begin": 21261, "end": 21320, "idx": 186}, {"begin": 21321, "end": 21444, "idx": 187}, {"begin": 21445, "end": 21569, "idx": 188}, {"begin": 21570, "end": 21647, "idx": 189}, {"begin": 21648, "end": 21799, "idx": 190}, {"begin": 21800, "end": 22020, "idx": 191}, {"begin": 22021, "end": 22179, "idx": 192}, {"begin": 22180, "end": 22371, "idx": 193}, {"begin": 22372, "end": 22626, "idx": 194}, {"begin": 22627, "end": 22746, "idx": 195}, {"begin": 22762, "end": 22991, "idx": 196}, {"begin": 22992, "end": 23155, "idx": 197}, {"begin": 23156, "end": 23225, "idx": 198}, {"begin": 23226, "end": 23359, "idx": 199}, {"begin": 23360, "end": 23428, "idx": 200}, {"begin": 23429, "end": 23537, "idx": 201}, {"begin": 23538, "end": 23572, "idx": 202}, {"begin": 23573, "end": 23585, "idx": 203}, {"begin": 23586, "end": 23700, "idx": 204}, {"begin": 23701, "end": 23826, "idx": 205}, {"begin": 23827, "end": 23900, "idx": 206}, {"begin": 23901, "end": 24068, "idx": 207}, {"begin": 24069, "end": 24088, "idx": 208}, {"begin": 24089, "end": 24162, "idx": 209}, {"begin": 24163, "end": 24293, "idx": 210}, {"begin": 24294, "end": 24533, "idx": 211}, {"begin": 24534, "end": 24703, "idx": 212}, {"begin": 24704, "end": 24816, "idx": 213}, {"begin": 24817, "end": 24870, "idx": 214}, {"begin": 24871, "end": 24881, "idx": 215}, {"begin": 24882, "end": 25140, "idx": 216}, {"begin": 25141, "end": 25309, "idx": 217}, {"begin": 25310, "end": 25417, "idx": 218}, {"begin": 25418, "end": 25567, "idx": 219}, {"begin": 25568, "end": 25644, "idx": 220}, {"begin": 25645, "end": 25703, "idx": 221}, {"begin": 25704, "end": 25776, "idx": 222}, {"begin": 25777, "end": 25978, "idx": 223}, {"begin": 25979, "end": 26109, "idx": 224}, {"begin": 26161, "end": 26248, "idx": 225}, {"begin": 26249, "end": 26310, "idx": 226}, {"begin": 26311, "end": 26439, "idx": 227}, {"begin": 26440, "end": 26464, "idx": 228}, {"begin": 26465, "end": 26607, "idx": 229}, {"begin": 26608, "end": 26823, "idx": 230}, {"begin": 26824, "end": 26932, "idx": 231}, {"begin": 26933, "end": 27044, "idx": 232}, {"begin": 27045, "end": 27076, "idx": 233}, {"begin": 27077, "end": 27177, "idx": 234}, {"begin": 27222, "end": 27347, "idx": 235}, {"begin": 27348, "end": 27492, "idx": 236}, {"begin": 27493, "end": 27560, "idx": 237}, {"begin": 27561, "end": 27646, "idx": 238}, {"begin": 27647, "end": 27675, "idx": 239}, {"begin": 27676, "end": 27736, "idx": 240}, {"begin": 27737, "end": 27863, "idx": 241}, {"begin": 27864, "end": 27930, "idx": 242}, {"begin": 27931, "end": 28111, "idx": 243}, {"begin": 28112, "end": 28205, "idx": 244}, {"begin": 28206, "end": 28296, "idx": 245}, {"begin": 28297, "end": 28368, "idx": 246}, {"begin": 28394, "end": 28545, "idx": 247}, {"begin": 28546, "end": 28616, "idx": 248}, {"begin": 28617, "end": 28792, "idx": 249}, {"begin": 28793, "end": 28900, "idx": 250}, {"begin": 28901, "end": 29017, "idx": 251}, {"begin": 29018, "end": 29192, "idx": 252}, {"begin": 29193, "end": 29368, "idx": 253}, {"begin": 29369, "end": 29472, "idx": 254}, {"begin": 29493, "end": 29652, "idx": 255}, {"begin": 29653, "end": 29892, "idx": 256}, {"begin": 29893, "end": 30007, "idx": 257}, {"begin": 30008, "end": 30184, "idx": 258}, {"begin": 30185, "end": 30379, "idx": 259}, {"begin": 30380, "end": 30491, "idx": 260}, {"begin": 30492, "end": 30687, "idx": 261}, {"begin": 30688, "end": 30830, "idx": 262}, {"begin": 30831, "end": 30975, "idx": 263}, {"begin": 30976, "end": 31218, "idx": 264}, {"begin": 31219, "end": 31386, "idx": 265}, {"begin": 31442, "end": 31546, "idx": 266}, {"begin": 31547, "end": 31660, "idx": 267}, {"begin": 31661, "end": 31689, "idx": 268}, {"begin": 31690, "end": 31868, "idx": 269}, {"begin": 31869, "end": 31928, "idx": 270}, {"begin": 31943, "end": 32077, "idx": 271}, {"begin": 32078, "end": 32182, "idx": 272}, {"begin": 32183, "end": 32293, "idx": 273}, {"begin": 32312, "end": 32396, "idx": 274}, {"begin": 32397, "end": 32575, "idx": 275}, {"begin": 32576, "end": 32705, "idx": 276}, {"begin": 32706, "end": 32805, "idx": 277}, {"begin": 32806, "end": 32987, "idx": 278}, {"begin": 32988, "end": 33151, "idx": 279}, {"begin": 33152, "end": 33246, "idx": 280}, {"begin": 33247, "end": 33341, "idx": 281}, {"begin": 33342, "end": 33405, "idx": 282}, {"begin": 33447, "end": 33519, "idx": 283}, {"begin": 33520, "end": 33598, "idx": 284}, {"begin": 33599, "end": 33695, "idx": 285}, {"begin": 33696, "end": 33753, "idx": 286}, {"begin": 33754, "end": 33827, "idx": 287}, {"begin": 33828, "end": 33889, "idx": 288}, {"begin": 33890, "end": 33991, "idx": 289}, {"begin": 33992, "end": 34086, "idx": 290}, {"begin": 34087, "end": 34263, "idx": 291}, {"begin": 34264, "end": 34338, "idx": 292}, {"begin": 34339, "end": 34449, "idx": 293}, {"begin": 34450, "end": 34648, "idx": 294}, {"begin": 34649, "end": 34651, "idx": 295}, {"begin": 34652, "end": 34778, "idx": 296}, {"begin": 34779, "end": 34895, "idx": 297}, {"begin": 34896, "end": 35119, "idx": 298}, {"begin": 35120, "end": 35420, "idx": 299}, {"begin": 35421, "end": 35648, "idx": 300}, {"begin": 35649, "end": 35799, "idx": 301}, {"begin": 35800, "end": 36149, "idx": 302}, {"begin": 36150, "end": 36212, "idx": 303}, {"begin": 36213, "end": 36461, "idx": 304}, {"begin": 36462, "end": 36464, "idx": 305}, {"begin": 36465, "end": 36595, "idx": 306}, {"begin": 36596, "end": 36804, "idx": 307}], "ReferenceToFigure": [{"begin": 9935, "end": 9936, "idx": 0}, {"begin": 28401, "end": 28402, "target": "#fig_1", "idx": 1}, {"begin": 29500, "end": 29501, "target": "#fig_2", "idx": 2}, {"begin": 29786, "end": 29787, "target": "#fig_2", "idx": 3}, {"begin": 29834, "end": 29841, "target": "#fig_3", "idx": 4}, {"begin": 30064, "end": 30065, "target": "#fig_2", "idx": 5}, {"begin": 30841, "end": 30842, "idx": 6}, {"begin": 31449, "end": 31450, "target": "#fig_4", "idx": 7}], "Abstract": [{"begin": 42, "end": 2398, "idx": 0}], "SectionFootnote": [{"begin": 36806, "end": 37318, "idx": 0}], "Footnote": [{"begin": 36817, "end": 36881, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 36882, "end": 36932, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 36933, "end": 37191, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 37192, "end": 37227, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 37228, "end": 37266, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 37267, "end": 37318, "id": "foot_5", "n": "6", "idx": 5}]}}