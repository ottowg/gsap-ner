{"text": "Meta-Learning with Sparse Experience Replay for Lifelong Language Learning\n\nAbstract:\nLifelong learning requires models that can continuously learn from sequential streams of data without suffering catastrophic forgetting due to shifts in data distributions. Deep learning models have thrived in the non-sequential learning paradigm; however, when used to learn a sequence of tasks, they fail to retain past knowledge and learn incrementally. We propose a novel approach to lifelong learning of language tasks based on meta-learning with sparse experience replay that directly optimizes to prevent forgetting. We show that under the realistic setting of performing a single pass on a stream of tasks and without any task identifiers, our method obtains state-of-the-art results on lifelong text classification and relation extraction. We analyze the effectiveness of our approach and further demonstrate its low computational and space complexity.\n\nMain:\n\n\n\n1 Introduction\nThe ability to learn tasks continuously during a lifetime and with limited supervision is a hallmark of human intelligence. This is enabled by efficient transfer of knowledge from past experience. On the contrary, when current deep learning methods are subjected to learning new tasks in a sequential manner, they suffer from catastrophic forgetting (Mc-Closkey and Cohen, 1989; Ratcliff, 1990; French, 1999), where previous information is lost due to the shift in data distribution. Non-stationarity is inevitable in the real world where data is continuously evolving. Thus, we need to design more robust machine learning mechanisms to deal with catastrophic interference.\nLifelong learning, also known as continual learning (Thrun, 1998), aims at developing models that can continuously learn from a stream of tasks in sequence without forgetting existing knowledge but rather building on the information acquired by previously learned tasks in order to learn new tasks (Chen and Liu, 2018). One conceptualization of this is to accelerate learning by positive transfer between tasks while minimizing interference with respect to network updates (Riemer et al., 2019). Manually-designed techniques for continual learning include regularization (Kirkpatrick et al., 2017) or gradient alignment (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019) to mitigate catastrophic forgetting, and have been shown effective in computer vision and reinforcement learning tasks. Meta-learning (Schmidhuber, 1987; Bengio et al., 1991; Thrun and Pratt, 1998) has been applied in continual learning with the objective of learning new tasks continually with a relatively small number of examples per task (Javed and White, 2019; Beaulieu et al., 2020) (in image classification) or in a traditional continual learning setup by interleaving with several past examples from a memory component, i.e. experience replay (Riemer et al., 2019; Obamuyide and Vlachos, 2019a) (in image classification, reinforcement learning and language processing).\nIn natural language processing (NLP), continual learning still remains relatively unexplored (Li et al., 2020). Despite the success of large pre-trained language models such as BERT (Devlin et al., 2019), they still require considerable amounts of in-domain examples for training on new tasks and are prone to catastrophic forgetting (Yogatama et al., 2019). Existing continual learning approaches to NLP tasks include purely replaybased methods (Wang et al., 2019; Han et al., 2020; d'Autume et al., 2019), a meta-learning based method (Obamuyide and Vlachos, 2019a; Wang et al., 2020) as well as a generative replay-based method (Sun et al., 2020).\nCurrently, most of the approaches to continual learning employ flawed experimental setups that have blind spots disguising certain weak points.\nBy assuming explicit task identifiers, distinct output heads per task, multiple training passes over the sequence of tasks, and the availability of large training times as well as computational and memory resources, they are essentially solving an easier problem compared to true continual learning (Farquhar and Gal, 2018). Specifically, while a high rate of experience replay (Lin, 1992) usually mitigates catastrophic forgetting, it comes closer to a multi-task learning (Caruana, 1997) than a lifelong learning setup and is computationally expensive when learning on a data stream in real-life applications. Continual learning methods in NLP suffer from these limitations too. An appropriate evaluation of continual learning is thus one where no task identifiers are available, without multiple epochs of training, with a shared output head as well as constraints on time, compute and memory.\nIn this paper, we propose a novel and efficient approach to lifelong learning on language processing tasks that overcomes the aforementioned shortcomings. We consider the realistic lifelong learning setup where only one pass over the training set is possible with constraints on the rate of experience replay, and no task identifiers are available. Our approach is based on meta-learning and experience replay that is sparse in time and size. We are the first to investigate meta-learning with sparse experience replay in the context of large-scale pre-trained language models, in contrast with previous works that take liberty in terms of how often experience replay is performed. Additionally, we conduct a systematic study of approaches that rely on pretrained models and that conform to the realistic lifelong learning setup.\nWe extend two algorithms, namely online meta-learning (OML) (Javed and White, 2019) and a neuromodulatory meta-learning algorithm (ANML) (Beaulieu et al., 2020) to the domain of NLP and augment them with an episodic memory module for experience replay, calling them OML-ER and ANML-ER respectively. While their original objective is to continually learn a new sequence of tasks during testing time, we enhance them for the conventional continual learning setup where evaluation is on previously seen tasks, thus directly addressing the problem of catastrophic forgetting. Furthermore, by realizing experience replay as a query set, we directly optimize to prevent forgetting.\nWe show that combining a pre-trained language model such as BERT along with meta-learning and sparse replay produces state-of-the-art performance on lifelong text classification and relation extraction benchmarks when compared against current methods under the same realistic setting. Through further experiments, we demonstrate that BERT combined with OML-ER results in an efficient form of lifelong learning, where most of the weight updates are performed on a single linear layer on top of BERT, while using a limited amount of memory during training and without any network adaptation during test-time. Therefore, our approach is considerably more efficient than previous work in terms of computational complexity as well as memory usage, enabling learning on a task stream without substantial overheads. We hope that our paper informs the NLP community about the right experimental design of continual learning and how metalearning methods enable efficient lifelong learning with limited replay and memory capacity. To facilitate further research in the field, we make our code publicly available 1.\n2 Background and Related Work\n\n2.1 Meta-learning\nIn meta-learning, a model is trained on several related tasks such that it can transfer knowledge and adapt to new tasks using only a few examples. The training set is referred to as meta-training set and the test set is referred to as meta-test set. They consist of episodes where each episode corresponds to a task, comprising a few training examples for adaptation called the support set and a separate set of examples for evaluation called the query set. The goal of meta-learning is to learn to adapt quickly from the support set such that the model can perform well on the query set.\nOptimization-based methods for meta-learning explicitly include generalizability in their objective function and optimize for the same. Modelagnostic meta-learning (MAML) algorithm (Finn et al., 2017) is an optimization-based method that seeks to train a model's initial parameters such that it can perform well on a new task after only a few gradient steps. During meta-training, it involves a two-level optimization process where task adaptation is performed using the support set in an inner-loop and meta-updates are performed using the query set in an outer-loop. Specifically, param-eters \u03b8 of the model f \u03b8 are updated to \u03b8 i for task T i in the inner-loop by m steps of gradient-based update U on the support set as:\u03b8 i = U (L s T i , \u03b8, \u03b1, m)\nwhere L s T i is the loss on the support set and \u03b1 is the inner-loop learning rate. The outer-loop objective is to have f \u03b8 i generalize well across tasks from a distribution p(T ):J(\u03b8) = T i \u223cp(T ) L q T i (f U (L s T i ,\u03b8,\u03b1,m) )\nwhere L q T i is the loss computed on the query set. The outer-loop optimization does the update with the outer-loop learning rate \u03b2 as:\u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207 \u03b8 T i \u223cp(T ) L q T i (f \u03b8 i )\nThis involves computing second-order gradients, i.e., the backward pass works through the update step in Equation 1, which is a computationally expensive process.  Finn et al. (2017) propose a first-order approximation, called FOMAML, which computes the gradients with respect to \u03b8 i rather than \u03b8. The outer-loop optimization step thus reduces to:\u03b8 \u2190 \u03b8 \u2212 \u03b2 T i \u223cp(T ) \u2207 \u03b8 i L q T i (f \u03b8 i )\nDuring meta-testing, new tasks are learned from the support sets and the performance is evaluated on the corresponding query sets.\nOptimization-based meta-learning methods (Finn et al., 2017; Nichol et al., 2018; Triantafillou et al., 2020) have been shown to work well for few-shot learning problems in NLP -specifically machine translation (Gu et al., 2018), relation classification (Obamuyide and Vlachos, 2019b), sentence-level semantic tasks (Dou et al., 2019; Bansal et al., 2019), text classification (Jiang et al., 2018), and word sense disambiguation (Holla et al., 2020).\n\n2.2 Continual learning\nCurrent approaches to prevent catastrophic forgetting can be grouped into one of several categories: (1) constrained optimization-based approaches with or without regularization (Kirkpatrick et al., 2017; Zenke et al., 2017; Chaudhry et al., 2018; Aljundi et al., 2018; Schwarz et al., 2018) that prevent large updates on weights that are important to previously seen tasks; (2) memorybased approaches (Rebuffi et al., 2017; Sprechmann et al., 2018; Wang et al., 2019; d'Autume et al., 2019) that replay examples stored in the memory; (3) generative replay-based approaches (Shin et al., 2017; Kemker and Kanan, 2018; Sun et al., 2020) that employ a generative model instead of a memory module; (4) architecture-based approaches (Rusu et al., 2016; Chen et al., 2016; Fernando et al., 2017) that either use different subsets of the network for different tasks or dynamically expand the networks; and (5) hybrid approaches that formulate optimization constraints based on examples in memory (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019). More recently, Riemer et al. (2019) proposed an approach based on a firstorder optimization-based meta-learning algorithm, Reptile (Nichol et al., 2018), augmented with experience replay. However, it involved interleaving every training example with several examples from memory, leading to a high replay rate.\nElastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), Gradient Episodic Memory (GEM) (Lopez-Paz and Ranzato, 2017) and Averaged-GEM (A-GEM) (Chaudhry et al., 2019) are three popular continual learning methods. EWC introduces a regularization term involving the Fisher information matrix that indicates the importance of each of the parameters to previous tasks. GEM solves a constrained optimization problem as a quadratic program involving gradients from all examples from previous tasks. A-GEM is a more efficient version of GEM since it solves a simpler constrained optimization problem based on gradients from randomly drawn samples from previous tasks in the memory.  Wang et al. (2019) propose an alignment model named EA-EMR that limits the distortion in the embedding space in an LSTM-based (Hochreiter and Schmidhuber, 1997) architecture for lifelong relation extraction. For the same task, Obamuyide and Vlachos (2019a) show that utilizing Reptile (Nichol et al., 2018) with memory can improve performance and call their method MLLRE.  Han et al. (2020) 2020) present a model based on GPT-2 (Radford et al., 2019), called LAMOL, that simultaneously learns to solve new tasks and to generate pseudo-samples from previous tasks for replay. They perform sequential learning on five tasks from decaNLP (McCann et al., 2018) as well as multiple datasets for text classification.\n\n2.3 Continual learning in NLP\nAll these methods are not yet well-suited for application in real-life scenarios -MbPA++ has slow inference, Meta-MbPA has a higher rate of sampling examples from memory, and other methods require task identifiers and multiple epochs of training. Our approach, on the other hand, alleviates all these problems.\n\n3 Overview\n\n\n3.1 Task formulation\nA typical continual learning setup consists of a stream of K tasks T 1 , T 2 , ..., T K . For supervised learning tasks, every task T i consists of a set of data points x j with labels y j , i.e., {(x j , y j )} N i j=1 that are locally i.i.d., where N i is the size of task T i . We consider the setting where the goal is to learn a function f \u03b8 with parameters \u03b8 by only making one pass over the stream of tasks and with no identifiers of tasks T i available. In multi-task learning, on the other hand, it is possible to draw samples i.i.d from all tasks along with training for multiple epochs. Therefore, multi-task learning is an upper bound to continual learning in terms of performance.\nWe propose an approach to continual learning with meta-learning and experience replay where the updates are similar to first-order MAML. We maintain an episodic memory (or simply called memory) M which stores previously seen examples. Episodes for meta-training are constructed from the stream of examples as well as randomly sampled examples from M. We perform experience replay sparsely, i.e., a small number of examples are drawn from M and only after seeing many examples from the stream (i.e. at long intervals), therefore being computationally inexpensive.\n\n3.2 Motivation\nRiemer et al. ( 2019) note that, given two sets of gradients for shared parameters \u03b8, interference occurs when the dot product of gradients is negative, and transfer occurs when their dot product is positive. Additionally, they show that Reptile (Nichol et al., 2018) implicitly maximizes the dot product between gradients within an episode and, hence, when coupled with experience replay, it could facilitate continual learning.\nConsider a first-order MAML setup that performs one step of SGD on each of the m batches in the support set during the inner-loop of an episode. Starting with parameters \u03b8 0 = \u03b8, it results in a sequence of parameters \u03b8 1 , ..., \u03b8 m using the losses L 1 , ..., L m . The meta-gradient computed on the query set of the episode is:g FOMAML = \u2202L q (\u03b8 m ) \u2202\u03b8 m\nUsing Taylor series approximation as in Nichol et al. (2018), the expected gradient under minibatch sampling could be expressed as:E [gFOMAML] = E \u2202L q (\u03b8m) \u2202\u03b8 \u2212 \u03b1 2 \u2202 \u2202\u03b8 m j=1 \u2202L j (\u03b8j\u22121) \u2202\u03b8 \u2022 \u2202L q (\u03b8m) \u2202\u03b8 + O(\u03b1 2 ) (6)\nwhere \u03b1 is the inner-loop learning rate. We provide a more detailed derivation in Appendix A.4. Outerloop gradient descent with this gradient approximately solves the following optimization problem:min \u03b8 E L q (\u03b8m) \u2212 \u03b1 2 m j=1 \u2202L j (\u03b8j\u22121) \u2202\u03b8 \u2022 \u2202L q (\u03b8m) \u2202\u03b8\nThis objective seeks to minimize the loss on the query set along with maximizing the dot product between the support and query set gradients. Thus, integrating previously seen examples into the query set in a first-order MAML framework could also potentially improve continual learning by minimizing interference and maximizing transfer.\n\n3.3 Episode generation and experience replay\nWe assume that data points arrive in mini-batches of a given size b and every data point has a probability p write of being written into an episodic memory module M. We construct episodes on-the-fly from the stream of mini-batches. Given a buffer size m, we construct episode i by taking m mini-batches as the support set S i and the next batch as the query set Q i .\nWe explicitly define our experience replay mechanism as consisting of two fixed hyperparameters -replay interval R I , which indicates the number of data points seen between two successive draws from memory, and replay rate r \u2208 [0, 1] which indicates the proportion of examples to draw from memory relative to R I . Thus, after every R I examples from the stream, r \u2022 R I examples are drawn from the memory.\nWe use these sampled examples from memory as the query set. To perform experience replay in an episodic fashion, we compute the replay frequency R F as follows (see Appendix A.3 for further details):R F = R I /b + 1 m + 1 (8)\nHence, every R F episodes, we draw a random batch of size r \u2022 R I from M as the query set.\nFor other episodes, the query set is obtained from the data stream. The support set for replay episodes is still constructed from the stream. A high r and/or a low R I ensures that information is not forgotten, but in order to be computationally efficient and adhere to continual learning, it is necessary that r is low and R I is high, so that the replay is sparsely performed, both in terms of size and time.\nDuring meta-testing, we randomly draw m batches from the memory as the support set and take the entire test set of the respective task as the query set for evaluation. This is done primarily in order to match the testing and training conditionsBERT x y h \u03d5 g W RLN PLN\nFigure 2 : Architecture of OML.  (Vinyals et al., 2016). Figure 1 provides an illustration of the structure of episodes and experience replay.\n\n4 Methods\n\n\n4.1 OML-ER\nThe original OML algorithm (Javed and White, 2019) was designed to solve new continual learning problems during meta-testing. Here, we extend it to our setup by augmenting it with an episodic memory module to perform experience replay (ER), and refer to it as OML-ER.\nThe model f \u03b8 is composed of two functionsa representation learning network (RLN) h \u03c6 with parameters \u03c6 and a prediction learning network (PLN) g W with parameters W such that \u03b8 = \u03c6 \u222a W and f \u03b8 (x) = g W (h \u03c6 (x)) for an input x. In each episode, the RLN is frozen while the PLN is fine-tuned during the inner-loop optimization. In the outer-loop, both the RLN and the PLN are meta-learned.\nDuring the inner-loop optimization in episode i, the PLN is fine-tuned on the support set minibatches S i with SGD to give:W i = SGD (L i , \u03c6, W , S i , \u03b1) (9)\nwhere L i is the loss function. Using the query set, the objective we optimize for is:J(\u03b8) = L i \u03c6, W i , Q i (10)\nDuring a regular episode, the objective encourages generalization to unseen data whereas during a replay episode, it promotes retention of knowledge from previously seen data.\nFor the outer-loop optimization, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate \u03b2 to update all parameters -both the RLN and PLN:\u03b8 \u2190 Adam(J(\u03b8), \u03b2)\nThe above optimization would involve secondorder gradients. Instead, we use the first-order variant where the gradients are taken with respect to \u03b8 i = \u03c6 \u222a W i . We use BERT BASE (Devlin et al., 2019) as the RLN (fully fine-tuned; output from the [CLS] token) and a single linear layer mapping to the classes as the PLN. The architecture of the model is shown in Figure 2. Beaulieu et al. (2020) proposed ANML that outperformed OML in solving new continual learning problems in image classification. Inspired by neuromodulatory processes in the brain, they design a context-dependent gating mechanism to achieve selective plasticity, i.e., limited and/or selective modification of parameters with new data. We refer to our extension of this method as ANML-ER.\n\n4.2 ANML-ER\nThe model f \u03b8 is composed of two networks -a regular prediction network (PN) and a neuromodulatory network (NM) that selectively gates the internal activations of the prediction network via element-wise multiplication. Formally, the NM is a function h \u03c6 with parameters \u03c6, and the PN isa composite function g W 2 \u2022 e W 1 with parameters W = W 1 \u222a W 2 .\nThe output is obtained as:f \u03b8 (x) = g W 2 (e W 1 (x) \u2022 h \u03c6 (x))\nIn the inner-loop, the NM is fixed while the PN is fine-tuned on the support set. Due to the choice of our notation, Equation 9 is the form of the innerloop here too. In the outer-loop, both the NM and the PN are updated as in Equation 11 with firstorder gradients.\nOur PN is the BERT BASE encoder followed by a linear layer mapping to the classes as in OML-ER. For the NM, we use BERT BASE followed by two linear layers (768 units) with ReLU non-linearity between them and a final sigmoid non-linearity to limit the gating signal to [0, 1]. We keep the NM BERT frozen throughout to reduce the total number of parameters. Our preliminary experiments indicated that fine-tuning the NM BERT in addition produces negligible improvements. Figure 3 presents the model architecture.\nAlgorithms 1 and 2 outline the meta-training and meta-testing procedure respectively that is common to both OML-ER and ANML-ER.for i = 1, 2, ... do S i \u2190 m batches from the stream if i = R F then Q i \u2190 sample(M, r \u2022 R I ) end else Q i \u2190 next batch from the stream write(M, Q i , p write ) end write(M, S i , p write ) W i = SGD(L i , \u03c6, W , S i , \u03b1) J(\u03b8) = L i (\u03c6, W i , Q i ) \u03b8 \u2190 Adam(J(\u03b8), \u03b2) endQ \u2190 T W = SGD(L, \u03c6, W , S, \u03b1) predict(Q, \u03c6, W )\n\n4.3 Baselines\nWe consider four BERT-based baselines to evaluate the effectiveness of our approach.\nSEQ We train our model \"traditionally\" on all tasks in a sequential manner i.e., one after the other, without replay. A-GEM It requires replay at every training step and task identifiers by default (Chaudhry et al., 2019), but we adapt it to our setting by randomly sampling data points from the memory in sparse intervals.\n\nREPLAY\nMTL We train our model in a \"traditional\" multitask setup for multiple epochs on mini-batches that are sampled i.i.d from the pool of all tasks. Thus, it serves as an upper bound for the performance of continual learning methods.  (Han et al., 2018). It consists of 44, 800 training sentences and 11, 200 test sentences, and a total of 80 relations along with their corresponding names available. Each sentence has a ground-truth relation as well as a set of 10 negative candidate relations. The goal is to predict the correct relation among them. To construct tasks for continual learning, they first perform K-means clustering over the average GloVe embeddings (Pennington et al., 2014) of the relation names to obtain 10 disjoint clusters. Each task then comprises of data points having ground-truth relations from the corresponding cluster. In any given task, the candidate relations that were not seen in earlier tasks are removed. But, if all the candidate relations are unseen, the last two candidates are retained. The evaluation metric is the accuracy on a single test set containing relations from all the clusters.\n\n5.2 Implementation\nFor text classification, we largely maintain the experimental setup of d' Autume et al. (2019). We consider four orders of the datasets (see Appendix A.1) and report the average results obtained from three independent runs. We also set p write = 1. While they perform replay by drawing 100 examples from memory for every 10, 000 examples from the stream, we draw 96 examples from memory for every 9, 600 examples which is more convenient with batch size b = 16. Thus, we have r = 0.01 and R I = 9600. We obtain the best hyperparameters by tuning on the first order of the datasets only. The learning rate for SEQ, A-GEM, REPLAY and MTL is 3e\u22125. MTL is trained for 2 epochs. For OML-ER, the inner-loop and outer-loop learning rates are 1e\u22123 and 1e\u22125 respectively whereas for ANML-ER, they are 3e\u22123 and 1e\u22125 respectively. The support set buffer size m for both of them is 5. We truncate the input sequence length to 300 for ANML-ER and 448 for the rest. The loss function is the cross-entropy loss across the 33 classes. For the evaluation of meta-learning methods, we con-struct five episodes at meta-test time, one for each of the datasets, where their query sets consist of the test sets of these datasets.\nFor relation extraction, we consider five orders of the tasks as in Wang et al. (2019). We report the average accuracy on the test set over the five orders, averaged over three independent runs. Sentencerelation pairs are concatenated with a [SEP] token between them to serve as the input. Since this is a smaller dataset, we set R I = 1600 and r = 0.01. Additionally, b = 4, m = 5 and p write = 1. Hyperparameter tuning is performed only on the first order. The learning rate is 3e\u22125 for SEQ, A-GEM and REPLAY. MTL is trained with a learning rate of 1e\u22125 for 3 epochs. The inner-loop and outer-loop learning rates are 1e\u22123 and 3e\u22125 for OML-ER as well as ANML-ER. All models are trained using the binary cross-entropy loss, treating the true sentence-relation pairs as the positive class and the incorrect pairs as the negative class. The prediction is obtained as an argmax over the logit scores. Meta-learning methods are evaluated using a single meta-test episode with the test set as the query set.\n\n6 Experiments and results\nText classification We present the average accuracy across the baselines and our models with standard deviations across runs in Table 1. We perform significance testing with a two-tailed paired t-test at a significance level of 0.05. Simply training on the datasets sequentially leads to extreme forgetting as reflected in the low accuracy of the SEQ model. With A-GEM, we get only a small, but significant gain (p = 0.008) compared to sequential training. By analyzing the frequency of constraint violations in Appendix A.6, we find that A-GEM updates on BERT often behave similar to that in SEQ, which explains its poor performance. REPLAY, on the other hand, drastically improves performance, indicating that BERT benefits substantially even from a sparse experience replay. MbPA++ is the current state-of-the-art on this benchmark under the realistic setup of excluding task identifiers, using sparse replay and a single training epoch.  Sun et al. (2020) re-implement MbPA++ and obtain a higher score than the original implementation. We surmise that this is partly attributed to the fact that they perform replay after every 100 steps along with dynamic batching and therefore likely resulting in a higher replay interval. Our approach, ANML-ER, achieves the highest accuracy, demonstrating that our meta-learning setup is more effective at mitigating catastrophic forgetting. OML-ER is almost as effective as ANML-ER, with the differences between the two being statistically insignificant (p = 0.993). Although LAMOL has a higher score, it is not directly comparable to our methods since it uses task identifiers and multiple epochs of training, and has a higher generative replay rate of 20%, all of which make the task easier. Meta-MbPA is not directly comparable either since it performs local adaptation on nearest neighbors obtained from the memory during all its inner loop updates, thus having a higher replay rate effectively. Our metalearning approach further narrows the gap with the MTL upper bound.\n\nRelation extraction\nWe report the average test set accuracy along with the standard deviation across the three runs in Table 2. We see that A-GEM performs similar to SEQ, with the differences being statistically insignificant (p = 0.218). Including sparse experience replay (REPLAY) again leads to a substantial increase in performance compared to SEQ. A low A-GEM performance compared to a simple replay method on this benchmark was also observed in Wang et al. (2019). OML-ER and ANML-ER significantly outperform all the baselines (p = 0.006 for OML-ER and p = 0.026 for ANML-ER when compared to REPLAY), and the former achieves the highest accuracy overall but, again, the differences between the two are not statistically significant (p = 0.098). Although not directly comparable, both of them outperform the previous state-of-the-art LSTM-based method EMAR (Han et al., 2020), despite it using task identities as additional information and training for multiple epochs. There is, however, a wide gap between OML-ER and the MTL upper bound. We return to this in a later analysis.\n\n7 Analysis\nAblation study To investigate the relative strengths of the various components in our approach, we perform an ablation study and report the results in Table 3. Meta-learning without replay leads to a large drop in performance, showing that experience replay, despite being sparse, is crucial. Interestingly however, meta-learning without replay still has considerably higher scores compared to SEQ, demonstrating that it is more resilient to catastrophic forgetting. Retrieving relevant exam- (Sun et al., 2020) 74.1 74.9 73.1 74.9 74.2 LAMOL (Sun et al., 2020) 76.7 77.2 76.1 76.1 76.5 Meta-MbPA (Wang et al., 2020) 77.9 76.7 77.3 77.6 77.3 SEQ 16.7 \u00b1 0.7 25.0 \u00b1 0.5 19.5 \u00b1 0.4 22.1 \u00b1 0.5 20.8 \u00b1 0.5 A-GEM 16.6 \u00b1 0.9 25.9 \u00b1 1.1 21.6 \u00b1 0.8 23.5 \u00b1 1.0 21.9 \u00b1 0. Effect of replay rate We noted previously that there exists a gap in performance between our best model and MTL. To analyze if increasing the replay rate can help narrow the gap, we train both REPLAY and OML-ER with a 2% and 4% replay rate 3, keeping R I the same as before (Table 4).\nOn text classification, OML-ER has similar performance (p = 0.936) with 2% replay rate and a small, significant improvement (p = 0.001) with 4% replay rate. The same trend is observed with REPLAY as the replay rate increases (p = 0.861 and p = 0.045) . In contrast, OML-ER and RE-PLAY improve by a significantly greater extent on relation extraction (p = 3e\u22124 and p = 0.048 respectively). We surmise this is because text classification has equally sized tasks whereas the tasks in relation extraction are imbalanced (see Appendix A.2). Since we employ uniform sampling for memory read/write, this imbalance is reflected in the memory, causing larger tasks to be replayed more often and underrepresented tasks to be forgotten more quickly. A higher replay rate therefore increases the chances of sufficiently revisiting all previous tasks, leading to better scores. Additionally, on both benchmarks, OML-ER outperforms RE-PLAY even with higher replay rates. There is still a wide gap between OML-ER with 4% replay and MTL, indicating there is scope for improvement. Effect of memory size In our experiments so far, we store all the examples in memory; however, this does not scale well when the number of tasks is very large. In order to investigate the effect of memory size on performance, we present the accuracy of OML-ER with 5% and 1% memory capacity in Table 5. We achieve this by setting p write to 0.05 and 0.01 respectively. There are insignificant changes (p = 0.074 and p = 0.952 respectively) in average accuracy even with reduced memory for text classification. MbPA++, on the other hand, was shown to have a drop of 3% accuracy with 10% memory capacity (d'Autume et al., 2019), which demonstrates that our method is more memory-efficient.\nPerformance on relation extraction suffers a small but significant drop (p = 0.040) with 1% memory. The difference is insignificant (p = 0.741) with 5% memory and, overall, can still be considered memory-efficient.\nEpisodic updates In addition to the automatic gradient alignment that comes with meta-learning, we believe that its episodic nature is another reason for its strength in lifelong learning. In text classification for example, SEQ has a replay every 600 optimizer steps whereas meta-learning, by way of its formulation, has a replay every 101 meta-optimizer steps (using Equation 8 with our hyperparameters) 4. Fewer updates between replays likely aids in knowledge retention. To probe deeper, we trained our REPLAY model such that replay occurs every 100 optimizer steps by setting R I = 1600, with everything else being the same. This achieves an accuracy of 74.4 \u00b1 0.2. Although this is now closer to our meta-learning methods, it is still significantly lower (p = 0.001 for OML-ER and p = 8e\u22125 for ANML-ER). Therefore, episodic updates in metalearning are an important part of the model, contributing positively to performance. For \"regular\" training to match the same level of performance, experience replay would need to be performed more often.\n\n8 Discussion\nContinual learning methods so far have relied on manual heuristics and/or have computational bottlenecks. MbPA++ is inexpensive during training due to sparse replay, but its inference is expensive since it requires retrieval of K nearest neighbors for every test example and multiple gradient steps on them. A-GEM, on the other hand, is slower to train due to its projection steps. OML-ER achieves the best of both worlds -its training is fast because its inner-loop, which makes up a large portion of the training, involves only updating the small PLN, and its inference is fast since it relies only on a small number of updates on randomly drawn examples from memory. Furthermore, it also retains its performance when the memory capacity is scaled down.\nOur method uses a simple, random write mechanism. Other strategies such as those based on surprise (Ramalho and Garnelo, 2019) and forgetting (Toneva et al., 2019) could further refine performance. Furthermore, the problem of task size imbalance could be mitigated with class-balancing reservoir sampling (Chrysakis and Moens, 2020).\nIn our experiments on text classification, we assume that all the classes are known beforehand.\nLifelong learning when the classes are unknown a priori and available only during each of the individual tasks is more challenging and would be an interesting extension.\nRecently, Knoblauch et al. (2020) showed theoretically that optimal continual learning is an NP-hard problem and requires perfect memorization of the past. An implication of this finding is that replay-based methods are more effective than regularization-based methods. Therefore, experience replay would perhaps remain a key component in designing future, more advanced methods.\nAnother promising direction for future work would be to integrate differential Hebbian plasticity (Miconi et al., 2018 (Miconi et al., , 2019) ) into a meta-learning framework for continual learning. Designing an appropriate neuromodulator for transformer-based language models or encouraging sparsity in them also requires additional work.\n\n9 Conclusion\nWe showed that pre-trained transformer-based language models, meta-learning and sparse experience replay produce a synergy that improves lifelong learning on language tasks in a realistic setup. This is an important step in moving away from manuallydesigned solutions into simpler, more generalizable methods to ultimately achieve human-like learning. Meta-learning could further be exploited for the combined setting of few-shot and lifelong learning. It might also be promising in learning distinct NLP tasks in a curriculum learning fashion.\n\nA Appendix\nA.1 Dataset order for text classification For text classification, the four different orderings of the datasets are:1. Yelp \u2192 AGNews \u2192 DBpedia \u2192 Amazon \u2192 Yahoo 2. DBpedia \u2192 Yahoo \u2192 AGNews \u2192 Amazon \u2192 Yelp 3. Yelp \u2192 Yahoo \u2192 Amazon \u2192 DBpedia \u2192 AGNews 4. AGNews \u2192 Yelp \u2192 Amazon \u2192 Yahoo \u2192 DBpedia\n\nA.2 Task distribution for relation extraction\nIn relation extraction, the size of each cluster is not balanced. Hence, each of the tasks vary in their size.\nIn Figure 4 we plot the number of relations and the number of sentences in each cluster. Overall, there is a great imbalance with respect to the task size, with cluster 2 and 6 having a disproportionately larger size compared to the other clusters.\n\nA.3 Expression for replay frequency\nIn REPLAY and A-GEM, since gradient updates occur after seeing a batch of size b from the stream, the replay frequency R F , i.e., the number of steps between the replay interval R I , is simply given byR F = R I b[(R F \u2212 1)(m + 1) + m] = R I R F \u2212 1 = R I /b \u2212 m m + 1 R F = R I /b + 1 m + 1\nwhere we round it up to the nearest integer so that replay is not performed before R I examples.\n\nA.4 Derivation of gradients\nConsider a first-order MAML setup that performs one step of SGD on each of the m batches in the support set during the inner-loop of an episode.\nStarting with parameters \u03b8 0 = \u03b8, it results in a sequence of parameters \u03b8 1 , ..., \u03b8 m using the losses L 1 , ..., L m . The query set could be considered as the (m + 1)-th batch that produces the metagradient for \u03b8 using L (m+1) = L q . We introduce the following two notations to denote the gradient and the Hessian with respect to the initial parameters \u03b8:\u1e21i = \u2202L i (\u03b8 i\u22121 ) \u2202\u03b8 (15) Hi = \u2202 2 L i (\u03b8 i\u22121 ) \u2202\u03b8 2\nUsing Taylor series approximation, Nichol et al. (2018) show that the meta-gradient can be written as: In Table 6, we summarize all the hyperparameters for text classification and relation extraction. We use the random seeds 42 -44 for the three independent runs. All models were trained on a system with a single Nvidia Titan RTX GPU and 45 GB memory.g FOMAML = \u2202L q (\u03b8 m ) \u2202\u03b8 m = \u1e21m+1 \u2212 \u03b1 Hm+1\n\nA.6 Frequency of constraint violations\nA-GEM solves a constrained optimization problem such that the dot product between the gradients from the current batch and a randomly drawn batch from the memory is greater than or equal to zero. We check constraint satisfaction by treating the model parameters as a single vector. To analyze the poor performance of A-GEM on our setup, we plot the average number of constraint violations across the four orders that occur per task in text classification in Figure 5. Note that the total number of optimizer steps per task is 7187 and replay 7DVNV 1RRIFRQVWUDLQWYLRODWLRQV %(57ILQHWXQLQJ )XOO 7RSOD\\HUV occurs about 11 times for each. When fine-tuning the whole of BERT, we have relatively few violations, meaning that no gradient correction is done most of the time. This perhaps relates to the finding by Merchant et al. (2020) that fine-tuning BERT primarily affects the top layers and does not lead to catastrophic forgetting of linguistic phenomena in the deeper layers. We see that the number of violations increase when we only fine-tune the top 2 layers of BERT. Yet, it was insufficient to reach the performance of a simple replay method.\n\nA.7 ANML visualization\nThe original OML and ANML models were shown to produce sparse representations with CNN encoders for images (Javed and White, 2019; Beaulieu et al., 2020). Sparse representations alleviate forgetting since only a few neurons are active for a given input. We visualize the representations from BERT before and after neuromodulation, along with the neuromodulatory signal, in our ANML-ER model in Figure 6. Clearly, none of the representations are sparse. Moreover, most of the neuromodulatory signal is composed of ones, further confirming our hypothesis that the neuromodulator does not play a significant role here. The lack of sparsity was also observed in OML-ER. Perhaps, a more sophisticated neuromodulatory mechanism is required to induce sparsity in pre-trained transformer-based language models.\n\nFootnotes:\n1: https://github.com/Nithin-Holla/ MetaLifelongLanguage\n3: The maximum replay rate for our meta-learning methods is 1/m = 20% i.e., replay every episode with m = 5\n4: However, we note that optimizer steps and meta-optimizer steps are not the same nor directly comparable as such.\n\nReferences:\n\n- Rahaf Aljundi, Francesca Babiloni, Mohamed Elho- seiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Confer- ence on Computer Vision (ECCV), pages 139-154.- Trapit Bansal, Rishikesh Jha, and Andrew McCallum. 2019. Learning to few-shot learn across diverse nat- ural language classification tasks. arXiv preprint arXiv:1911.03863.\n\n- Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff Clune, and Nick Cheney. 2020. Learning to continually learn. arXiv preprint arXiv:2002.09571.\n\n- Y. Bengio, S. Bengio, and J. Cloutier. 1991. Learn- ing a synaptic learning rule. In IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii, pages 969 vol.2-. Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41-75.\n\n- Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. 2018. Riemannian walk for incremental learning: Understanding forget- ting and intransigence. In The European Conference on Computer Vision (ECCV).\n\n- Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2019. Efficient lifelong learning with A-GEM. In International Conference on Learning Representations.\n\n- Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. 2016. Net2net: Accelerating learning via knowl- edge transfer. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Pro- ceedings.\n\n- Zhiyuan Chen and Bing Liu. 2018. Lifelong machine learning. Synthesis Lectures on Artificial Intelli- gence and Machine Learning, 12(3):1-207.\n\n- Aristotelis Chrysakis and Marie-Francine Moens. 2020. Online continual learning from imbalanced data. In Proceedings of Machine Learning and Systems 2020, pages 8303-8312.\n\n- Cyprien de Masson d'Autume, Sebastian Ruder, Ling- peng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems 32, pages 13143-13152.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\n- Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos. 2019. Investigating meta-learning algorithms for low-resource natural language understanding tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 1192- 1197, Hong Kong, China. Association for Computa- tional Linguistics.\n\n- Sebastian Farquhar and Yarin Gal. 2018. Towards Robust Evaluations of Continual Learning. Life- long Learning: A Reinforcement Learning Approach Workshop at ICML.\n\n- Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. 2017. Pathnet: Evolu- tion channels gradient descent in super neural net- works.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th In- ternational Conference on Machine Learning, vol- ume 70 of Proceedings of Machine Learning Re- search, pages 1126-1135, International Convention Centre, Sydney, Australia. PMLR.\n\n- Robert M French. 1999. Catastrophic forgetting in con- nectionist networks. Trends in Cognitive Sciences, 3(4):128-135.\n\n- Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li, and Kyunghyun Cho. 2018. Meta-learning for low- resource neural machine translation. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622-3631, Brussels, Belgium. Association for Computational Linguistics.\n\n- Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2020. Contin- ual relation learning via episodic memory activation and reconsolidation. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 6429-6440, Online. Association for Computational Linguistics.\n\n- Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classifica- tion dataset with state-of-the-art evaluation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 4803- 4809, Brussels, Belgium. Association for Computa- tional Linguistics.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.\n\n- Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, and Ekaterina Shutova. 2020. Learning to learn to disambiguate: Meta-learning for few-shot word sense disambiguation. In Findings of the Associa- tion for Computational Linguistics: EMNLP 2020, pages 4517-4533, Online. Association for Compu- tational Linguistics.\n\n- Khurram Javed and Martha White. 2019. Meta- learning representations for continual learning. In Advances in Neural Information Processing Systems 32, pages 1820-1830.\n\n- Xiang Jiang, Mohammad Havaei, Gabriel Chartrand, Hassan Chouaib, Thomas Vincent, Andrew Jesson, Nicolas Chapados, and Stan Matwin. 2018. Atten- tive task-agnostic meta-learning for few-shot text classification. In The Second Workshop on Met- aLearning at NeurIPS.\n\n- Ronald Kemker and Christopher Kanan. 2018. Fear- net: Brain-inspired model for incremental learning. In International Conference on Learning Represen- tations.\n\n- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\n- James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag- nieszka Grabska-Barwinska, et al. 2017. Over- coming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.\n\n- Jeremias Knoblauch, Hisham Husain, and Tom Diethe. 2020. Optimal continual learning has perfect mem- ory and is np-hard. In Proceedings of Machine Learning and Systems 2020, pages 338-348.\n\n- Yuanpeng Li, Liang Zhao, Kenneth Church, and Mo- hamed Elhoseiny. 2020. Compositional language continual learning. In International Conference on Learning Representations.\n\n- Long-Ji Lin. 1992. Self-improving reactive agents based on reinforcement learning, planning and teach- ing. In Machine Learning, pages 293-321.\n\n- David Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems 30, pages 6467-6476.\n\n- Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language de- cathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730.\n\n- Michael McCloskey and Neal J. Cohen. 1989. Catas- trophic interference in connectionist networks: The sequential learning problem. Psychology of Learn- ing and Motivation -Advances in Research and The- ory, 24(C):109-165.\n\n- Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. 2020. What happens to BERT embeddings during fine-tuning? arXiv preprint arXiv:2004.14448.\n\n- Thomas Miconi, Aditya Rawal, Jeff Clune, and Ken- neth O. Stanley. 2019. Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity. In International Confer- ence on Learning Representations.\n\n- Thomas Miconi, Kenneth O. Stanley, and Jeff Clune. 2018. Differentiable plasticity: training plastic neu- ral networks with backpropagation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stock- holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 3556-3565. PMLR.\n\n- Alex Nichol, Joshua Achiam, and John Schulman. 2018. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Abiola Obamuyide and Andreas Vlachos. 2019a. Meta-learning improves lifelong relation extraction. In Proceedings of the 4th Workshop on Represen- tation Learning for NLP (RepL4NLP-2019), pages 224-229, Florence, Italy. Association for Computa- tional Linguistics.\n\n- Abiola Obamuyide and Andreas Vlachos. 2019b. Model-agnostic meta-learning for relation classifica- tion with limited supervision. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 5873-5879, Florence, Italy. Association for Computational Linguistics.\n\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1532-1543, Doha, Qatar. Asso- ciation for Computational Linguistics.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\n- Tiago Ramalho and Marta Garnelo. 2019. Adaptive posterior learning: few-shot learning with a surprise- based memory module. In International Conference on Learning Representations.\n\n- Roger Ratcliff. 1990. Connectionist models of recog- nition memory: constraints imposed by learning and forgetting functions. Psychological Review, 97(2):285.\n\n- Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. 2017. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001-2010.\n\n- Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. 2019. Learning to learn without forgetting by max- imizing transfer and minimizing interference. In International Conference on Learning Representa- tions.\n\n- Andrei A Rusu, Neil C Rabinowitz, Guillaume Des- jardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks. arXiv preprint arXiv:1606.04671.\n\n- Jurgen Schmidhuber. 1987. Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta...-hook. Diploma thesis, Tech- nische Universitat Munchen, Germany, 14 May.\n\n- Jonathan Schwarz, Wojciech Czarnecki, Je- lena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. 2018. Progress & compress: A scalable framework for continual learning. In ICML, pages 4535-4544. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual learning with deep generative replay. In Advances in Neural Information Process- ing Systems 30, pages 2990-2999.\n\n- P. Sprechmann, S. Jayakumar, J. Rae, A. Pritzel, A. Puigdom\u00e8nech, B. Uria, O. Vinyals, D. Hassabis, R. Pascanu, and C. Blundell. 2018. Memory-based Parameter Adaptation. In ICLR 2018.\n\n- Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2020. Lamol: Language modeling for lifelong lan- guage learning. In International Conference on Learning Representations.\n\n- Sebastian Thrun. 1998. Lifelong learning algorithms. In Learning to learn, pages 181-209. Springer.\n\n- Sebastian Thrun and Lorien Pratt. 1998. Learning to Learn. Kluwer Academic Publishers, USA.\n\n- Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geof- frey J. Gordon. 2019. An empirical study of exam- ple forgetting during deep neural network learning. In International Conference on Learning Represen- tations.\n\n- Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pas- cal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Man- zagol, and Hugo Larochelle. 2020. Meta-dataset: A dataset of datasets for learning to learn from few ex- amples. In International Conference on Learning Representations.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Ko- ray Kavukcuoglu, and Daan Wierstra. 2016. Match- ing networks for one shot learning. In Advances in Neural Information Processing Systems 29, pages 3630-3638.\n\n- Hong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, and William Yang Wang. 2019. Sen- tence embedding alignment for lifelong relation ex- traction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 796-806, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n- Zirui Wang, Sanket Vaibhav Mehta, Barnabas Poczos, and Jaime Carbonell. 2020. Efficient meta lifelong- learning with limited memory. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP), pages 535-548, Online. Association for Computational Linguistics.\n\n- Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tom\u00e1s Kocisk\u00fd, Mike Chrzanowski, Ling- peng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelligence. CoRR, abs/1901.11373.\n\n- Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual learning through synaptic intelli- gence. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3987-3995, International Convention Centre, Syd- ney, Australia. PMLR.\n\n- Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification. In Advances in Neural Information Pro- cessing Systems 28, pages 649-657.\n\n", "annotations": {"ReferenceToTable": [{"begin": 25727, "end": 25728, "target": "#tab_2", "idx": 0}, {"begin": 27737, "end": 27738, "target": "#tab_3", "idx": 1}, {"begin": 28865, "end": 28866, "target": "#tab_4", "idx": 2}, {"begin": 29750, "end": 29751, "target": "#tab_5", "idx": 3}, {"begin": 37260, "end": 37261, "idx": 4}], "ReferenceToFootnote": [{"begin": 7298, "end": 7299, "target": "#foot_0", "idx": 0}, {"begin": 29709, "end": 29710, "target": "#foot_1", "idx": 1}, {"begin": 32128, "end": 32129, "target": "#foot_2", "idx": 2}], "SectionMain": [{"begin": 955, "end": 39558, "idx": 0}], "ReferenceToFormula": [{"begin": 12526, "end": 12530, "idx": 0}, {"begin": 14512, "end": 14516, "idx": 1}], "SectionReference": [{"begin": 39853, "end": 53761, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 955, "idx": 0}], "Div": [{"begin": 86, "end": 947, "idx": 0}, {"begin": 958, "end": 7330, "idx": 1}, {"begin": 7332, "end": 10073, "idx": 2}, {"begin": 10075, "end": 12845, "idx": 3}, {"begin": 12847, "end": 13187, "idx": 4}, {"begin": 13189, "end": 13200, "idx": 5}, {"begin": 13202, "end": 14479, "idx": 6}, {"begin": 14481, "end": 16098, "idx": 7}, {"begin": 16100, "end": 18060, "idx": 8}, {"begin": 18062, "end": 18072, "idx": 9}, {"begin": 18074, "end": 20123, "idx": 10}, {"begin": 20125, "end": 21776, "idx": 11}, {"begin": 21778, "end": 22200, "idx": 12}, {"begin": 22202, "end": 23334, "idx": 13}, {"begin": 23336, "end": 25565, "idx": 14}, {"begin": 25567, "end": 27610, "idx": 15}, {"begin": 27612, "end": 28695, "idx": 16}, {"begin": 28697, "end": 32771, "idx": 17}, {"begin": 32773, "end": 34862, "idx": 18}, {"begin": 34864, "end": 35421, "idx": 19}, {"begin": 35423, "end": 35725, "idx": 20}, {"begin": 35727, "end": 36132, "idx": 21}, {"begin": 36134, "end": 36559, "idx": 22}, {"begin": 36561, "end": 37543, "idx": 23}, {"begin": 37545, "end": 38731, "idx": 24}, {"begin": 38733, "end": 39558, "idx": 25}], "Head": [{"begin": 958, "end": 972, "n": "1", "idx": 0}, {"begin": 7332, "end": 7349, "n": "2.1", "idx": 1}, {"begin": 10075, "end": 10097, "n": "2.2", "idx": 2}, {"begin": 12847, "end": 12876, "n": "2.3", "idx": 3}, {"begin": 13189, "end": 13199, "n": "3", "idx": 4}, {"begin": 13202, "end": 13222, "n": "3.1", "idx": 5}, {"begin": 14481, "end": 14495, "n": "3.2", "idx": 6}, {"begin": 16100, "end": 16144, "n": "3.3", "idx": 7}, {"begin": 18062, "end": 18071, "n": "4", "idx": 8}, {"begin": 18074, "end": 18084, "n": "4.1", "idx": 9}, {"begin": 20125, "end": 20136, "n": "4.2", "idx": 10}, {"begin": 21778, "end": 21791, "n": "4.3", "idx": 11}, {"begin": 22202, "end": 22208, "idx": 12}, {"begin": 23336, "end": 23354, "n": "5.2", "idx": 13}, {"begin": 25567, "end": 25592, "n": "6", "idx": 14}, {"begin": 27612, "end": 27631, "idx": 15}, {"begin": 28697, "end": 28707, "n": "7", "idx": 16}, {"begin": 32773, "end": 32785, "n": "8", "idx": 17}, {"begin": 34864, "end": 34876, "n": "9", "idx": 18}, {"begin": 35423, "end": 35433, "idx": 19}, {"begin": 35727, "end": 35772, "idx": 20}, {"begin": 36134, "end": 36169, "idx": 21}, {"begin": 36561, "end": 36588, "idx": 22}, {"begin": 37545, "end": 37583, "idx": 23}, {"begin": 38733, "end": 38755, "idx": 24}], "Paragraph": [{"begin": 86, "end": 947, "idx": 0}, {"begin": 973, "end": 1646, "idx": 1}, {"begin": 1647, "end": 2997, "idx": 2}, {"begin": 2998, "end": 3648, "idx": 3}, {"begin": 3649, "end": 3792, "idx": 4}, {"begin": 3793, "end": 4689, "idx": 5}, {"begin": 4690, "end": 5519, "idx": 6}, {"begin": 5520, "end": 6195, "idx": 7}, {"begin": 6196, "end": 7300, "idx": 8}, {"begin": 7301, "end": 7330, "idx": 9}, {"begin": 7350, "end": 7939, "idx": 10}, {"begin": 7940, "end": 8664, "idx": 11}, {"begin": 8692, "end": 8873, "idx": 12}, {"begin": 8923, "end": 9059, "idx": 13}, {"begin": 9100, "end": 9448, "idx": 14}, {"begin": 9492, "end": 9622, "idx": 15}, {"begin": 9623, "end": 10073, "idx": 16}, {"begin": 10098, "end": 11452, "idx": 17}, {"begin": 11453, "end": 12845, "idx": 18}, {"begin": 12877, "end": 13187, "idx": 19}, {"begin": 13223, "end": 13916, "idx": 20}, {"begin": 13917, "end": 14479, "idx": 21}, {"begin": 14496, "end": 14925, "idx": 22}, {"begin": 14926, "end": 15255, "idx": 23}, {"begin": 15283, "end": 15414, "idx": 24}, {"begin": 15504, "end": 15702, "idx": 25}, {"begin": 15761, "end": 16098, "idx": 26}, {"begin": 16145, "end": 16512, "idx": 27}, {"begin": 16513, "end": 16920, "idx": 28}, {"begin": 16921, "end": 17120, "idx": 29}, {"begin": 17147, "end": 17237, "idx": 30}, {"begin": 17238, "end": 17648, "idx": 31}, {"begin": 17649, "end": 17893, "idx": 32}, {"begin": 17918, "end": 18060, "idx": 33}, {"begin": 18085, "end": 18352, "idx": 34}, {"begin": 18353, "end": 18743, "idx": 35}, {"begin": 18744, "end": 18867, "idx": 36}, {"begin": 18904, "end": 18990, "idx": 37}, {"begin": 19019, "end": 19194, "idx": 38}, {"begin": 19195, "end": 19346, "idx": 39}, {"begin": 19364, "end": 20123, "idx": 40}, {"begin": 20137, "end": 20423, "idx": 41}, {"begin": 20490, "end": 20516, "idx": 42}, {"begin": 20554, "end": 20819, "idx": 43}, {"begin": 20820, "end": 21330, "idx": 44}, {"begin": 21331, "end": 21458, "idx": 45}, {"begin": 21792, "end": 21876, "idx": 46}, {"begin": 21877, "end": 22200, "idx": 47}, {"begin": 22209, "end": 23334, "idx": 48}, {"begin": 23355, "end": 24562, "idx": 49}, {"begin": 24563, "end": 25565, "idx": 50}, {"begin": 25593, "end": 27610, "idx": 51}, {"begin": 27632, "end": 28695, "idx": 52}, {"begin": 28708, "end": 29753, "idx": 53}, {"begin": 29754, "end": 31506, "idx": 54}, {"begin": 31507, "end": 31721, "idx": 55}, {"begin": 31722, "end": 32771, "idx": 56}, {"begin": 32786, "end": 33541, "idx": 57}, {"begin": 33542, "end": 33875, "idx": 58}, {"begin": 33876, "end": 33971, "idx": 59}, {"begin": 33972, "end": 34141, "idx": 60}, {"begin": 34142, "end": 34521, "idx": 61}, {"begin": 34522, "end": 34862, "idx": 62}, {"begin": 34877, "end": 35421, "idx": 63}, {"begin": 35434, "end": 35550, "idx": 64}, {"begin": 35773, "end": 35883, "idx": 65}, {"begin": 35884, "end": 36132, "idx": 66}, {"begin": 36170, "end": 36373, "idx": 67}, {"begin": 36463, "end": 36559, "idx": 68}, {"begin": 36589, "end": 36733, "idx": 69}, {"begin": 36734, "end": 37094, "idx": 70}, {"begin": 37148, "end": 37500, "idx": 71}, {"begin": 37584, "end": 38731, "idx": 72}, {"begin": 38756, "end": 39558, "idx": 73}], "ReferenceToBib": [{"begin": 1323, "end": 1351, "idx": 0}, {"begin": 1352, "end": 1367, "target": "#b40", "idx": 1}, {"begin": 1368, "end": 1381, "target": "#b15", "idx": 2}, {"begin": 1699, "end": 1712, "target": "#b48", "idx": 3}, {"begin": 1945, "end": 1965, "target": "#b7", "idx": 4}, {"begin": 2120, "end": 2141, "target": "#b42", "idx": 5}, {"begin": 2218, "end": 2244, "target": "#b25", "idx": 6}, {"begin": 2267, "end": 2296, "target": "#b29", "idx": 7}, {"begin": 2297, "end": 2319, "target": "#b5", "idx": 8}, {"begin": 2454, "end": 2473, "target": "#b44", "idx": 9}, {"begin": 2474, "end": 2494, "target": "#b3", "idx": 10}, {"begin": 2495, "end": 2517, "target": "#b49", "idx": 11}, {"begin": 2662, "end": 2685, "target": "#b21", "idx": 12}, {"begin": 2686, "end": 2708, "target": "#b2", "idx": 13}, {"begin": 2871, "end": 2892, "target": "#b42", "idx": 14}, {"begin": 2893, "end": 2922, "target": "#b35", "idx": 15}, {"begin": 3091, "end": 3108, "target": "#b27", "idx": 16}, {"begin": 3180, "end": 3201, "target": "#b10", "idx": 17}, {"begin": 3332, "end": 3355, "target": "#b55", "idx": 18}, {"begin": 3444, "end": 3463, "target": "#b53", "idx": 19}, {"begin": 3464, "end": 3481, "target": "#b17", "idx": 20}, {"begin": 3482, "end": 3504, "idx": 21}, {"begin": 3535, "end": 3565, "target": "#b35", "idx": 22}, {"begin": 3566, "end": 3584, "target": "#b54", "idx": 23}, {"begin": 3629, "end": 3647, "target": "#b47", "idx": 24}, {"begin": 4092, "end": 4116, "target": "#b12", "idx": 25}, {"begin": 4171, "end": 4182, "target": "#b28", "idx": 26}, {"begin": 4267, "end": 4282, "idx": 27}, {"begin": 5580, "end": 5603, "target": "#b21", "idx": 28}, {"begin": 5657, "end": 5680, "target": "#b2", "idx": 29}, {"begin": 8121, "end": 8139, "target": "#b14", "idx": 30}, {"begin": 9264, "end": 9282, "target": "#b14", "idx": 31}, {"begin": 9664, "end": 9683, "target": "#b14", "idx": 32}, {"begin": 9684, "end": 9704, "target": "#b35", "idx": 33}, {"begin": 9705, "end": 9732, "target": "#b51", "idx": 34}, {"begin": 9834, "end": 9851, "target": "#b16", "idx": 35}, {"begin": 9877, "end": 9906, "target": "#b36", "idx": 36}, {"begin": 9939, "end": 9957, "target": "#b11", "idx": 37}, {"begin": 9958, "end": 9977, "target": "#b1", "idx": 38}, {"begin": 10000, "end": 10020, "target": "#b22", "idx": 39}, {"begin": 10052, "end": 10072, "target": "#b20", "idx": 40}, {"begin": 10276, "end": 10302, "target": "#b25", "idx": 41}, {"begin": 10303, "end": 10322, "target": "#b56", "idx": 42}, {"begin": 10323, "end": 10345, "target": "#b4", "idx": 43}, {"begin": 10346, "end": 10367, "target": "#b0", "idx": 44}, {"begin": 10368, "end": 10389, "target": "#b45", "idx": 45}, {"begin": 10500, "end": 10522, "target": "#b41", "idx": 46}, {"begin": 10523, "end": 10547, "target": "#b46", "idx": 47}, {"begin": 10548, "end": 10566, "target": "#b53", "idx": 48}, {"begin": 10567, "end": 10589, "idx": 49}, {"begin": 10672, "end": 10691, "target": "#b45", "idx": 50}, {"begin": 10692, "end": 10715, "target": "#b23", "idx": 51}, {"begin": 10716, "end": 10733, "target": "#b47", "idx": 52}, {"begin": 10827, "end": 10846, "target": "#b43", "idx": 53}, {"begin": 10847, "end": 10865, "target": "#b6", "idx": 54}, {"begin": 10866, "end": 10888, "target": "#b13", "idx": 55}, {"begin": 11088, "end": 11117, "target": "#b29", "idx": 56}, {"begin": 11118, "end": 11140, "target": "#b5", "idx": 57}, {"begin": 11157, "end": 11177, "target": "#b42", "idx": 58}, {"begin": 11273, "end": 11294, "target": "#b35", "idx": 59}, {"begin": 11488, "end": 11514, "target": "#b25", "idx": 60}, {"begin": 11547, "end": 11576, "target": "#b29", "idx": 61}, {"begin": 11602, "end": 11625, "target": "#b5", "idx": 62}, {"begin": 12135, "end": 12153, "target": "#b53", "idx": 63}, {"begin": 12261, "end": 12295, "target": "#b19", "idx": 64}, {"begin": 12362, "end": 12391, "target": "#b35", "idx": 65}, {"begin": 12420, "end": 12441, "target": "#b35", "idx": 66}, {"begin": 12508, "end": 12525, "target": "#b17", "idx": 67}, {"begin": 12563, "end": 12585, "target": "#b38", "idx": 68}, {"begin": 12770, "end": 12791, "target": "#b30", "idx": 69}, {"begin": 14742, "end": 14763, "target": "#b35", "idx": 70}, {"begin": 15323, "end": 15343, "target": "#b35", "idx": 71}, {"begin": 17951, "end": 17973, "target": "#b52", "idx": 72}, {"begin": 18112, "end": 18135, "target": "#b21", "idx": 73}, {"begin": 19543, "end": 19564, "target": "#b10", "idx": 74}, {"begin": 19737, "end": 19759, "target": "#b2", "idx": 75}, {"begin": 22075, "end": 22098, "target": "#b5", "idx": 76}, {"begin": 22440, "end": 22458, "target": "#b18", "idx": 77}, {"begin": 22872, "end": 22897, "target": "#b37", "idx": 78}, {"begin": 23429, "end": 23449, "idx": 79}, {"begin": 24631, "end": 24649, "target": "#b53", "idx": 80}, {"begin": 26535, "end": 26552, "target": "#b47", "idx": 81}, {"begin": 28063, "end": 28081, "target": "#b53", "idx": 82}, {"begin": 28474, "end": 28492, "target": "#b17", "idx": 83}, {"begin": 29201, "end": 29219, "target": "#b47", "idx": 84}, {"begin": 29251, "end": 29269, "target": "#b47", "idx": 85}, {"begin": 29305, "end": 29324, "target": "#b54", "idx": 86}, {"begin": 31421, "end": 31444, "idx": 87}, {"begin": 33641, "end": 33668, "target": "#b39", "idx": 88}, {"begin": 33684, "end": 33705, "target": "#b50", "idx": 89}, {"begin": 33847, "end": 33874, "target": "#b8", "idx": 90}, {"begin": 34152, "end": 34175, "target": "#b26", "idx": 91}, {"begin": 34620, "end": 34640, "target": "#b34", "idx": 92}, {"begin": 34641, "end": 34666, "target": "#b33", "idx": 93}, {"begin": 37183, "end": 37203, "target": "#b35", "idx": 94}, {"begin": 38391, "end": 38413, "target": "#b32", "idx": 95}, {"begin": 38863, "end": 38886, "target": "#b21", "idx": 96}, {"begin": 38887, "end": 38909, "target": "#b2", "idx": 97}], "ReferenceString": [{"begin": 39868, "end": 40108, "id": "b0", "idx": 0}, {"begin": 40110, "end": 40282, "id": "b1", "idx": 1}, {"begin": 40286, "end": 40459, "id": "b2", "idx": 2}, {"begin": 40463, "end": 40715, "id": "b3", "idx": 3}, {"begin": 40719, "end": 40950, "id": "b4", "idx": 4}, {"begin": 40954, "end": 41135, "id": "b5", "idx": 5}, {"begin": 41139, "end": 41396, "id": "b6", "idx": 6}, {"begin": 41400, "end": 41542, "id": "b7", "idx": 7}, {"begin": 41546, "end": 41717, "id": "b8", "idx": 8}, {"begin": 41721, "end": 41930, "id": "b9", "idx": 9}, {"begin": 41934, "end": 42356, "id": "b10", "idx": 10}, {"begin": 42360, "end": 42771, "id": "b11", "idx": 11}, {"begin": 42775, "end": 42937, "id": "b12", "idx": 12}, {"begin": 42941, "end": 43151, "id": "b13", "idx": 13}, {"begin": 43155, "end": 43484, "id": "b14", "idx": 14}, {"begin": 43488, "end": 43607, "id": "b15", "idx": 15}, {"begin": 43611, "end": 43916, "id": "b16", "idx": 16}, {"begin": 43920, "end": 44258, "id": "b17", "idx": 17}, {"begin": 44262, "end": 44635, "id": "b18", "idx": 18}, {"begin": 44639, "end": 44744, "id": "b19", "idx": 19}, {"begin": 44748, "end": 45062, "id": "b20", "idx": 20}, {"begin": 45066, "end": 45232, "id": "b21", "idx": 21}, {"begin": 45236, "end": 45499, "id": "b22", "idx": 22}, {"begin": 45503, "end": 45662, "id": "b23", "idx": 23}, {"begin": 45666, "end": 45887, "id": "b24", "idx": 24}, {"begin": 45891, "end": 46201, "id": "b25", "idx": 25}, {"begin": 46205, "end": 46393, "id": "b26", "idx": 26}, {"begin": 46397, "end": 46568, "id": "b27", "idx": 27}, {"begin": 46572, "end": 46715, "id": "b28", "idx": 28}, {"begin": 46719, "end": 46889, "id": "b29", "idx": 29}, {"begin": 46893, "end": 47079, "id": "b30", "idx": 30}, {"begin": 47083, "end": 47304, "id": "b31", "idx": 31}, {"begin": 47308, "end": 47464, "id": "b32", "idx": 32}, {"begin": 47468, "end": 47701, "id": "b33", "idx": 33}, {"begin": 47705, "end": 48064, "id": "b34", "idx": 34}, {"begin": 48068, "end": 48458, "id": "b35", "idx": 35}, {"begin": 48462, "end": 48762, "id": "b36", "idx": 36}, {"begin": 48766, "end": 49057, "id": "b37", "idx": 37}, {"begin": 49061, "end": 49201, "id": "b38", "idx": 38}, {"begin": 49205, "end": 49385, "id": "b39", "idx": 39}, {"begin": 49389, "end": 49547, "id": "b40", "idx": 40}, {"begin": 49551, "end": 49800, "id": "b41", "idx": 41}, {"begin": 49804, "end": 50060, "id": "b42", "idx": 42}, {"begin": 50064, "end": 50275, "id": "b43", "idx": 43}, {"begin": 50279, "end": 50479, "id": "b44", "idx": 44}, {"begin": 50483, "end": 50899, "id": "b45", "idx": 45}, {"begin": 50903, "end": 51086, "id": "b46", "idx": 46}, {"begin": 51090, "end": 51256, "id": "b47", "idx": 47}, {"begin": 51260, "end": 51359, "id": "b48", "idx": 48}, {"begin": 51363, "end": 51454, "id": "b49", "idx": 49}, {"begin": 51458, "end": 51718, "id": "b50", "idx": 50}, {"begin": 51722, "end": 52046, "id": "b51", "idx": 51}, {"begin": 52050, "end": 52264, "id": "b52", "idx": 52}, {"begin": 52268, "end": 52687, "id": "b53", "idx": 53}, {"begin": 52691, "end": 52990, "id": "b54", "idx": 54}, {"begin": 52994, "end": 53255, "id": "b55", "idx": 55}, {"begin": 53259, "end": 53569, "id": "b56", "idx": 56}, {"begin": 53573, "end": 53759, "id": "b57", "idx": 57}], "Sentence": [{"begin": 86, "end": 258, "idx": 0}, {"begin": 259, "end": 442, "idx": 1}, {"begin": 443, "end": 609, "idx": 2}, {"begin": 610, "end": 834, "idx": 3}, {"begin": 835, "end": 947, "idx": 4}, {"begin": 973, "end": 1096, "idx": 5}, {"begin": 1097, "end": 1169, "idx": 6}, {"begin": 1170, "end": 1456, "idx": 7}, {"begin": 1457, "end": 1542, "idx": 8}, {"begin": 1543, "end": 1646, "idx": 9}, {"begin": 1647, "end": 1966, "idx": 10}, {"begin": 1967, "end": 2142, "idx": 11}, {"begin": 2143, "end": 2439, "idx": 12}, {"begin": 2440, "end": 2997, "idx": 13}, {"begin": 2998, "end": 3109, "idx": 14}, {"begin": 3110, "end": 3356, "idx": 15}, {"begin": 3357, "end": 3648, "idx": 16}, {"begin": 3649, "end": 3792, "idx": 17}, {"begin": 3793, "end": 4117, "idx": 18}, {"begin": 4118, "end": 4404, "idx": 19}, {"begin": 4405, "end": 4473, "idx": 20}, {"begin": 4474, "end": 4689, "idx": 21}, {"begin": 4690, "end": 4844, "idx": 22}, {"begin": 4845, "end": 5038, "idx": 23}, {"begin": 5039, "end": 5132, "idx": 24}, {"begin": 5133, "end": 5371, "idx": 25}, {"begin": 5372, "end": 5519, "idx": 26}, {"begin": 5520, "end": 5818, "idx": 27}, {"begin": 5819, "end": 6091, "idx": 28}, {"begin": 6092, "end": 6195, "idx": 29}, {"begin": 6196, "end": 6480, "idx": 30}, {"begin": 6481, "end": 6802, "idx": 31}, {"begin": 6803, "end": 7004, "idx": 32}, {"begin": 7005, "end": 7216, "idx": 33}, {"begin": 7217, "end": 7300, "idx": 34}, {"begin": 7301, "end": 7330, "idx": 35}, {"begin": 7350, "end": 7497, "idx": 36}, {"begin": 7498, "end": 7600, "idx": 37}, {"begin": 7601, "end": 7808, "idx": 38}, {"begin": 7809, "end": 7939, "idx": 39}, {"begin": 7940, "end": 8075, "idx": 40}, {"begin": 8076, "end": 8298, "idx": 41}, {"begin": 8299, "end": 8508, "idx": 42}, {"begin": 8509, "end": 8664, "idx": 43}, {"begin": 8692, "end": 8775, "idx": 44}, {"begin": 8776, "end": 8873, "idx": 45}, {"begin": 8923, "end": 8975, "idx": 46}, {"begin": 8976, "end": 9059, "idx": 47}, {"begin": 9100, "end": 9262, "idx": 48}, {"begin": 9263, "end": 9398, "idx": 49}, {"begin": 9399, "end": 9448, "idx": 50}, {"begin": 9492, "end": 9622, "idx": 51}, {"begin": 9623, "end": 10073, "idx": 52}, {"begin": 10098, "end": 11141, "idx": 53}, {"begin": 11142, "end": 11329, "idx": 54}, {"begin": 11330, "end": 11452, "idx": 55}, {"begin": 11453, "end": 11671, "idx": 56}, {"begin": 11672, "end": 11823, "idx": 57}, {"begin": 11824, "end": 11951, "idx": 58}, {"begin": 11952, "end": 12133, "idx": 59}, {"begin": 12134, "end": 12342, "idx": 60}, {"begin": 12343, "end": 12506, "idx": 61}, {"begin": 12507, "end": 12709, "idx": 62}, {"begin": 12710, "end": 12845, "idx": 63}, {"begin": 12877, "end": 13123, "idx": 64}, {"begin": 13124, "end": 13187, "idx": 65}, {"begin": 13223, "end": 13312, "idx": 66}, {"begin": 13313, "end": 13503, "idx": 67}, {"begin": 13504, "end": 13684, "idx": 68}, {"begin": 13685, "end": 13820, "idx": 69}, {"begin": 13821, "end": 13916, "idx": 70}, {"begin": 13917, "end": 14053, "idx": 71}, {"begin": 14054, "end": 14151, "idx": 72}, {"begin": 14152, "end": 14479, "idx": 73}, {"begin": 14496, "end": 14704, "idx": 74}, {"begin": 14705, "end": 14925, "idx": 75}, {"begin": 14926, "end": 15070, "idx": 76}, {"begin": 15071, "end": 15192, "idx": 77}, {"begin": 15193, "end": 15255, "idx": 78}, {"begin": 15283, "end": 15414, "idx": 79}, {"begin": 15504, "end": 15544, "idx": 80}, {"begin": 15545, "end": 15702, "idx": 81}, {"begin": 15761, "end": 15902, "idx": 82}, {"begin": 15903, "end": 16098, "idx": 83}, {"begin": 16145, "end": 16376, "idx": 84}, {"begin": 16377, "end": 16512, "idx": 85}, {"begin": 16513, "end": 16828, "idx": 86}, {"begin": 16829, "end": 16920, "idx": 87}, {"begin": 16921, "end": 16980, "idx": 88}, {"begin": 16981, "end": 17120, "idx": 89}, {"begin": 17147, "end": 17237, "idx": 90}, {"begin": 17238, "end": 17305, "idx": 91}, {"begin": 17306, "end": 17379, "idx": 92}, {"begin": 17380, "end": 17648, "idx": 93}, {"begin": 17649, "end": 17816, "idx": 94}, {"begin": 17817, "end": 17893, "idx": 95}, {"begin": 17918, "end": 17949, "idx": 96}, {"begin": 17950, "end": 17974, "idx": 97}, {"begin": 17975, "end": 18060, "idx": 98}, {"begin": 18085, "end": 18210, "idx": 99}, {"begin": 18211, "end": 18352, "idx": 100}, {"begin": 18353, "end": 18582, "idx": 101}, {"begin": 18583, "end": 18681, "idx": 102}, {"begin": 18682, "end": 18743, "idx": 103}, {"begin": 18744, "end": 18867, "idx": 104}, {"begin": 18904, "end": 18935, "idx": 105}, {"begin": 18936, "end": 18990, "idx": 106}, {"begin": 19019, "end": 19194, "idx": 107}, {"begin": 19195, "end": 19346, "idx": 108}, {"begin": 19364, "end": 19423, "idx": 109}, {"begin": 19424, "end": 19525, "idx": 110}, {"begin": 19526, "end": 19684, "idx": 111}, {"begin": 19685, "end": 19863, "idx": 112}, {"begin": 19864, "end": 20070, "idx": 113}, {"begin": 20071, "end": 20123, "idx": 114}, {"begin": 20137, "end": 20355, "idx": 115}, {"begin": 20356, "end": 20423, "idx": 116}, {"begin": 20490, "end": 20516, "idx": 117}, {"begin": 20554, "end": 20635, "idx": 118}, {"begin": 20636, "end": 20720, "idx": 119}, {"begin": 20721, "end": 20819, "idx": 120}, {"begin": 20820, "end": 20915, "idx": 121}, {"begin": 20916, "end": 21095, "idx": 122}, {"begin": 21096, "end": 21175, "idx": 123}, {"begin": 21176, "end": 21288, "idx": 124}, {"begin": 21289, "end": 21330, "idx": 125}, {"begin": 21331, "end": 21458, "idx": 126}, {"begin": 21792, "end": 21876, "idx": 127}, {"begin": 21877, "end": 21994, "idx": 128}, {"begin": 21995, "end": 22200, "idx": 129}, {"begin": 22209, "end": 22353, "idx": 130}, {"begin": 22354, "end": 22438, "idx": 131}, {"begin": 22439, "end": 22459, "idx": 132}, {"begin": 22460, "end": 22605, "idx": 133}, {"begin": 22606, "end": 22700, "idx": 134}, {"begin": 22701, "end": 22756, "idx": 135}, {"begin": 22757, "end": 22951, "idx": 136}, {"begin": 22952, "end": 23053, "idx": 137}, {"begin": 23054, "end": 23145, "idx": 138}, {"begin": 23146, "end": 23231, "idx": 139}, {"begin": 23232, "end": 23334, "idx": 140}, {"begin": 23355, "end": 23450, "idx": 141}, {"begin": 23451, "end": 23578, "idx": 142}, {"begin": 23579, "end": 23603, "idx": 143}, {"begin": 23604, "end": 23816, "idx": 144}, {"begin": 23817, "end": 23855, "idx": 145}, {"begin": 23856, "end": 23941, "idx": 146}, {"begin": 23942, "end": 23999, "idx": 147}, {"begin": 24000, "end": 24028, "idx": 148}, {"begin": 24029, "end": 24174, "idx": 149}, {"begin": 24175, "end": 24227, "idx": 150}, {"begin": 24228, "end": 24306, "idx": 151}, {"begin": 24307, "end": 24373, "idx": 152}, {"begin": 24374, "end": 24562, "idx": 153}, {"begin": 24563, "end": 24650, "idx": 154}, {"begin": 24651, "end": 24757, "idx": 155}, {"begin": 24758, "end": 24852, "idx": 156}, {"begin": 24853, "end": 24917, "idx": 157}, {"begin": 24918, "end": 24961, "idx": 158}, {"begin": 24962, "end": 25021, "idx": 159}, {"begin": 25022, "end": 25074, "idx": 160}, {"begin": 25075, "end": 25132, "idx": 161}, {"begin": 25133, "end": 25226, "idx": 162}, {"begin": 25227, "end": 25397, "idx": 163}, {"begin": 25398, "end": 25460, "idx": 164}, {"begin": 25461, "end": 25565, "idx": 165}, {"begin": 25593, "end": 25729, "idx": 166}, {"begin": 25730, "end": 25826, "idx": 167}, {"begin": 25827, "end": 25950, "idx": 168}, {"begin": 25951, "end": 26049, "idx": 169}, {"begin": 26050, "end": 26227, "idx": 170}, {"begin": 26228, "end": 26370, "idx": 171}, {"begin": 26371, "end": 26533, "idx": 172}, {"begin": 26534, "end": 26632, "idx": 173}, {"begin": 26633, "end": 26821, "idx": 174}, {"begin": 26822, "end": 26975, "idx": 175}, {"begin": 26976, "end": 27101, "idx": 176}, {"begin": 27102, "end": 27328, "idx": 177}, {"begin": 27329, "end": 27534, "idx": 178}, {"begin": 27535, "end": 27610, "idx": 179}, {"begin": 27632, "end": 27739, "idx": 180}, {"begin": 27740, "end": 27850, "idx": 181}, {"begin": 27851, "end": 27964, "idx": 182}, {"begin": 27965, "end": 28082, "idx": 183}, {"begin": 28083, "end": 28362, "idx": 184}, {"begin": 28363, "end": 28586, "idx": 185}, {"begin": 28587, "end": 28656, "idx": 186}, {"begin": 28657, "end": 28695, "idx": 187}, {"begin": 28708, "end": 29000, "idx": 188}, {"begin": 29001, "end": 29174, "idx": 189}, {"begin": 29175, "end": 29581, "idx": 190}, {"begin": 29582, "end": 29753, "idx": 191}, {"begin": 29754, "end": 29910, "idx": 192}, {"begin": 29911, "end": 30006, "idx": 193}, {"begin": 30007, "end": 30142, "idx": 194}, {"begin": 30143, "end": 30492, "idx": 195}, {"begin": 30493, "end": 30618, "idx": 196}, {"begin": 30619, "end": 30710, "idx": 197}, {"begin": 30711, "end": 30818, "idx": 198}, {"begin": 30819, "end": 30978, "idx": 199}, {"begin": 30979, "end": 31121, "idx": 200}, {"begin": 31122, "end": 31187, "idx": 201}, {"begin": 31188, "end": 31328, "idx": 202}, {"begin": 31329, "end": 31506, "idx": 203}, {"begin": 31507, "end": 31606, "idx": 204}, {"begin": 31607, "end": 31721, "idx": 205}, {"begin": 31722, "end": 31910, "idx": 206}, {"begin": 31911, "end": 32130, "idx": 207}, {"begin": 32131, "end": 32196, "idx": 208}, {"begin": 32197, "end": 32351, "idx": 209}, {"begin": 32352, "end": 32392, "idx": 210}, {"begin": 32393, "end": 32531, "idx": 211}, {"begin": 32532, "end": 32651, "idx": 212}, {"begin": 32652, "end": 32771, "idx": 213}, {"begin": 32786, "end": 32891, "idx": 214}, {"begin": 32892, "end": 33093, "idx": 215}, {"begin": 33094, "end": 33167, "idx": 216}, {"begin": 33168, "end": 33455, "idx": 217}, {"begin": 33456, "end": 33541, "idx": 218}, {"begin": 33542, "end": 33591, "idx": 219}, {"begin": 33592, "end": 33739, "idx": 220}, {"begin": 33740, "end": 33875, "idx": 221}, {"begin": 33876, "end": 33971, "idx": 222}, {"begin": 33972, "end": 34141, "idx": 223}, {"begin": 34142, "end": 34297, "idx": 224}, {"begin": 34298, "end": 34411, "idx": 225}, {"begin": 34412, "end": 34521, "idx": 226}, {"begin": 34522, "end": 34721, "idx": 227}, {"begin": 34722, "end": 34862, "idx": 228}, {"begin": 34877, "end": 35071, "idx": 229}, {"begin": 35072, "end": 35228, "idx": 230}, {"begin": 35229, "end": 35329, "idx": 231}, {"begin": 35330, "end": 35421, "idx": 232}, {"begin": 35434, "end": 35550, "idx": 233}, {"begin": 35773, "end": 35838, "idx": 234}, {"begin": 35839, "end": 35883, "idx": 235}, {"begin": 35884, "end": 35972, "idx": 236}, {"begin": 35973, "end": 36132, "idx": 237}, {"begin": 36170, "end": 36373, "idx": 238}, {"begin": 36463, "end": 36559, "idx": 239}, {"begin": 36589, "end": 36733, "idx": 240}, {"begin": 36734, "end": 36855, "idx": 241}, {"begin": 36856, "end": 36972, "idx": 242}, {"begin": 36973, "end": 37094, "idx": 243}, {"begin": 37148, "end": 37348, "idx": 244}, {"begin": 37349, "end": 37411, "idx": 245}, {"begin": 37412, "end": 37500, "idx": 246}, {"begin": 37584, "end": 37779, "idx": 247}, {"begin": 37780, "end": 37865, "idx": 248}, {"begin": 37866, "end": 38051, "idx": 249}, {"begin": 38052, "end": 38218, "idx": 250}, {"begin": 38219, "end": 38351, "idx": 251}, {"begin": 38352, "end": 38559, "idx": 252}, {"begin": 38560, "end": 38654, "idx": 253}, {"begin": 38655, "end": 38731, "idx": 254}, {"begin": 38756, "end": 38910, "idx": 255}, {"begin": 38911, "end": 39009, "idx": 256}, {"begin": 39010, "end": 39159, "idx": 257}, {"begin": 39160, "end": 39208, "idx": 258}, {"begin": 39209, "end": 39371, "idx": 259}, {"begin": 39372, "end": 39421, "idx": 260}, {"begin": 39422, "end": 39558, "idx": 261}], "ReferenceToFigure": [{"begin": 17925, "end": 17926, "target": "#fig_3", "idx": 0}, {"begin": 17982, "end": 17983, "target": "#fig_1", "idx": 1}, {"begin": 19734, "end": 19735, "target": "#fig_3", "idx": 2}, {"begin": 21296, "end": 21297, "target": "#fig_2", "idx": 3}, {"begin": 35894, "end": 35895, "target": "#fig_7", "idx": 4}, {"begin": 38049, "end": 38050, "target": "#fig_10", "idx": 5}, {"begin": 39157, "end": 39158, "idx": 6}], "Abstract": [{"begin": 76, "end": 947, "idx": 0}], "SectionFootnote": [{"begin": 39560, "end": 39851, "idx": 0}], "Footnote": [{"begin": 39571, "end": 39627, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 39628, "end": 39735, "id": "foot_1", "n": "3", "idx": 1}, {"begin": 39736, "end": 39851, "id": "foot_2", "n": "4", "idx": 2}]}}