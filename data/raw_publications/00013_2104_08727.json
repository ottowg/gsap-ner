{"text": "EMNLP-Findings'21 GOOAQ : Open Question Answering with Diverse Answer Types\n\nAbstract:\nWhile day-to-day questions come with a variety of answer types, the current open questionanswering (QA) literature represents isolated efforts on niche response types, with a heavy focus on specific kinds of short responses (people, places, etc.). To address this gap, we present GOOAQ, a large-scale dataset collected from Google questions and answers, containing 3 million questions with diverse answer types ranging from factual short answers to snippets to collections. Our human evaluation shows that 94% of the mined answers are accurate, enabling fine-tuning a pre-trained language model for answering GOOAQ questions. We use this dataset to study inherent differences between models producing different answer types, and observe interesting trends. For example, in line with recent work, LM's strong performance on GOOAQ's shortanswer questions heavily benefits from annotated data. However, their surprisingly high quality in generating coherent and accurate answers for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. Moreover, we show that GOOAQ is a valuable training resource, resulting in strong performance on the recent ELI5 long-answers dataset. We release GOOAQ to facilitate further research on improving QA with diverse response types. 1\n\n\n1 Introduction\nResearch in \"open\" question answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021).\nIn contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., 'what is' or 'can you' questions), a collection of items such as ingredients ('what are kinds of', 'things to') or of steps towards a goal such as unlocking a phone ('how to'), etc.\nEven when the answer is short, it can have rich types, e.g., unit conversion, time zone conversion, or a variety of knowledge look-up ('how much', 'when is ', etc.). 2 Such answer type diversity is not represented in any existing dataset.\nMotivated by this, we introduce GOOAQ (pronounced guac like guacamole), the first open QA benchmark containing questions with all of the above answer types within a unified dataset, collected using the same, coherent process. GOOAQ contains 3 million questions with short, snippet, or collection answers, such as the ones shown in Fig. 1. Besides supporting research on various types of answers, GOOAQ enables a quantitative study of the inherent differences in systems across different answer types.\nGOOAQ questions are automatically mined from Google's search-autocomplete feature and thus, we hypothesize, represent popular queries of realworld interest. Such questions also trigger 'answer boxes' in the search results, containing responses deemed best by Google, which we extract and refer to as Google answers. Our human evaluation ( \u00a73.2) found the collected questions and answers to be of Question: what is the difference between an assignment and a delegation? Answer(snippet): The difference is that an assignment can't increase another party's obligations. Delegation, on the other hand, is a method of using a contract to transfer one party's obligations to another party. Assigning rights is usually easier than delegating, and fewer restrictions are in place.\nquestions w/ short answers questions w/ snippet answers questions w/ collection answers Figure 1 : Examples from GOOAQ showing different types of the questions considered in this study. Each input is a natural language question, mapped to textual answer(s). The questions/answers come with answer type which are inferred from meta information of the search results.\nhigh quality (over 94% valid answers). GOOAQ provides a unified test bed to study inherent differences between questions. To do so, we fine-tune generative pre-trained language models (LMs) (Lewis et al., 2020; Raffel et al., 2020) on different subsets of GOOAQ, and ask whether models trained for different answer types:\n(Q 1 ) benefit similarly from pre-training? (Q 2 ) benefit similarly from labeled data? (Q 3 ) benefit similarly from larger models?\nTo understand the contribution of pre-training, (Q 1 ), we train the powerful T5 language model (Raffel et al., 2020) on GOOAQ with a small amount of labeled data. While LMs struggle, as expected, in this setting on short response questions, they perform surprisingly well in generating snippet and collection responses. 3 We hypothesize this is because response fluency and coherence have a much higher weight in such questions, and these factors remarkably benefit from the LM pre-training objective. Regarding the value of labelled data, (Q 2 ), we observe the opposite trend: short response questions benefit consistently from increasing amounts of supervised (labeled) data, whereas both snippet and collection response questions show minimal gains (e.g., only 5-10% improvement when going from 2k training examples to 200k or even 2 million). Lastly, on the benefit of model size, (Q 3 ), we find larger models to be more effective in all cases as expected, but the gains are much more pronounced for snippet and collection 3 Over 30-40% of our best model's snippet and collection answers were preferred by crowdworkers over Google's answers; achieving 50% here would mean parity with Google.\nresponse generation (20+%) as compared to short responses (5-10%), under human evaluation.\nAdditionally, we expect GOOAQ to facilitate further research on models for answering snippet and collection response questions. While the largest models we consider score surprisingly high on these questions, they are still far from reaching Google's quality under either automated or human evaluations. Importantly, due to little benefit observed from more labeled data on such questions, further progress requires rethinking the approach and devising new solutions.\nLastly, we find GOOAQ to be a valuable resource for training models. On the long-answer dataset ELI5 (Fan et al., 2019), T5 trained only on our snippet questions performs on par with state-of-theart models trained on ELI5 data.\nOur closing remarks describe why we aren't simply replicating an existing QA system at Google, place our findings in context, and discuss future uses of GOOAQ, such as creating a neural knowledge-base or a question generation system.\n\nContributions. Our contributions are threefold:\n1. We present GOOAQ, a collection of 3 million question-answer pairs with a diverse set of answers, along with a crowdsourced assessment of its quality. 2. We benchmark state-of-the-art models on GOOAQ, both in terms of automatic and human judgments, and observe remarkable differences in how models behave on different answer types. 3. We demonstrate that GOOAQ is also a valuable model training resource by showing strong generalization to ELI5 (Fan et al., 2019).\n\n2 Related Work\nA closely related work is the Natural-Questions (NQ) dataset (Kwiatkowski et al., 2019; Lee et al., 2019) which contains questions written by Google users, and answers that were manually extracted from Wikipedia articles. While our questions (extracted via autocomplete) were also likely frequently asked by Google users, our dataset represents a different and wider distribution of questions ( \u00a73. (Fan et al., 2019; Krishna et al., 2021), containing questions/answers mined from Reddit forums. In contrast, GOOAQ is collected differently and is several orders of magnitude larger than ELI5. Empirically, we show that models trained on GOOAQ transfer surprisingly well to ELI5 ( \u00a75.3), indicating GOOAQ's broad coverage.\nIt is worth highlighting that there is precedent for using search engines to create resources for the analysis of AI systems. Search engines harness colossal amounts of click information to help them effectively map input queries to a massive collection of information available in their index (Brin and Page, 1998; Joachims, 2002; Berant et al., 2013; Joachims et al., 2017). Although academic researchers do not have direct access to information collected from the users of search engines, search results can act as a proxy for them and all the complex engineering behind them. In particular, the GOOAQ dataset used in this study probably is not representative of a single QA system in Google; on the contrary, we hypothesize, this data is produced by a complex combination of many systems, various forms of user feedback, as well as expert annotation/verification of highly popular responses.\n\n3 GOOAQ dataset\nWe describe how GOOAQ was collected, followed by dataset statistics and quality assessment.\n\n3.1 Dataset Construction\nConstructing this dataset involved two main steps, extracting questions from search auto-complete and extracting answers from answer boxes.\n\n3.1.1 Query Extraction\nTo extract a rich yet natural set of questions we use Google auto-completion. 4 A similar strategy was also used by Berant et al. (2013), albeit in the context of a slightly different study. We start with a seed set of question terms (e.g., 'who', 'where', etc.; the complete list is in Appendix A.) We bootstrap based on this set, by repeatedly querying prefixes of previously extracted questions, in order to discover longer and richer sets of questions. Such questions extracted from the autocomplete algorithm reflect popular questions posed by users of Google. We filter out any questions shorter than 5 tokens as they are often incomplete questions. This process yields over \u223c5M questions, which were collected over a span of 6 months. The average length of the questions is about 8 tokens.\n\n3.1.2 Answer Extraction\nTo mine answers to our collected questions, we extract the Google answer boxes shown on top of the search results when the questions are issued to Google. There are a variety of answer boxes. The most common kind involves highlighted sentences (extracted from various websites) that contain the answer to a given question. These form the snippet and collection answers in GOOAQ. In some cases, the answer box shows the answer directly, possibly in addition to the textual snippet. Similarly, unitconversion and time-conversion they each have distinct answer boxes. Some technical details of the answer extraction is included in Appendix B.\nAfter the answer extraction step, we have all the necessary information to create a question in GOOAQ, such as the examples in Fig. 1.\nAnswer Type Categories. We use the HTML tags of the search results to infer answer type tags for each answer. The overall list of types are shown in Table 1 (examples in Fig. 1). We define 'short' response questions to be the union of 'knowledge', 'unit-conversion', 'time-conversion', and short answers from the 'snippet' responses.\nTable 1 summarizes various statistics about GOOAQ broken down into different question/answer types. Of the 5M collected questions, about half resulted in successful answer extraction from answer boxes. The largest type of questions received 'snippet' answers with over 2.7M responses (examples shown in the left-most column of Fig. 1). The other major category is 'collection' answers with 329k questions (examples shown on the right-most column of Fig. 1).\n\n3.2 Quality Assessment of GOOAQ\nWe perform a crowdsourcing experiment to assess the quality of the extracted questions and their answers. We use Amazon Mechanical Turk (AMT) to annotate about 2.5k randomly selected questionanswer pairs. The annotators were asked to annotate (1) whether a given question makes sense and, if so, (2) whether the provided answer is complete.\nAnnotation details. Since our task is focused on English, we required workers to be based in a country with a population predominantly of native English speakers (e.g., USA, Canada, UK, and Australia) and have completed at least 5000 HITs with \u2265 99% assignment approval rate. Additionally, we have a qualification test with half-a-dozen questions all of which need to be answered correctly by our annotators. To prevent biased judgements, we also ask annotators to avoid using Google search (which is what we used to mine GOOAQ) when annotating the quality of shown instances. Each example is annotated by 3 independent annotators and aggregated via a majority vote of the 3 labels.\nAssessment results. We compute aggregate statistics for (1) average rating of questions and\n(2) average rating of the answer quality, among valid questions. As can be seen in the results in Table 1 only a small percentage of the questions were deemed 'invalid'. Additionally, among the 'valid' questions, a high percentage of the answers were deemed high-quality for most of the question/answer types. This indicates a reasonable qual-ity of GOOAQ question-answer pairs, as evaluated directly, independent from any systems. (Examples of invalid questions/answers are provided in Appendix C.)\n\n3.3 Dataset Analysis\nTo better understand the content of GOOAQ, we present several distributions from the data. Fig. 2 shows the length distribution of GOOAQ questions and that of NQ (Kwiatkowski et al., 2019). While a vast majority of NQ questions contain 8-10 tokens, GOOAQ questions have a somewhat broader range of lengths.\n(a) GOOAQ  To gain insights about the type of questions, we study the distribution of the most frequent opening bigrams of the questions (Fig. 3). Among the short answer questions, the majority are informationseeking questions about counts ('how many'), places ('where is'), values ('how much'), and people ('who is'). They also include 'what is' questions, which can cover a wide variety of openended queries with short answers (e.g., what is the time difference . . .?, what is the length of X?, etc.). Among the snippet questions, the dominant pattern is 'what is', which typically is an open-ended question about entities (e.g., 'what is X?' or 'what is the difference between X and Y?'). Among the collection response questions, most questions are about steps or ingredients needed to accomplish a goal ('how to ' and 'what are'). A comparison with the bigram distribution of NQ (Fig. 3; right) highlights that GOOAQ represents a different and wider class of questions. Specifically, NQ has many 'who', 'when', and 'how many' questions, while GOOAQ dominantly contains 'how' and 'what' questions, which typically require explanatory responses.\nIn terms of the different reasoning types, GOOAQ has an extremely long-tail of reasoning challenges, due to our data collection procedure. For example, we observed many challenges such as\n\n4 Task Setup and Models\nGOOAQ naturally forms a dataset for the task of open QA, where the input is a question and the output is its answer. Unlike the reading comprehension setting, the context for answering the question is not provided as part of the input. In particular, we consider the so-called 'closed-book' setup (Roberts et al., 2020) where the model (e.g., a language model) is expected to use background knowledge stored within it, without access to any additional explicit information retrieval mechanism. 5\n\n4.1 Problem Setup\nWe split GOOAQ into three sub-tasks: (T short ) short responses questions, (T snippet ) snippet responses questions, and (T collection ) collection response questions. We train and evaluate models for each of these sub-tasks separately. We define them as different sub-tasks since by merely reading the 5 In our early experiments, we considered informationretrieval (IR) systems in conjunction to LMs (i.e., an 'openbook' setup). We observed that IR results are quite noisy for most open questions. Hence, a system trained with the retrieved documents did not benefit from them (the model learned to ignore the noisy retrieval results). Similar observations were also made by Krishna et al. (2021, Sec3.1) (\"generations are similar irrespective of type of retrievals\"). questions it might not be clear whether its response should be short, a snippet, or a collection, Data splits. For each sub-task, we randomly sample test and dev sets such that each evaluation split contains at least 500 instances of each response type. We experiment with varying training data sizes to better understand the value of labeled data.  Lewis et al. (2021) have shown that leakage from training data to the evaluation sets often results in unrealistically high scores. To minimize this issue, we create training splits by selecting the most dissimilar instances to our evaluation splits. The measure of similarity for each training instance is computed as the maximum amount of tokenoverlap with any of the instances in the test/dev set (computed over both questions and answers). Using the most dissimilar subset of the training instances, we create training splits of the following sizes: 2k, 20k, 200k. For T snippet , we also have a 2M training set since this sub-task has more data.\n\n4.2 Evaluation Metrics\nAutomatic evaluation. We use the ROUGE-L metric (Lin, 2004), which is a common metric for assessing the quality of models for text generation tasks. The results of the automatic evaluation for each sub-task are shown in the top row of Fig. 4.\nHuman evaluation. We additionally perform human evaluation which is generally known to provide more accurate evaluation for generated text. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the Google answer for each question (without revealing the source of the answers).\nThe annotation interface is shown in Fig. 5, which is essentially the same template used for the quality assessment of the dataset ( \u00a73.2), except that here the crowdworkers are shown a pair of The short-answer sub-tasks (T short ; left) have a relatively low performance when supervised with 2k instances. However, they benefit more than the long-answer sub-tasks (T snippet & T collection ) from more labeled data. Additionally, we observe that the gap between the two systems is bigger in terms of human evaluation (compared to the corresponding gap in terms of automatic evaluation), especially in the long response tasks (middle & right). responses for each question-the reference answer (extracted from Google) and the one generated by the model-turning the task into a comparative one. Before annotating each instance, we remind the annotators to avoid using Google. Then we ask them to check if the provided question is clear enough and makes sense. Upon indicating 'yes', they choose between the Google answer, the generated answer by our model, or indicate that they are equally good (by selecting 'tie').\nFor each question, we obtain annotations from 5 independent annotators and aggregate via a majority vote. 6 The model receives a credit of 1 if the majority vote favors the model's prediction, 0.5 if the majority vote is the 'tie' label, and 0 otherwise. The overall accuracy score for the model is computed by averaging instance-level scores, after discarding questions annotated as invalid ('this question makes no sense').\nThe resulting human-evaluation metric indicates how often were model predictions preferred over Google's answers. In this evaluation, 50% is the mark where the annotators are not able to distinguish the model's responses from Google's answers in any meaningful way. The results of human evaluation are shown in the bottom row of Fig. 4.\n\n4.3 Models\nFor our evaluation, we use the T5 model (Raffel et al., 2020), a recent text-to-text framework that has achieved state-of-the-art results on a variety of tasks, including open QA (Roberts et al., 2020). The models are trained to produce answer string, given the question string as input. We use two model sizes that capture the two extremes: the smallest model ('small') and the largest model ('11B'). Both models were trained for 20k steps on the training splits, dumping checkpoints every 2k steps (with 196,608 tokens per batch on v3-128 TPUs) with the default hyperparameters. We select the checkpoint with the highest score on the 'dev' set and report its corresponding 'test' score.\n\n5 Empirical Results and Analyses\nIn this section, we evaluate the behavior of models for various answer types ( \u00a75.1). We further show how GOOAQ can support research in answering questions with long answers ( \u00a75.2; \u00a75.3).\n\n5.1 Models vs. Various Answer Types\n(Q 1 ) Model pre-training is surprisingly effective on the snippet and collection answer subtasks Both automatic and human evaluations of these two classes of questions (Fig. 4; middle & right) demonstrate that the T5-11B model is surprisingly effective at answering them, with only 2k training examples. For example, crowdworkers even prefer the model's answer over Google's in 30% of the cases. 7 This is in contrast with short answer questions, where the model's accuracy is only around 10% and crowdworkers prefer Google's answers in about 90% of the cases.\nTo understand this observation, one needs to put into perspective several factors that are at play. First, short answer questions typically ask for encyclopedic knowledge and, therefore, correctness of the answers matters the most. In snippet and collection questions, we suspect coherence of the response carries a heavier weight. This is partly due to the nature of the questions, which can be responded to in a variety of ways. For example, the snippet response to the question of how many calories burned 30 minutes crossfit? (Fig. 1) could refer to a range of calorie consumption, depend on the choice of activity during crossfit, or vary by the attributes of the person working out. All of these responses would be equally correct.\n(Q 2 ) Labeled data is more helpful for short answer questions. Based again on both the automatic and human evaluations (Fig. 4; left), the performance of both small and 11B parameter models on the short response questions quickly improves as we increase the amount of training data, especially beyond 20k. This is in contrast with snippet and collection questions, where even 200k labeled instances don't appear to help much, indicating that in these question types, model pre-training contributes more than labeled data does.\n(Q 3 ) Human evaluation accentuates the gap between the 'small' and '11B' models, especially on snippet and collection response questions. This is visually evident from the gap between the blue and red curves in the bottom row vs. the top row of Fig. 4. This is compatible with recent work of Min et al. (2021), who also observed that the gap between two reasonably different systems is bigger when using human evaluation. We hypothesize this is due to the crudeness of automatic evaluation metrics, and an indication of the necessity of human evaluation to distinguish between nuanced differences among generated responses.\nWhat is perhaps more interesting (and not evident from prior work) is that the gap between automatic and human evaluation is larger for the snippet and collection questions than short answer questions, especially for the T5-small model. This is, at least partly, due to the inaccuracy of automatic metrics in evaluating long text.\n\n5.2 GOOAQ as a challenge for LMs\nOne can view GOOAQ as a challenge for NLP, for building self-contained models that achieve performance comparable to Google's answers.\nAs mentioned earlier, our human evaluation measures the comparative quality of the model predictions and our reference responses (Google's answers). Hence, a value of 50% in this evaluation is an indication that the predictions are on par with (i.e., indistinguishable from) the ground-truth responses (defined in 'human-evaluation' \u00a74.2).\nAs the bottom row of Fig. 4 shows, the T5-11B model comes quite close to Google's answers but is still not quite at par with it. We hope this gap will encourage further research in building stronger models, especially for the snippet and collection answer questions where more labeled data doesn't appear to be a promising way to increase accuracy.\n\n5.2.1 Error Analysis\nTo gain an intuition about the mistakes made by the models, we conducted a small-scale errors analysis of model predictions. For each model, we (one of the authors) annotated 30 predictions, and labeled them with the following error categories inspired from existing evaluations of text summarization (Chaganty et al., 2018) The results of our error analysis are summarized in Table 2. As expected, the 'small' model makes more errors across all categories, and suffers particularly from redundancy and incompleteness. Overall, both models have very little incoherence, which is to be expected from their strong pre-training.\n\n5.3 Extrinsic Utility of GOOAQ\nTo showcase the value of GOOAQ as a model training resource, we train our models on questions from GOOAQ and evaluate them on ELI5 (Fan et al., 2019), a relatively recent dataset with longanswer questions extracted from Reddit posts. Our evaluation, summarized in Table 3, shows that both our small and 11B T5 models trained on GOOAQ's snippet-answer subset (no training on ELI5) perform quite well (21.8 and 22.9, respectively) when evaluated on ELI5. They are even better than the same architectures trained with ELI5's own training data (19.0 and 22.7, resp.) and on par with retrieval based state-of-the-art models (23.4). Complementary to these results, a T5-11B model trained on ELI5 and evaluated on GOOAQ results in 22.6%, much lower than \u223c28.9% in Table 4.\nWe hypothesize that despite GOOAQ being collected differently than ELI5, a notable portion of ELI5 is covered by GOOAQ, indicating good coverage of common questions posed by ordinary users.\n\n6 Closing Remarks\nWe studied open QA under diverse response types. To this end, we collected GOOAQ, a very large set of QA pairs mined from Google, with a variety of short and long answer types, all of which are collected using a unified, coherent process, enabling a cross-type comparison. The auto-complete system used for our question collection likely reflects a natural distribution of questions asked by users.\nWe benchmarked two variants of a state-ofthe-art self-contained text generation model (T5, without retrieval) on three different sub-tasks of GOOAQ: short, snippet, and collection response questions. Our analysis, using both automatic and human evaluations, brings out the distinct behavior of LMs on long and short response questions. For example, while short response models benefit heavily from more labeled data, the surprisingly strong performance of long response models is driven mostly by their pre-training. We also demonstrate that GOOAQ is a valuable resource for training models by showing high performance on an extrinsic task, ELI5, while using only GOOAQ data for training.\nScope of our conclusions. One must be careful in taking our specific conclusions out of the context of this study (i.e., the dataset at hand, the models, the evaluation metrics used, etc.). While we expect our findings to be fairly general, it may be possible to come up with a different long-form QA dataset where the trends across answer types differ.\nKnowledge leakage across train and evaluation sets has been shown to significantly inflate performance numbers on recent open QA datasets (Lewis et al., 2021; Emami et al., 2020). Similar concerns have motivated our careful training/evaluation splits of the data ( \u00a74) and experiments with varying training set sizes. Nevertheless, we found it challenging to define (and assess) the amount of such leakage, and welcome such studies on GOOAQ.\nAre we mimicking Google's QA? A reader might question the value of this work by noting that the website from which GOOAQ was mined had likely also used a QA system to begin with. In other words, are we basically reverse-engineer Google's internal QA system (Kilgarriff, 2007)?\nWhile we (the authors) are not aware of how Google answer box system works, we suspect that it is much more complex than a single QA system built using a single LM like T5 (Raffel et al., 2020). The system, besides incorporating one or more QA models, likely makes heavy use of implicit user feedback (e.g., information contained in billions of clicks, the structure of web links, etc.), in addition to explicit feedback from users and possibly some expert curation of answers to common questions. Moreover, Google's system may decide which questions to display answers for, and probably limits itself to the answers that it is most confident in.\nThus, the data in Google's answer boxes likely captures a variety of signals that contribute towards its high-quality. We believe aiming for a 'standard' NLP QA system that's on par with Google QA is therefore a challenging and worthwhile goal.\nFuture uses of GOOAQ. One challenge in the progress on long-form QA is response evaluation. To facilitate future work on GOOAQ and replicability of our human evaluations, we have released the templates used for crowdsourcing human judgements. Efforts on text generation tasks such as ours will benefit from-and should in turn benefit advances in-proposals for streamlining human evaluation of models (Khashabi et al., 2021).\nWe hope our analysis and data will benefit the understanding of and further development of QA systems for dealing with diverse response types.\nWhile we used GOOAQ for the purposes of QA, we expect this data to have a variety of use-cases, such as building a knowledge-base accessible via question queries (Bosselut et al., 2019), creating a better question generation system, etc. We leave such investigation to future work.\n\nA Query Terms\nThe list of terms used for bootstrapping questions: \"who\", \"whom\", \"whose\", \"what\", \"which\", \"when\", \"where\", \"why\", \"how\", \"should\", \"would\", \"wouldn't\", \"can\", \"can't\", \"will\", \"won't\", \"aren't\", \"do\", \"does\", \"has\", \"have\", \"am\", \"are\", \"is\", \"shouldn't\", \"isn't\", \"could\", \"couldn't\", \"does\", \"don't\", \"must\", \"may\", \"ought\".\n\nB Extracting Answers from Google\nFor technical reasons, the answer extraction was done in two steps. (1) We first scrape the search results for all of our questions. This is the main extraction bottleneck as there is no official APIs to provide the answer boxes. Therefore, one needs to extract them directly from the HTML search results. We use Selenium 8 which simulates browser experience. Note one cannot send too many queries to Google in a short span of time (due to various query limits). Therefore, we ensured to have enough delays between our queries (otherwise, we'd be blocked). Overall, this extraction process was done in 3 months. Subsequent to extracting the search HTML results, ( 2) we extract answer strings from the HTML content of the search results. Answer types are also inferred at this stage, based on the HTML tags around the answer.\n\nC Invalid Questions and Answers\nBased on the human evaluation of GOOAQ in \u00a73.2, we should example of erroneous instances. Figure 6 shows examples of invalid questions. Often the questions are deemed invalid since they're under-defined or significantly deviate from the proper English. Figure 7 shows examples of invalid answers (to valid questions). Invalid answers often do not sufficiently address the topic of the given question. Type: collection Question: how long does it take to get through john wayne airport? Answer: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] '21+ Min.'] Figure 7 : examples of invalid answers (to valid questions)\n\nD Error Analysis of Model Predictions\nHere are examples of hallucination, redundancy, and incompleteness errors in model predictions based on our human analysis discussed in \u00a75.2.1. As noted earlier, all models exhibit very few incoherence errors, mainly because this aspect is directly addressed during model pre-training.\nQuestion: how much does it cost to get court married? Google ans: Your application must be notarized. Please include at least one phone number where you can be reached during the day. Enclose a check or money order for $69 payable to King County. The check or money order must include your address. Predicted ans: The cost of a court marriage is between $40,000 and $50,000. The cost of a court marriage is between $40,000 and $50,000. The cost of a court marriage is between $40,000 and $50,000. Explanation: In this example, the model has made up numbers that do not seem reasonable (hence, a 'hallucination' error.) Additionally, it is repeating the same point (hence, a 'redundancy' error.)\nQuestion: who went to the superbowl last year? Google ans: Here's your answer: The Eagles won Super Bowl LII on Feb. 4, 2018, beating the Patriots 4133 to win the league championship.\nPredicted ans: The Super Bowl was a national championship game between the New York Giants and the New York Giants. The team was a member of the New York Giants, who starred in the Super Bowl in the 1970s. Explanation: In this example, there is repetition in the model's prediction (hence, a 'redundancy' error.)\nQuestion: what courses do you need to be a kinesiologist? Google ans: To become a kinesiologist, you typically need either a bachelor's or a master's degree in Kinesiology or a related field, such as Human Kinetics, Exercise Science, or Exercise Physiology. Predicted ans: To become a kinesiologist, you will need to complete a four-year degree in kinesiology, biology, chemistry, chemistry, physics. Explanation: In this example, the model provides some relevant points, but the response is not as detailed as the gold response (hence, an 'incompleteness' error.)\n\nFootnotes:\n1: The dataset is available at https://github.com/ allenai/gooaq under an appropriate license.\n2: In contrast, the short responses in existing datasets typically inquire about people, dates, and counts. For instance, 65% of Natural Questions (Kwiatkowski et al., 2019) begin with 'who', 'when', or 'how many'; cf.\n3: Fig 3.\n4: http://google.com/complete/search?client=chrome&q=...\n6: Ties occurred infrequently (e.g., in 6% of the cases when evaluating our largest T5 model) and were broken at random.\n7: Across all experiments, the model's and Google's answers were deemed a \"tie\" in fewer than 10% of the cases.\n8: https://github.com/SeleniumHQ/selenium/\n\nReferences:\n\n- Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of EMNLP, pages 1533-1544.- Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. 2021. Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Chal- lenge. arXiv preprint arXiv:2102.03315.\n\n- Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai- tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for au- tomatic knowledge graph construction. In Proceed- ings of ACL, pages 4762-4779.\n\n- Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Com- puter Networks, 30:107-117.\n\n- Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of ACL, pages 643-653.\n\n- Ali Emami, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2020. An analysis of dataset overlap on winograd-style tasks. In Proceedings of COLING, pages 5855-5865.\n\n- Angela Fan, Yacine Jernite, Ethan Perez, David Grang- ier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of ACL, pages 3558-3567.\n\n- Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of KDD, pages 133-142.\n\n- Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2017. Accurately inter- preting clickthrough data as implicit feedback. In ACM SIGIR Forum, pages 4-11. Acm New York, NY, USA.\n\n- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Dis- tantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of ACL, pages 1601-1611.\n\n- Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, and Daniel S Weld. 2021. GENIE: A leader- board for human-in-the-loop evaluation of text gen- eration. arXiv preprint arXiv:2101.06561.\n\n- Adam Kilgarriff. 2007. Googleology is bad science. Computational linguistics, 33(1):147-151.\n\n- Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answer- ing. In Proceedings of NAACL.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a bench- mark for question answering research. TACL, 7:453- 466.\n\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of ACL, pages 6086-6096.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre- training for Natural Language Generation, Transla- tion, and Comprehension. In Proceedings of ACL, pages 7871-7880.\n\n- Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and answer test-train overlap in open-domain question answering datasets. In Pro- ceedings of EACL, pages 1000-1008.\n\n- Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.\n\n- Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi, Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palo- maki, et al. 2021. NeurIPS 2020 EfficientQA Com- petition: Systems, Analyses and Lessons Learned. arXiv preprint arXiv:2101.00133. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. JMLR, 21(140):1-67.\n\n- Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? Proceedings of EMNLP.\n\n", "annotations": {"Abstract": [{"begin": 77, "end": 1455, "idx": 0}], "Head": [{"begin": 1458, "end": 1472, "n": "1", "idx": 0}, {"begin": 7035, "end": 7082, "idx": 1}, {"begin": 7551, "end": 7565, "n": "2", "idx": 2}, {"begin": 9185, "end": 9200, "n": "3", "idx": 3}, {"begin": 9294, "end": 9318, "n": "3.1", "idx": 4}, {"begin": 9460, "end": 9482, "n": "3.1.1", "idx": 5}, {"begin": 10281, "end": 10304, "n": "3.1.2", "idx": 6}, {"begin": 11873, "end": 11904, "n": "3.2", "idx": 7}, {"begin": 13522, "end": 13542, "n": "3.3", "idx": 8}, {"begin": 15188, "end": 15211, "n": "4", "idx": 9}, {"begin": 15709, "end": 15726, "n": "4.1", "idx": 10}, {"begin": 17499, "end": 17521, "n": "4.2", "idx": 11}, {"begin": 19966, "end": 19976, "n": "4.3", "idx": 12}, {"begin": 20667, "end": 20699, "n": "5", "idx": 13}, {"begin": 20890, "end": 20925, "n": "5.1", "idx": 14}, {"begin": 23711, "end": 23743, "n": "5.2", "idx": 15}, {"begin": 24569, "end": 24589, "n": "5.2.1", "idx": 16}, {"begin": 25217, "end": 25247, "n": "5.3", "idx": 17}, {"begin": 26205, "end": 26222, "n": "6", "idx": 18}, {"begin": 30127, "end": 30140, "idx": 19}, {"begin": 30472, "end": 30504, "idx": 20}, {"begin": 31332, "end": 31363, "idx": 21}, {"begin": 32025, "end": 32062, "idx": 22}], "ReferenceToBib": [{"begin": 1820, "end": 1842, "target": "#b19", "idx": 0}, {"begin": 1843, "end": 1862, "target": "#b16", "idx": 1}, {"begin": 1998, "end": 2018, "target": "#b9", "idx": 2}, {"begin": 2019, "end": 2036, "target": "#b14", "idx": 3}, {"begin": 2037, "end": 2066, "target": "#b1", "idx": 4}, {"begin": 2636, "end": 2644, "idx": 5}, {"begin": 4372, "end": 4396, "idx": 6}, {"begin": 4549, "end": 4569, "target": "#b15", "idx": 7}, {"begin": 4570, "end": 4590, "idx": 8}, {"begin": 4910, "end": 4931, "idx": 9}, {"begin": 6673, "end": 6691, "target": "#b6", "idx": 10}, {"begin": 7530, "end": 7548, "target": "#b6", "idx": 11}, {"begin": 7627, "end": 7653, "target": "#b13", "idx": 12}, {"begin": 7654, "end": 7671, "target": "#b14", "idx": 13}, {"begin": 7965, "end": 7983, "target": "#b6", "idx": 14}, {"begin": 7984, "end": 8005, "target": "#b12", "idx": 15}, {"begin": 8582, "end": 8603, "target": "#b3", "idx": 16}, {"begin": 8604, "end": 8619, "target": "#b7", "idx": 17}, {"begin": 8620, "end": 8640, "target": "#b0", "idx": 18}, {"begin": 8641, "end": 8663, "target": "#b8", "idx": 19}, {"begin": 9599, "end": 9619, "target": "#b0", "idx": 20}, {"begin": 13705, "end": 13731, "target": "#b13", "idx": 21}, {"begin": 14667, "end": 14684, "idx": 22}, {"begin": 15509, "end": 15531, "target": "#b19", "idx": 23}, {"begin": 16403, "end": 16429, "idx": 24}, {"begin": 16847, "end": 16866, "target": "#b16", "idx": 25}, {"begin": 17570, "end": 17581, "target": "#b17", "idx": 26}, {"begin": 20017, "end": 20038, "idx": 27}, {"begin": 20156, "end": 20178, "target": "#b19", "idx": 28}, {"begin": 23047, "end": 23064, "target": "#b18", "idx": 29}, {"begin": 24891, "end": 24914, "target": "#b4", "idx": 30}, {"begin": 25379, "end": 25397, "target": "#b6", "idx": 31}, {"begin": 27803, "end": 27823, "target": "#b16", "idx": 32}, {"begin": 27824, "end": 27843, "target": "#b5", "idx": 33}, {"begin": 28364, "end": 28381, "target": "#b11", "idx": 34}, {"begin": 28556, "end": 28577, "idx": 35}, {"begin": 29676, "end": 29699, "target": "#b10", "idx": 36}, {"begin": 30006, "end": 30029, "target": "#b2", "idx": 37}, {"begin": 31857, "end": 31860, "idx": 38}, {"begin": 31861, "end": 31864, "idx": 39}, {"begin": 31865, "end": 31868, "idx": 40}, {"begin": 31869, "end": 31872, "idx": 41}, {"begin": 31873, "end": 31876, "idx": 42}, {"begin": 31877, "end": 31880, "idx": 43}, {"begin": 31881, "end": 31884, "idx": 44}, {"begin": 31885, "end": 31888, "idx": 45}, {"begin": 31889, "end": 31892, "idx": 46}, {"begin": 31893, "end": 31896, "idx": 47}, {"begin": 31897, "end": 31901, "idx": 48}, {"begin": 31902, "end": 31906, "idx": 49}, {"begin": 31907, "end": 31911, "idx": 50}, {"begin": 31912, "end": 31916, "idx": 51}, {"begin": 31917, "end": 31921, "idx": 52}, {"begin": 31922, "end": 31926, "idx": 53}, {"begin": 31927, "end": 31931, "idx": 54}, {"begin": 31932, "end": 31936, "idx": 55}, {"begin": 31937, "end": 31941, "idx": 56}, {"begin": 31942, "end": 31946, "idx": 57}, {"begin": 31947, "end": 31951, "idx": 58}, {"begin": 31952, "end": 31963, "idx": 59}, {"begin": 34360, "end": 34386, "target": "#b13", "idx": 60}], "ReferenceToFootnote": [{"begin": 2646, "end": 2647, "target": "#foot_1", "idx": 0}, {"begin": 9561, "end": 9562, "target": "#foot_3", "idx": 1}, {"begin": 19308, "end": 19309, "target": "#foot_4", "idx": 2}, {"begin": 21323, "end": 21324, "target": "#foot_5", "idx": 3}, {"begin": 30827, "end": 30828, "target": "#foot_6", "idx": 4}], "SectionFootnote": [{"begin": 34107, "end": 34774, "idx": 0}], "ReferenceString": [{"begin": 34791, "end": 34955, "id": "b0", "idx": 0}, {"begin": 34957, "end": 35273, "id": "b1", "idx": 1}, {"begin": 35277, "end": 35505, "id": "b2", "idx": 2}, {"begin": 35509, "end": 35639, "id": "b3", "idx": 3}, {"begin": 35643, "end": 35809, "id": "b4", "idx": 4}, {"begin": 35813, "end": 35987, "id": "b5", "idx": 5}, {"begin": 35991, "end": 36163, "id": "b6", "idx": 6}, {"begin": 36167, "end": 36280, "id": "b7", "idx": 7}, {"begin": 36284, "end": 36485, "id": "b8", "idx": 8}, {"begin": 36489, "end": 36690, "id": "b9", "idx": 9}, {"begin": 36694, "end": 36937, "id": "b10", "idx": 10}, {"begin": 36941, "end": 37033, "id": "b11", "idx": 11}, {"begin": 37037, "end": 37167, "id": "b12", "idx": 12}, {"begin": 37171, "end": 37432, "id": "b13", "idx": 13}, {"begin": 37436, "end": 37604, "id": "b14", "idx": 14}, {"begin": 37608, "end": 37904, "id": "b15", "idx": 15}, {"begin": 37908, "end": 38094, "id": "b16", "idx": 16}, {"begin": 38098, "end": 38222, "id": "b17", "idx": 17}, {"begin": 38226, "end": 38742, "id": "b18", "idx": 18}, {"begin": 38746, "end": 38894, "id": "b19", "idx": 19}], "ReferenceToTable": [{"begin": 11235, "end": 11236, "idx": 0}, {"begin": 11420, "end": 11421, "idx": 1}, {"begin": 13125, "end": 13126, "idx": 2}, {"begin": 24973, "end": 24974, "target": "#tab_6", "idx": 3}, {"begin": 25518, "end": 25519, "target": "#tab_7", "idx": 4}, {"begin": 26011, "end": 26012, "idx": 5}], "Footnote": [{"begin": 34118, "end": 34212, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 34213, "end": 34431, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 34432, "end": 34441, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 34442, "end": 34498, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 34499, "end": 34619, "id": "foot_4", "n": "6", "idx": 4}, {"begin": 34620, "end": 34731, "id": "foot_5", "n": "7", "idx": 5}, {"begin": 34732, "end": 34774, "id": "foot_6", "n": "8", "idx": 6}], "ReferenceToFormula": [{"begin": 31169, "end": 31170, "idx": 0}], "Paragraph": [{"begin": 87, "end": 1455, "idx": 0}, {"begin": 1473, "end": 2067, "idx": 1}, {"begin": 2068, "end": 2479, "idx": 2}, {"begin": 2480, "end": 2718, "idx": 3}, {"begin": 2719, "end": 3219, "idx": 4}, {"begin": 3220, "end": 3992, "idx": 5}, {"begin": 3993, "end": 4358, "idx": 6}, {"begin": 4359, "end": 4680, "idx": 7}, {"begin": 4681, "end": 4813, "idx": 8}, {"begin": 4814, "end": 6012, "idx": 9}, {"begin": 6013, "end": 6103, "idx": 10}, {"begin": 6104, "end": 6571, "idx": 11}, {"begin": 6572, "end": 6799, "idx": 12}, {"begin": 6800, "end": 7033, "idx": 13}, {"begin": 7083, "end": 7549, "idx": 14}, {"begin": 7566, "end": 8287, "idx": 15}, {"begin": 8288, "end": 9183, "idx": 16}, {"begin": 9201, "end": 9292, "idx": 17}, {"begin": 9319, "end": 9458, "idx": 18}, {"begin": 9483, "end": 10279, "idx": 19}, {"begin": 10305, "end": 10944, "idx": 20}, {"begin": 10945, "end": 11079, "idx": 21}, {"begin": 11080, "end": 11413, "idx": 22}, {"begin": 11414, "end": 11871, "idx": 23}, {"begin": 11905, "end": 12245, "idx": 24}, {"begin": 12246, "end": 12928, "idx": 25}, {"begin": 12929, "end": 13020, "idx": 26}, {"begin": 13021, "end": 13520, "idx": 27}, {"begin": 13543, "end": 13849, "idx": 28}, {"begin": 13850, "end": 14998, "idx": 29}, {"begin": 14999, "end": 15186, "idx": 30}, {"begin": 15212, "end": 15707, "idx": 31}, {"begin": 15727, "end": 17497, "idx": 32}, {"begin": 17522, "end": 17764, "idx": 33}, {"begin": 17765, "end": 18085, "idx": 34}, {"begin": 18086, "end": 19201, "idx": 35}, {"begin": 19202, "end": 19627, "idx": 36}, {"begin": 19628, "end": 19964, "idx": 37}, {"begin": 19977, "end": 20665, "idx": 38}, {"begin": 20700, "end": 20888, "idx": 39}, {"begin": 20926, "end": 21487, "idx": 40}, {"begin": 21488, "end": 22225, "idx": 41}, {"begin": 22226, "end": 22753, "idx": 42}, {"begin": 22754, "end": 23378, "idx": 43}, {"begin": 23379, "end": 23709, "idx": 44}, {"begin": 23744, "end": 23878, "idx": 45}, {"begin": 23879, "end": 24218, "idx": 46}, {"begin": 24219, "end": 24567, "idx": 47}, {"begin": 24590, "end": 25215, "idx": 48}, {"begin": 25248, "end": 26013, "idx": 49}, {"begin": 26014, "end": 26203, "idx": 50}, {"begin": 26223, "end": 26621, "idx": 51}, {"begin": 26622, "end": 27310, "idx": 52}, {"begin": 27311, "end": 27664, "idx": 53}, {"begin": 27665, "end": 28106, "idx": 54}, {"begin": 28107, "end": 28383, "idx": 55}, {"begin": 28384, "end": 29030, "idx": 56}, {"begin": 29031, "end": 29275, "idx": 57}, {"begin": 29276, "end": 29700, "idx": 58}, {"begin": 29701, "end": 29843, "idx": 59}, {"begin": 29844, "end": 30125, "idx": 60}, {"begin": 30141, "end": 30470, "idx": 61}, {"begin": 30505, "end": 31330, "idx": 62}, {"begin": 31364, "end": 32023, "idx": 63}, {"begin": 32063, "end": 32348, "idx": 64}, {"begin": 32349, "end": 33043, "idx": 65}, {"begin": 33044, "end": 33227, "idx": 66}, {"begin": 33228, "end": 33540, "idx": 67}, {"begin": 33541, "end": 34105, "idx": 68}], "SectionHeader": [{"begin": 0, "end": 1455, "idx": 0}], "SectionReference": [{"begin": 34776, "end": 38896, "idx": 0}], "Sentence": [{"begin": 87, "end": 334, "idx": 0}, {"begin": 335, "end": 560, "idx": 1}, {"begin": 561, "end": 712, "idx": 2}, {"begin": 713, "end": 843, "idx": 3}, {"begin": 844, "end": 977, "idx": 4}, {"begin": 978, "end": 1225, "idx": 5}, {"begin": 1226, "end": 1360, "idx": 6}, {"begin": 1361, "end": 1455, "idx": 7}, {"begin": 1473, "end": 1687, "idx": 8}, {"begin": 1688, "end": 1863, "idx": 9}, {"begin": 1864, "end": 2067, "idx": 10}, {"begin": 2068, "end": 2229, "idx": 11}, {"begin": 2230, "end": 2479, "idx": 12}, {"begin": 2480, "end": 2718, "idx": 13}, {"begin": 2719, "end": 2944, "idx": 14}, {"begin": 2945, "end": 3057, "idx": 15}, {"begin": 3058, "end": 3219, "idx": 16}, {"begin": 3220, "end": 3376, "idx": 17}, {"begin": 3377, "end": 3535, "idx": 18}, {"begin": 3536, "end": 3688, "idx": 19}, {"begin": 3689, "end": 3786, "idx": 20}, {"begin": 3787, "end": 3903, "idx": 21}, {"begin": 3904, "end": 3992, "idx": 22}, {"begin": 3993, "end": 4178, "idx": 23}, {"begin": 4179, "end": 4250, "idx": 24}, {"begin": 4251, "end": 4358, "idx": 25}, {"begin": 4359, "end": 4397, "idx": 26}, {"begin": 4398, "end": 4480, "idx": 27}, {"begin": 4481, "end": 4680, "idx": 28}, {"begin": 4681, "end": 4724, "idx": 29}, {"begin": 4725, "end": 4768, "idx": 30}, {"begin": 4769, "end": 4813, "idx": 31}, {"begin": 4814, "end": 4977, "idx": 32}, {"begin": 4978, "end": 5136, "idx": 33}, {"begin": 5137, "end": 5316, "idx": 34}, {"begin": 5317, "end": 5662, "idx": 35}, {"begin": 5663, "end": 6012, "idx": 36}, {"begin": 6013, "end": 6103, "idx": 37}, {"begin": 6104, "end": 6231, "idx": 38}, {"begin": 6232, "end": 6407, "idx": 39}, {"begin": 6408, "end": 6571, "idx": 40}, {"begin": 6572, "end": 6640, "idx": 41}, {"begin": 6641, "end": 6799, "idx": 42}, {"begin": 6800, "end": 7033, "idx": 43}, {"begin": 7083, "end": 7235, "idx": 44}, {"begin": 7236, "end": 7416, "idx": 45}, {"begin": 7417, "end": 7549, "idx": 46}, {"begin": 7566, "end": 7787, "idx": 47}, {"begin": 7788, "end": 8061, "idx": 48}, {"begin": 8062, "end": 8158, "idx": 49}, {"begin": 8159, "end": 8287, "idx": 50}, {"begin": 8288, "end": 8413, "idx": 51}, {"begin": 8414, "end": 8664, "idx": 52}, {"begin": 8665, "end": 8867, "idx": 53}, {"begin": 8868, "end": 9183, "idx": 54}, {"begin": 9201, "end": 9292, "idx": 55}, {"begin": 9319, "end": 9458, "idx": 56}, {"begin": 9483, "end": 9562, "idx": 57}, {"begin": 9563, "end": 9673, "idx": 58}, {"begin": 9674, "end": 9939, "idx": 59}, {"begin": 9940, "end": 10048, "idx": 60}, {"begin": 10049, "end": 10138, "idx": 61}, {"begin": 10139, "end": 10224, "idx": 62}, {"begin": 10225, "end": 10279, "idx": 63}, {"begin": 10305, "end": 10459, "idx": 64}, {"begin": 10460, "end": 10496, "idx": 65}, {"begin": 10497, "end": 10627, "idx": 66}, {"begin": 10628, "end": 10683, "idx": 67}, {"begin": 10684, "end": 10785, "idx": 68}, {"begin": 10786, "end": 10869, "idx": 69}, {"begin": 10870, "end": 10944, "idx": 70}, {"begin": 10945, "end": 11079, "idx": 71}, {"begin": 11080, "end": 11103, "idx": 72}, {"begin": 11104, "end": 11189, "idx": 73}, {"begin": 11190, "end": 11258, "idx": 74}, {"begin": 11259, "end": 11413, "idx": 75}, {"begin": 11414, "end": 11513, "idx": 76}, {"begin": 11514, "end": 11615, "idx": 77}, {"begin": 11616, "end": 11749, "idx": 78}, {"begin": 11750, "end": 11871, "idx": 79}, {"begin": 11905, "end": 12010, "idx": 80}, {"begin": 12011, "end": 12109, "idx": 81}, {"begin": 12110, "end": 12245, "idx": 82}, {"begin": 12246, "end": 12265, "idx": 83}, {"begin": 12266, "end": 12521, "idx": 84}, {"begin": 12522, "end": 12654, "idx": 85}, {"begin": 12655, "end": 12822, "idx": 86}, {"begin": 12823, "end": 12928, "idx": 87}, {"begin": 12929, "end": 12948, "idx": 88}, {"begin": 12949, "end": 13020, "idx": 89}, {"begin": 13021, "end": 13085, "idx": 90}, {"begin": 13086, "end": 13190, "idx": 91}, {"begin": 13191, "end": 13330, "idx": 92}, {"begin": 13331, "end": 13452, "idx": 93}, {"begin": 13453, "end": 13520, "idx": 94}, {"begin": 13543, "end": 13633, "idx": 95}, {"begin": 13634, "end": 13732, "idx": 96}, {"begin": 13733, "end": 13849, "idx": 97}, {"begin": 13850, "end": 13996, "idx": 98}, {"begin": 13997, "end": 14168, "idx": 99}, {"begin": 14169, "end": 14317, "idx": 100}, {"begin": 14318, "end": 14354, "idx": 101}, {"begin": 14355, "end": 14542, "idx": 102}, {"begin": 14543, "end": 14685, "idx": 103}, {"begin": 14686, "end": 14824, "idx": 104}, {"begin": 14825, "end": 14998, "idx": 105}, {"begin": 14999, "end": 15137, "idx": 106}, {"begin": 15138, "end": 15186, "idx": 107}, {"begin": 15212, "end": 15328, "idx": 108}, {"begin": 15329, "end": 15447, "idx": 109}, {"begin": 15448, "end": 15707, "idx": 110}, {"begin": 15727, "end": 15894, "idx": 111}, {"begin": 15895, "end": 15963, "idx": 112}, {"begin": 15964, "end": 16156, "idx": 113}, {"begin": 16157, "end": 16225, "idx": 114}, {"begin": 16226, "end": 16363, "idx": 115}, {"begin": 16364, "end": 16496, "idx": 116}, {"begin": 16497, "end": 16607, "idx": 117}, {"begin": 16608, "end": 16750, "idx": 118}, {"begin": 16751, "end": 16845, "idx": 119}, {"begin": 16846, "end": 16978, "idx": 120}, {"begin": 16979, "end": 17097, "idx": 121}, {"begin": 17098, "end": 17290, "idx": 122}, {"begin": 17291, "end": 17415, "idx": 123}, {"begin": 17416, "end": 17497, "idx": 124}, {"begin": 17522, "end": 17543, "idx": 125}, {"begin": 17544, "end": 17670, "idx": 126}, {"begin": 17671, "end": 17764, "idx": 127}, {"begin": 17765, "end": 17782, "idx": 128}, {"begin": 17783, "end": 17904, "idx": 129}, {"begin": 17905, "end": 18085, "idx": 130}, {"begin": 18086, "end": 18392, "idx": 131}, {"begin": 18393, "end": 18502, "idx": 132}, {"begin": 18503, "end": 18729, "idx": 133}, {"begin": 18730, "end": 18878, "idx": 134}, {"begin": 18879, "end": 18959, "idx": 135}, {"begin": 18960, "end": 19043, "idx": 136}, {"begin": 19044, "end": 19201, "idx": 137}, {"begin": 19202, "end": 19309, "idx": 138}, {"begin": 19310, "end": 19456, "idx": 139}, {"begin": 19457, "end": 19627, "idx": 140}, {"begin": 19628, "end": 19741, "idx": 141}, {"begin": 19742, "end": 19893, "idx": 142}, {"begin": 19894, "end": 19964, "idx": 143}, {"begin": 19977, "end": 20179, "idx": 144}, {"begin": 20180, "end": 20264, "idx": 145}, {"begin": 20265, "end": 20378, "idx": 146}, {"begin": 20379, "end": 20557, "idx": 147}, {"begin": 20558, "end": 20665, "idx": 148}, {"begin": 20700, "end": 20785, "idx": 149}, {"begin": 20786, "end": 20888, "idx": 150}, {"begin": 20926, "end": 21230, "idx": 151}, {"begin": 21231, "end": 21324, "idx": 152}, {"begin": 21325, "end": 21487, "idx": 153}, {"begin": 21488, "end": 21587, "idx": 154}, {"begin": 21588, "end": 21719, "idx": 155}, {"begin": 21720, "end": 21819, "idx": 156}, {"begin": 21820, "end": 21918, "idx": 157}, {"begin": 21919, "end": 22017, "idx": 158}, {"begin": 22018, "end": 22176, "idx": 159}, {"begin": 22177, "end": 22225, "idx": 160}, {"begin": 22226, "end": 22289, "idx": 161}, {"begin": 22290, "end": 22532, "idx": 162}, {"begin": 22533, "end": 22753, "idx": 163}, {"begin": 22754, "end": 22892, "idx": 164}, {"begin": 22893, "end": 23007, "idx": 165}, {"begin": 23008, "end": 23176, "idx": 166}, {"begin": 23177, "end": 23378, "idx": 167}, {"begin": 23379, "end": 23615, "idx": 168}, {"begin": 23616, "end": 23709, "idx": 169}, {"begin": 23744, "end": 23878, "idx": 170}, {"begin": 23879, "end": 24027, "idx": 171}, {"begin": 24028, "end": 24218, "idx": 172}, {"begin": 24219, "end": 24347, "idx": 173}, {"begin": 24348, "end": 24567, "idx": 174}, {"begin": 24590, "end": 24714, "idx": 175}, {"begin": 24715, "end": 24975, "idx": 176}, {"begin": 24976, "end": 25108, "idx": 177}, {"begin": 25109, "end": 25215, "idx": 178}, {"begin": 25248, "end": 25481, "idx": 179}, {"begin": 25482, "end": 25700, "idx": 180}, {"begin": 25701, "end": 25874, "idx": 181}, {"begin": 25875, "end": 26013, "idx": 182}, {"begin": 26014, "end": 26203, "idx": 183}, {"begin": 26223, "end": 26271, "idx": 184}, {"begin": 26272, "end": 26495, "idx": 185}, {"begin": 26496, "end": 26621, "idx": 186}, {"begin": 26622, "end": 26821, "idx": 187}, {"begin": 26822, "end": 26957, "idx": 188}, {"begin": 26958, "end": 27138, "idx": 189}, {"begin": 27139, "end": 27310, "idx": 190}, {"begin": 27311, "end": 27336, "idx": 191}, {"begin": 27337, "end": 27500, "idx": 192}, {"begin": 27501, "end": 27664, "idx": 193}, {"begin": 27665, "end": 27844, "idx": 194}, {"begin": 27845, "end": 27982, "idx": 195}, {"begin": 27983, "end": 28106, "idx": 196}, {"begin": 28107, "end": 28136, "idx": 197}, {"begin": 28137, "end": 28285, "idx": 198}, {"begin": 28286, "end": 28383, "idx": 199}, {"begin": 28384, "end": 28578, "idx": 200}, {"begin": 28579, "end": 28881, "idx": 201}, {"begin": 28882, "end": 29030, "idx": 202}, {"begin": 29031, "end": 29149, "idx": 203}, {"begin": 29150, "end": 29275, "idx": 204}, {"begin": 29276, "end": 29297, "idx": 205}, {"begin": 29298, "end": 29367, "idx": 206}, {"begin": 29368, "end": 29518, "idx": 207}, {"begin": 29519, "end": 29700, "idx": 208}, {"begin": 29701, "end": 29843, "idx": 209}, {"begin": 29844, "end": 30081, "idx": 210}, {"begin": 30082, "end": 30125, "idx": 211}, {"begin": 30141, "end": 30470, "idx": 212}, {"begin": 30505, "end": 30572, "idx": 213}, {"begin": 30573, "end": 30637, "idx": 214}, {"begin": 30638, "end": 30734, "idx": 215}, {"begin": 30735, "end": 30810, "idx": 216}, {"begin": 30811, "end": 30864, "idx": 217}, {"begin": 30865, "end": 30967, "idx": 218}, {"begin": 30968, "end": 31061, "idx": 219}, {"begin": 31062, "end": 31116, "idx": 220}, {"begin": 31117, "end": 31242, "idx": 221}, {"begin": 31243, "end": 31330, "idx": 222}, {"begin": 31364, "end": 31453, "idx": 223}, {"begin": 31454, "end": 31499, "idx": 224}, {"begin": 31500, "end": 31616, "idx": 225}, {"begin": 31617, "end": 31681, "idx": 226}, {"begin": 31682, "end": 31764, "idx": 227}, {"begin": 31765, "end": 31848, "idx": 228}, {"begin": 31849, "end": 32023, "idx": 229}, {"begin": 32063, "end": 32206, "idx": 230}, {"begin": 32207, "end": 32348, "idx": 231}, {"begin": 32349, "end": 32402, "idx": 232}, {"begin": 32403, "end": 32450, "idx": 233}, {"begin": 32451, "end": 32532, "idx": 234}, {"begin": 32533, "end": 32595, "idx": 235}, {"begin": 32596, "end": 32647, "idx": 236}, {"begin": 32648, "end": 32723, "idx": 237}, {"begin": 32724, "end": 32784, "idx": 238}, {"begin": 32785, "end": 32845, "idx": 239}, {"begin": 32846, "end": 32967, "idx": 240}, {"begin": 32968, "end": 33043, "idx": 241}, {"begin": 33044, "end": 33090, "idx": 242}, {"begin": 33091, "end": 33227, "idx": 243}, {"begin": 33228, "end": 33343, "idx": 244}, {"begin": 33344, "end": 33433, "idx": 245}, {"begin": 33434, "end": 33540, "idx": 246}, {"begin": 33541, "end": 33598, "idx": 247}, {"begin": 33599, "end": 33798, "idx": 248}, {"begin": 33799, "end": 33941, "idx": 249}, {"begin": 33942, "end": 34105, "idx": 250}], "ReferenceToFigure": [{"begin": 2227, "end": 2228, "idx": 0}, {"begin": 3055, "end": 3056, "idx": 1}, {"begin": 4088, "end": 4089, "idx": 2}, {"begin": 11077, "end": 11078, "idx": 3}, {"begin": 11255, "end": 11256, "idx": 4}, {"begin": 11746, "end": 11747, "idx": 5}, {"begin": 11868, "end": 11869, "idx": 6}, {"begin": 13639, "end": 13640, "target": "#fig_0", "idx": 7}, {"begin": 13993, "end": 13994, "idx": 8}, {"begin": 14740, "end": 14741, "idx": 9}, {"begin": 17762, "end": 17763, "target": "#fig_1", "idx": 10}, {"begin": 18128, "end": 18129, "target": "#fig_2", "idx": 11}, {"begin": 19962, "end": 19963, "target": "#fig_1", "idx": 12}, {"begin": 21101, "end": 21102, "target": "#fig_1", "idx": 13}, {"begin": 22024, "end": 22025, "idx": 14}, {"begin": 22352, "end": 22353, "target": "#fig_1", "idx": 15}, {"begin": 23005, "end": 23006, "target": "#fig_1", "idx": 16}, {"begin": 24245, "end": 24246, "target": "#fig_1", "idx": 17}, {"begin": 31461, "end": 31462, "target": "#fig_4", "idx": 18}, {"begin": 31624, "end": 31625, "idx": 19}, {"begin": 31971, "end": 31972, "idx": 20}], "Div": [{"begin": 87, "end": 1455, "idx": 0}, {"begin": 1458, "end": 7033, "idx": 1}, {"begin": 7035, "end": 7549, "idx": 2}, {"begin": 7551, "end": 9183, "idx": 3}, {"begin": 9185, "end": 9292, "idx": 4}, {"begin": 9294, "end": 9458, "idx": 5}, {"begin": 9460, "end": 10279, "idx": 6}, {"begin": 10281, "end": 11871, "idx": 7}, {"begin": 11873, "end": 13520, "idx": 8}, {"begin": 13522, "end": 15186, "idx": 9}, {"begin": 15188, "end": 15707, "idx": 10}, {"begin": 15709, "end": 17497, "idx": 11}, {"begin": 17499, "end": 19964, "idx": 12}, {"begin": 19966, "end": 20665, "idx": 13}, {"begin": 20667, "end": 20888, "idx": 14}, {"begin": 20890, "end": 23709, "idx": 15}, {"begin": 23711, "end": 24567, "idx": 16}, {"begin": 24569, "end": 25215, "idx": 17}, {"begin": 25217, "end": 26203, "idx": 18}, {"begin": 26205, "end": 30125, "idx": 19}, {"begin": 30127, "end": 30470, "idx": 20}, {"begin": 30472, "end": 31330, "idx": 21}, {"begin": 31332, "end": 32023, "idx": 22}, {"begin": 32025, "end": 34105, "idx": 23}], "SectionMain": [{"begin": 1455, "end": 34105, "idx": 0}]}}