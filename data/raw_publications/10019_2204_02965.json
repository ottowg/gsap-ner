{"text": "LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification\n\nAbstract:\nWe introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self information of network parameters in a reparameterized latent space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. We achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10 dataset as well as 35% smaller model size and 42% structured sparsity on ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model compression methods. Code is available at https://github.com/Sharath-girish/LilNetX.\nPreprint. Under review.\n\nMain:\n\n\n\n1 Introduction\nRecent research in deep neural networks (DNNs) has shown that large performance gains can be achieved on a variety of computer vision tasks simply by employing larger parameter-heavy and computationally intensive architectures [13, 26]. However, as the DNNs proliferate in the industry, they often need to be trained repeatedly, transmitted over the network to different devices, and need to perform under hardware constraints with minimal loss in accuracy, all at the same time. Hence, finding ways to reduce the storage size of the models on the devices while simultaneously improving their run-time is of utmost importance. This paper proposes a general purpose neural network training framework to jointly optimize the model parameters for accuracy, model size on the disk and computation, on any given task.\nOver the last few years, the research on training smaller and efficient DNNs has followed two seemingly parallel tracks with different goals: One line of work focuses on model compression to deal with the storage and communication network bottlenecks when deploying a large number of models over the air. While they achieve high levels of compression in terms of memory, their focus is not on reducing computation. They either require additional algorithms with some form of post hoc training [71] or quantize the network parameters at the cost of network performance [10, 39]. The other line of work focuses on reducing computation through various model pruning techniques [16]. The focus of these works is to decrease the number of Floating Point Operations (FLOPs) of the network at the inference time, albeit they are also able to achieve some compression due to fewer parameters. Typically, the cost of storing these pruned networks on disk is much higher than the dedicated model compression works. ResNet-50\nLilNetX (Ours)\nOktay [52] Wiedemann [69] Frankle [16] Lin [41] Reduction in Model Size % Reduction in Slice FLOPs Figure 1 : Our method jointly optimizes for size on disk (x-axis), as well as slice FLOPs (y-axis). We compare various approaches using the base ResNet-50 [26] architecture on ImageNet dataset [11] and plot FLOPs vs. size for models with similar accuracy. Prior methods optimize for either quantization (\u25a0) or pruning (\u25b2) objectives. Our approach, LilNetX, enables training a deep model while simultaneously optimizing for computation (using structured sparsity) as well as model size. Refer Tab. 1 for details.\nIn this work, we bridge the gap between the two and show for the first time that it is indeed possible to jointly optimize the model in terms of both the compression to reduce disk space as well as structured sparsity to reduce computation (Fig. 1). We propose to perform model compression by penalizing the entropy of weights that are quantized in a reparameterized latent space. This idea of reparameterized quantization [52] is extremely effective in reducing the effective model size on the disk, however, requires the full dense model during the inference. To address this shortcoming, we introduce key design changes to encourage structured and unstructured parameter sparsity in the model and enable tradeoff with model compression rates and accuracy. Our priors reside in reparameterized latent space while encouraging sparsity in the model space. More specifically, we introduce the notion of slice sparsity, a form of structured sparsity where each K \u00d7 K slice is fully zero for a convolutional kernel of filter size K. Unlike unstructured sparsity which have irregular memory access and offer little practical speedups, slice-structured sparsity allows for convolutions to be represented as block sparse matrix multiplications which can be exploited for computational gains and inference speedups through dedicated libraries like NVIDIA's CuSparse library [51]. Slice structured sparsity hits the middle ground between the advantages of high sparsity ratios from unstructured sparsity and practical inference speedups from fully structured sparsity.\nExtensive experimentation on three datasets show that our framework achieves extremely high levels of sparsity with little to no performance drops. In addition to increased sparsity, the introduced priors show gains even in model compression compared to Oktay et al. [52]. By varying the weight of the priors, we establish a trade-off between model size and accuracy as well as sparsity and accuracy. As our method achieves high levels of model compression along with structured sparsity, we dub it LilNetX -Lightweight Networks with EXtreme Compression and Structured Sparsification. We summarize our contributions below.\n\u2022 We introduce LilNetX, an algorithm to jointly perform model compression as well as structured and unstructured sparsification for direct computational gains in network inference. Our algorithm can be trained end-to-end using a single joint optimization objective and does not require post-hoc training or post-processing.\n\u2022 With extensive ablation studies and results, we show the effectiveness of our approach while outperforming existing approaches in both model compression and pruning in most networks and dataset setups.\n\n2 Related Work\nTypical model compression methods usually follow some form of quantization, parameter pruning, or both. Both lines of work focus on reducing the size of the model on the disk, and/or increasing the speed of the network during the inference time, while maintaining an acceptable level of classification accuracy. In this section, we discuss prominent quantization and pruning techniques.\n\n2.1 Model Pruning\nIt was well established early on that it is possible to prune a large number of neural network weights without significant loss in the performance [23, 37, 54]. Based on the stage of training at which weights are pruned, we can categorize model pruning methods broadly into three classes.\nPruning at initialization techniques such as SNIP [38], GraSP [65], NTT [42], and SynFlow [61] aim to prune the neural networks without any training by removing the weights least salient with respect to the loss for few images from the dataset. Even though these methods surpass the trivial baseline of random network pruning at initialization, [19] suggests that accuracy of these methods on standard classification benchmarks remains below the dense network obtained after training.\nPruning after training introduced by the Lottery Ticket Hypothesis [16], adapted by various works [3, 5, 6, 12, 17, 18, 20, 47, 49, 72, 75], prune the neural network weights based on the magnitude after the network is trained. Typically the dense network is reset to the initial weights after pruning and retrained from scratch. This results in additional training cost, however, the resulting network can achieve similar performance as the dense network, and better performance than the sparse networks obtained without training.\nThe third class of methods, perform the pruning while training [22, 23, 33, 55, 61, 64]. In these methods, the network is iteratively pruned during training based on either the magnitude of weights or their gradients. These heuristics have shown to perform well, however can only induce unstructured sparsity in the network which is not hardware friendly. Further, in these methods the pruning heuristic is often agnostic to the actual accuracy criteria for which the network is being optimized. Structured pruning methods [27-29, 40, 43, 45, 46, 68, 73, 80, 82] on the other hand, introduce additional priors or structure on the weights to be pruned, and have been shown to be effective in improving the inference speeds of the model on both CPUs and GPUs.\n\n2.2 Model Quantization\nQuantization methods for compressing neural network discretize the parameters of a network to a small, finite set of values, so that they can be further stored efficiently using one or more entropy coding methods [31, 56]. Some of the earlier methods resort to uniform quantization of weights to binary or tertiary representation of the weights [10, 30, 32, 39, 53, 79, 81]. Several other methods have focused on non-uniform Scalar Quantization (SQ) techniques [1, 50, 63, 70, 77, 78]. In SQ methods, neural network parameters or parameter groups can be quantized, both uniformly or non-uniformly, before or afer training. The maximum possible set of values that the representers (of the neural network parameters) can take is given by Kronecker product of representable scalar elements.\nVector Quantization (VQ) [7, 8, 21, 60, 67] on the other hand, is a more general technique, where the representers can take any value. VQ can be done by clustering of CNN layers at various rate-accuracy trade-offs [15, 59], hashing [7, 8], or residual quantization [21]. In practice, Scalar Quantization (SQ) is preferred over VQ due to its simplicity and effectiveness without additional encoding complexity. Young et al. [74] provide an overview of various transform based quantization techniques for compressing CNNs.\nOktay et al. [52] propose to warp the network parameters in a 'latent' space using a transformation, and perform scalar quantization in the learned latent space during model training. This allows SQ to be more flexible while at the same time, computationally more feasible as compared to VQ. This approach while effective at reducing the model size on the disk, doesn't provide any computation gains during inference since the decoded model is dense. In this paper, we build upon [52], to jointly train a model for both compression and computational gains. We describe our approach in Sec. A standard CNN comprises of a sequence of convolutional and fullyconnected layers. We reparameterize the parameters W i of each of these layers as W i in a quantized latent space. CNN parameters can be computed using a learned purely linear transform \u03a8 of the latent parameters. Linearity of transform allows sparsity in the quantized latents to translate into the sparsity of network parameters. Further, we organize each parameter tensor as a set of slices (depicted as colored bands) corresponding to different channels. Additional loss terms are then introduced to encourage slice sparsity and jointly optimize for accuracy-rate-computation. while highlighting our contributions. We show the benefits of our approach in terms of even smaller model size and better computational gains while maintaining high levels of accuracy in Sec. 4.\n\n3 Approach\nWe consider the task of classification using a convolutional neural network (CNN), although our approach can be trivially extended to other tasks such as object detection or generative modeling. Given a dataset of N images and their corresponding labels {x i , y i } N i=1 , our goal is to train a CNN with parameters \u0398 that is jointly optimized to: 1) maximize classification accuracy, 2) minimize the number of bits required to store the model on disk, and 3) minimize the computational cost of inference in the model. To keep our method end-to-end trainable, we formulate it as minimization of a joint objective that allows for an accuracy-rate-computation trade-off as belowL(\u0398) = L acc (\u0398) + L rate (\u0398) + L compute (\u0398).\nFor the task of classification, the accuracy term L acc (\u0398) is the usual cross-entropy loss. It encourages the model to maximize prediction accuracy. The rate term L rate (\u0398) encourages the model have a small disk size. We modify Oktay et al. [52] in the formulation of this term, where a self-information penalty encourages the parameters to have a small bit representation. The rate term, while encouraging smaller model size, doesn't lead to computational gains as the learned parameters are still dense. Our computation term L compute (\u0398) addresses this issue by introducing a structured sparsity inducing term that encourages the model to be more amenable to computational optimizations. Refer to Figure 2 for a high-level overview of our approach. In the following sections we describe the rate and computation terms in more detail.\n\n3.1 Rate term\nWe formulate our rate term by building upon Oktay et al. [52].The CNN weights \u0398 are reparameterized in terms of a latent representation. The latent representation is quantized and compressed while the network parameters are implicitly defined as a transform of the latent representation.\nA typical CNN consists of a sequence of convolutional and dense layers. For each of these layers, we denote weights as W and bias as b,\u0398 = W 1 , b 1 , W 2 , b 2 , . . . , W N , b N\nwhere N is the total number of layers in the network, and W k , b k represent weight and bias parameters of the kth layer. Each of these parameters can take continuous values during inference.\nHowever these parameters are stored using their discrete reparameterized forms belonging to a corresponding set\u03a6 = W 1 , b 1 , W 2 , b 2 , . . . , W N , b N\nFor each convolutional layer, W is a weight tensor of dimensions C in \u00d7 C out \u00d7 K \u00d7 K, where C in is the number of input channels, and C out is the number of output channels, and K denotes the filter width and height. The corresponding reparameterized weight tensor W is represented by a two-dimensional (2D) matrix of size C in C out \u00d7 K 2 . Similarly, for each dense layer, W is a tensor of dimension C in \u00d7 C out and its corresponding reparameterized weight W can be represented by a matrix of dimension C in C out \u00d7 1. All the biases can be reparameterized in the same way as dense layers. Each reparameterization can hence be represented by a quantized 2D matrix W \u2208 Z CinCout\u00d7l where l = 1 for dense weights (and biases) while l = K 2 for convolutional weights. Each in W represents a sample drawn from an l-dimensional discrete probability distribution. In order to convert parameters from latent space \u03a6 to model space \u0398, [52] introduced learnable affine transform \u03a8 for convolutional, dense, and bias weights.W = reshape(\u03a8 scale ( W T + \u03a8 shift ))\nwhere \u03a8 scale \u2208 R l\u00d7l , \u03a8 shift \u2208 R l are the affine transformation parameters. Also note that model partitioning is performed such that different kinds of layers use different pair of transform parameters (\u03a8 scale , \u03a8 shift ). That is, convolutional layers have their own transform, while dense layers have their own, and so on.\nAs \u03a6 consists of discrete parameters which are difficult to optimize, continuous surrogates W are maintained for each quantized parameter W . W is thus simply obtained by rounding the individual elements of W to the nearest integer. Rate minimization is achieved by enforcing an entropy penalty on the discrete weights. Since discrete entropy is non-differentiable, a continuous proxy is used instead. A set of continuous density functions implemented as small neural networks are fitted to W + n where n \u223c U(\u2212 1 2 , 1 2 ) is a matrix whose elements are drawn from the uniform distribution. We recap that each latent weight layer W is a matrix in Z CinCout\u00d7l and can be rewritten as a factorized probability model as shown in Eq. (5). The entropy of model weights can now be minimized directly by minimizing the negative log likelihood which serve as an approximation to the self-information I (Eq. ( 5)). The rate term is then the sum of all the self-information terms.q( W ) = CinCout j=1 l i=1 q i ( W j,i ) and I( W ) = \u2212 log 2 q( W )L rate (\u0398) = \u03bb I \u03d5\u2208\u03a6 I( \u03d5)\nHere q i represents the probability model for the i th element of an l\u2212dimensional row slice of W , \u03bb I is a trade-off parameter that permits specification of relative weight with respect to other terms. A straight-through estimator [2] is used to back-propagate the gradients from the classification loss to parameter latents.\n\n3.2 Sparsity priors\nWhile the rate term described in previous section encourages a smaller representation of the model in terms of the number of bits required to store the model on disk, it doesn't reduce the number of parameters in a way that leads to computational gains. To address this, we introduce a few key changes and then formulate our computation term as structured sparsity priors that lead to reduced computation. Note that we formulate all the priors in the reparameterized latent space to decouple from the affine transform parameters \u03a8 scale , \u03a8 shift and to be consistent with the L rate (\u0398) term that is also applied in the same space.\nUnstructured sparsity with zero mean prior: Our first contribution derives from the observation that even if the latent representation of the parameters is all zero, the resulting parameters may still be non-zero. For example, in order to enforce structural sparsity in the convolutional layers in the model space W , we require each K \u00d7 K slice to be all zero. However, this is only possible if (\u03a8 shift + W ) in Eq. 4 is a zero vector itself or lies in the null space of \u03a8 scale . We notice that the latter does not occur in most practical situations especially when the vector is discrete. Therefore, we remove the shift parameter \u03a8 shift and make the affine transform a purely linear transform. Now a zero vector in latent space results in a zero vector in the model space. Note that any single non zero element, in the latents corresponding to a slice, causes the full transformed vector to be nonzero and does not yield any sparsity in the model space.\nNext, we observe that zero entropy distributions over latents do not necessarily imply zero valued latents. They could take on any constant value. Therefore, we introduce a zero-mean prior on the latent representation of the parameters to encourage the latent parameters to go to zero as the entropy of their distribution decreases. This penalty encourages the ground state of the zero entropy distributions to be zero value, as opposed to an arbitrary constant values in absence of this penalty. As a result this penalty encourages unstructured sparsity in the latents. We use a simple gaussian as a zero mean prior that results in an l 2 penalty on the continuous surrogates W of the individual weight latents. A laplacian prior, resulting in a l 1 penalty, can also be used along with/instead of the gaussian prior. While l 1 penalty usually leads to sparsity on its own, in conjunction with rate penalty both seemed to have similar effect of encouraging unstructured sparsity. We experimented with both the priors and observed similar performance (an ablation study is provided in Sec. B of the appendix).\nGroup sparsity prior for computation: Now note that W can be viewed as being represented as a set of groups (or slices), where each group is of size K \u00d7 K corresponding to a same size slice of W . These slices going to zero as a whole lead to a block sparse structure in the corresponding transform matrix for the parameter W , which can then be exploited for computational gains. To encourage individual slices to go to zero as a whole we propose to use a group sparsity regularization [76] on the W slices as followingL group = Cin\u00d7Cout j=1 \u221a \u03c1 j \u2225 W j \u2225 2 .\nHere W j is the jth slice of the latent weight matrix, and \u03c1 j is a term to account for varying group sizes. Our overall computation term based on structured and unstructured sparsity becomesL compute (\u0398) = \u03bb U \u2225 W \u2225 2 2 + \u03bb S Cin\u00d7Cout j=1 \u221a \u03c1 j \u2225 W j \u2225 2\nNote that \u03bb U and \u03bb S are trade-off parameters for the individual sparsity terms.\n\n3.3 Discussion\nThe overall loss function is the combination of cross-entropy loss (for classification), self-information of the reparameterizations, and the regularization for structured and unstructured sparsity as following+ \u03bbU \u2225 W \u2225 2 2 Unstructured Sparsity + \u03bbS C in \u00d7Cout j=1 \u221a \u03c1j\u2225 W j \u22252\nStructured Sparsity (9) Without the group prior, the gaussian prior and the learned prior used by the rate penalty assume the individual weights in each slice to be independent draws. The group prior enforces structure on the weights and assigns higher likelihood for fully sparse or dense weight slices. Combined, all these priors enforce a slice-wise sparsity in the decoded weights. This sparsity, which we term slicesparsity, hits a middle ground between completely unstructured sparsity where individual weights are independently zero and fully structured sparsity where an entire filter is zero. Note that convolutions with slice-sparse weight tensors, when represented as matrix multiplications (as done by the popular im2col algorithm) [4], form block sparse matrices. Libraries, such as NVIDIA's Cusparse library [51], can exploit such block sparse structure to achieve speedups in comparison to dense matrix multiplications or entirely unstructured sparsity. We use the term Slice FLOPs (or SFLOPs) to denote the direct computational gain we can obtain through block or slice sparsity.\n\n4 Experiments\nDatasets. We consider three datasets in our experiments. CIFAR-10 and CIFAR-100 datasets [35] consist of 50000 training and 10000 test color images each of size 32 \u00d7 32. CIFAR-10 has 10 classes (with 6000 images per class) and CIFAR-100 has 100 classes (with 600 images per class). For large scale experiments, we use ILSVRC2012 (ImageNet) dataset [11]. It has 1.2 million images for training, 50000 images for the test and 1000 classes.\nNetwork Architectures. For CIFAR-10 and CIFAR-100 datasets, we show results using two architectures -VGG-16 [58] and ResNet-20 with a width multiplier of 4 (ResNet-20-4) [26]. VGG-16 is a commonly used architecture consisting of 13 convolutional layers of kernel size 3 \u00d7 3 and 3 dense or fully-connected layers. Dense layers are resized to adapt to CIFAR's 32 \u00d7 32 image size, as done in the baseline approaches [52]. ResNet-20-4 consists of 3 ResNet groups, each with 3 residual blocks. All the convolutional layer are of size 3 \u00d7 3, and there is a single dense layer at the end.\nFor the ImageNet experiments, we use ResNet-18/50 [26], an 18/50-layer network comprising of one 7 \u00d7 7 convolutional layer, one dense layer and the remaining 3 \u00d7 3 convolutional layers. We also run experiments on the MobileNet-V2 architecture [57] which consists of depthwise separable convolutions and inverted Bottleneck blocks.\n\n4.1 Implementation Details\nWe train all of our models from scratch. CIFAR-10 and CIFAR-100 experiments are trained for 200 epochs. We use the FFCV library [36] for faster ImageNet training, with a batchsize of 2048/512 for ResNet-18/50 split across 4 GPUs. We train ResNet-18/50 for 35/32 epochs in order to keep the range of the uncompressed network accuracies similar to other works for a fair comparison. CIFAR-10 and CIFAR-100 training schedules amount to much fewer iterations compared to previous works. Nevertheless, we show strong performance in terms of model compression and sparsity outperforming existing model compression works while converging faster with relatively fewer epochs.\nWe use two optimizers, one for optimizing the parameters of the entropy model q( W ), and the other for optimizing the decoder matrix \u03a8 and the model parameters \u03a6. Entropy model is optimized using Adam [34] with a learning rate of 10 \u22124 for all our experiments. The remaining parameters are optimized using Adam with a learning rate of 0.01 for CIFAR-10 experiments and a learning rate of 0.02/0.006/0.01 for ResNet-18/50/MobileNet-v2 respectively on ImageNet with a cosine decay schedule. Our model compression results are reported using the torchac library [48] which does arithmetic coding of the weights given probability tables for the quantized values which we obtain from the trained entropy models. We do not compress the biases and batch normalization parameters and include the additional sizes from these parameters as well as the parameters \u03a8 whenever we report the model size unless mentioned otherwise. The entropy model for VGG-16 consists of a parameter group for each dense layer and a parameter group for all 3 \u00d7 3 convolutions leading to four weight decoders/probability models for each parameter group. For ResNet-20-4 we use zero padding shortcut type A as defined in [26], which leads to only 2 parameter groups, one for the final dense layer and the other for all 3 \u00d7 3 convolutions. For ResNet-18 trained on ImageNet, we use three parameter groups, for the initial 7x7 convolution, 3 \u00d7 3 convolutions, as well as the dense layer. ResNet-50 consists of an additional parameter group for 1 \u00d7 1 convolutions. MobileNet-V2 consists of 3 parameter groups for the initial 3x3 convolution, final dense layer and the remaining 3x3 convolution. Initialization of the network weights and the decoders are non-trivial in terms of this decoding scheme. We provide an explanation of our initialization approach in Sec. C in our appendix.\nRecall from the Eq. ( 9) that in addition to the usual cross-entropy loss for classification, we have three additional loss terms and coefficients.\nCompression coefficient. The rate term allows us to trade-off between model size (bit-rate) and accuracy. We fix the coefficient \u03bb I = 10 \u22124 in our experiments.\nTable 1 : Comparison of our approach against other model compression techniques. We show two cases of our method with best error rate and also extreme compression at higher error rate. We achieve higher compression along with the added computational benefits of high slice sparsity. Best corresponds to our best model in terms of accuracy while Extreme is matching the range of error of baselines if exists. * corresponds to unstructured sparsity which is generally higher than structured sparsity. \u2212 implies that the work perform pruning but do not report numbers in their paper. Sparsity coefficients. Note that the sparsity terms not only improve the model's inference speed (by reducing the number of FLOPs) but also reduces the entropy of latent weights W as most of the weights become zero. By varying the two trade-off parameters, one for each sparsity term, we obtain different points on the pareto curves for accuracy-model size trade-off and accuracy-sparsity trade-off. We study each of these trade-offs extensively in Sec. 5.\n\n4.2 Comparison with compression methods\nAs discussed in Section 2, existing approaches for model compression follow either quantization, pruning, or both. We compare with the state of the art methods in each of these two categories. Among model quantization methods, we use Oktay et al. [52] and Dubey et al. [14] for comparison. Note that while [52] 's method compresses the model for storage and transmission, the uncompressed model is dense and doesn't provide any computational speedups.  [14] prunes the filters along with quantization and helps with speed-up as well. Our results are summarized in Table 1. Unless otherwise noted, we use the numbers reported by the original papers. Since we do not have access to many prior art models, we compare using overall sparsity. For the CIFAR-10 dataset, we achieve the best results in compression while also achieving a lower Top-1 error rate for both VGG-16 and ResNet-20-4.\nFor VGG-16 we obtain the best performance in the error range of \u223c 7% at 129KB which is a 465x compression compared to the baseline model. At the \u223c 10% error range, we outperform [52] in terms of model compression and also with a 99.2% slice sparsity. For ResNet-20-4, compared to [52] we achieve almost 3 times the compression rate at a similar error rate of \u223c 10.1, while simultaneously achieving extremely high levels of slice sparsity (98.73%) in the network weights. Similar results hold for the case of CIFAR-100 where we achieve a 137\u00d7 compression in model size with 86.67% sparsity with little to no drop in accuracy compared to the uncompressed model.\nFor ResNet-18 trained on ImageNet, we achieve 46\u00d7 compression as compared to the uncompressed model with a small drop in accuracy. The compressed network achieves a group sparsity of 58.3%. For an extreme compression case, we achieve higher levels of compression (54\u00d7) and sparsity (\u223c 65%) at the cost of \u223c 2% accuracy compared to uncompressed model. Figure 3 : Top-1 Accuracy vs. the % reduction in SFLOPs (left)/FLOPs (middle)/Speedup (Right) as defined in \u00a7 3.3. Speedup is the ratio of CPU inference times for the uncompressed to our compressed models on the CIFAR-10 test set. Hue goes from light to dark as we the \u03bbS, while the marker size increases with \u03bbU . Note that SFLOPs have a higher reduction due to higher slice sparsity compared to full structure sparsity while still offering computational gains. We obtain high % reduction in SFLOPs and FLOPs coupled with inference speedups with little loss in accuracy.\nTo show the generality of our approach, we additionally show experiments using the MobileNetV2 [57] architecture trained on ImageNet. Our approach achieves almost 21\u00d7 compression with respect to the uncompressed model in terms of model size and achieves a sparsity of 56.8% with almost no drop in Top-1 error on ImageNet (32.8%). A more extreme variant, with higher sparsity constraints achieves 25\u00d7 compression with a sparsity of 64.7%. The results show that our approach works well for different types of architectures such as MobileNetV2 which consist of Inverted BottleNeck layers coupled with Depthwise Separable Convolutions. Additionally, even though MobileNet is already optimized towards size (number of parameters) and computation (FLOPs), we are able to obtain high levels of additional compression and computation reduction with little to no loss in top-1 accuracy highighting the efficacy of our approach in different settings.\nWe therefore conclude that the LilNetX framework outperforms state-of-the-art approaches in model compression by a significant margin while also achieving sparsification of the network weights for computational gains.\n\n4.3 FLOPs/Speedup-Accuracy Tradeoff\nWhile we show compression gains with respect to state-of-the-art in Sec.4.2, here we highlight the computational gains we get through slice sparsity. As explained in Sec. 3.3, slice sparsity enables us to perform block sparse multiplications which offer network speedups. We use SFLOPs to denote the theoretical FLOPs obtained by computing using only non zero weight slices. Additionally we show the FLOPs corresponding to the fully structured regime by removing entire filters or input channels of a weight tensor which are completely zeros. While not directly optimizing for FLOPs, a high slice sparsity also provides the added benefits of full structured sparsity. Through structured sparsity, we also obtain computational gains in terms of inference speedups for the compressed network. Thus, both types of sparsity can offer computational speedups with no hardware modifications. We show the tradeoff between SFLOPs, FLOPs and speedups against Top-1 Accuracy in Figure 3. Speedups are measured on CPU by timing the forward pass of the full CIFAR-10 test set with a batch size of 128. The ratio of time taken between the uncompressed model to the compressed model provides the speedup gains reported in Figure 3. Note the high % reduction in SFLOPs compared to FLOPs due to allowing for less constrained sparsity while still offering computational gains. We see that we obtain high SFLOPs reduction (\u223c90%) with modest accuracy drops while also getting high levels of FLOP reduction. We additionally show the speedups in inference time for the compressed network by removing entire filters or input channels for convolutional layers based on their structural sparsity. We see from Fig. 3 that we obtain almost a 1.5\u00d7 speedup compared to uncompressed network with no drop in Top-1 accuracy. Even higher values of speedup at 2\u00d7 are obtained for modest drops in accuracy \u223c 1%. Thus, our framework allows for computational gains directly through slice sparsity (SFLOPs) or fully structured sparsity (FLOPs).\n\n5 Analysis\nIn this section, we analyze the effect of the different priors in detail. We use the ResNet-20-4 architecture trained on CIFAR-10/100 datasets. We use a constant compression coefficient \u03bb I = 10 \u22124 . Since both the unstructured sparsity and the structured sparsity coefficients \u03bb U , and \u03bb S impact overall sparsity, we keep one of them fixed, and analyze the impact of varying the other on model performance (accuracy), model size (bit-rate), and the slice sparsity (fraction). As the size of the weight decoders and the uncompressed parameters is fixed, we analyze model sizes for the compressed parameters this section.\n\n5.1 Effect of Unstructured Sparsity Regularization\nFig. 4 shows model performance as we vary \u03bb U at fixed values of \u03bb S . Figure 4 (a) and 4(b) show the impact on the top-1 accuracy as a function of model size. For CIFAR-10 dataset, when \u03bb S = 0, the accuracy of the model increases from 92.2% to 93.5% as we decrease \u03bb U , however, at the cost of increased model size (\u223c20KB and \u223c150KB respectively). Similar trend exists for other values of \u03bb S . We also observe that increasing \u03bb S (blue/circle to orange/cross to green/square), allows the network to reach higher accuracy for a fixed model size and \u03bb U .\nFigure 4 (c) and 4(d) show the impact on accuracy as a function of slice sparsity or SFLOPs. At \u03bb S = 0, increasing the value of \u03bb U , causes the accuracy to drop from 93.5% to 92.4% while slice sparsity increases from 74% to 98%. Again, we observe a similar trend at different values of \u03bb S . Also, as we increase \u03bb S , the plot shifts towards right showing we can achieve higher slice sparsity at a given accuracy.\nIn both cases, CIFAR-100 shows a similar but a bit more noisy trend as compared to CIFAR-10. We hypothesize that the models trained on CIFAR-100 do not completely converge by the end of 200 epochs in some cases (results shown in Sec. D of appendix), leading to more noise. x-axis shows the sparsity of the latent weights while y-axis shows the slice sparsity of the latent weights is same as slice sparsity of decoded weights and is correlated with speed-up in the model inference.\n\n5.2 Effect of Structured Sparsity Regularization\nFigure 5 shows how the performance of the model varies as we vary \u03bb S at fixed values of \u03bb U . Figure 5 (a) and 5(b) show the accuracy and model size trade-off. For CIFAR-10, when \u03bb U = 0 (blue circle), we can achieve 93.2% accuracy at 120KB, however, as we increase the penalty, model size decreases sharply to 50KB while keeping the accuracy at around 92.9%. As we increase \u03bb U , the plots moves slightly up and left, which means that we can achieve even lower model sizes at the same values of accuracy.\nFigure 5 (c) and 5(d) show the impact on the top-1 accuracy as a function of slice sparsity or the model's computational efficiency. At \u03bb U = 0, model's slice sparsity increases with increasing \u03bb S as expected, albeit at the cost of lower accuracy. Increasing the \u03bb U , allows for a wider trade-off range between accuracy and slice sparsity and also yields higher accuracy for the same levels of sparsity.\n\n5.3 Structured vs. unstructured sparsity\nFigure 6 shows the effect of \u03bb U and \u03bb S on slice sparsity of the latent weights which is same as slice sparsity of decoded weights (and is also a proxy for the speed-up in the model inference), and unstructured sparsity in latent weights (which is correlated with the model compression or the model size on disk). At a fixed \u03bb S , we observe that increasing \u03bb U increasing the sparsity of the latent weights as well as the slice sparsity. However, as we increase \u03bb S we notice that the plots shift upwards which implies higher structural sparsity for any fixed number of latent weight unstructured sparsity. We see that our structured sparsity priors are effective in forcing non zero weights to lie in fewer weight slices thus leading to higher structured sparsity.\n\n6 Conclusion\nWe propose a novel framework for training a deep neural network, while simultaneously optimizing for the model size, to reduce storage cost, and structured sparsity, to reduce computation cost. To the best of our knowledge, this is the first work on model compression that add priors for structured pruning of weights in a reparameterized discrete weight space.\nExperiments on three datasets, and three network architectures show that our approach achieves state of the performance in terms of simultaneous compression and reduction in FLOPs which directly translate to inference speedups. We also perform extensive ablation studies to verify that the proposed sparsity and entropy priors allow us to easily control the accuracy-rate-computation trade-off, which is an important consideration for practical deployment of models. We have provided our code for the benefit of community and reproducibility.\nrespectively. We see that l 2 group norm outperforms its l \u221e counterpart for both datasets. However, l 1 norm has little additional effect in terms of l 2 weight norm. Additionally, the l 2 group norm yields lesser slice sparsity for a given sparsity (c,g) highlighting the importance of l \u221e for high structured sparsity. While l \u221e leads to higher sparsity, it also shows higher model size for a given slice sparsity. Thus, there is an inherent tradeoff for l \u221e which leads to more sparsity but also larger model sizes (d,h).\n\nC Initialization of Continuous Surrogates\nThe initialization of the continuous surrogate W of a latent space weight W and the decoder matrix \u03a8 plays an important in the neural network training. Na\u00efve He initialization [25] commonly used in training ResNet classifiers does not work in our case since small values of W get rounded to zero before decoding. Such an initialization results in zero gradients for updating the parameters and the loss becomes stagnant. To overcome this issue, we propose a modification to the initialization of the different parameters. In our framework, we recap that the decoded weights used in a forward pass are obtained usingW = reshape( W \u03a8)\nwhere W is a matrix in Z CinCout\u00d7l and \u03a8 is a matrix in Z l\u00d7l (where l = 1 for dense weights (and biases) while l = K 2 for convolutional weights).\nOur goal is to initialize W and \u03a8 such that the decoded weights W follow He initialization [25]. First, since W is rounded to nearest integer (to obtain latent space weights W ), we assume its elements to be drawn from a uniform distribution in [\u2212b, b] where b > 0.5 in order to enforce atleast some non-zero weights after rounding to nearest integer. Next, we take the elements of \u03a8 to be a normal distribution with mean 0 and variance v.\nAssuming the parameters to be i.i.d., and Var(X) denoting the variance of any individual element in matrix X,Var(W ) = l \u00d7 Var(\u03a8) \u00d7 Var( W )\nAssuming a RELU activation, with f denoting the total number of channels (fan-in or fan-out) for a layer, LHS of Equation (11), using the He initializer becomes 2 f , RHS on the other hand can be obtained analytically2 f = l \u00d7 v \u00d7 (2b + 1) 2 \u2212 1 12 =\u21d2 b = 24 lvf + 1 \u2212 1 2 , v = 24 lf ((2b + 1) 2 \u2212 1)\nEquation (12) gives us a relationship between b (defining the uniform distribution of W ) and v (defining the normal distribution of \u03a8). Note that l and f values are constant and known for each layer.\nFor a weight decoder corresponding to a parameter group, the maximum value of f in that group enforces the smallest value of b which should be above a minimum limit b min . Denoting f max as the maximum fan-in or fan-out value for a parameter group, we get on the corresponding value of f , we then initialize the elements of W to be drawn from a uniform distribution in the interval [\u2212b, b] and elements of \u03a8 to be drawn from N (0, v).\nNote that f = f max =\u21d2 b = b min which shows that the minimum boundary corresponds to the layer with maximum channels (fan-in or fan-out) f .\nBy choosing an appropriate value of b min we obtain good initial values of the gradient which allows the network to converge well as training progresses. b min offers an intuitive way of initializing the discrete weights. Too small a value leads to most of the weights being set to zero while too large a value can lead to exploding gradients. In practice, we find that this initialization approach works well for Cifar experiments. For ImageNet experiments, we vary the variance of the weights manually and fix based on the best value.\n\nD CIFAR-100 Convergence\nWe analyze the convergence of 3 different runs for ResNet-20-4 trained on the CIFAR-100 dataset with varying values of \u03bb S and \u03bb U . Results are shown in Fig. 9 when trained for 200 epochs. We see that validation accuracy (on the right y-axis) continues to increase towards the end of training between 190-200 epochs. At the same time, validation loss (on the left y-axis) also decreases. This suggests that the model hasn't fully converged by the end of 200 epochs. We hypothesize that this is an artifact of the dataset as well as the cosine decay schedule where learning rate decreases drastically towards the end of training and is not maintained for longer for better convergence. MIT CIFAR-100 [35] MIT ImageNet [11] BSD 3-Clause\n\nE Code and License\nTable 2 lists all datasets we used and their licenses.\nOur code is available at https://github.com/Sharath-girish/LilNetX with the MIT License.\n\nFootnotes:\n\nReferences:\n\n- Banner, R., Nahshan, Y., Hoffer, E., Soudry, D.: Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723 (2018)- Bengio, Y., L\u00e9onard, N., Courville, A.: Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013)\n\n- Brix, C., Bahar, P., Ney, H.: Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. ACL (2020). https://doi.org/10.18653/v1/2020.acl-main.360, https://www.aclweb.org/anthology/2020.acl-main.360\n\n- Chellapilla, K., Puri, S., Simard, P.: High performance convolutional neural networks for document processing. In: Tenth international workshop on frontiers in handwriting recognition. Suvisoft (2006)\n\n- Chen, H., He, B., Wang, H., Ren, Y., Lim, S.N., Shrivastava, A.: Nerv: Neural representations for videos. arXiv preprint arXiv:2110.13903 (2021)\n\n- Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., Carbin, M.: The lottery ticket hypothesis for pre-trained bert networks. In: NeurIPS (2020)\n\n- Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural networks with the hashing trick. In: International conference on machine learning. pp. 2285-2294. PMLR (2015)\n\n- Chen, W., Wilson, J., Tyree, S., Weinberger, K.Q., Chen, Y.: Compressing convolutional neural networks in the frequency domain. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 1475-1484 (2016)\n\n- Choi, Y., et al.: Compression of deep cnns under joint sparsity constraints. arXiv:1805.08303 (2018)\n\n- Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural networks with binary weights during propagations. In: Advances in neural information processing systems. pp. 3123-3131 (2015)\n\n- Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n\n- Desai, S., Zhan, H., Aly, A.: Evaluating lottery tickets under distributional shifts. arXiv preprint arXiv:1910.12708 (2019)\n\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n\n- Dubey, A., Chatterjee, M., Ahuja, N.: Coreset-based neural network compression. In: Proceed- ings of the European Conference on Computer Vision (ECCV). pp. 454-470 (2018)\n\n- Faraone, J., Fraser, N., Blott, M., Leong, P.H.: Syq: Learning symmetric quantization for efficient deep neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4300-4309 (2018)\n\n- Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018)\n\n- Frankle, J., Dziugaite, G.K., Roy, D., Carbin, M.: Linear mode connectivity and the lottery ticket hypothesis. In: International Conference on Machine Learning. pp. 3259-3269. PMLR (2020)\n\n- Frankle, J., Dziugaite, G.K., Roy, D.M., Carbin, M.: Stabilizing the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611 (2019)\n\n- Frankle, J., Dziugaite, G.K., Roy, D.M., Carbin, M.: Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576 (2020)\n\n- Girish, S., Maiya, S.R., Gupta, K., Chen, H., Davis, L.S., Shrivastava, A.: The lottery ticket hypothesis for object recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 762-771 (2021)\n\n- Gong, Y., Liu, L., Yang, M., Bourdev, L.: Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115 (2014)\n\n- Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015)\n\n- Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626 (2015)\n\n- Havasi, M., Peharz, R., Hern\u00e1ndez-Lobato, J.M.: Minimal random code learning: Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440 (2018)\n\n- He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proceedings of the IEEE international conference on computer vision. pp. 1026-1034 (2015)\n\n- He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro- ceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n\n- He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating deep convolu- tional neural networks. arXiv preprint arXiv:1808.06866 (2018)\n\n- He, Y., Liu, P., Wang, Z., Hu, Z., Yang, Y.: Filter pruning via geometric median for deep convolutional neural networks acceleration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4340-4349 (2019)\n\n- He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural networks. In: Proceedings of the IEEE international conference on computer vision. pp. 1389-1397 (2017)\n\n- Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research 18(1), 6869-6898 (2017)\n\n- Huffman, D.A.: A method for the construction of minimum-redundancy codes. Proceedings of the IRE 40(9), 1098-1101 (1952)\n\n- Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and training of neural networks for efficient integer-arithmetic-only inference. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2704-2713 (2018)\n\n- de Jorge, P., Sanyal, A., Behl, H.S., Torr, P.H., Rogez, G., Dokania, P.K.: Progressive skele- tonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081 (2020)\n\n- Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\n- Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)\n\n- Leclerc, G., Ilyas, A., Engstrom, L., Park, S.M., Salman, H., Madry, A.: ffcv. https://github. com/libffcv/ffcv/ (2022)\n\n- LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Advances in neural information processing systems. pp. 598-605 (1990)\n\n- Lee, N., Ajanthan, T., Torr, P.H.: Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 (2018)\n\n- Li, F., Zhang, B., Liu, B.: Ternary weight networks. arXiv preprint arXiv:1605.04711 (2016)\n\n- Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016)\n\n- Lin, S., Ji, R., Li, Y., Deng, C., Li, X.: Toward compact convnets via structure-sparsity regularized filter pruning. IEEE transactions on neural networks and learning systems 31(2), 574-588 (2019)\n\n- Liu, T., Zenke, F.: Finding trainable sparse networks through neural tangent transfer. In: International Conference on Machine Learning. pp. 6336-6347. PMLR (2020)\n\n- Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolutional networks through network slimming. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2736-2744 (2017)\n\n- Louizos, C., Ullrich, K., Welling, M.: Bayesian compression for deep learning. arXiv preprint arXiv:1705.08665 (2017)\n\n- Louizos, C., Welling, M., Kingma, D.P.: Learning sparse neural networks through l_0 regular- ization. arXiv:1712.01312 (2017)\n\n- Luo, J.H., Wu, J., Lin, W.: Thinet: A filter level pruning method for deep neural network compression. In: Proceedings of the IEEE international conference on computer vision. pp. 5058-5066 (2017)\n\n- Malach, E., Yehudai, G., Shalev-Schwartz, S., Shamir, O.: Proving the lottery ticket hypothesis: Pruning is all you need. In: International Conference on Machine Learning. pp. 6682-6691. PMLR (2020)\n\n- Mentzer, F., Agustsson, E., Tschannen, M., Timofte, R., Van Gool, L.: Practical full resolution learned lossless image compression. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\n- Movva, R., Zhao, J.Y.: Dissecting lottery ticket transformers: Structural and behavioral study of sparse neural machine translation. ArXiv abs/2009.13270 (2020)\n\n- Nagel, M., Baalen, M.v., Blankevoort, T., Welling, M.: Data-free quantization through weight equalization and bias correction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1325-1334 (2019)\n\n- Narang, S., et al.: Block-sparse recurrent neural networks. arXiv:1711.02782 (2017)\n\n- Oktay, D., Ball\u00e9, J., Singh, S., Shrivastava, A.: Scalable model compression by entropy penalized reparameterization. arXiv preprint arXiv:1906.06624 (2019)\n\n- Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: Xnor-net: Imagenet classification using binary convolutional neural networks. In: European conference on computer vision. pp. 525-542. Springer (2016)\n\n- Reed, R.: Pruning algorithms-a survey. IEEE transactions on Neural Networks 4(5), 740-747 (1993)\n\n- Renda, A., Frankle, J., Carbin, M.: Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389 (2020)\n\n- Rissanen, J., Langdon, G.: Universal modeling and coding. IEEE Transactions on Information Theory 27(1), 12-23 (1981)\n\n- Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4510-4520 (2018)\n\n- Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recogni- tion. arXiv preprint arXiv:1409.1556 (2014)\n\n- Son, S., Nah, S., Lee, K.M.: Clustering convolutional kernels to compress deep neural networks. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 216-232 (2018)\n\n- Stock, P., Joulin, A., Gribonval, R., Graham, B., J\u00e9gou, H.: And the bit goes down: Revisiting the quantization of neural networks. arXiv preprint arXiv:1907.05686 (2019)\n\n- Tanaka, H., Kunin, D., Yamins, D.L., Ganguli, S.: Pruning neural networks without any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467 (2020)\n\n- Tu, C.H., Lee, J.H., Chan, Y.M., Chen, C.S.: Pruning depthwise separable convolutions for mobilenet compression. In: 2020 International Joint Conference on Neural Networks (IJCNN). pp. 1-8. IEEE (2020)\n\n- Tung, F., Mori, G.: Clip-q: Deep network compression learning by in-parallel pruning- quantization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7873-7882 (2018)\n\n- Verdenius, S., Stol, M., Forr\u00e9, P.: Pruning via iterative ranking of sensitivity statistics. arXiv preprint arXiv:2006.00896 (2020)\n\n- Wang, C., Zhang, G., Grosse, R.: Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376 (2020)\n\n- Wang, K., Liu, Z., Lin, Y., Lin, J., Han, S.: Haq: Hardware-aware automated quantization with mixed precision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8612-8620 (2019)\n\n- Wang, Y., Xu, C., You, S., Tao, D., Xu, C.: Cnnpack: Packing convolutional neural networks in the frequency domain. In: NIPS. vol. 1, p. 3 (2016)\n\n- Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in deep neural networks. Advances in neural information processing systems 29, 2074-2082 (2016)\n\n- Wiedemann, S., Kirchhoffer, H., Matlage, S., Haase, P., Marban, A., Marinc, T., Neumann, D., Osman, A., Marpe, D., Schwarz, H., et al.: Deepcabac: Context-adaptive binary arithmetic coding for deep neural network compression. arXiv preprint arXiv:1905.08318 (2019)\n\n- Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural networks for mobile devices. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4820-4828 (2016)\n\n- Yeom, S.K., Seegerer, P., Lapuschkin, S., Binder, A., Wiedemann, S., M\u00fcller, K.R., Samek, W.: Pruning by explaining: A novel criterion for deep neural network pruning. Pattern Recognition 115, 107899 (2021)\n\n- You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.G., Wang, Z., Lin, Y.: Drawing early-bird tickets: Toward more efficient training of deep networks. In: ICLR (2020), https://openreview.net/forum?id=BJxsrgStvr\n\n- You, Z., Yan, K., Ye, J., Ma, M., Wang, P.: Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks. arXiv preprint arXiv:1909.08174 (2019)\n\n- Young, S., Wang, Z., Taubman, D., Girod, B.: Transform quantization for cnn compression. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)\n\n- Yu, H., S, S.E., Y, Y.T., Morcos, A.S.: Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP. In: ICLR (2020), https://openreview.net/forum?id= S1xnXRVFwH\n\n- Yuan, M., Lin, Y.: Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68(1), 49-67 (2006)\n\n- Zhang, D., Yang, J., Ye, D., Hua, G.: Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In: Proceedings of the European conference on computer vision (ECCV). pp. 365-382 (2018)\n\n- Zhou, A., Yao, A., Guo, Y., Xu, L., Chen, Y.: Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 (2017)\n\n- Zhou, A., Yao, A., Wang, K., Chen, Y.: Explicit loss-error-aware quantization for low-bit deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 9426-9435 (2018)\n\n- Zhou, Y., Zhang, Y., Wang, Y., Tian, Q.: Accelerate cnn via recursive bayesian pruning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3306-3315 (2019)\n\n- Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization. arXiv preprint arXiv:1612.01064 (2016)\n\n- Zhuang, Z., Tan, M., Zhuang, B., Liu, J., Guo, Y., Wu, Q., Huang, J., Zhu, J.: Discrimination- aware channel pruning for deep neural networks. arXiv preprint arXiv:1810.11809 (2018)\n\n", "annotations": {"ReferenceToTable": [{"begin": 25995, "end": 25996, "idx": 0}, {"begin": 27638, "end": 27639, "idx": 1}, {"begin": 41913, "end": 41914, "target": "#tab_2", "idx": 2}], "SectionMain": [{"begin": 1132, "end": 42050, "idx": 0}], "ReferenceToFormula": [{"begin": 16263, "end": 16264, "target": "#formula_4", "idx": 0}, {"begin": 25702, "end": 25703, "idx": 1}], "SectionReference": [{"begin": 42064, "end": 56725, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1132, "idx": 0}], "Div": [{"begin": 102, "end": 1124, "idx": 0}, {"begin": 1135, "end": 6315, "idx": 1}, {"begin": 6317, "end": 6718, "idx": 2}, {"begin": 6720, "end": 8800, "idx": 3}, {"begin": 8802, "end": 11564, "idx": 4}, {"begin": 11566, "end": 13140, "idx": 5}, {"begin": 13142, "end": 16754, "idx": 6}, {"begin": 16756, "end": 20376, "idx": 7}, {"begin": 20378, "end": 21768, "idx": 8}, {"begin": 21770, "end": 23134, "idx": 9}, {"begin": 23136, "end": 27026, "idx": 10}, {"begin": 27028, "end": 30695, "idx": 11}, {"begin": 30697, "end": 32739, "idx": 12}, {"begin": 32741, "end": 33374, "idx": 13}, {"begin": 33376, "end": 34883, "idx": 14}, {"begin": 34885, "end": 35846, "idx": 15}, {"begin": 35848, "end": 36656, "idx": 16}, {"begin": 36658, "end": 38101, "idx": 17}, {"begin": 38103, "end": 41125, "idx": 18}, {"begin": 41127, "end": 41886, "idx": 19}, {"begin": 41888, "end": 42050, "idx": 20}], "Head": [{"begin": 1135, "end": 1149, "n": "1", "idx": 0}, {"begin": 6317, "end": 6331, "n": "2", "idx": 1}, {"begin": 6720, "end": 6737, "n": "2.1", "idx": 2}, {"begin": 8802, "end": 8824, "n": "2.2", "idx": 3}, {"begin": 11566, "end": 11576, "n": "3", "idx": 4}, {"begin": 13142, "end": 13155, "n": "3.1", "idx": 5}, {"begin": 16756, "end": 16775, "n": "3.2", "idx": 6}, {"begin": 20378, "end": 20392, "n": "3.3", "idx": 7}, {"begin": 21770, "end": 21783, "n": "4", "idx": 8}, {"begin": 23136, "end": 23162, "n": "4.1", "idx": 9}, {"begin": 27028, "end": 27067, "n": "4.2", "idx": 10}, {"begin": 30697, "end": 30732, "n": "4.3", "idx": 11}, {"begin": 32741, "end": 32751, "n": "5", "idx": 12}, {"begin": 33376, "end": 33426, "n": "5.1", "idx": 13}, {"begin": 34885, "end": 34933, "n": "5.2", "idx": 14}, {"begin": 35848, "end": 35888, "n": "5.3", "idx": 15}, {"begin": 36658, "end": 36670, "n": "6", "idx": 16}, {"begin": 38103, "end": 38144, "idx": 17}, {"begin": 41127, "end": 41150, "idx": 18}, {"begin": 41888, "end": 41906, "idx": 19}], "Paragraph": [{"begin": 102, "end": 1100, "idx": 0}, {"begin": 1101, "end": 1124, "idx": 1}, {"begin": 1150, "end": 1962, "idx": 2}, {"begin": 1963, "end": 2977, "idx": 3}, {"begin": 2978, "end": 2992, "idx": 4}, {"begin": 2993, "end": 3603, "idx": 5}, {"begin": 3604, "end": 5164, "idx": 6}, {"begin": 5165, "end": 5787, "idx": 7}, {"begin": 5788, "end": 6111, "idx": 8}, {"begin": 6112, "end": 6315, "idx": 9}, {"begin": 6332, "end": 6718, "idx": 10}, {"begin": 6738, "end": 7026, "idx": 11}, {"begin": 7027, "end": 7511, "idx": 12}, {"begin": 7512, "end": 8042, "idx": 13}, {"begin": 8043, "end": 8800, "idx": 14}, {"begin": 8825, "end": 9612, "idx": 15}, {"begin": 9613, "end": 10133, "idx": 16}, {"begin": 10134, "end": 11564, "idx": 17}, {"begin": 11577, "end": 12255, "idx": 18}, {"begin": 12302, "end": 13140, "idx": 19}, {"begin": 13156, "end": 13443, "idx": 20}, {"begin": 13444, "end": 13579, "idx": 21}, {"begin": 13625, "end": 13817, "idx": 22}, {"begin": 13818, "end": 13929, "idx": 23}, {"begin": 13975, "end": 14993, "idx": 24}, {"begin": 15032, "end": 15361, "idx": 25}, {"begin": 15362, "end": 16332, "idx": 26}, {"begin": 16427, "end": 16754, "idx": 27}, {"begin": 16776, "end": 17408, "idx": 28}, {"begin": 17409, "end": 18367, "idx": 29}, {"begin": 18368, "end": 19477, "idx": 30}, {"begin": 19478, "end": 19998, "idx": 31}, {"begin": 20039, "end": 20230, "idx": 32}, {"begin": 20295, "end": 20376, "idx": 33}, {"begin": 20393, "end": 20603, "idx": 34}, {"begin": 20673, "end": 21768, "idx": 35}, {"begin": 21784, "end": 22221, "idx": 36}, {"begin": 22222, "end": 22803, "idx": 37}, {"begin": 22804, "end": 23134, "idx": 38}, {"begin": 23163, "end": 23830, "idx": 39}, {"begin": 23831, "end": 25679, "idx": 40}, {"begin": 25680, "end": 25827, "idx": 41}, {"begin": 25828, "end": 25988, "idx": 42}, {"begin": 25989, "end": 27026, "idx": 43}, {"begin": 27068, "end": 27953, "idx": 44}, {"begin": 27954, "end": 28613, "idx": 45}, {"begin": 28614, "end": 29536, "idx": 46}, {"begin": 29537, "end": 30477, "idx": 47}, {"begin": 30478, "end": 30695, "idx": 48}, {"begin": 30733, "end": 32739, "idx": 49}, {"begin": 32752, "end": 33374, "idx": 50}, {"begin": 33427, "end": 33984, "idx": 51}, {"begin": 33985, "end": 34401, "idx": 52}, {"begin": 34402, "end": 34883, "idx": 53}, {"begin": 34934, "end": 35440, "idx": 54}, {"begin": 35441, "end": 35846, "idx": 55}, {"begin": 35889, "end": 36656, "idx": 56}, {"begin": 36671, "end": 37032, "idx": 57}, {"begin": 37033, "end": 37575, "idx": 58}, {"begin": 37576, "end": 38101, "idx": 59}, {"begin": 38145, "end": 38760, "idx": 60}, {"begin": 38778, "end": 38925, "idx": 61}, {"begin": 38926, "end": 39365, "idx": 62}, {"begin": 39366, "end": 39475, "idx": 63}, {"begin": 39507, "end": 39724, "idx": 64}, {"begin": 39809, "end": 40009, "idx": 65}, {"begin": 40010, "end": 40446, "idx": 66}, {"begin": 40447, "end": 40588, "idx": 67}, {"begin": 40589, "end": 41125, "idx": 68}, {"begin": 41151, "end": 41886, "idx": 69}, {"begin": 41907, "end": 41961, "idx": 70}, {"begin": 41962, "end": 42050, "idx": 71}], "ReferenceToBib": [{"begin": 1377, "end": 1381, "target": "#b12", "idx": 0}, {"begin": 1382, "end": 1385, "target": "#b25", "idx": 1}, {"begin": 2456, "end": 2460, "target": "#b70", "idx": 2}, {"begin": 2531, "end": 2535, "target": "#b9", "idx": 3}, {"begin": 2536, "end": 2539, "target": "#b38", "idx": 4}, {"begin": 2637, "end": 2641, "target": "#b15", "idx": 5}, {"begin": 2999, "end": 3003, "target": "#b51", "idx": 6}, {"begin": 3014, "end": 3018, "target": "#b68", "idx": 7}, {"begin": 3027, "end": 3031, "target": "#b15", "idx": 8}, {"begin": 3036, "end": 3040, "target": "#b40", "idx": 9}, {"begin": 3247, "end": 3251, "target": "#b25", "idx": 10}, {"begin": 3285, "end": 3289, "target": "#b10", "idx": 11}, {"begin": 4027, "end": 4031, "target": "#b51", "idx": 12}, {"begin": 4971, "end": 4975, "target": "#b50", "idx": 13}, {"begin": 5432, "end": 5436, "target": "#b51", "idx": 14}, {"begin": 6885, "end": 6889, "target": "#b22", "idx": 15}, {"begin": 6890, "end": 6893, "target": "#b36", "idx": 16}, {"begin": 6894, "end": 6897, "target": "#b53", "idx": 17}, {"begin": 7077, "end": 7081, "target": "#b37", "idx": 18}, {"begin": 7089, "end": 7093, "target": "#b64", "idx": 19}, {"begin": 7099, "end": 7103, "target": "#b41", "idx": 20}, {"begin": 7117, "end": 7121, "target": "#b60", "idx": 21}, {"begin": 7372, "end": 7376, "target": "#b18", "idx": 22}, {"begin": 7579, "end": 7583, "target": "#b15", "idx": 23}, {"begin": 7610, "end": 7613, "target": "#b2", "idx": 24}, {"begin": 7614, "end": 7616, "target": "#b4", "idx": 25}, {"begin": 7617, "end": 7619, "target": "#b5", "idx": 26}, {"begin": 7620, "end": 7623, "target": "#b11", "idx": 27}, {"begin": 7624, "end": 7627, "target": "#b16", "idx": 28}, {"begin": 7628, "end": 7631, "target": "#b17", "idx": 29}, {"begin": 7632, "end": 7635, "target": "#b19", "idx": 30}, {"begin": 7636, "end": 7639, "target": "#b46", "idx": 31}, {"begin": 7640, "end": 7643, "target": "#b48", "idx": 32}, {"begin": 7644, "end": 7647, "target": "#b71", "idx": 33}, {"begin": 7648, "end": 7651, "target": "#b74", "idx": 34}, {"begin": 8106, "end": 8110, "target": "#b21", "idx": 35}, {"begin": 8111, "end": 8114, "target": "#b22", "idx": 36}, {"begin": 8115, "end": 8118, "target": "#b32", "idx": 37}, {"begin": 8119, "end": 8122, "target": "#b54", "idx": 38}, {"begin": 8123, "end": 8126, "target": "#b60", "idx": 39}, {"begin": 8127, "end": 8130, "target": "#b63", "idx": 40}, {"begin": 8566, "end": 8605, "idx": 41}, {"begin": 9038, "end": 9042, "target": "#b30", "idx": 42}, {"begin": 9043, "end": 9046, "target": "#b55", "idx": 43}, {"begin": 9170, "end": 9174, "target": "#b9", "idx": 44}, {"begin": 9175, "end": 9178, "target": "#b29", "idx": 45}, {"begin": 9179, "end": 9182, "target": "#b31", "idx": 46}, {"begin": 9183, "end": 9186, "target": "#b38", "idx": 47}, {"begin": 9187, "end": 9190, "target": "#b52", "idx": 48}, {"begin": 9191, "end": 9194, "target": "#b78", "idx": 49}, {"begin": 9195, "end": 9198, "target": "#b80", "idx": 50}, {"begin": 9286, "end": 9289, "target": "#b0", "idx": 51}, {"begin": 9290, "end": 9293, "target": "#b49", "idx": 52}, {"begin": 9294, "end": 9297, "target": "#b62", "idx": 53}, {"begin": 9298, "end": 9301, "target": "#b69", "idx": 54}, {"begin": 9302, "end": 9305, "target": "#b76", "idx": 55}, {"begin": 9306, "end": 9309, "target": "#b77", "idx": 56}, {"begin": 9638, "end": 9641, "target": "#b6", "idx": 57}, {"begin": 9642, "end": 9644, "target": "#b7", "idx": 58}, {"begin": 9645, "end": 9648, "target": "#b20", "idx": 59}, {"begin": 9649, "end": 9652, "target": "#b59", "idx": 60}, {"begin": 9653, "end": 9656, "target": "#b66", "idx": 61}, {"begin": 9827, "end": 9831, "target": "#b14", "idx": 62}, {"begin": 9832, "end": 9835, "target": "#b58", "idx": 63}, {"begin": 9845, "end": 9848, "target": "#b6", "idx": 64}, {"begin": 9849, "end": 9851, "target": "#b7", "idx": 65}, {"begin": 9878, "end": 9882, "target": "#b20", "idx": 66}, {"begin": 10036, "end": 10040, "target": "#b73", "idx": 67}, {"begin": 10147, "end": 10151, "target": "#b51", "idx": 68}, {"begin": 10614, "end": 10618, "target": "#b51", "idx": 69}, {"begin": 12545, "end": 12549, "target": "#b51", "idx": 70}, {"begin": 13213, "end": 13217, "target": "#b51", "idx": 71}, {"begin": 14905, "end": 14909, "target": "#b51", "idx": 72}, {"begin": 16092, "end": 16095, "target": "#b4", "idx": 73}, {"begin": 16660, "end": 16663, "target": "#b1", "idx": 74}, {"begin": 19965, "end": 19969, "target": "#b75", "idx": 75}, {"begin": 20693, "end": 20696, "target": "#b8", "idx": 76}, {"begin": 21417, "end": 21420, "target": "#b3", "idx": 77}, {"begin": 21495, "end": 21499, "target": "#b50", "idx": 78}, {"begin": 21873, "end": 21877, "target": "#b34", "idx": 79}, {"begin": 22132, "end": 22136, "target": "#b10", "idx": 80}, {"begin": 22330, "end": 22334, "target": "#b57", "idx": 81}, {"begin": 22392, "end": 22396, "target": "#b25", "idx": 82}, {"begin": 22635, "end": 22639, "target": "#b51", "idx": 83}, {"begin": 22854, "end": 22858, "target": "#b25", "idx": 84}, {"begin": 23047, "end": 23051, "target": "#b56", "idx": 85}, {"begin": 23291, "end": 23295, "target": "#b35", "idx": 86}, {"begin": 24033, "end": 24037, "target": "#b33", "idx": 87}, {"begin": 24390, "end": 24394, "target": "#b47", "idx": 88}, {"begin": 25020, "end": 25024, "target": "#b25", "idx": 89}, {"begin": 27315, "end": 27319, "target": "#b51", "idx": 90}, {"begin": 27337, "end": 27341, "target": "#b13", "idx": 91}, {"begin": 27374, "end": 27378, "target": "#b51", "idx": 92}, {"begin": 27521, "end": 27525, "target": "#b13", "idx": 93}, {"begin": 28132, "end": 28136, "target": "#b51", "idx": 94}, {"begin": 28234, "end": 28238, "target": "#b51", "idx": 95}, {"begin": 29632, "end": 29636, "target": "#b56", "idx": 96}, {"begin": 38321, "end": 38325, "target": "#b24", "idx": 97}, {"begin": 39017, "end": 39021, "target": "#b24", "idx": 98}, {"begin": 39629, "end": 39633, "target": "#b10", "idx": 99}, {"begin": 39818, "end": 39822, "target": "#b11", "idx": 100}, {"begin": 41851, "end": 41855, "target": "#b34", "idx": 101}, {"begin": 41869, "end": 41873, "target": "#b10", "idx": 102}], "Sentence": [{"begin": 102, "end": 258, "idx": 0}, {"begin": 259, "end": 457, "idx": 1}, {"begin": 458, "end": 736, "idx": 2}, {"begin": 737, "end": 1036, "idx": 3}, {"begin": 1037, "end": 1100, "idx": 4}, {"begin": 1101, "end": 1110, "idx": 5}, {"begin": 1111, "end": 1124, "idx": 6}, {"begin": 1150, "end": 1386, "idx": 7}, {"begin": 1387, "end": 1629, "idx": 8}, {"begin": 1630, "end": 1776, "idx": 9}, {"begin": 1777, "end": 1962, "idx": 10}, {"begin": 1963, "end": 2267, "idx": 11}, {"begin": 2268, "end": 2377, "idx": 12}, {"begin": 2378, "end": 2540, "idx": 13}, {"begin": 2541, "end": 2642, "idx": 14}, {"begin": 2643, "end": 2847, "idx": 15}, {"begin": 2848, "end": 2967, "idx": 16}, {"begin": 2968, "end": 2977, "idx": 17}, {"begin": 2978, "end": 2992, "idx": 18}, {"begin": 2993, "end": 3191, "idx": 19}, {"begin": 3192, "end": 3347, "idx": 20}, {"begin": 3348, "end": 3425, "idx": 21}, {"begin": 3426, "end": 3577, "idx": 22}, {"begin": 3578, "end": 3603, "idx": 23}, {"begin": 3604, "end": 3853, "idx": 24}, {"begin": 3854, "end": 3984, "idx": 25}, {"begin": 3985, "end": 4165, "idx": 26}, {"begin": 4166, "end": 4362, "idx": 27}, {"begin": 4363, "end": 4459, "idx": 28}, {"begin": 4460, "end": 4976, "idx": 29}, {"begin": 4977, "end": 5164, "idx": 30}, {"begin": 5165, "end": 5312, "idx": 31}, {"begin": 5313, "end": 5437, "idx": 32}, {"begin": 5438, "end": 5565, "idx": 33}, {"begin": 5566, "end": 5749, "idx": 34}, {"begin": 5750, "end": 5787, "idx": 35}, {"begin": 5788, "end": 5968, "idx": 36}, {"begin": 5969, "end": 6111, "idx": 37}, {"begin": 6112, "end": 6315, "idx": 38}, {"begin": 6332, "end": 6435, "idx": 39}, {"begin": 6436, "end": 6643, "idx": 40}, {"begin": 6644, "end": 6718, "idx": 41}, {"begin": 6738, "end": 6898, "idx": 42}, {"begin": 6899, "end": 7026, "idx": 43}, {"begin": 7027, "end": 7271, "idx": 44}, {"begin": 7272, "end": 7511, "idx": 45}, {"begin": 7512, "end": 7738, "idx": 46}, {"begin": 7739, "end": 7840, "idx": 47}, {"begin": 7841, "end": 8042, "idx": 48}, {"begin": 8043, "end": 8131, "idx": 49}, {"begin": 8132, "end": 8260, "idx": 50}, {"begin": 8261, "end": 8398, "idx": 51}, {"begin": 8399, "end": 8538, "idx": 52}, {"begin": 8539, "end": 8800, "idx": 53}, {"begin": 8825, "end": 9047, "idx": 54}, {"begin": 9048, "end": 9199, "idx": 55}, {"begin": 9200, "end": 9310, "idx": 56}, {"begin": 9311, "end": 9447, "idx": 57}, {"begin": 9448, "end": 9612, "idx": 58}, {"begin": 9613, "end": 9747, "idx": 59}, {"begin": 9748, "end": 9883, "idx": 60}, {"begin": 9884, "end": 10022, "idx": 61}, {"begin": 10023, "end": 10133, "idx": 62}, {"begin": 10134, "end": 10317, "idx": 63}, {"begin": 10318, "end": 10425, "idx": 64}, {"begin": 10426, "end": 10584, "idx": 65}, {"begin": 10585, "end": 10690, "idx": 66}, {"begin": 10691, "end": 10723, "idx": 67}, {"begin": 10724, "end": 10806, "idx": 68}, {"begin": 10807, "end": 10903, "idx": 69}, {"begin": 10904, "end": 11002, "idx": 70}, {"begin": 11003, "end": 11120, "idx": 71}, {"begin": 11121, "end": 11247, "idx": 72}, {"begin": 11248, "end": 11407, "idx": 73}, {"begin": 11408, "end": 11564, "idx": 74}, {"begin": 11577, "end": 11771, "idx": 75}, {"begin": 11772, "end": 12097, "idx": 76}, {"begin": 12098, "end": 12255, "idx": 77}, {"begin": 12302, "end": 12394, "idx": 78}, {"begin": 12395, "end": 12451, "idx": 79}, {"begin": 12452, "end": 12521, "idx": 80}, {"begin": 12522, "end": 12677, "idx": 81}, {"begin": 12678, "end": 12809, "idx": 82}, {"begin": 12810, "end": 12994, "idx": 83}, {"begin": 12995, "end": 13055, "idx": 84}, {"begin": 13056, "end": 13140, "idx": 85}, {"begin": 13156, "end": 13292, "idx": 86}, {"begin": 13293, "end": 13443, "idx": 87}, {"begin": 13444, "end": 13515, "idx": 88}, {"begin": 13516, "end": 13579, "idx": 89}, {"begin": 13625, "end": 13747, "idx": 90}, {"begin": 13748, "end": 13817, "idx": 91}, {"begin": 13818, "end": 13929, "idx": 92}, {"begin": 13975, "end": 14192, "idx": 93}, {"begin": 14193, "end": 14317, "idx": 94}, {"begin": 14318, "end": 14497, "idx": 95}, {"begin": 14498, "end": 14568, "idx": 96}, {"begin": 14569, "end": 14742, "idx": 97}, {"begin": 14743, "end": 14835, "idx": 98}, {"begin": 14836, "end": 14993, "idx": 99}, {"begin": 15032, "end": 15111, "idx": 100}, {"begin": 15112, "end": 15259, "idx": 101}, {"begin": 15260, "end": 15361, "idx": 102}, {"begin": 15362, "end": 15594, "idx": 103}, {"begin": 15595, "end": 15681, "idx": 104}, {"begin": 15682, "end": 15763, "idx": 105}, {"begin": 15764, "end": 15952, "idx": 106}, {"begin": 15953, "end": 16096, "idx": 107}, {"begin": 16097, "end": 16260, "idx": 108}, {"begin": 16261, "end": 16267, "idx": 109}, {"begin": 16268, "end": 16332, "idx": 110}, {"begin": 16427, "end": 16630, "idx": 111}, {"begin": 16631, "end": 16754, "idx": 112}, {"begin": 16776, "end": 17029, "idx": 113}, {"begin": 17030, "end": 17181, "idx": 114}, {"begin": 17182, "end": 17408, "idx": 115}, {"begin": 17409, "end": 17622, "idx": 116}, {"begin": 17623, "end": 17770, "idx": 117}, {"begin": 17771, "end": 17891, "idx": 118}, {"begin": 17892, "end": 18001, "idx": 119}, {"begin": 18002, "end": 18107, "idx": 120}, {"begin": 18108, "end": 18186, "idx": 121}, {"begin": 18187, "end": 18367, "idx": 122}, {"begin": 18368, "end": 18475, "idx": 123}, {"begin": 18476, "end": 18514, "idx": 124}, {"begin": 18515, "end": 18700, "idx": 125}, {"begin": 18701, "end": 18864, "idx": 126}, {"begin": 18865, "end": 18938, "idx": 127}, {"begin": 18939, "end": 19080, "idx": 128}, {"begin": 19081, "end": 19186, "idx": 129}, {"begin": 19187, "end": 19348, "idx": 130}, {"begin": 19349, "end": 19457, "idx": 131}, {"begin": 19458, "end": 19477, "idx": 132}, {"begin": 19478, "end": 19674, "idx": 133}, {"begin": 19675, "end": 19858, "idx": 134}, {"begin": 19859, "end": 19998, "idx": 135}, {"begin": 20039, "end": 20147, "idx": 136}, {"begin": 20148, "end": 20230, "idx": 137}, {"begin": 20295, "end": 20376, "idx": 138}, {"begin": 20393, "end": 20603, "idx": 139}, {"begin": 20673, "end": 20856, "idx": 140}, {"begin": 20857, "end": 20977, "idx": 141}, {"begin": 20978, "end": 21058, "idx": 142}, {"begin": 21059, "end": 21274, "idx": 143}, {"begin": 21275, "end": 21449, "idx": 144}, {"begin": 21450, "end": 21641, "idx": 145}, {"begin": 21642, "end": 21768, "idx": 146}, {"begin": 21784, "end": 21793, "idx": 147}, {"begin": 21794, "end": 21840, "idx": 148}, {"begin": 21841, "end": 21953, "idx": 149}, {"begin": 21954, "end": 22065, "idx": 150}, {"begin": 22066, "end": 22137, "idx": 151}, {"begin": 22138, "end": 22221, "idx": 152}, {"begin": 22222, "end": 22244, "idx": 153}, {"begin": 22245, "end": 22397, "idx": 154}, {"begin": 22398, "end": 22534, "idx": 155}, {"begin": 22535, "end": 22640, "idx": 156}, {"begin": 22641, "end": 22710, "idx": 157}, {"begin": 22711, "end": 22803, "idx": 158}, {"begin": 22804, "end": 22989, "idx": 159}, {"begin": 22990, "end": 23134, "idx": 160}, {"begin": 23163, "end": 23203, "idx": 161}, {"begin": 23204, "end": 23266, "idx": 162}, {"begin": 23267, "end": 23392, "idx": 163}, {"begin": 23393, "end": 23543, "idx": 164}, {"begin": 23544, "end": 23645, "idx": 165}, {"begin": 23646, "end": 23830, "idx": 166}, {"begin": 23831, "end": 24092, "idx": 167}, {"begin": 24093, "end": 24320, "idx": 168}, {"begin": 24321, "end": 24537, "idx": 169}, {"begin": 24538, "end": 24747, "idx": 170}, {"begin": 24748, "end": 24953, "idx": 171}, {"begin": 24954, "end": 25137, "idx": 172}, {"begin": 25138, "end": 25284, "idx": 173}, {"begin": 25285, "end": 25360, "idx": 174}, {"begin": 25361, "end": 25490, "idx": 175}, {"begin": 25491, "end": 25595, "idx": 176}, {"begin": 25596, "end": 25660, "idx": 177}, {"begin": 25661, "end": 25679, "idx": 178}, {"begin": 25680, "end": 25827, "idx": 179}, {"begin": 25828, "end": 25852, "idx": 180}, {"begin": 25853, "end": 25933, "idx": 181}, {"begin": 25934, "end": 25988, "idx": 182}, {"begin": 25989, "end": 26069, "idx": 183}, {"begin": 26070, "end": 26173, "idx": 184}, {"begin": 26174, "end": 26271, "idx": 185}, {"begin": 26272, "end": 26396, "idx": 186}, {"begin": 26397, "end": 26487, "idx": 187}, {"begin": 26488, "end": 26569, "idx": 188}, {"begin": 26570, "end": 26592, "idx": 189}, {"begin": 26593, "end": 26785, "idx": 190}, {"begin": 26786, "end": 26969, "idx": 191}, {"begin": 26970, "end": 27026, "idx": 192}, {"begin": 27068, "end": 27182, "idx": 193}, {"begin": 27183, "end": 27260, "idx": 194}, {"begin": 27261, "end": 27357, "idx": 195}, {"begin": 27358, "end": 27519, "idx": 196}, {"begin": 27520, "end": 27601, "idx": 197}, {"begin": 27602, "end": 27640, "idx": 198}, {"begin": 27641, "end": 27716, "idx": 199}, {"begin": 27717, "end": 27805, "idx": 200}, {"begin": 27806, "end": 27953, "idx": 201}, {"begin": 27954, "end": 28091, "idx": 202}, {"begin": 28092, "end": 28204, "idx": 203}, {"begin": 28205, "end": 28424, "idx": 204}, {"begin": 28425, "end": 28613, "idx": 205}, {"begin": 28614, "end": 28744, "idx": 206}, {"begin": 28745, "end": 28803, "idx": 207}, {"begin": 28804, "end": 28964, "idx": 208}, {"begin": 28965, "end": 29079, "idx": 209}, {"begin": 29080, "end": 29195, "idx": 210}, {"begin": 29196, "end": 29279, "idx": 211}, {"begin": 29280, "end": 29427, "idx": 212}, {"begin": 29428, "end": 29536, "idx": 213}, {"begin": 29537, "end": 29670, "idx": 214}, {"begin": 29671, "end": 29866, "idx": 215}, {"begin": 29867, "end": 29974, "idx": 216}, {"begin": 29975, "end": 30168, "idx": 217}, {"begin": 30169, "end": 30477, "idx": 218}, {"begin": 30478, "end": 30695, "idx": 219}, {"begin": 30733, "end": 30882, "idx": 220}, {"begin": 30883, "end": 30903, "idx": 221}, {"begin": 30904, "end": 31004, "idx": 222}, {"begin": 31005, "end": 31107, "idx": 223}, {"begin": 31108, "end": 31275, "idx": 224}, {"begin": 31276, "end": 31400, "idx": 225}, {"begin": 31401, "end": 31523, "idx": 226}, {"begin": 31524, "end": 31617, "idx": 227}, {"begin": 31618, "end": 31821, "idx": 228}, {"begin": 31822, "end": 31949, "idx": 229}, {"begin": 31950, "end": 32091, "idx": 230}, {"begin": 32092, "end": 32219, "idx": 231}, {"begin": 32220, "end": 32404, "idx": 232}, {"begin": 32405, "end": 32525, "idx": 233}, {"begin": 32526, "end": 32609, "idx": 234}, {"begin": 32610, "end": 32739, "idx": 235}, {"begin": 32752, "end": 32825, "idx": 236}, {"begin": 32826, "end": 32895, "idx": 237}, {"begin": 32896, "end": 32951, "idx": 238}, {"begin": 32952, "end": 33230, "idx": 239}, {"begin": 33231, "end": 33374, "idx": 240}, {"begin": 33427, "end": 33497, "idx": 241}, {"begin": 33498, "end": 33586, "idx": 242}, {"begin": 33587, "end": 33777, "idx": 243}, {"begin": 33778, "end": 33824, "idx": 244}, {"begin": 33825, "end": 33984, "idx": 245}, {"begin": 33985, "end": 34077, "idx": 246}, {"begin": 34078, "end": 34215, "idx": 247}, {"begin": 34216, "end": 34278, "idx": 248}, {"begin": 34279, "end": 34401, "idx": 249}, {"begin": 34402, "end": 34494, "idx": 250}, {"begin": 34495, "end": 34635, "idx": 251}, {"begin": 34636, "end": 34674, "idx": 252}, {"begin": 34675, "end": 34883, "idx": 253}, {"begin": 34934, "end": 35094, "idx": 254}, {"begin": 35095, "end": 35294, "idx": 255}, {"begin": 35295, "end": 35440, "idx": 256}, {"begin": 35441, "end": 35573, "idx": 257}, {"begin": 35574, "end": 35689, "idx": 258}, {"begin": 35690, "end": 35846, "idx": 259}, {"begin": 35889, "end": 36203, "idx": 260}, {"begin": 36204, "end": 36328, "idx": 261}, {"begin": 36329, "end": 36497, "idx": 262}, {"begin": 36498, "end": 36656, "idx": 263}, {"begin": 36671, "end": 36864, "idx": 264}, {"begin": 36865, "end": 37032, "idx": 265}, {"begin": 37033, "end": 37260, "idx": 266}, {"begin": 37261, "end": 37499, "idx": 267}, {"begin": 37500, "end": 37575, "idx": 268}, {"begin": 37576, "end": 37589, "idx": 269}, {"begin": 37590, "end": 37667, "idx": 270}, {"begin": 37668, "end": 37743, "idx": 271}, {"begin": 37744, "end": 37897, "idx": 272}, {"begin": 37898, "end": 37993, "idx": 273}, {"begin": 37994, "end": 38101, "idx": 274}, {"begin": 38145, "end": 38296, "idx": 275}, {"begin": 38297, "end": 38457, "idx": 276}, {"begin": 38458, "end": 38565, "idx": 277}, {"begin": 38566, "end": 38666, "idx": 278}, {"begin": 38667, "end": 38760, "idx": 279}, {"begin": 38778, "end": 38925, "idx": 280}, {"begin": 38926, "end": 39022, "idx": 281}, {"begin": 39023, "end": 39277, "idx": 282}, {"begin": 39278, "end": 39365, "idx": 283}, {"begin": 39366, "end": 39475, "idx": 284}, {"begin": 39507, "end": 39724, "idx": 285}, {"begin": 39809, "end": 39945, "idx": 286}, {"begin": 39946, "end": 40009, "idx": 287}, {"begin": 40010, "end": 40182, "idx": 288}, {"begin": 40183, "end": 40446, "idx": 289}, {"begin": 40447, "end": 40588, "idx": 290}, {"begin": 40589, "end": 40742, "idx": 291}, {"begin": 40743, "end": 40810, "idx": 292}, {"begin": 40811, "end": 40932, "idx": 293}, {"begin": 40933, "end": 41021, "idx": 294}, {"begin": 41022, "end": 41125, "idx": 295}, {"begin": 41151, "end": 41283, "idx": 296}, {"begin": 41284, "end": 41340, "idx": 297}, {"begin": 41341, "end": 41468, "idx": 298}, {"begin": 41469, "end": 41539, "idx": 299}, {"begin": 41540, "end": 41617, "idx": 300}, {"begin": 41618, "end": 41836, "idx": 301}, {"begin": 41837, "end": 41886, "idx": 302}, {"begin": 41907, "end": 41961, "idx": 303}, {"begin": 41962, "end": 42028, "idx": 304}, {"begin": 42029, "end": 42050, "idx": 305}], "ReferenceToFigure": [{"begin": 3099, "end": 3100, "idx": 0}, {"begin": 3850, "end": 3851, "idx": 1}, {"begin": 13011, "end": 13012, "target": "#fig_1", "idx": 2}, {"begin": 28972, "end": 28973, "idx": 3}, {"begin": 31707, "end": 31708, "idx": 4}, {"begin": 31947, "end": 31948, "idx": 5}, {"begin": 32422, "end": 32423, "idx": 6}, {"begin": 33432, "end": 33433, "target": "#fig_4", "idx": 7}, {"begin": 33505, "end": 33506, "target": "#fig_4", "idx": 8}, {"begin": 33992, "end": 33993, "target": "#fig_4", "idx": 9}, {"begin": 34941, "end": 34942, "idx": 10}, {"begin": 35036, "end": 35037, "idx": 11}, {"begin": 35448, "end": 35449, "idx": 12}, {"begin": 35896, "end": 35897, "idx": 13}, {"begin": 41310, "end": 41311, "idx": 14}], "Abstract": [{"begin": 92, "end": 1124, "idx": 0}], "SectionFootnote": [{"begin": 42052, "end": 42062, "idx": 0}], "ReferenceString": [{"begin": 42079, "end": 42245, "id": "b0", "idx": 0}, {"begin": 42247, "end": 42416, "id": "b1", "idx": 1}, {"begin": 42420, "end": 42655, "id": "b2", "idx": 2}, {"begin": 42659, "end": 42859, "id": "b3", "idx": 3}, {"begin": 42863, "end": 43007, "id": "b4", "idx": 4}, {"begin": 43011, "end": 43166, "id": "b5", "idx": 5}, {"begin": 43170, "end": 43357, "id": "b6", "idx": 6}, {"begin": 43361, "end": 43613, "id": "b7", "idx": 7}, {"begin": 43617, "end": 43717, "id": "b8", "idx": 8}, {"begin": 43721, "end": 43924, "id": "b9", "idx": 9}, {"begin": 43928, "end": 44137, "id": "b10", "idx": 10}, {"begin": 44141, "end": 44265, "id": "b11", "idx": 11}, {"begin": 44269, "end": 44531, "id": "b12", "idx": 12}, {"begin": 44535, "end": 44705, "id": "b13", "idx": 13}, {"begin": 44709, "end": 44934, "id": "b14", "idx": 14}, {"begin": 44938, "end": 45075, "id": "b15", "idx": 15}, {"begin": 45079, "end": 45266, "id": "b16", "idx": 16}, {"begin": 45270, "end": 45404, "id": "b17", "idx": 17}, {"begin": 45408, "end": 45571, "id": "b18", "idx": 18}, {"begin": 45575, "end": 45810, "id": "b19", "idx": 19}, {"begin": 45814, "end": 45960, "id": "b20", "idx": 20}, {"begin": 45964, "end": 46139, "id": "b21", "idx": 21}, {"begin": 46143, "end": 46292, "id": "b22", "idx": 22}, {"begin": 46296, "end": 46464, "id": "b23", "idx": 23}, {"begin": 46468, "end": 46691, "id": "b24", "idx": 24}, {"begin": 46695, "end": 46881, "id": "b25", "idx": 25}, {"begin": 46885, "end": 47044, "id": "b26", "idx": 26}, {"begin": 47048, "end": 47289, "id": "b27", "idx": 27}, {"begin": 47293, "end": 47474, "id": "b28", "idx": 28}, {"begin": 47478, "end": 47705, "id": "b29", "idx": 29}, {"begin": 47709, "end": 47829, "id": "b30", "idx": 30}, {"begin": 47833, "end": 48122, "id": "b31", "idx": 31}, {"begin": 48126, "end": 48323, "id": "b32", "idx": 32}, {"begin": 48327, "end": 48430, "id": "b33", "idx": 33}, {"begin": 48434, "end": 48530, "id": "b34", "idx": 34}, {"begin": 48534, "end": 48653, "id": "b35", "idx": 35}, {"begin": 48657, "end": 48790, "id": "b36", "idx": 36}, {"begin": 48794, "end": 48934, "id": "b37", "idx": 37}, {"begin": 48938, "end": 49029, "id": "b38", "idx": 38}, {"begin": 49033, "end": 49169, "id": "b39", "idx": 39}, {"begin": 49173, "end": 49370, "id": "b40", "idx": 40}, {"begin": 49374, "end": 49537, "id": "b41", "idx": 41}, {"begin": 49541, "end": 49760, "id": "b42", "idx": 42}, {"begin": 49764, "end": 49881, "id": "b43", "idx": 43}, {"begin": 49885, "end": 50010, "id": "b44", "idx": 44}, {"begin": 50014, "end": 50210, "id": "b45", "idx": 45}, {"begin": 50214, "end": 50412, "id": "b46", "idx": 46}, {"begin": 50416, "end": 50643, "id": "b47", "idx": 47}, {"begin": 50647, "end": 50807, "id": "b48", "idx": 48}, {"begin": 50811, "end": 51035, "id": "b49", "idx": 49}, {"begin": 51039, "end": 51122, "id": "b50", "idx": 50}, {"begin": 51126, "end": 51282, "id": "b51", "idx": 51}, {"begin": 51286, "end": 51489, "id": "b52", "idx": 52}, {"begin": 51493, "end": 51589, "id": "b53", "idx": 53}, {"begin": 51593, "end": 51730, "id": "b54", "idx": 54}, {"begin": 51734, "end": 51851, "id": "b55", "idx": 55}, {"begin": 51855, "end": 52075, "id": "b56", "idx": 56}, {"begin": 52079, "end": 52215, "id": "b57", "idx": 57}, {"begin": 52219, "end": 52403, "id": "b58", "idx": 58}, {"begin": 52407, "end": 52577, "id": "b59", "idx": 59}, {"begin": 52581, "end": 52751, "id": "b60", "idx": 60}, {"begin": 52755, "end": 52956, "id": "b61", "idx": 61}, {"begin": 52960, "end": 53163, "id": "b62", "idx": 62}, {"begin": 53167, "end": 53298, "id": "b63", "idx": 63}, {"begin": 53302, "end": 53442, "id": "b64", "idx": 64}, {"begin": 53446, "end": 53664, "id": "b65", "idx": 65}, {"begin": 53668, "end": 53813, "id": "b66", "idx": 66}, {"begin": 53817, "end": 53986, "id": "b67", "idx": 67}, {"begin": 53990, "end": 54254, "id": "b68", "idx": 68}, {"begin": 54258, "end": 54468, "id": "b69", "idx": 69}, {"begin": 54472, "end": 54678, "id": "b70", "idx": 70}, {"begin": 54682, "end": 54906, "id": "b71", "idx": 71}, {"begin": 54910, "end": 55090, "id": "b72", "idx": 72}, {"begin": 55094, "end": 55252, "id": "b73", "idx": 73}, {"begin": 55256, "end": 55444, "id": "b74", "idx": 74}, {"begin": 55448, "end": 55632, "id": "b75", "idx": 75}, {"begin": 55636, "end": 55846, "id": "b76", "idx": 76}, {"begin": 55850, "end": 56018, "id": "b77", "idx": 77}, {"begin": 56022, "end": 56237, "id": "b78", "idx": 78}, {"begin": 56241, "end": 56426, "id": "b79", "idx": 79}, {"begin": 56430, "end": 56538, "id": "b80", "idx": 80}, {"begin": 56542, "end": 56723, "id": "b81", "idx": 81}]}}