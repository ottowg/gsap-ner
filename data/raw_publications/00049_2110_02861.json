{"text": "8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION\n\nAbstract:\nStateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-sourceour 8-bit optimizers as a drop-in replacement that only requires a two-line code change.\n\n\nFigure 1 : Schematic of 8-bit optimizers via block-wise dynamic quantization, see Section 2 for more details. After the optimizer update is performed in 32-bit, the state tensor is chunked into blocks, normalized by the absolute maximum value of each block. Then dynamic quantization is performed, and the index is stored. For dequantization, a lookup in the index is performed, with subsequent denormalization by multiplication with the block-wise absolute maximum value. Outliers are confined to a single block through block-wise quantization, and their effect on normalization is limited.\nand rare large ones. However, to be practical, 8-bit optimizers need to be fast enough to not slow down training, which is especially difficult for non-linear methods that require more complex data structures to maintain the quantization buckets. Finally, to maintain stability with huge models beyond 1B parameters, a quantization method needs to not only have a good mean error but excellent worse case performance since a single large quantization error can cause the entire training run to diverge.\nWe introduce a new block-wise quantization approach that addresses all three of these challenges. Block-wise quantization splits input tensors into blocks and performs quantization on each block independently. This block-wise division reduces the effect of outliers on the quantization process since they are isolated to particular blocks, thereby improving stability and performance, especially for large-scale models. Block-wise processing also allows for high optimizer throughput since each normalization can be computed independently in each core. This contrasts with tensor-wide normalization, which requires slow cross-core synchronization that is highly dependent on task-core scheduling. We combine block-wise quantization with two novel methods for stable, high-performance 8-bit optimizers: dynamic quantization and a stable embedding layer. Dynamic quantization is an extension of dynamic tree quantization for unsigned input data. The stable embedding layer is a variation of a standard word embedding layer that supports more aggressive quantization by normalizing the highly non-uniform distribution of inputs to avoid extreme gradient variation.\nOur 8-bit optimizers maintain 32-bit performance at a fraction of the original memory footprint.\nWe show this for a broad range of tasks: 1.5B and 355M parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14+WMT'16 machine translation, MoCo v2 contrastive image pretraining+finetuning, and RoBERTa pretraining. We also report additional ablations and sensitivity analysis showing that all components -block-wise quantization, dynamic quantization, and stable embedding layer -are crucial for these results and that 8-bit Adam can be used as a simple drop-in replacement for 32-bit Adam, with no hyperparameter changes. We open-source our custom CUDA kernels and provide a PyTorch implementation that enables 8-bit optimization by changing two lines of code.\n\n1 BACKGROUND\n1.1 STATEFUL OPTIMIZERS An optimizer updates the parameters w of a neural network by using the gradient of the loss with respect to the weight g t = \u2202L \u2202w at update iteration t. Stateful optimizers compute statistics of the gradient with respect to each parameter over time for accelerated optimization. Two of the most commonly used stateful optimizers are Adam (Kingma and Ba, 2014), and SGD with momentum (Qian, 1999) -or Momentum for short. Without damping and scaling constants, the update rules of these optimizers are given by:Momentum(g t , w t\u22121 , m t\u22121 ) = \uf8f1 \uf8f2 \uf8f3 m 0 = g 0 Initialization m t = \u03b2 1 m t\u22121 + g t State 1 update w t = w t\u22121 \u2212 \u03b1 \u2022 m t Weight update (1) Adam(g t , w t\u22121 , m t\u22121 , r t\u22121 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 r 0 = m 0 = 0 Initialization m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )g t State 1 update r t = \u03b2 2 r t\u22121 + (1 \u2212 \u03b2 2 )g 2 t State 2 update w t = w t\u22121 \u2212 \u03b1 \u2022 mt \u221a rt+ Weight update, (2)\nwhere \u03b2 1 and \u03b2 2 are smoothing constants, is a small constant, and \u03b1 is the learning rate.\nFor 32-bit states, Momentum and Adam consume 4 and 8 bytes per parameter. That is 4 GB and 8 GB for a 1B parameter model. Our 8-bit non-linear quantization reduces these costs to 1 GB and 2 GB.\n\n1.2 NON-LINEAR QUANTIZATION\nQuantization compresses numeric representations to save space at the cost of precision. Quantization is the mapping of a k-bit integer to a real element in D, that is,Q map : [0, 2 k \u2212 1] \u2192 D.\nFor example, the IEEE 32-bit floating point data type maps the indices 0...2 32 \u2212 1 to the domain [-3.4e38, +3.4e38]. We use the following notation:Q map (i) = Q map i = q i , for example Q map (2 31 + 131072) = 2.\n03125, for the IEEE 32-bit floating point data type.\nTo perform general quantization from one data type into another we require three steps. (1) Compute a normalization constant N that transforms the input tensor T into the range of the domain D of the target quantization data type Q map , (2) for each element of T/N find the closest corresponding value q i in the domain D, (3) store the index i corresponding to q i in the quantized output tensor T Q . To receive the dequantized tensor T D we look up the index and denormalize:T D i = Q map (T Q i ) \u2022 N .\nTo perform this procedure for dynamic quantization we first normalize into the range [-1, 1] through division by the absolute maximum value: N = max(|T|).\nThen we find the closest values via a binary search: Dynamic Tree quantization (Dettmers, 2016) is a method that yields low quantization error for both small and large magnitude values. Unlike data types with fixed exponent and fraction, dynamic tree quantization uses a datatype with a dynamic exponent and fraction that can change with each number. It is made up of four parts, as seen in Figure 2 : (1) The first bit of the data type is reserved for a sign.T Q i = 2 n arg min j=0 |Q map j \u2212 T i N | (3) 1.3 DYNAMIC TREE QUANTIZATION\n(2) The number of subsequent zero bits indicates the magnitude of the exponent.\n(3) The first bit that is set to one indicates that all following values are reserved for (4) linear quantization. By moving the indicator bit, numbers can have a large exponent 10 \u22127 or precision as high as 1/63. Compared to linear quantization, dynamic tree quantization has better absolute and relative quantization errors for non-uniform distributions. Dynamic tree quantization is strictly defined to quantize numbers in the range [-1.0, 1.0], which is ensured by performing tensor-level absolute max normalization.\n2 8-BIT OPTIMIZERS Our 8-bit optimizers have three components: (1) block-wise quantization that isolates outliers and distributes the error more equally over all bits; (2) dynamic quantization, which quantizes both small and large values with high precision; and (3) a stable embedding layer to improve stability during optimization for models with word embeddings.\nWith these components, performing an optimizer update with 8-bit states is straightforward. We dequantize the 8-bit optimizer states to 32-bit, perform the update, and then quantize the states back to 8-bit for storage. We do this 8-bit to 32-bit conversion element-by-element in registers, which means no slow copies to GPU memory or additional temporary memory are needed to perform quantization and dequantization. For GPUs, this makes 8-bit optimizers faster than regular 32-bit optimizers, as we show in Section 3.\n\n2.1 BLOCK-WISE QUANTIZATION\nOur block-wise quantization reduces the cost of computing normalization and improves quantization precision by isolating outliers. In order to dynamically quantize a tensor, as defined in Section 1.2, we need to normalize the tensor into the range [-1, 1]. Such normalization requires a reduction over the entire tensor, which entails multiple synchronizations across GPU cores. Block-wise dynamic quantization reduces this cost by chunking an input tensor into small blocks of size B = 2048 and performing normalization independently in each core across this block.\nMore formally, using the notation introduced in Section 1.2, in block-wise quantization, we treat T as a one-dimensional sequence of elements that we chunk in blocks of size B. This means for an input tensor T with n elements we have n/B blocks. We proceed to compute a normalization constant for each block:N b = max(|T b |)\n, where b is the index of the block 0..n/B. With this block-wise normalization constant, each block can be quantized independently:T Q bi = 2 n arg min j=0 |Q map j \u2212 T bi N b | 0<i<B\nThis approach has several advantages, both for stability and efficiency. First, each block normalization can be computed independently. Thus no synchronization between cores is required, and throughput is enhanced.\nSecondly, it is also much more robust to outliers in the input tensor. For example, to contrast blockwise and regular quantization, if we create an input tensor with one million elements sampled from the standard normal distribution, we expect less than 1% of elements of the tensor will be in the range [3, +\u221e). However, since we normalize the input tensor into the range [-1,1] this means the maximum values of the distribution determine the range of quantization buckets. This means if the input tensor contains an outlier with magnitude 5, the quantization buckets reserved for numbers between 3 and 5 will mostly go unused since less than 1% of numbers are in this range. With blockwise quantization, the effect of outliers is limited to a single block. As such, most bits are used effectively in other blocks.\nFurthermore, because outliers represent the absolute maximum value in the input tensor, blockwise quantization approximates outlier values without any error. This guarantees that the largest optimizer states, arguably the most important, will always be quantized with full precision. This property makes block-wise dynamic quantization both robust and precise and is essential for good training performance in practice.\n\n2.2 DYNAMIC QUANTIZATION\nIn this work, we extend dynamic tree quantization (Section 1.3) for non-signed input tensors by re-purposing the sign bit. Since the second Adam state is strictly positive, the sign bit is not needed. Instead of just removing the sign bit, we opt to extend dynamic tree quantization with a fixed bit for the fraction. This extension is motivated by the observation that the second Adam state varies around 3-5 orders of magnitude during the training of a language model. In comparison, dynamic tree quantization already has a range of 7 orders of magnitude. We refer to this quantization as dynamic quantization to distinguish it from dynamic tree quantization in our experiments. A study of additional quantization data types and their performance is detailed in Appendix F.\n\n2.3 STABLE EMBEDDING LAYER\nOur stable embedding layer is a standard word embedding layer variation (Devlin et al., 2019) designed to ensure stable training for NLP tasks. This embedding layer supports more aggressive quantization by normalizing the highly non-uniform distribution of inputs to avoid extreme gradient variation. See Appendix C for a discussion of why commonly adopted embedding layers (Ott et al., 2019) are so unstable.\nWe initialize the Stable Embedding Layer with Xavier uniform initialization (Glorot and Bengio, 2010) and apply layer normalization (Ba et al., 2016) Experimental Setup We compare the performance of 8-bit optimizers to their 32-bit counterparts on a range of challenging public benchmarks. These benchmarks either use Adam (Kingma and Ba, 2014), AdamW (Loshchilov and Hutter, 2018), or Momentum (Qian, 1999).\nWe do not change any hyperparameters or precision of weights, gradients, and activations/input gradients for each experimental setting compared to the public baseline-the only change is to replace 32-bit optimizers with 8-bit optimizers. This means that for most experiments, we train in 16-bit mixed-precision (Micikevicius et al., 2017). We also compare with Adafactor (Shazeer and Stern, 2018), with the time-independent formulation for \u03b2 2 (Shazeer and Stern, 2018) -which is the same formulation used in Adam. We also do not change any hyperparameters for Adafactor.\nWe report on benchmarks in neural machine translation (Ott et al., 2018) 2 trained on WMT'16 (Sennrich et al., 2016) and evaluated on en-de WMT'14 (Mach\u00e1\u010dek and Bojar, 2014), large-scale language modeling (Lewis et al., 2021; Brown et al., 2020) and RoBERTa pretraining (Liu et al., 2019) on English CC-100 + RoBERTa corpus (Nagel, 2016; Gokaslan and Cohen, 2019; Zhu et al., 2015; Wenzek et al., 2020), finetuning the pretrained masked language model RoBERTa (Liu et al., 2019) 3 on GLUE (Wang et al., 2018a), ResNet-50 v1.5 image classification (He et al., 2016) 4 on ImageNet-1k (Deng et al., 2009), and Moco v2 contrastive image pretraining and linear finetuning (Chen et al., 2020b) 5 on ImageNet-1k (Deng et al., 2009).\nWe use the stable embedding layer for all NLP tasks except for finetuning on GLUE. Beyond this, we follow the exact experimental setup outlined in the referenced papers and codebases. We consistently report replication results for each benchmark with public codebases and report median accuracy, perplexity, or BLEU over ten random seeds for GLUE, three random seeds for others tasks, and a single random seed for large scale language modeling. While it is standard to report means and standard errors on some tasks, others use median performance. We opted to report medians for all tasks for consistency.\n\nResults\nIn Table 1, we see that 8-bit optimizers match replicated 32-bit performance for all tasks. While Adafactor is competitive with 8-bit Adam, 8-bit Adam uses less memory and provides faster optimization. Our 8-bit optimizers save up to 8.5 GB of GPU memory for our largest 1.5B pa-Table 1 : Median performance on diverse NLP and computer vision tasks: GLUE, object classification with (Moco v2) and without pretraining (CLS), machine translation (MT), and large-scale language modeling (LM). While 32-bit Adafactor is competitive with 8-bit Adam, it uses almost twice as much memory and trains slower. 8-bit Optimizers match or exceed replicated 32-bit performance on all tasks. We observe no instabilities for 8-bit optimizers. Time is total GPU time on V100 GPUs, except for RoBERTa and GPT3 pretraining, which were done on A100 GPUs. rameter language model and 2.0 GB for RoBERTa. Thus, 8-bit optimizers maintain performance and improve accessibility to the finetuning of large models for those that cannot afford GPUs with large memory buffers. We show models that are now accessible with smaller GPUs in Table 2. A breakdown of individual dataset results on GLUE can be found in Appendix B).\nThe broad range of tasks and competitive results demonstrate that 8-bit optimizers are a robust and effective replacement for 32-bit optimizers, do not require any additional changes in hyperparameters, and save a significant amount of memory while speeding up training slightly.\n\n4 ANALYSIS\nWe analyze our method in two ways. First, we ablate all 8-bit optimizer components and show that they are necessary for good performance. Second, we look at the sensitivity to hyperparameters compared to 32-bit Adam and show that 8-bit Adam with block-wise dynamic quantization is a reliable replacement that does not require further hyperparameter tuning.\nExperimental Setup We perform our analysis on a strong 32-bit Adam baseline for language modeling with transformers (Vaswani et al., 2017). We subsample from the RoBERTa corpus (Liu et al., 2019) which consists of the English sub-datasets: Books (Zhu et al., 2015), Stories (Trinh and Le, 2018), OpenWebText-1 (Gokaslan and Cohen, 2019), Wikipedia, and CC-News (Nagel, 2016). We use a 50k token BPE encoded vocabulary (Sennrich et al., 2015). We find the best 2-GPU-day transformer baseline for 32-bit Adam with multiple hyperparameter searches that take in a total of 440 GPU days. Key hyperparameters include 10 layers with a model dimension of 1024, a fully connected hidden dimension of 8192, 16 heads, and input sub-sequences with a length of 512 tokens each. The final model has 209m parameters. Ablation Analysis For the ablation analysis, we compare small and large-scale language modeling perplexity and training stability against a 32-bit Adam baseline. We ablate components individually and include combinations of methods that highlight their interactions. The baseline method uses linear quantization, and we add dynamic quantization, block-wise quantization, and the stable embedding layer to demonstrate their effect. To test optimization stability for small-scale language modeling, we run each setting with different hyperparameters and report median performance across all successful runs. A successful run is a run that does not crash due to exploding gradients or diverges in the loss. We use the hyperparameters {1e-8, 1e-7, 1e-6}, \u03b2 1 {0.90, 0.87, 0.93}, \u03b2 2 {0.999, 0.99, 0.98} and small changes in learning rates. We also include some partial ablations for large-scale models beyond 1B parameters. In the large-scale setting, we run several seeds with the same hyperparameters. We use a single seed for 32-bit Adam, five seeds for 8-bit Adam at 1.3B parameters, and a single seed for 8-bit Adam at 1.5B parameters. 6 Results are shown in Table 3.\nThe Ablations show that dynamic quantization, block-wise quantization, and the stable embedding layer are critical for either performance or stability. In addition, block-wise quantization is critical for large-scale language model stability.\n\nSensitivity Analysis\nWe compare the perplexity of 32-bit Adam vs 8-bit Adam + Stable Embedding as we change the optimizer hyperparameters: learning rate, betas, and . We change each hyperparameter individually from the baseline hyperparameters \u03b2 1 =0.9, \u03b2 2 =0.995, =1e-7, and lr=0.0163 and run two random seeds for both 8-bit and 32-bit Adam for each setting. If 8-bit Adam is perfectly insensitive to hyperparameters compared to 32-bit Adam, we would expect the same constant offset in performance for any hyperparameter combination. The results can be seen in Figure 3. The results show a relatively steady gap between 8-bit and 32-bit Adam, suggesting that 8-bit Adam does not require any further hyperparameter tuning compared to 32-bit Adam.\nFigure 3 : Sensitivity analysis of 8-bit vs 32-bit Adam hyperparameters. We can see that there is little variance between 8 and 32-bit performance, which suggests that 8-bit Adam can be used as a drop-in replacement for 32-bit Adam without any further hyperparameter tuning.\n\n5 RELATED WORK\nCompressing & Distributing Optimizer States While 16-bit Adam has been used in several publications, the stability of 16-bit Adam was first explicitly studied for a text-to-image generation model DALL-E (Ramesh et al., 2021). They show that a stable embedding layer, tensor-wise scaling constants for both Adam states, and multiple loss scaling blocks are critical to achieving stability during training. Our work reduces the memory footprint of Adam further, from 16 to 8-bit. In addition, we achieve stability by developing new training procedures and non-linear quantization, both of which complement previous developments.\nAdafactor (Shazeer and Stern, 2018) uses a different strategy to save memory. All optimizer states are still 32-bit, but the second Adam state is factorized by a row-column outer product resulting in a comparable memory footprint to 16-bit Adam. Alternatively, Adafactor can also be used without using the first moment (\u03b2 1 = 0.0) (Shazeer and Stern, 2018). This version is as memory efficient as 8-bit Adam, but unlike 8-bit Adam, hyperparameters for this Adafactor variant need to be re-tuned to achieve good performance. We compare 8-bit Adam with Adafactor \u03b2 1 > 0.0 in our experiments.\nAdaGrad (Duchi et al., 2011) adapts the gradient with aggregate training statistics over the entire training run. AdaGrad that uses only the main diagonal as optimizer state and extensions of AdaGrad such as SM3 (Anil et al., 2019) and extreme tensoring (Chen et al., 2020a) can be more efficient than 8-bit Adam. We include some initial comparison with AdaGrad in Appendix H.\nOptimizer sharding (Rajbhandari et al., 2020) splits optimizer states across multiple accelerators such as GPUs/TPUs. While very effective, it can only be used if multiple accelerators are available and data parallelism is used. Optimizer sharding can also have significant communication overhead (Rajbhandari et al., 2021). Our 8-bit optimizers work with all kinds of parallelism. They can also complement optimizer sharding, as they reduce communication overhead by 75%.\nGeneral Memory Reduction Techniques Other complementary methods for efficient training can be either distributed or local. Distributed approaches spread out the memory of a model across several accelerators such as GPUs/TPUs. Such approaches are model parallelism (Krizhevsky et al., 2009), pipeline parallelism (Krizhevsky et al., 2009; Huang et al., 2018; Harlap et al., 2018), and operator parallelism (Lepikhin et al., 2020). These approaches are useful if one has multiple accelerators available. Our 8-bit optimizers are useful for both single and multiple devices.\nLocal approaches work for a single accelerator. They include gradient checkpointing (Chen et al., 2016), reversible residual connections (Gomez et al., 2017), and offloading (Pudipeddi et al., 2020; Rajbhandari et al., 2021). All these methods save memory at the cost of increased computational or communication costs. Our 8-bit optimizers reduce the memory footprint of the model while maintaining 32-bit training speed.\nQuantization Methods and Data Types While our work is the first to apply 8-bit quantization to optimizer statistics, quantization for neural network model compression, training, and inference are well-studied problems. One of the most common formats of 8-bit quantization is to use data types composed of static sign, exponent, and fraction bits. The most common combination is 5 bits for the exponent and 2 bits for the fraction (Wang et al., 2018b; Sun et al., 2019; Cambier et al., 2020; Mellempudi et al., 2019) with either no normalization or min-max normalization. These data types offer high precision for small magnitude values but have large errors for large magnitude values since only 2 bits are assigned to the fraction. Other methods improve quantization through soft constraints (Li et al., 2021) or more general uniform affine quantizations (Pappalardo, 2021).\nData types lower than 8-bit are usually used to prepare a model for deployment, and the main focus is on improving network inference speed and memory footprint rather than maintaining accuracy.\nThere are methods that use 1-bit (Courbariaux and Bengio, 2016; Rastegari et al., 2016; Courbariaux et al., 2015), 2-bit/3 values (Zhu et al., 2017; Choi et al., 2019), 4-bits (Li et al., 2019), more bits (Courbariaux et al., 2014), or a variable amount of bits (Gong et al., 2019). See also Qin et al. (2020) for a survey on binary neural networks. While these low-bit quantization techniques allow for efficient storage, they likely lead to instability when used for optimizer states.\nThe work most similar to our block-wise quantization is work on Hybrid Block Floating Point (HBFP) (Drumond et al., 2018) which uses a 24-bit fraction data type with a separate exponent for each tile in matrix multiplication to perform 24-bit matrix multiplication. However, unlike HBFP, block-wise dynamic quantization has the advantage of having both block-wise normalization and a dynamic exponent for each number. This allows for a much broader range of important values since optimizer state values vary by about 5 orders of magnitude. Furthermore, unlike HBFP, block-wise quantization approximates the maximum magnitude values within each block without any quantization error, which is critical for optimization stability, particularly for large networks.\n\n6 DISCUSSION & LIMITATIONS\nHere we have shown that high precision quantization can yield 8-bit optimizers that maintain 32-bit optimizer performance without requiring any change in hyperparameters. One of the main limitations of our work is that 8-bit optimizers for natural language tasks require a stable embedding layer to be trained to 32-bit performance. On the other hand, we show that 32-bit optimizers also benefit from a stable embedding layer. As such, the stable embedding layer could be seen as a general replacement for other embedding layers.\nWe show that 8-bit optimizers reduce the memory footprint and accelerate optimization on a wide range of tasks. However, since 8-bit optimizers reduce only the memory footprint proportional to the number of parameters, models that use large amounts of activation memory, such as convolutional networks, have few benefits from using 8-bit optimizers. Thus, 8-bit optimizers are most beneficial for training or finetuning models with many parameters on highly memory-constrained GPUs.\nFurthermore, there remain sources of instability that, to our knowledge, are not well understood. For example, we observed that models with over 1B parameters often have hard systemic divergence, where many parameters simultaneously cause exploding gradients. In other cases, a single parameter among those 1B parameters assumed a value too large, caused an exploding gradient, and led to a cascade of instability. It might be that this rare cascading instability is related to the phenomena where instability disappears after reloading a model checkpoint and rolling a new random seeda method standard for training huge models. Cascading instability might also be related to the observation that the larger a model is, the more unstable it becomes. For 8-bit optimizers, handling outliers through block-wise quantization and the stable embedding layer was key for stability. We hypothesize that that extreme outliers are related to cascading instability. If such phenomena were better understood, it could lead to better 8-bit optimizers and stable training in general.\n\nA BROADER IMPACT\nOur 8-bit optimizers enable training models that previously could not be trained on various GPUs, as shown in Table 2. Furthermore, while many options exist to reduce the memory footprint via parallelism (Rajbhandari et al., 2020; Lepikhin et al., 2020) our 8-bit optimizers are one of the few options that can reduce the optimizer memory footprint significantly for single devices without degrading performance. Therefore, it is likely that our 8-bit optimizers will improve access to larger models -especially for the users that have the least resources.\n\nB GLUE SCORE BREAKDOWN\n\n\nC STABILITY OF EMBEDDING LAYERS\nHighly variable gradients can lead to unpredictable optimization behavior and instability that manifests as divergence or exploding gradients. Low precision optimziers can amplify variance of gradient updates due to the noise introduced during quantization. While our 8-bit optimizers appear to be stable for convolutional networks, similar to Ramesh et al. (2021), we find that word embedding layers are a major source of instability.\nThe main instability from the word embedding layer comes from the fact that it is a sparse layer with non-uniform distribution of inputs which can produce maximum gradient magnitudes 100x larger than other layers. For dense layers, if given n samples arranged into k mini-batches the sum of gradients of all mini-batches is always the same independent of how the n samples are arranged into k mini-batches. For embedding gradients, this depends on the arrangement of samples into mini-batches. This is because most deep learning frameworks normalize the gradient by the number of total tokens in the mini-batch, rather than the frequency of each individual token. This approximation allows stable learning with a single learning rate rather than variable learning rates that depend on token frequency in each individual mini-batch. However a side-effect of this method is that the magnitude of gradients for a particular token can vary widely with batch sizes and between different mini-batches.\nThere are multiple recipes for initialization word embedding layers. One of the most common recipes used in all models trained with fairseq (Ott et al., 2019) such as RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), large NMT models (Ott et al., 2018), and sparse expert models (Lewis et al., 2021), is the following: Initialize the word embedding layer withN (0, 1/ \u221a k)\nwhere k is the embedding size of the embedding layer and to scale the outputs by \u221a k. This scheme has a variance of one at the start of training for the output distribution to ensure good gradient flow.\nWe find this approach to induce some instability for 8-bit optimizers. We develop the stable embedding layer to solve this instability problem.\nWhile the full recipe for our stable embedding layer is new, components of it has been used before. The layer norm after the embedding has been used before in work such as Devlin et al. (2019) and Radford et al. (2019) and enhanced precision for this particular layer was used in Ramesh et al. (2021). As pointed out above, these elements are not standard and the stable embedding layer combines three aspects that are all important: (1) enhanced precision, (2) layer norm, and (3) Xavier initialization.\n\nD QUANTIZATION ERROR ANALYSIS\nTo gain more insights into why block-wise dynamic quantization works so well and how it could be improved, we performed a quantization error analysis of Adam quantization errors during language model training. Adam quantization errors are the deviations between the quantized 8-bit Adam update and the 32-bit Adam updates: |u 8 \u2212 u 16 |, where u k = s k 1 /s k 2 for k bits. See Background Section 1.1 for details on Adam.\nA good 8-bit quantization has the property that, for a given input distribution, the inputs are only rarely quantized into intervals with high quantization error and most often quantized into intervals with low error.\nIn 8-bit, there are 255\u00d7256 possible 8-bit Adam updates, 256 possible values for the first and 256 for the second Adam state. We look at the average quantization error of each of these possible updates to see where the largest errors are and we plot histograms to see how often do these values with error occur. Taken together, these two perspectives give a detailed view of the magnitude of deviations and how often large deviations occur.\nWe study these questions by looking at how often each of the 256 values for both Adam states are used during language model training. We also analyze the average error for each of the inputs quantized to each of the 256 values. With this analysis it is easy to find regions of high use and high error, and visualize their overlap. An overlap of these regions is associated with large frequent errors that cause unstable training. The quantization error analysis is shown in Figure 4.\nThe plots show two things: (1) The region of high usage (histogram) shows how often each combination of 256\u00d7256 bit values is used for the first Adam state s 1 (exponentially smoothed running sum) and the second Adam state s 2 (exponentially smoothed running squared sum). ( 2 We can see that block-wise dynamic quantization has the smallest overlap between regions of high use and high error. While the absolute Adam quantization error of block-wise dynamic quantization is 0.0061, which is not much lower than that of dynamic quantization with 0.0067, the plots can also be interpreted as block-wise dynamic having rarer large errors that likely contribute to improved stability during optimization.\n\nE FINE-GRAINED OPTIMIZER RUNTIME PERFORMANCE\nTable 5 shows optimizer performance that is benchmarked in isolation without any training. We use a large sample of a normal distribution and benchmark the average time to perform 100 optimizer updates per billion parameters in milliseconds. The plot shows that for linear quantization regions of high usage and high error overlap. For dynamic quantization regions with high relative error are used infrequently while only small regions have high usage and high absolute error. Block-wise dynamic quantization spreads out the usage over a large space and has the lowest overlap between regions of high use and errors. This means that not only is the overall error of block-wise dynamic quantization lower, but also that large errors for individual parameter updates are rarer compared to other methods, thus improving stability. See the main text for more details.\nPublished as a conference paper at ICLR 2022 quantization twice as low as dynamic quantization for any normal distribution it has sporadic large errors that lead to large Adam errors and poor model performance (see Figure 5) and even with state-of-the-art quantile estimation algorithms (see Section G) quantile quantization is too slow to be practical. An overview of quantization performance of this additional quantization data types compared to dynamic quantization (without block-wise quantization) can be found in Table 6. We normalize the values into the range [-1,1]. With this, -1 indicates the largest negative value, 0 the value that is closest to 0, and so forth. See Figure 6 for a visualization of this normalization. Quantile quantization has large errors for large values, while dynamic quantization has small errors for both small and large values while the bulk of the errors is concentrated in intermediate values. we expect that small values in the second state r t to produce large Adam updates. To get a better quantization error for small values we can switch the dynamic exponent and the base exponent. For regular dynamic quantization the base exponent is 10 0 = 1 and each zero bit decreases the exponent by a factor of 10 for a minimum value of 10 \u22127 . We invert this starting with base 10 \u22127 and each zero bit increases the exponent by 10 for a maximum value of 1. We denote this quantization as inverse dynamic quantization.\n\nF.2 QUANTILE QUANTIZATION: A LOSSY MINIMUM ENTROPY ENCODING\nA lossy minimum entropy encoding with k bits has the property that for any input data, the quantized outputs take the value of each of the 2 k different bit representations equally often.\nMore formally, a lossy minimum entropy encoding can be described in the following way. Given an infinite stream of sampled real numbers x i where x i is distributed as X, an arbitrary probability distribution, a lossy minimum entropy encoding is given by the k-bit quantization map Q map \u2208 R 2 k which maps values q \u2208 R 2 k to indices 0, 1, . . . 2 k which has the property that if any number of elements x i from the stream are quantized to x q i we do not gain any information which is predictive of future x q j>i . One way to fulfill this property for arbitrary probability distributions X, is to divide the probability distribution function f X into 2 k bins where each bin has equal area and the mid-points of these bins are values q of the quantization map Q map . Empirically, this is equivalent to a histogram with 2 k bins where each bin contains equal number of values.\nHow do we find the mid-points for each histogram bin? This is equivalent to finding the 2 k nonoverlapping values x for the cumulative distribution function F X with equal probability mass. These values can most easily be found by using its inverse function, the quantile function Q X = F \u22121 X . We can find the mid-points of each of the histogram bins by using the mid-points between 2 k +1 equally spaced quantiles over the range of probabilities [0, 1]:q i = Q X i 2 k +1 + Q X i+1 2 k +1 2 ,\nTo find q empirically, we can estimate sample quantiles for a tensor T with unknown distribution X by finding the 2 k equally spaced sample quantiles via T's empirical cumulative distribution function.\nWe refer to this quantization as quantile quantization.\nTo estimate sample quantiles efficiently, we devise a specialized approximate quantile estimation algorithm, SRAM-Quantiles, which is more than 75x faster than other approximate quantile estimation approaches (Govindaraju et al., 2005; Dunning and Ertl, 2019). SRAM-Quantiles uses a divide-and-conquer strategy to perform sorting solely in fast SRAM. More details on this algorithm can be found in the Appendix Section G.\n\nG SRAM-QUANTILES: A FAST QUANTILE ESTIMATION ALGORITHM\nTo estimate sample quantiles of a tensor one needs to determine the empirical cumulative distribution function (eCDF) of that tensor. The easiest way to find the eCDF is to sort a given tensor. Once sorted, the quantiles can be found by using the value at index i = q \u00d7 n where i is the index into the sorted array, q is the desired quantile and n is the total elements in the tensor. While simple, this process of estimating quantiles is computationally expensive and would render training with quantile quantization too slow to be useful.\nSimilar to other quantile estimation approaches, our GPU algorithm, SRAM-Quantiles, uses a sliding windows over the data for fast, approximate quantile estimation with minimal resources. Greenwald and Khanna (2001)'s quantile estimation algorithm uses dynamic bin histograms over sliding windows to estimate quantiles. Extensions of this algorithm accelerate estimation by using more efficient data structures and estimation algorithms (Dunning and Ertl, 2019) or by using GPUs (Govindaraju et al., 2005). The main difference between this work an ours is that we only compute a limit set of quantiles that are known a priori -256, to be exact -while previous work focuses on general statistics which help to produce any quantile a posteriori. Thus we can devise a highly specialized algorithm which offers faster estimation.\nThe idea behind our algorithm comes from the fact that sorting is slow because it involves repeated loads and stores from main memory (DRAM) when executing divide-and-conquer sorting algorithms. We can significantly improve performance of quantile estimation if we restructure quantile estimation to respect memory hierarchies of the device on which the algorithm is executed.\nOn a GPU, programmable SRAM -known as shared memory -is 15x faster than DRAM but has a limit size of around 64 kb per core. The SRAM-Quantiles algorithm is simple. Instead of finding the full eCDF we find the eCDF for a subset of values of the tensor that fits into SRAM (about 4096 32-bit values). Once we found the quantiles for each subset, we average the quantiles atomically in DRAM.\nThis algorithm works, because the arithmetic mean is an unbiased estimator for the population mean and samples quantiles estimated via eCDFs are asymptotically unbiased estimators of the population quantile (Chen and Kelton, 2001). Thus the more subset quantiles we average, the better the estimate of the tensor-wide quantiles.\nFor estimating 256 quantiles on a large stream of numbers, our algorithm takes on average 0.064 ns to process one element in the stream, whereas the fastest general algorithms take 300 ns (Govindaraju et al., 2005) and 5 ns (Dunning and Ertl, 2019).\n\nH ADAGRAD COMPARISONS\nWhile the main aim in this work is to investigate how the most commonly used optimizers, such as Adam (Kingma and Ba, 2014) and Momentum (Qian, 1999), can be used as 8-bit variants without any further hyperparameter tuning, it can be of interest to consider the behavior of our 8-bit methods under different scenarios. For example, one difference between Adam/Momentum and AdaGrad (Duchi et al., 2011) is that AdaGrad accumulates gradients statistics over the entire course of training while Adam/Momentum use a smoothed exponential decay over time. As such, this could lead to very different 8-bit quantization behavior where there are large difference between the magnitude of different optimizer states. Such large differences could induce a large quantization error and degrade performance of 8-bit optimizers.\nTo investigate this, we train small 209M parameter language models on the RoBERTa corpus (Liu et al., 2019). We use the AdaGrad hyperparameters introduced by Keskar et al. (2019). Results are shown in Table 7. From the results we can see that our 8-bit methods do not work as well for Ada-Grad. One hypothesis is that this is due to the the wide range of gradient statistics of AdaGrad which comes from averaging the gradient over the entire course of training. To prevent poor quantization in such scenarios, stochastic rounding proved to be very effective from our initial experiments with other 8-bit optimizer. While we abandoned stochastic rounding because we did not see any benefits for Adam and Momentum, it could be an effective solution for AdaGrad. We leave such improved 8-bit quantization methods for AdaGrad to future work.\nWhile AdaGrad falls short in this experiments in terms of perplexity compared to Adam, AdaGrad's performance might be improved by adding a momentum term. We leave such improvements for future work.  8. We can see that both Xavier initialization and the layer norm improves performance. While we can see performance difference in this setup, the models are too small to study instabilities that usually occur only at larger scales. As such, it is as expected that 32-bit optimizer states for the embedding layer makes no difference in either perplexity or stability.\nThe best setup to test the stable embedding layer's effect on instabilities at large scale is to train large models and record instabilities. However, since a single model with more than 1B parameters takes roughly 300 GPU days to run, and multiple random seeds are need to study instability, an ablation study of that scale is out of our computational budget. As such, we are unable to study the stabilizing effects of the Stable Embedding layer beyond showing that it affects perplexity at the small scale.\n\nFootnotes:\n1: We study 8-bit optimization with current best practice model and gradient representations (typically 16-bit mixed precision), to isolate optimization challenges. Future work could explore further compressing all three.\n2: https://github.com/pytorch/fairseq/tiny/master/examples/scaling_nmt/README.md\n3: https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md\n4: https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/\n5: https://github.com/facebookresearch/moco\n6: We chose not to do the full ablations with such large models because each training run takes one GPU year.\n\nReferences:\n\n- Anil, R., Gupta, V., Koren, T., and Singer, Y. (2019). Memory efficient adaptive optimization. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9746-9755.- Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n\n- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n\n- Cambier, L., Bhiwandiwalla, A., Gong, T., Elibol, O. H., Nekuii, M., and Tang, H. (2020). Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\n- Chen, E. J. and Kelton, W. D. (2001). Quantile and histogram estimation. In Proceeding of the 2001 Winter Simulation Conference (Cat. No. 01CH37304), volume 1, pages 451-459. IEEE.\n\n- Chen, T., Xu, B., Zhang, C., and Guestrin, C. (2016). Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\n\n- Chen, X., Agarwal, N., Hazan, E., Zhang, C., and Zhang, Y. (2020a). Extreme tensoring for low- memory preconditioning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\n- Chen, X., Fan, H., Girshick, R., and He, K. (2020b). Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297.\n\n- Choi, J., Venkataramani, S., Srinivasan, V., Gopalakrishnan, K., Wang, Z., and Chuang, P. (2019). Accurate and efficient 2-bit quantized neural networks. In Talwalkar, A., Smith, V., and Zaharia, M., editors, Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 -April 2, 2019. mlsys.org.\n\n- Courbariaux, M. and Bengio, Y. (2016). Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR, abs/1602.02830.\n\n- Courbariaux, M., Bengio, Y., and David, J. (2015). Binaryconnect: Training deep neural networks with binary weights during propagations. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3123-3131.\n\n- Courbariaux, M., Bengio, Y., and David, J.-P. (2014). Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024.\n\n- Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee.\n\n- Dettmers, T. (2016). 8-bit approximations for parallelism in deep learning. International Conference on Learning Representations (ICLR).\n\n- Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.\n\n- Drumond, M., Lin, T., Jaggi, M., and Falsafi, B. (2018). Training dnns with hybrid block float- ing point. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 451-461.\n\n- Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7).\n\n- Dunning, T. and Ertl, O. (2019). Computing extremely accurate quantiles using t-digests. arXiv preprint arXiv:1902.04023.\n\n- Fedus, W., Zoph, B., and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961.\n\n- Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings.\n\n- Gokaslan, A. and Cohen, V. (2019). Openwebtext corpus.\n\n- Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. (2017). The reversible residual network: Backpropagation without storing activations. arXiv preprint arXiv:1707.04585.\n\n- Gong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., and Yan, J. (2019). Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In 2019 IEEE/CVF Interna- tional Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 4851-4860. IEEE.\n\n- Govindaraju, N. K., Raghuvanshi, N., and Manocha, D. (2005). Fast and approximate stream mining of quantiles and frequencies using graphics processors. In Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 611-622.\n\n- Greenwald, M. and Khanna, S. (2001). Space-efficient online computation of quantile summaries. ACM SIGMOD Record, 30(2):58-66.\n\n- Harlap, A., Narayanan, D., Phanishayee, A., Seshadri, V., Devanur, N., Ganger, G., and Gib- bons, P. (2018). Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377.\n\n- He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.\n\n- Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. (2020). Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701.\n\n- Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. (2018). Gpipe: Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965.\n\n- Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n\n- Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. (2019). CTRL: A conditional transformer language model for controllable generation. CoRR, abs/1909.05858.\n\n- Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\n- Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.\n\n- Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. (2020). Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.\n\n- Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. (2021). Base layers: Simplify- ing training of large, sparse models. arXiv preprint arXiv:2103.16716.\n\n- Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettle- moyer, L. (2020). BART: denoising sequence-to-sequence pre-training for natural language gen- eration, translation, and comprehension. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics.\n\n- Li, J. B., Qu, S., Li, X., Strubell, E., and Metze, F. (2021). End-to-end quantized training via log-barrier extensions.\n\n- Li, R., Wang, Y., Liang, F., Qin, H., Yan, J., and Fan, R. (2019). Fully quantized network for object detection. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 2810-2819. Computer Vision Foundation / IEEE.\n\n- Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\n- Loshchilov, I. and Hutter, F. (2018). Fixing weight decay regularization in adam.\n\n- Mach\u00e1\u010dek, M. and Bojar, O. (2014). Results of the wmt14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293-301.\n\n- Mellempudi, N., Srinivasan, S., Das, D., and Kaul, B. (2019). Mixed precision training with 8-bit floating point. CoRR, abs/1905.12334.\n\n- Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Hous- ton, M., Kuchaiev, O., Venkatesh, G., et al. (2017). Mixed precision training. arXiv preprint arXiv:1710.03740.\n\n- Nagel, S. (2016). Cc-news.\n\n- Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. (2019). fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038.\n\n- Ott, M., Edunov, S., Grangier, D., and Auli, M. (2018). Scaling neural machine translation. arXiv preprint arXiv:1806.00187.\n\n- Pappalardo, A. (2021). Xilinx/brevitas.\n\n- Pudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S. (2020). Training large neural net- works with constant memory using a new execution algorithm. arXiv preprint arXiv:2002.05645.\n\n- Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks : the official journal of the International Neural Network Society, 12 1:145-151.\n\n- Qin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. (2020). Binary neural networks: A survey. CoRR, abs/2004.03333.\n\n- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\n- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\n\n- Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. (2020). Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE.\n\n- Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and He, Y. (2021). Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. arXiv preprint arXiv:2104.07857.\n\n- Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092.\n\n- Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. (2016). Xnor-net: Imagenet classification using binary convolutional neural networks. In Leibe, B., Matas, J., Sebe, N., and Welling, M., editors, Computer Vision -ECCV 2016 -14th European Conference, Amsterdam, The Nether- lands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages 525-542. Springer.\n\n- Sennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\n\n- Sennrich, R., Haddow, B., and Birch, A. (2016). Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891.\n\n- Shazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR.\n\n- Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron- lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.\n\n- Sun, X., Choi, J., Chen, C., Wang, N., Venkataramani, S., Srinivasan, V., Cui, X., Zhang, W., and Gopalakrishnan, K. (2019). Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 4901-4910.\n\n- Trinh, T. H. and Le, Q. V. (2018). A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.\n\n- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo- sukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n\n- Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018a). Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.\n\n- Wang, N., Choi, J., Brand, D., Chen, C., and Gopalakrishnan, K. (2018b). Training deep neural networks with 8-bit floating point numbers. In Bengio, S., Wallach, H. M., Larochelle, H., Grau- man, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 7686-7695.\n\n- Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm\u00e1n, F., Joulin, A., and Grave, E. (2020). CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceed- ings of the 12th Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France. European Language Resources Association.\n\n- Zhu, C., Han, S., Mao, H., and Dally, W. J. (2017). Trained ternary quantization. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\n\n- Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27.\n\n", "annotations": {"Abstract": [{"begin": 46, "end": 1790, "idx": 0}], "Head": [{"begin": 4831, "end": 4843, "n": "1", "idx": 0}, {"begin": 6032, "end": 6059, "n": "1.2", "idx": 1}, {"begin": 9209, "end": 9236, "n": "2.1", "idx": 2}, {"begin": 11766, "end": 11790, "n": "2.2", "idx": 3}, {"begin": 12568, "end": 12594, "n": "2.3", "idx": 4}, {"begin": 15319, "end": 15326, "idx": 5}, {"begin": 16803, "end": 16813, "n": "4", "idx": 6}, {"begin": 19386, "end": 19406, "idx": 7}, {"begin": 20410, "end": 20424, "n": "5", "idx": 8}, {"begin": 25807, "end": 25833, "n": "6", "idx": 9}, {"begin": 27919, "end": 27935, "idx": 10}, {"begin": 28494, "end": 28516, "idx": 11}, {"begin": 28519, "end": 28550, "idx": 12}, {"begin": 31214, "end": 31243, "idx": 13}, {"begin": 33513, "end": 33557, "idx": 14}, {"begin": 35878, "end": 35937, "idx": 15}, {"begin": 38184, "end": 38238, "idx": 16}, {"begin": 40951, "end": 40972, "idx": 17}], "ReferenceToBib": [{"begin": 5207, "end": 5228, "target": "#b31", "idx": 0}, {"begin": 5252, "end": 5264, "target": "#b48", "idx": 1}, {"begin": 6351, "end": 6368, "idx": 2}, {"begin": 7263, "end": 7278, "target": "#b13", "idx": 3}, {"begin": 10902, "end": 10908, "idx": 4}, {"begin": 12667, "end": 12688, "target": "#b14", "idx": 5}, {"begin": 12969, "end": 12987, "target": "#b44", "idx": 6}, {"begin": 13081, "end": 13106, "target": "#b19", "idx": 7}, {"begin": 13137, "end": 13154, "target": "#b1", "idx": 8}, {"begin": 13328, "end": 13349, "target": "#b31", "idx": 9}, {"begin": 13357, "end": 13386, "target": "#b39", "idx": 10}, {"begin": 13400, "end": 13412, "target": "#b48", "idx": 11}, {"begin": 13725, "end": 13752, "target": "#b42", "idx": 12}, {"begin": 13785, "end": 13810, "target": "#b58", "idx": 13}, {"begin": 13858, "end": 13883, "target": "#b58", "idx": 14}, {"begin": 14040, "end": 14058, "target": "#b45", "idx": 15}, {"begin": 14079, "end": 14102, "target": "#b57", "idx": 16}, {"begin": 14133, "end": 14159, "target": "#b40", "idx": 17}, {"begin": 14191, "end": 14211, "target": "#b34", "idx": 18}, {"begin": 14212, "end": 14231, "target": "#b2", "idx": 19}, {"begin": 14256, "end": 14274, "target": "#b38", "idx": 20}, {"begin": 14310, "end": 14323, "target": "#b43", "idx": 21}, {"begin": 14324, "end": 14349, "target": "#b20", "idx": 22}, {"begin": 14350, "end": 14367, "target": "#b67", "idx": 23}, {"begin": 14368, "end": 14388, "target": "#b65", "idx": 24}, {"begin": 14446, "end": 14464, "target": "#b38", "idx": 25}, {"begin": 14475, "end": 14495, "target": "#b63", "idx": 26}, {"begin": 14533, "end": 14550, "target": "#b26", "idx": 27}, {"begin": 14568, "end": 14587, "target": "#b12", "idx": 28}, {"begin": 14653, "end": 14673, "target": "#b7", "idx": 29}, {"begin": 14691, "end": 14710, "target": "#b12", "idx": 30}, {"begin": 17287, "end": 17309, "target": "#b62", "idx": 31}, {"begin": 17348, "end": 17366, "target": "#b38", "idx": 32}, {"begin": 17417, "end": 17435, "target": "#b67", "idx": 33}, {"begin": 17445, "end": 17465, "target": "#b61", "idx": 34}, {"begin": 17481, "end": 17507, "target": "#b20", "idx": 35}, {"begin": 17532, "end": 17545, "target": "#b43", "idx": 36}, {"begin": 17589, "end": 17612, "target": "#b56", "idx": 37}, {"begin": 20628, "end": 20649, "target": "#b54", "idx": 38}, {"begin": 21062, "end": 21087, "target": "#b58", "idx": 39}, {"begin": 21383, "end": 21408, "target": "#b58", "idx": 40}, {"begin": 21651, "end": 21671, "target": "#b16", "idx": 41}, {"begin": 21855, "end": 21874, "target": "#b0", "idx": 42}, {"begin": 21897, "end": 21916, "target": "#b6", "idx": 43}, {"begin": 22039, "end": 22065, "target": "#b52", "idx": 44}, {"begin": 22317, "end": 22343, "target": "#b53", "idx": 45}, {"begin": 22757, "end": 22782, "target": "#b32", "idx": 46}, {"begin": 22805, "end": 22830, "target": "#b32", "idx": 47}, {"begin": 22831, "end": 22850, "target": "#b28", "idx": 48}, {"begin": 22851, "end": 22871, "target": "#b25", "idx": 49}, {"begin": 22898, "end": 22921, "target": "#b33", "idx": 50}, {"begin": 23149, "end": 23168, "target": "#b5", "idx": 51}, {"begin": 23202, "end": 23222, "target": "#b21", "idx": 52}, {"begin": 23239, "end": 23263, "target": "#b47", "idx": 53}, {"begin": 23264, "end": 23289, "target": "#b53", "idx": 54}, {"begin": 23917, "end": 23937, "target": "#b64", "idx": 55}, {"begin": 23938, "end": 23955, "target": "#b60", "idx": 56}, {"begin": 23956, "end": 23977, "target": "#b3", "idx": 57}, {"begin": 23978, "end": 24002, "target": "#b41", "idx": 58}, {"begin": 24280, "end": 24297, "target": "#b36", "idx": 59}, {"begin": 24343, "end": 24361, "target": "#b46", "idx": 60}, {"begin": 24590, "end": 24620, "target": "#b9", "idx": 61}, {"begin": 24621, "end": 24644, "target": "#b55", "idx": 62}, {"begin": 24645, "end": 24670, "target": "#b10", "idx": 63}, {"begin": 24687, "end": 24705, "target": "#b66", "idx": 64}, {"begin": 24706, "end": 24724, "target": "#b8", "idx": 65}, {"begin": 24733, "end": 24750, "target": "#b37", "idx": 66}, {"begin": 24762, "end": 24788, "target": "#b11", "idx": 67}, {"begin": 24819, "end": 24838, "target": "#b22", "idx": 68}, {"begin": 24849, "end": 24866, "target": "#b49", "idx": 69}, {"begin": 25143, "end": 25165, "target": "#b15", "idx": 70}, {"begin": 28140, "end": 28166, "target": "#b52", "idx": 71}, {"begin": 28167, "end": 28189, "target": "#b33", "idx": 72}, {"begin": 28895, "end": 28915, "target": "#b54", "idx": 73}, {"begin": 30123, "end": 30141, "target": "#b44", "idx": 74}, {"begin": 30158, "end": 30176, "target": "#b38", "idx": 75}, {"begin": 30183, "end": 30203, "target": "#b35", "idx": 76}, {"begin": 30222, "end": 30240, "target": "#b45", "idx": 77}, {"begin": 30267, "end": 30287, "target": "#b34", "idx": 78}, {"begin": 30880, "end": 30900, "target": "#b14", "idx": 79}, {"begin": 30905, "end": 30926, "target": "#b50", "idx": 80}, {"begin": 30988, "end": 31008, "target": "#b54", "idx": 81}, {"begin": 34991, "end": 34997, "idx": 82}, {"begin": 37970, "end": 37996, "target": "#b23", "idx": 83}, {"begin": 37997, "end": 38020, "target": "#b17", "idx": 84}, {"begin": 39216, "end": 39240, "target": "#b17", "idx": 85}, {"begin": 39258, "end": 39284, "target": "#b23", "idx": 86}, {"begin": 40578, "end": 40601, "target": "#b4", "idx": 87}, {"begin": 40888, "end": 40914, "target": "#b23", "idx": 88}, {"begin": 40924, "end": 40948, "target": "#b17", "idx": 89}, {"begin": 41075, "end": 41096, "target": "#b31", "idx": 90}, {"begin": 41110, "end": 41122, "target": "#b48", "idx": 91}, {"begin": 41354, "end": 41374, "target": "#b16", "idx": 92}, {"begin": 41877, "end": 41895, "target": "#b38", "idx": 93}, {"begin": 41946, "end": 41966, "target": "#b30", "idx": 94}], "ReferenceToFootnote": [{"begin": 14059, "end": 14060, "target": "#foot_1", "idx": 0}, {"begin": 14465, "end": 14466, "target": "#foot_2", "idx": 1}, {"begin": 14551, "end": 14552, "target": "#foot_3", "idx": 2}, {"begin": 14674, "end": 14675, "target": "#foot_4", "idx": 3}, {"begin": 19110, "end": 19111, "target": "#foot_5", "idx": 4}], "SectionFootnote": [{"begin": 43702, "end": 44346, "idx": 0}], "ReferenceString": [{"begin": 44363, "end": 44758, "id": "b0", "idx": 0}, {"begin": 44760, "end": 44864, "id": "b1", "idx": 1}, {"begin": 44868, "end": 45080, "id": "b2", "idx": 2}, {"begin": 45084, "end": 45404, "id": "b3", "idx": 3}, {"begin": 45408, "end": 45588, "id": "b4", "idx": 4}, {"begin": 45592, "end": 45725, "id": "b5", "idx": 5}, {"begin": 45729, "end": 45977, "id": "b6", "idx": 6}, {"begin": 45981, "end": 46121, "id": "b7", "idx": 7}, {"begin": 46125, "end": 46450, "id": "b8", "idx": 8}, {"begin": 46454, "end": 46609, "id": "b9", "idx": 9}, {"begin": 46613, "end": 47016, "id": "b10", "idx": 10}, {"begin": 47020, "end": 47171, "id": "b11", "idx": 11}, {"begin": 47175, "end": 47393, "id": "b12", "idx": 12}, {"begin": 47397, "end": 47533, "id": "b13", "idx": 13}, {"begin": 47537, "end": 48024, "id": "b14", "idx": 14}, {"begin": 48028, "end": 48424, "id": "b15", "idx": 15}, {"begin": 48428, "end": 48595, "id": "b16", "idx": 16}, {"begin": 48599, "end": 48720, "id": "b17", "idx": 17}, {"begin": 48724, "end": 48895, "id": "b18", "idx": 18}, {"begin": 48899, "end": 49165, "id": "b19", "idx": 19}, {"begin": 49169, "end": 49223, "id": "b20", "idx": 20}, {"begin": 49227, "end": 49399, "id": "b21", "idx": 21}, {"begin": 49403, "end": 49722, "id": "b22", "idx": 22}, {"begin": 49726, "end": 49978, "id": "b23", "idx": 23}, {"begin": 49982, "end": 50108, "id": "b24", "idx": 24}, {"begin": 50112, "end": 50315, "id": "b25", "idx": 25}, {"begin": 50319, "end": 50509, "id": "b26", "idx": 26}, {"begin": 50513, "end": 50730, "id": "b27", "idx": 27}, {"begin": 50734, "end": 50966, "id": "b28", "idx": 28}, {"begin": 50970, "end": 51175, "id": "b29", "idx": 29}, {"begin": 51179, "end": 51354, "id": "b30", "idx": 30}, {"begin": 51358, "end": 51466, "id": "b31", "idx": 31}, {"begin": 51470, "end": 51567, "id": "b32", "idx": 32}, {"begin": 51571, "end": 51794, "id": "b33", "idx": 33}, {"begin": 51798, "end": 51968, "id": "b34", "idx": 34}, {"begin": 51972, "end": 52463, "id": "b35", "idx": 35}, {"begin": 52467, "end": 52587, "id": "b36", "idx": 36}, {"begin": 52591, "end": 52868, "id": "b37", "idx": 37}, {"begin": 52872, "end": 53082, "id": "b38", "idx": 38}, {"begin": 53086, "end": 53167, "id": "b39", "idx": 39}, {"begin": 53171, "end": 53335, "id": "b40", "idx": 40}, {"begin": 53339, "end": 53474, "id": "b41", "idx": 41}, {"begin": 53478, "end": 53685, "id": "b42", "idx": 42}, {"begin": 53689, "end": 53715, "id": "b43", "idx": 43}, {"begin": 53719, "end": 53907, "id": "b44", "idx": 44}, {"begin": 53911, "end": 54035, "id": "b45", "idx": 45}, {"begin": 54039, "end": 54078, "id": "b46", "idx": 46}, {"begin": 54082, "end": 54272, "id": "b47", "idx": 47}, {"begin": 54276, "end": 54452, "id": "b48", "idx": 48}, {"begin": 54456, "end": 54579, "id": "b49", "idx": 49}, {"begin": 54583, "end": 54736, "id": "b50", "idx": 50}, {"begin": 54740, "end": 54967, "id": "b51", "idx": 51}, {"begin": 54971, "end": 55218, "id": "b52", "idx": 52}, {"begin": 55222, "end": 55402, "id": "b53", "idx": 53}, {"begin": 55406, "end": 55576, "id": "b54", "idx": 54}, {"begin": 55580, "end": 55984, "id": "b55", "idx": 55}, {"begin": 55988, "end": 56129, "id": "b56", "idx": 56}, {"begin": 56133, "end": 56270, "id": "b57", "idx": 57}, {"begin": 56274, "end": 56442, "id": "b58", "idx": 58}, {"begin": 56446, "end": 56655, "id": "b59", "idx": 59}, {"begin": 56659, "end": 57168, "id": "b60", "idx": 60}, {"begin": 57172, "end": 57282, "id": "b61", "idx": 61}, {"begin": 57286, "end": 57464, "id": "b62", "idx": 62}, {"begin": 57468, "end": 57668, "id": "b63", "idx": 63}, {"begin": 57672, "end": 58103, "id": "b64", "idx": 64}, {"begin": 58107, "end": 58433, "id": "b65", "idx": 65}, {"begin": 58437, "end": 58673, "id": "b66", "idx": 66}, {"begin": 58677, "end": 58965, "id": "b67", "idx": 67}], "ReferenceToTable": [{"begin": 15336, "end": 15337, "idx": 0}, {"begin": 15612, "end": 15613, "idx": 1}, {"begin": 16440, "end": 16441, "target": "#tab_2", "idx": 2}, {"begin": 19139, "end": 19140, "target": "#tab_3", "idx": 3}, {"begin": 28052, "end": 28053, "target": "#tab_2", "idx": 4}, {"begin": 33564, "end": 33565, "target": "#tab_6", "idx": 5}, {"begin": 34949, "end": 34950, "target": "#tab_7", "idx": 6}, {"begin": 41995, "end": 41996, "target": "#tab_8", "idx": 7}, {"begin": 42825, "end": 42826, "target": "#tab_9", "idx": 8}], "Footnote": [{"begin": 43713, "end": 43934, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 43935, "end": 44015, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 44016, "end": 44097, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 44098, "end": 44192, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 44193, "end": 44236, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 44237, "end": 44346, "id": "foot_5", "n": "6", "idx": 5}], "ReferenceToFormula": [{"begin": 33085, "end": 33086, "idx": 0}], "Paragraph": [{"begin": 56, "end": 1790, "idx": 0}, {"begin": 1793, "end": 2384, "idx": 1}, {"begin": 2385, "end": 2887, "idx": 2}, {"begin": 2888, "end": 4049, "idx": 3}, {"begin": 4050, "end": 4146, "idx": 4}, {"begin": 4147, "end": 4829, "idx": 5}, {"begin": 4844, "end": 5378, "idx": 6}, {"begin": 5745, "end": 5836, "idx": 7}, {"begin": 5837, "end": 6030, "idx": 8}, {"begin": 6060, "end": 6227, "idx": 9}, {"begin": 6253, "end": 6401, "idx": 10}, {"begin": 6468, "end": 6520, "idx": 11}, {"begin": 6521, "end": 7000, "idx": 12}, {"begin": 7029, "end": 7183, "idx": 13}, {"begin": 7184, "end": 7644, "idx": 14}, {"begin": 7721, "end": 7800, "idx": 15}, {"begin": 7801, "end": 8321, "idx": 16}, {"begin": 8322, "end": 8687, "idx": 17}, {"begin": 8688, "end": 9207, "idx": 18}, {"begin": 9237, "end": 9803, "idx": 19}, {"begin": 9804, "end": 10112, "idx": 20}, {"begin": 10130, "end": 10261, "idx": 21}, {"begin": 10314, "end": 10528, "idx": 22}, {"begin": 10529, "end": 11344, "idx": 23}, {"begin": 11345, "end": 11764, "idx": 24}, {"begin": 11791, "end": 12566, "idx": 25}, {"begin": 12595, "end": 13004, "idx": 26}, {"begin": 13005, "end": 13413, "idx": 27}, {"begin": 13414, "end": 13985, "idx": 28}, {"begin": 13986, "end": 14711, "idx": 29}, {"begin": 14712, "end": 15317, "idx": 30}, {"begin": 15327, "end": 16521, "idx": 31}, {"begin": 16522, "end": 16801, "idx": 32}, {"begin": 16814, "end": 17170, "idx": 33}, {"begin": 17171, "end": 19141, "idx": 34}, {"begin": 19142, "end": 19384, "idx": 35}, {"begin": 19407, "end": 20133, "idx": 36}, {"begin": 20134, "end": 20408, "idx": 37}, {"begin": 20425, "end": 21051, "idx": 38}, {"begin": 21052, "end": 21642, "idx": 39}, {"begin": 21643, "end": 22019, "idx": 40}, {"begin": 22020, "end": 22492, "idx": 41}, {"begin": 22493, "end": 23064, "idx": 42}, {"begin": 23065, "end": 23486, "idx": 43}, {"begin": 23487, "end": 24362, "idx": 44}, {"begin": 24363, "end": 24556, "idx": 45}, {"begin": 24557, "end": 25043, "idx": 46}, {"begin": 25044, "end": 25805, "idx": 47}, {"begin": 25834, "end": 26363, "idx": 48}, {"begin": 26364, "end": 26846, "idx": 49}, {"begin": 26847, "end": 27917, "idx": 50}, {"begin": 27936, "end": 28492, "idx": 51}, {"begin": 28551, "end": 28986, "idx": 52}, {"begin": 28987, "end": 29982, "idx": 53}, {"begin": 29983, "end": 30347, "idx": 54}, {"begin": 30361, "end": 30563, "idx": 55}, {"begin": 30564, "end": 30707, "idx": 56}, {"begin": 30708, "end": 31212, "idx": 57}, {"begin": 31244, "end": 31666, "idx": 58}, {"begin": 31667, "end": 31884, "idx": 59}, {"begin": 31885, "end": 32325, "idx": 60}, {"begin": 32326, "end": 32809, "idx": 61}, {"begin": 32810, "end": 33511, "idx": 62}, {"begin": 33558, "end": 34422, "idx": 63}, {"begin": 34423, "end": 35876, "idx": 64}, {"begin": 35938, "end": 36125, "idx": 65}, {"begin": 36126, "end": 37006, "idx": 66}, {"begin": 37007, "end": 37463, "idx": 67}, {"begin": 37503, "end": 37704, "idx": 68}, {"begin": 37705, "end": 37760, "idx": 69}, {"begin": 37761, "end": 38182, "idx": 70}, {"begin": 38239, "end": 38779, "idx": 71}, {"begin": 38780, "end": 39604, "idx": 72}, {"begin": 39605, "end": 39981, "idx": 73}, {"begin": 39982, "end": 40370, "idx": 74}, {"begin": 40371, "end": 40699, "idx": 75}, {"begin": 40700, "end": 40949, "idx": 76}, {"begin": 40973, "end": 41787, "idx": 77}, {"begin": 41788, "end": 42625, "idx": 78}, {"begin": 42626, "end": 43191, "idx": 79}, {"begin": 43192, "end": 43700, "idx": 80}], "SectionHeader": [{"begin": 0, "end": 1790, "idx": 0}], "SectionReference": [{"begin": 44348, "end": 58967, "idx": 0}], "Sentence": [{"begin": 56, "end": 219, "idx": 0}, {"begin": 220, "end": 451, "idx": 1}, {"begin": 452, "end": 599, "idx": 2}, {"begin": 600, "end": 724, "idx": 3}, {"begin": 725, "end": 824, "idx": 4}, {"begin": 825, "end": 936, "idx": 5}, {"begin": 937, "end": 1307, "idx": 6}, {"begin": 1308, "end": 1687, "idx": 7}, {"begin": 1688, "end": 1790, "idx": 8}, {"begin": 1793, "end": 1902, "idx": 9}, {"begin": 1903, "end": 2050, "idx": 10}, {"begin": 2051, "end": 2115, "idx": 11}, {"begin": 2116, "end": 2265, "idx": 12}, {"begin": 2266, "end": 2384, "idx": 13}, {"begin": 2385, "end": 2405, "idx": 14}, {"begin": 2406, "end": 2631, "idx": 15}, {"begin": 2632, "end": 2887, "idx": 16}, {"begin": 2888, "end": 2985, "idx": 17}, {"begin": 2986, "end": 3097, "idx": 18}, {"begin": 3098, "end": 3307, "idx": 19}, {"begin": 3308, "end": 3440, "idx": 20}, {"begin": 3441, "end": 3584, "idx": 21}, {"begin": 3585, "end": 3740, "idx": 22}, {"begin": 3741, "end": 3831, "idx": 23}, {"begin": 3832, "end": 4049, "idx": 24}, {"begin": 4050, "end": 4146, "idx": 25}, {"begin": 4147, "end": 4382, "idx": 26}, {"begin": 4383, "end": 4690, "idx": 27}, {"begin": 4691, "end": 4829, "idx": 28}, {"begin": 4844, "end": 5021, "idx": 29}, {"begin": 5022, "end": 5147, "idx": 30}, {"begin": 5148, "end": 5288, "idx": 31}, {"begin": 5289, "end": 5378, "idx": 32}, {"begin": 5745, "end": 5836, "idx": 33}, {"begin": 5837, "end": 5910, "idx": 34}, {"begin": 5911, "end": 5958, "idx": 35}, {"begin": 5959, "end": 6030, "idx": 36}, {"begin": 6060, "end": 6147, "idx": 37}, {"begin": 6148, "end": 6227, "idx": 38}, {"begin": 6253, "end": 6370, "idx": 39}, {"begin": 6371, "end": 6401, "idx": 40}, {"begin": 6468, "end": 6520, "idx": 41}, {"begin": 6521, "end": 6608, "idx": 42}, {"begin": 6609, "end": 6924, "idx": 43}, {"begin": 6925, "end": 7000, "idx": 44}, {"begin": 7029, "end": 7183, "idx": 45}, {"begin": 7184, "end": 7369, "idx": 46}, {"begin": 7370, "end": 7534, "idx": 47}, {"begin": 7535, "end": 7644, "idx": 48}, {"begin": 7721, "end": 7800, "idx": 49}, {"begin": 7801, "end": 7915, "idx": 50}, {"begin": 7916, "end": 8014, "idx": 51}, {"begin": 8015, "end": 8157, "idx": 52}, {"begin": 8158, "end": 8321, "idx": 53}, {"begin": 8322, "end": 8687, "idx": 54}, {"begin": 8688, "end": 8779, "idx": 55}, {"begin": 8780, "end": 8907, "idx": 56}, {"begin": 8908, "end": 9105, "idx": 57}, {"begin": 9106, "end": 9207, "idx": 58}, {"begin": 9237, "end": 9367, "idx": 59}, {"begin": 9368, "end": 9493, "idx": 60}, {"begin": 9494, "end": 9615, "idx": 61}, {"begin": 9616, "end": 9803, "idx": 62}, {"begin": 9804, "end": 10049, "idx": 63}, {"begin": 10050, "end": 10112, "idx": 64}, {"begin": 10130, "end": 10173, "idx": 65}, {"begin": 10174, "end": 10261, "idx": 66}, {"begin": 10314, "end": 10386, "idx": 67}, {"begin": 10387, "end": 10449, "idx": 68}, {"begin": 10450, "end": 10528, "idx": 69}, {"begin": 10529, "end": 10599, "idx": 70}, {"begin": 10600, "end": 10841, "idx": 71}, {"begin": 10842, "end": 11003, "idx": 72}, {"begin": 11004, "end": 11205, "idx": 73}, {"begin": 11206, "end": 11287, "idx": 74}, {"begin": 11288, "end": 11344, "idx": 75}, {"begin": 11345, "end": 11502, "idx": 76}, {"begin": 11503, "end": 11628, "idx": 77}, {"begin": 11629, "end": 11764, "idx": 78}, {"begin": 11791, "end": 11913, "idx": 79}, {"begin": 11914, "end": 11991, "idx": 80}, {"begin": 11992, "end": 12108, "idx": 81}, {"begin": 12109, "end": 12261, "idx": 82}, {"begin": 12262, "end": 12348, "idx": 83}, {"begin": 12349, "end": 12471, "idx": 84}, {"begin": 12472, "end": 12566, "idx": 85}, {"begin": 12595, "end": 12738, "idx": 86}, {"begin": 12739, "end": 12895, "idx": 87}, {"begin": 12896, "end": 13004, "idx": 88}, {"begin": 13005, "end": 13294, "idx": 89}, {"begin": 13295, "end": 13413, "idx": 90}, {"begin": 13414, "end": 13651, "idx": 91}, {"begin": 13652, "end": 13753, "idx": 92}, {"begin": 13754, "end": 13928, "idx": 93}, {"begin": 13929, "end": 13985, "idx": 94}, {"begin": 13986, "end": 14711, "idx": 95}, {"begin": 14712, "end": 14794, "idx": 96}, {"begin": 14795, "end": 14895, "idx": 97}, {"begin": 14896, "end": 15156, "idx": 98}, {"begin": 15157, "end": 15259, "idx": 99}, {"begin": 15260, "end": 15317, "idx": 100}, {"begin": 15327, "end": 15418, "idx": 101}, {"begin": 15419, "end": 15528, "idx": 102}, {"begin": 15529, "end": 15816, "idx": 103}, {"begin": 15817, "end": 15926, "idx": 104}, {"begin": 15927, "end": 16003, "idx": 105}, {"begin": 16004, "end": 16053, "idx": 106}, {"begin": 16054, "end": 16161, "idx": 107}, {"begin": 16162, "end": 16208, "idx": 108}, {"begin": 16209, "end": 16373, "idx": 109}, {"begin": 16374, "end": 16442, "idx": 110}, {"begin": 16443, "end": 16521, "idx": 111}, {"begin": 16522, "end": 16801, "idx": 112}, {"begin": 16814, "end": 16848, "idx": 113}, {"begin": 16849, "end": 16951, "idx": 114}, {"begin": 16952, "end": 17170, "idx": 115}, {"begin": 17171, "end": 17310, "idx": 116}, {"begin": 17311, "end": 17546, "idx": 117}, {"begin": 17547, "end": 17613, "idx": 118}, {"begin": 17614, "end": 17753, "idx": 119}, {"begin": 17754, "end": 17935, "idx": 120}, {"begin": 17936, "end": 17972, "idx": 121}, {"begin": 17973, "end": 18134, "idx": 122}, {"begin": 18135, "end": 18239, "idx": 123}, {"begin": 18240, "end": 18403, "idx": 124}, {"begin": 18404, "end": 18578, "idx": 125}, {"begin": 18579, "end": 18676, "idx": 126}, {"begin": 18677, "end": 18808, "idx": 127}, {"begin": 18809, "end": 18892, "idx": 128}, {"begin": 18893, "end": 18972, "idx": 129}, {"begin": 18973, "end": 19111, "idx": 130}, {"begin": 19112, "end": 19141, "idx": 131}, {"begin": 19142, "end": 19293, "idx": 132}, {"begin": 19294, "end": 19384, "idx": 133}, {"begin": 19407, "end": 19552, "idx": 134}, {"begin": 19553, "end": 19746, "idx": 135}, {"begin": 19747, "end": 19921, "idx": 136}, {"begin": 19922, "end": 19958, "idx": 137}, {"begin": 19959, "end": 20133, "idx": 138}, {"begin": 20134, "end": 20206, "idx": 139}, {"begin": 20207, "end": 20408, "idx": 140}, {"begin": 20425, "end": 20650, "idx": 141}, {"begin": 20651, "end": 20829, "idx": 142}, {"begin": 20830, "end": 20902, "idx": 143}, {"begin": 20903, "end": 21051, "idx": 144}, {"begin": 21052, "end": 21129, "idx": 145}, {"begin": 21130, "end": 21297, "idx": 146}, {"begin": 21298, "end": 21409, "idx": 147}, {"begin": 21410, "end": 21575, "idx": 148}, {"begin": 21576, "end": 21642, "idx": 149}, {"begin": 21643, "end": 21756, "idx": 150}, {"begin": 21757, "end": 21956, "idx": 151}, {"begin": 21957, "end": 22019, "idx": 152}, {"begin": 22020, "end": 22137, "idx": 153}, {"begin": 22138, "end": 22248, "idx": 154}, {"begin": 22249, "end": 22344, "idx": 155}, {"begin": 22345, "end": 22401, "idx": 156}, {"begin": 22402, "end": 22492, "idx": 157}, {"begin": 22493, "end": 22615, "idx": 158}, {"begin": 22616, "end": 22718, "idx": 159}, {"begin": 22719, "end": 22922, "idx": 160}, {"begin": 22923, "end": 22994, "idx": 161}, {"begin": 22995, "end": 23064, "idx": 162}, {"begin": 23065, "end": 23112, "idx": 163}, {"begin": 23113, "end": 23290, "idx": 164}, {"begin": 23291, "end": 23383, "idx": 165}, {"begin": 23384, "end": 23486, "idx": 166}, {"begin": 23487, "end": 23705, "idx": 167}, {"begin": 23706, "end": 23833, "idx": 168}, {"begin": 23834, "end": 24057, "idx": 169}, {"begin": 24058, "end": 24219, "idx": 170}, {"begin": 24220, "end": 24362, "idx": 171}, {"begin": 24363, "end": 24556, "idx": 172}, {"begin": 24557, "end": 24839, "idx": 173}, {"begin": 24840, "end": 24906, "idx": 174}, {"begin": 24907, "end": 25043, "idx": 175}, {"begin": 25044, "end": 25309, "idx": 176}, {"begin": 25310, "end": 25461, "idx": 177}, {"begin": 25462, "end": 25584, "idx": 178}, {"begin": 25585, "end": 25805, "idx": 179}, {"begin": 25834, "end": 26004, "idx": 180}, {"begin": 26005, "end": 26166, "idx": 181}, {"begin": 26167, "end": 26260, "idx": 182}, {"begin": 26261, "end": 26363, "idx": 183}, {"begin": 26364, "end": 26475, "idx": 184}, {"begin": 26476, "end": 26713, "idx": 185}, {"begin": 26714, "end": 26846, "idx": 186}, {"begin": 26847, "end": 26944, "idx": 187}, {"begin": 26945, "end": 27106, "idx": 188}, {"begin": 27107, "end": 27261, "idx": 189}, {"begin": 27262, "end": 27475, "idx": 190}, {"begin": 27476, "end": 27596, "idx": 191}, {"begin": 27597, "end": 27722, "idx": 192}, {"begin": 27723, "end": 27802, "idx": 193}, {"begin": 27803, "end": 27917, "idx": 194}, {"begin": 27936, "end": 28348, "idx": 195}, {"begin": 28349, "end": 28492, "idx": 196}, {"begin": 28551, "end": 28693, "idx": 197}, {"begin": 28694, "end": 28808, "idx": 198}, {"begin": 28809, "end": 28986, "idx": 199}, {"begin": 28987, "end": 29200, "idx": 200}, {"begin": 29201, "end": 29393, "idx": 201}, {"begin": 29394, "end": 29480, "idx": 202}, {"begin": 29481, "end": 29650, "idx": 203}, {"begin": 29651, "end": 29818, "idx": 204}, {"begin": 29819, "end": 29982, "idx": 205}, {"begin": 29983, "end": 30051, "idx": 206}, {"begin": 30052, "end": 30347, "idx": 207}, {"begin": 30361, "end": 30446, "idx": 208}, {"begin": 30447, "end": 30563, "idx": 209}, {"begin": 30564, "end": 30634, "idx": 210}, {"begin": 30635, "end": 30707, "idx": 211}, {"begin": 30708, "end": 30807, "idx": 212}, {"begin": 30808, "end": 31009, "idx": 213}, {"begin": 31010, "end": 31212, "idx": 214}, {"begin": 31244, "end": 31453, "idx": 215}, {"begin": 31454, "end": 31618, "idx": 216}, {"begin": 31619, "end": 31666, "idx": 217}, {"begin": 31667, "end": 31884, "idx": 218}, {"begin": 31885, "end": 32010, "idx": 219}, {"begin": 32011, "end": 32196, "idx": 220}, {"begin": 32197, "end": 32325, "idx": 221}, {"begin": 32326, "end": 32459, "idx": 222}, {"begin": 32460, "end": 32553, "idx": 223}, {"begin": 32554, "end": 32656, "idx": 224}, {"begin": 32657, "end": 32755, "idx": 225}, {"begin": 32756, "end": 32809, "idx": 226}, {"begin": 32810, "end": 33082, "idx": 227}, {"begin": 33083, "end": 33203, "idx": 228}, {"begin": 33204, "end": 33511, "idx": 229}, {"begin": 33558, "end": 33648, "idx": 230}, {"begin": 33649, "end": 33799, "idx": 231}, {"begin": 33800, "end": 33889, "idx": 232}, {"begin": 33890, "end": 34035, "idx": 233}, {"begin": 34036, "end": 34175, "idx": 234}, {"begin": 34176, "end": 34386, "idx": 235}, {"begin": 34387, "end": 34422, "idx": 236}, {"begin": 34423, "end": 34776, "idx": 237}, {"begin": 34777, "end": 34951, "idx": 238}, {"begin": 34952, "end": 34998, "idx": 239}, {"begin": 34999, "end": 35098, "idx": 240}, {"begin": 35099, "end": 35154, "idx": 241}, {"begin": 35155, "end": 35356, "idx": 242}, {"begin": 35357, "end": 35439, "idx": 243}, {"begin": 35440, "end": 35549, "idx": 244}, {"begin": 35550, "end": 35702, "idx": 245}, {"begin": 35703, "end": 35815, "idx": 246}, {"begin": 35816, "end": 35876, "idx": 247}, {"begin": 35938, "end": 36125, "idx": 248}, {"begin": 36126, "end": 36212, "idx": 249}, {"begin": 36213, "end": 36644, "idx": 250}, {"begin": 36645, "end": 36897, "idx": 251}, {"begin": 36898, "end": 37006, "idx": 252}, {"begin": 37007, "end": 37060, "idx": 253}, {"begin": 37061, "end": 37196, "idx": 254}, {"begin": 37197, "end": 37302, "idx": 255}, {"begin": 37303, "end": 37463, "idx": 256}, {"begin": 37503, "end": 37704, "idx": 257}, {"begin": 37705, "end": 37760, "idx": 258}, {"begin": 37761, "end": 38021, "idx": 259}, {"begin": 38022, "end": 38111, "idx": 260}, {"begin": 38112, "end": 38182, "idx": 261}, {"begin": 38239, "end": 38372, "idx": 262}, {"begin": 38373, "end": 38432, "idx": 263}, {"begin": 38433, "end": 38623, "idx": 264}, {"begin": 38624, "end": 38779, "idx": 265}, {"begin": 38780, "end": 38966, "idx": 266}, {"begin": 38967, "end": 39098, "idx": 267}, {"begin": 39099, "end": 39285, "idx": 268}, {"begin": 39286, "end": 39522, "idx": 269}, {"begin": 39523, "end": 39604, "idx": 270}, {"begin": 39605, "end": 39799, "idx": 271}, {"begin": 39800, "end": 39981, "idx": 272}, {"begin": 39982, "end": 40105, "idx": 273}, {"begin": 40106, "end": 40145, "idx": 274}, {"begin": 40146, "end": 40280, "idx": 275}, {"begin": 40281, "end": 40370, "idx": 276}, {"begin": 40371, "end": 40602, "idx": 277}, {"begin": 40603, "end": 40699, "idx": 278}, {"begin": 40700, "end": 40949, "idx": 279}, {"begin": 40973, "end": 41291, "idx": 280}, {"begin": 41292, "end": 41522, "idx": 281}, {"begin": 41523, "end": 41679, "idx": 282}, {"begin": 41680, "end": 41787, "idx": 283}, {"begin": 41788, "end": 41896, "idx": 284}, {"begin": 41897, "end": 41967, "idx": 285}, {"begin": 41968, "end": 42082, "idx": 286}, {"begin": 42083, "end": 42249, "idx": 287}, {"begin": 42250, "end": 42402, "idx": 288}, {"begin": 42403, "end": 42547, "idx": 289}, {"begin": 42548, "end": 42625, "idx": 290}, {"begin": 42626, "end": 42779, "idx": 291}, {"begin": 42780, "end": 42823, "idx": 292}, {"begin": 42824, "end": 42827, "idx": 293}, {"begin": 42828, "end": 42911, "idx": 294}, {"begin": 42912, "end": 43056, "idx": 295}, {"begin": 43057, "end": 43191, "idx": 296}, {"begin": 43192, "end": 43333, "idx": 297}, {"begin": 43334, "end": 43552, "idx": 298}, {"begin": 43553, "end": 43700, "idx": 299}], "ReferenceToFigure": [{"begin": 1800, "end": 1801, "idx": 0}, {"begin": 7582, "end": 7583, "target": "#fig_1", "idx": 1}, {"begin": 19956, "end": 19957, "target": "#fig_5", "idx": 2}, {"begin": 20141, "end": 20142, "target": "#fig_5", "idx": 3}, {"begin": 32807, "end": 32808, "target": "#fig_3", "idx": 4}, {"begin": 34645, "end": 34646, "target": "#fig_4", "idx": 5}, {"begin": 35110, "end": 35111, "target": "#fig_6", "idx": 6}], "Div": [{"begin": 56, "end": 1790, "idx": 0}, {"begin": 1793, "end": 4829, "idx": 1}, {"begin": 4831, "end": 6030, "idx": 2}, {"begin": 6032, "end": 9207, "idx": 3}, {"begin": 9209, "end": 11764, "idx": 4}, {"begin": 11766, "end": 12566, "idx": 5}, {"begin": 12568, "end": 15317, "idx": 6}, {"begin": 15319, "end": 16801, "idx": 7}, {"begin": 16803, "end": 19384, "idx": 8}, {"begin": 19386, "end": 20408, "idx": 9}, {"begin": 20410, "end": 25805, "idx": 10}, {"begin": 25807, "end": 27917, "idx": 11}, {"begin": 27919, "end": 28492, "idx": 12}, {"begin": 28494, "end": 28517, "idx": 13}, {"begin": 28519, "end": 31212, "idx": 14}, {"begin": 31214, "end": 33511, "idx": 15}, {"begin": 33513, "end": 35876, "idx": 16}, {"begin": 35878, "end": 38182, "idx": 17}, {"begin": 38184, "end": 40949, "idx": 18}, {"begin": 40951, "end": 43700, "idx": 19}], "SectionMain": [{"begin": 1790, "end": 43700, "idx": 0}]}}