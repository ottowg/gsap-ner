{"text": "Feature Correlation-guided Knowledge Transfer for Federated Self-supervised Learning\n\nAbstract:\nTo eliminate the requirement of fully-labeled data for supervised model training in traditional Federated Learning (FL), extensive attention has been paid to the application of Self-supervised Learning (SSL) approaches on FL to tackle the label scarcity problem. Previous works on Federated SSL generally fall into two categories: parameter-based model aggregation (i.e., FedAvg, applicable to homogeneous cases) or data-based feature sharing (i.e., knowledge distillation, applicable to heterogeneous cases) to achieve knowledge transfer among multiple unlabeled clients. Despite the progress, all of them inevitably rely on some assumptions, such as homogeneous models or the existence of an additional public dataset, which hinder the universality of the training frameworks for more general scenarios. Therefore, in this paper, we propose a novel and general method named Federated Self-supervised Learning with Feature-correlation based Aggregation (FedFoA) to tackle the above limitations in a communication-efficient and privacy-preserving manner. Our insight is to utilize feature correlation to align the feature mappings and calibrate the local model updates across clients during their local training process. More specifically, we design a factorization-based method to extract the cross-feature relation matrix from the local representations. Then, the relation matrix can be regarded as a carrier of semantic information to perform the aggregation phase. We prove that FedFoA is a model-agnostic training framework and can be easily compatible with state-of-the-art unsupervised FL methods. Extensive empirical experiments demonstrate that our proposed approach outperforms the state-of-the-art methods by a significant margin.\n\nMain:\n\n\n\nIn conventional FedSSL, knowledge transfer usually relies on local data information sharing (i.e., with the supervision of pseudo label [12], prototype [30], clustering centroid [18]); While in (b), we focus on utilizing feature-correlation to transfer knowledge among different clients.\na promising paradigm by collaboratively training machine learning models without exposing the raw data of each client. As a common practice in FL, the global model is usually obtained by periodically averaging the updated model parameters from the clients in a centralized server. While existing FL methods assume that data is fully labeled so that supervised learning can be used for the model update on each client, in some real-world applications, labeling all the data is usually unrealistic due to high labor costs and the requirement of expert knowledge. For example, in medical diagnosis, the patient would be reluctant to contribute to the labeling process for privacy concerns [21].\nIn this case, Self-supervised Learning (SSL) can be regarded as a promising paradigm to tackle the label scarcity problem in FL. With the concept of learning remarkable representations on a set of unlabeled data, SSL allows higher-level data representations to be extracted from raw data using deep neural networks, such that the learned representations can be easily used for downstream tasks [27]. Many prior works have applied SSL to FL by directly extending unsupervised learning approaches to decentralized settings, where only unlabeled data are available at each client, such as BYOL [14], SimSam [3]. Considering the statistical characteristics of decentralized data, i.e., non-independent and identically distributed (Non-IID) data, FedU [33], FedReID [35], and FedEMA [34] propose to dynamically control the knowledge transfer between global model and local models, while FedCA [30] focuses on sharing features on an additional public dataset to reduce the divergence among different local models.\nHowever, we found that these solutions above still fail to achieve the desired performance due to the following two limitations: 1) inconsistency of feature representations. In FL environment, limited unlabeled training data in each client may lead to overfitting of local models, resulting in discrepancy in feature representations from client to client. Worse still, Non-IID data would aggravate the inconsistency between representations without unified information among clients. Although some previous works conduct the feature alignment by introducing an auxiliary public dataset for feature sharing (i.e., Fig. 1(a)), the selection of a public dataset may bring new challenges such as communication overhead. Therefore, it is critical to design a data-free knowledge transfer scheme to calibrate the divergence among local models. 2) Dependence on homogeneous model aggregation. Most of works rely on uploading the local models to perform knowledge aggregation, which requires all clients to hold homogeneous model architecture. This setting greatly limits the universality of the training framework in various scenarios, especially for clients with heterogeneous neural architectures.\nTo tackle these limitations, in this paper, we propose a novel and general method named Federated Self-supervised Learning with Feature-correlation based Aggregation (Fed-FoA) to achieve data-free and model-agnostic collaborative training framework in a communication-efficient and privacy-preserving manner. The key insight of FedFoA is to utilize feature correlation to align the feature mappings and calibrate the local model updates across clients during their local training process (i.e., Fig. 1(b)). More specifically, we design a factorization-based method to extract the crossfeature relation matrix from the local representations. Then, the relation matrix can be regarded as a carrier of semantic information to perform the aggregation phase. We prove that FedFoA is a model-agnostic training framework and can be easily compatible with state-of-the-art unsupervised FL methods. Extensive empirical experiments on various benchmarks and downstream tasks demonstrate that Fed-FoA outperforms the state-of-the-art methods by a significant margin in terms of model accuracy and communication efficiency. The contributions of the paper are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to explore the shared cross-feature correlation among different clients, and explicitly reveal the benefits of featurecorrelation-based knowledge transfer in federated selfsupervised learning.\n\u2022 We design a factorization-based method to extract shared feature correlation from local data and use it as a medium for knowledge transfer.\n\u2022 We propose a general training framework to align feature representations among different clients so as to perform unbiased knowledge aggregation process.\n\u2022 Extensive experiments on three typical image classification tasks and two different data settings show the superior performance of FedFoA over the state-of-theart approaches.\n\n2. Related Work\nIn this section, we summarize the prior related work in three aspects: federated learning, self-supervised learning and federated self-supervised learning.\n\n2.1. Federated Learning\nFederated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train one global model through a parameter server without sharing their raw data. In the classic algorithm FedAvg [19], the global model parameters are calculated by a weighted average of local clients' models, in which the weight of a local model is positively correlated with the data volume. Another line of works focuses on client loss rather than data volume [16, 20]. Instead of directly averaging the global model, Wang et al. [29] proposed to match neurons in a layer-wise manner before aggregation to achieve better convergence. Fedpara [11] proposed to utilize a factorizationbased method to compress parameter size and reduce communication overhead of weights aggregation. However, all these weighted aggregation schemes rely on knowledge transfer via model parameters, which require all clients to hold homogeneous model architecture. To further address model heterogeneity in FL, FedMD [15] and FedDF [17] design distillation-based methods that use additional public datasets or generated data as the common input and aggregate the features among clients. Instead of introducing an additional dataset, FedProto [26] generates label-associated feature prototypes computed by the private datasets to perform knowledge transfer. However, this methodology is only applicable to supervised learning tasks, where labeled data are available for each client.\n\n2.2. Self-supervised Learning\nSelf-supervised learning (SSL) aims to learn a general feature representation from unlabeled dataset. The preliminaries of SSL can be classified into two different categories: generative methods and discriminative methods. The principle of generative methods is to learn the representations via generating pixels of input [7, 28]. While in discriminative methods, proxy tasks are used to guide representation learning [6, 22, 24, 32]. As one promising branch, contrastive learning-based methods [1, 23] define two augmented samples of the same input as a positive pair, and two different samples as a negative pair. In the training process, they use contrastive loss to minimize the distance between positive pairs while maximizing the distance between negative pairs. MoCo [10] generates the negative samples from a memory bank and SimCLR [2] find the negative pairs in large batch size. BYOL [8] and SimSiam [3] explore more efficient approaches that skip the negative pairs training and only focus on positive pairs.\n\n2.3. Federated Self-supervised Learning\nFederated self-supervised learning (FedSSL) is still a nascent topic with the goal of learning representations from decentralized unlabeled data while protecting data privacy. Some prior works on FedSSL develop a direct combination of self-supervised learning techniques with FL, such as [13, 27]. To further tackle the non-independent and identically distributed (Non-IID) data challenges in FL, clustering-based frameworks [4, 18], auto-encoding-based method [30], and contrastive learning-based approaches (i.e., FedU [33] based on Siamese, FedReID [35], FedX [9] and FedAMA [34] based on BYOL) are proposed to reduce the divergence in feature representations among different clients caused by heterogeneous local training processes. However, all above methods require that all clients own homogeneous network architecture, which may be infeasible in some real-world applications. The federated selfsupervised learning in the heterogeneous systems has not been explored yet. To the best of our knowledge, this work is the first attempt to tackle the model heterogeneity problem in the FedSSL setup.\n\n3. Methodology\nIn this section, we explain the motivation and technical details of FedFoA. In federated unsupervised learning, though various clients may hold different views of valuable representations, the relationship between features is homogeneous since their dataset belong to a unit task (Fig. 2). Instead of directly aggregating the model weights, we align the features among clients from a new perspective: the semantic relationship between features.\n\n3.1. Problem Statement\nIn this subsection, we first review the standard FedSSL problem. The objective of federated self-supervised learning is to assemble the knowledge from multiple decentralized clients to learn general representations for downstream tasks. Supposing there are N clients with heterogeneous neural networks in the federated self-supervised learning system. Each client holds an unlabeled datasetD i = {X i }.\nWe uniformly express all the self-supervised learning loss functions, both contrastive and non-contrastive, as f (\u03c9), where \u03c9 represents the model parameters of the online encoder. In federated learning, the global objective is to learn a general model parameter set that can fit the data from all participated clients. The global loss function can be expressed as:f (\u03c9) = N i=1 D i D f i (\u03c9 i ),\nwhereD i represents the data volume of client i, D = N i=1 D i .\nThe goal of conventional federated learning is to solve the following optimization problem:\u03c9 * = arg min \u03c9 f (\u03c9).\nTaking contrastive loss as an example, in the local training of FedSSL, each client samples a mini-batch of data and uses data augmentation to generate two different views. The same image on different views will be taken as positive pair, and two different images will be regarded as negative pair. Suppose the batch size is m, the conventional contrastive loss is represented as:c (z i , z j ) = \u2212 log exp(sim(z i , z j )/\u03c4 ) 2m k=1 1 k =i exp(sim(z i , z k )/\u03c4 ) ,\nwhere sim(u, v) represents the cosine similarity between vector u and v. 1 \u2208 {0, 1} represents the positive pair indicator and \u03c4 denotes a temperature parameter.\nHowever, in the heterogeneous federated learning system, it is impossible to maintain a shared unified global model. Each client might hold a unique model and the dimensions of the model parameters among clients may not match. As a result, parameter-based model aggregation becomes infeasible. Though the model weights' knowledge can not be transferred, the knowledge in feature correlation can still be transferred in a model-agnostic way. Our goal is Figure 2. The feature-correlation can be extracted by using QRbased method [5]. For well trained models on the same task, their feature correlation tends to be similar.\nto develop a novel method that can extract the feature correlation knowledge and use it to replace the weights aggregation in heterogeneous federated self-supervised learning.\n\n3.2. QR-based Feature-correlation Extraction\nIn this section, we introduce the motivation and methodology of feature correlation extraction in FedFoA.\nLike every color in the real-world can be represented as a linear combinations of red, green, and blue, we believe that the feature representation can also be factorized into semantic basis vectors and the linear coefficients that maps the basis vector space to the original feature vector space.\nTo factorize the feature representation into the basis vectors and corresponding linear coefficients, we utilize a simple but effective factorization method, QR decomposition [5, 25]. Though Singular Value Decomposition (SVD) can also complete this task, in our scenario, QR decomposition has three advantages: 1) QR decomposition is numerically stable and the solution is unique. By contrast, SVD has multiple factorization solutions over one matrix. It is difficult for clients to share knowledge via an unstable carrier. 2) QR decomposition is computationally efficient. QR decomposition has been applied in Principal Component Analysis (PCA) [25] and proved to be more efficient than SVD-based method.\n3) The R matrix of QR composition has double physical meaning in our scenario. Except for the linear coefficients, R also represents the linear correlations among original feature vector space. For distributed clients with the same learning task, we believe sharing feature correlations and utilizing it in the training process is beneficial. It can facilitate clients to learn a more general feature representations.\nSuppose the size of client i's dataset is m and the dimension of feature representation is n, then the learned feature can be represented as a m\u00d7n matrix ZF qr (Z i t,b ) \u2192 Q i t,b , R i t,b ,\nwhere the dimension of Q i t,b is a m \u00d7 n and R i t,b is a n \u00d7 n matrix. The QR decomposition extracts the basis vectorQ i t,b out of Z i t,b\n, and R i t,b is the linear coefficients that maps the basis space back to the original feature space. Hence,Z i t,b , Q i t,b , R i t,b satisfy Z i t,b t = Q i t,b R i t,b .\nIt should be noticed that QR decomposition is highly sequentially biased. In the calculation process, it automatically sets the first column of the given matrix as the first basis vector. In the following iterations, it calculates the linearly independent part of current row from previous basis and then uses the result as the current row's basis. In other words, the R matrix also represents the cross-feature relation of a given collection of feature representations. Each value in the diagonal of R matrix represents the independence of corresponding feature vector. Since the QR decomposition is highly sequentially biased, the first feature vector is the most independent one. As a result, the value on the diagonal of R follow a decreasing pattern.\nAs the learning process continues, the independence of feature vectors increases. We use the trace of R matrix as the measurement of the independence of current feature representation. In Appendix A, we empirically prove that the downstream performance is positively correlated with the independence of feature representation.\n\n3.3. Mutual Understanding Mechanism\nAfter extracting the feature-correlation matrix by QR decomposition, the next step is to transfer knowledge through it. Instead of directly facilitating clients to learn a general cross-feature correlation matrix, we encourage each client to understand each other's cross-feature correlation. Motivated by this, we design a mutual-understanding mechanism to guide the training of clients.\nSuppose client i calculates it's own feature representation Z i , and receives a feature correlation matrix R j from client j. If i can recreate Z i based on R j and one basis vectors Q, we can say that i understands j's feature correlation. The optimal recreation can be formulated as:Q * = min Q ||Z i \u2212 QR j || 2 , s.t. Q Q = I (5)\nThis optimization problem can be solved by a auxiliary SVD decomposition: This optimization is proved in Lemma 1. Once we have the optimization solution, each client can recreate its own feature representation based on the feature correlation matrix of others. A successful recreation means two clients have a good mutual understanding. Guided by this logic, we design a regularization term to facilitates clients to enhance their mutual understanding:Q * = V U , s.t. U, \u03a3, V = SVD(R j Z i ) (6)A (Z i , R j ) = ||Z i \u2212 Q * R j ||\nThe Regularization term A can be regarded as a plugand-play module that can be used standalone or stacked on top of existing FedSSL training frameworks.\nLemma 1. Given a m \u00d7 n matrix Z and a n \u00d7 n matrix R, the optimal orthogonal matrix Q that minimizes ||QR\u2212Z|| 2 is equal to V U , where U , \u03a3, V is the SVD decomposed result of RZ .\nSince V , Q, U are three orthogonal matrices, their product is also an orthogonal matrix. So we have following inequality:Tr(V QU ) \u2264 Tr(I) = 1\nThe boundary condition that makes V QU = I is Q = V U , so that we have the following conclusion:Q * = V U\nwhich completes the proof.\n\n3.4. Round-wise Approximation\nWe then explain the advantages of communication efficiency in this section. In the ideal setting, client i only needs to upload its correlation matrix R i t for every training batch. Though the feature correlation matrix R i t has a relatively smaller shape than the original feature Z i t , the communication frequency is high since R i t updates every batch. The communication overhead will become unbearable if we use R i t to calculate our regularization term for every batch.\nTo further reduce the communication overhead, we use round-wise aggregation rather than batch-wise aggregation. Client i calculates the local average correlation matrix in one training round Ri t by:Ri t = 1 B B b=0 R i t,b ,\nwhere B is the total batch number in one training round. Then i updates Ri t to the memory bank as its representative correlation matrix. Since QR decomposition is numerically stable and the solution is unique, such an approximation does not undermine the semantic information in feature correlation. The experiment in Appendix C empirically proves that this round-wise approximation can save the traffic cost in a large volume while still maintain outstanding performance.6 for batch b \u2208 1, 2, \u2022 \u2022 \u2022 , B do 7 Z i t,b \u2190 F (X i t,b , \u03c9 i ) 8 Q i t,b , R i t,b \u2190 F qr (Z i t,b ) 9 = c (Z i t,b ) 10 if round t > t warm then 11 \u2190 FedFoA( , Z i t,b , R i t,b , R) 12 \u03c9 i \u2190 \u03c9 i \u2212 (\u03c9 i ) 13 Ri t \u2190 1 B B b=0 R i t,b 14 update memory bank Ri t \u2192 R 15 Function FedFoA( , Z i t,b , R i t,b , R): 16 for client j \u2208 N do 17 if j = i then 18 Rj t\u22121 \u2190 R 19 if Tr( Rj t\u22121 ) > Tr(R i t,b ) then 20 U, \u03a3, V \u2190 SVD( Rj t\u22121 , Z i t,b ) 21 Q * = V U 22 A = ||Z i t,b \u2212 Q * Rj t\u22121 || 23 \u2190 + \u03bb A 24 return\n\n3.5. FedFoA\nIn this section, we explain the FedFoA framework in details. We summarize the workflow of FedFoA in Algorithm 1. The feature space among clients might vary in the heterogeneous federated learning system. To unify the dimension, we add one linear calibration layer at the tail part to calibrate the various feature dimension among clients into an identical projection space. In our framework, the conventional contrastive loss is also calculated based on the projection space. In every training round, all clients calculate their correlation matrix and then upload them to the global memory bank R. Since the semantic information might be chaotic in the first few training rounds, we set warm-up limits t warm . The feature correlation knowledge transfer starts after the first t warm rounds. In the following training iterations, each client follows a Person-to-Person (P2P) protocol to access another client's feature correlation matrix. As mentioned in Sec. 3.2, the trace of the R ma-trix can be used to measure the learning process. Once the chosen correlation matrix has a higher trace, client i uses the chosen correlation matrix to calculate our regularization term by Eq. 7. We use a hyper-parameter \u03bb to control the trade-off between traditional contrastive loss and our regularization term.= C + \u03bb A ()\nDuring our feature correlation knowledge transfer process, the local feature privacy is guaranteed since the shared part is only a R matrix. In the QR decomposition, the Q matrix is independent of the R matrix. There is no feasible solution to calculate Q by R. Without Q, it is impossible to derive the original local feature by R matrix only. We can also understand it from a security perspective: the feature correlation knowledge is encoded as a private key Q and a public key R. The feature correlation knowledge transfer can be achieved by only communicating the public key without accessing the private key. Hence, information privacy is guaranteed.\n\n4. Evaluation\n\n\n4.1. Experiment Setup\nIn this section, we provide some basic experiment settings, and the detailed version is provided in Appendix B. First, we set two scenarios to evaluate the performance: heterogeneous federated learning system and homogeneous federated learning system. Since FedFoA plays different roles in these two scenarios, we compare the performance under two experiment settings. Heterogeneous setting: We choose BYOL [8], FedMD [15], and FedDF [17] as our baselines. Though FedMD, and FedDF are proposed to solve the heterogeneous federated learning in supervised setting, their methodology are general solutions and do not rely on labeled public dataset. In the heterogeneous setting, we use 4 clients with 4 different model architectures: ResNet-18, ResNet-34, AlexNet and VGG-9. After training, we use the downstream task to individually evaluate the accuracy of each client. And we use the mean accuracy as the evaluation criteria. Homogeneous setting: We choose FedBYOL, FedU [33] and FedEMA [34] as our baselines. FedFoA aims to transfer the feature correlation knowledge among clients. Hence, the benefit of FedFoA is orthogonal to baselines. We combine FedFoA with three baselines as integrated algorithms and demonstrate the improvement brought by FedFoA. In the homogeneous system, we set Resnet18 as the global model and evaluate the downstream task performance on the global model. Other general setting: Our experiments are established on two datasets: CIFAR-10 and CIFAR-100. The data distribution follows the independent and identical distribution. We set 500 as the batch size, 0.032 as the learning rate, and 256 as the projection dimension. The downstream task follows a linear evaluation protocol, and we train a single linear layer of 200 epochs to evaluate the performance. In FedFoA, we set the warm up rounds as 5 and \u03bb as 0.01. All experiments are conducted on NVIDIA RTX 3090.\n\n4.2. Performance Evaluation\nIn this section, we compare the performance of FedFoA under heterogeneous setting and homogeneous setting. We summarize the experiment result of CIFAR-10 and CIFAR-100 under heterogeneous federated learning under in Table 1 and Table 2. We set the BYOL algorithm as the core contrastive learning algorithm for each client to do local training. Without any aggregation method, the performance among clients varies. The model architecture that has the highest down stream accuracy is ResNet-18 while the lowest is AlexNet. We calculate the average accuracy as the evaluation criterion. We apply FedMD, FedDF, and Fed-FoA as aggregation methods to improve their performance.\nHeterogeneous Case. As Table 1 shows, FedMD slightly improves the accuracy from 74.33% to 74.41%, and FedDF improves the accuracy to 75.55%. FedFoA can improve the accuracy to 77.35%. Hence, our FedFoA outperforms the other two baselines by a significant margin. The experiment on CIFAR-100 also demonstrates this conclusion.\nBy aligning the feature representations among heterogeneous clients over a public dataset, FedMD successfully improves the accuracy of AlexNet and VGG-9 but undermines the performance of ResNet-18 and ResNet-34 on a small scale. Though the overall influence of FedMD is still positive, it shows the limitations of FedMD that it relies heavily on the quantity and quality of the public dataset. FedDF uses a collaboratively generated dataset as the carrier of knowledge to prevent the limitation of the public dataset. It aims to share knowledge at the feature level and force all clients to have a common feature representation.\nIn our analysis, FedFoA has three advantages that make it outstanding: 1). FedFoA uses the knowledge embedded in the private dataset so that it does not require any public dataset. 2). Instead of assembling feature representations among heterogeneous clients, FedFoA facilitates clients to have a mutual understanding of each other's feature correlation. 3). To prevent the negative effect of the clients with lower performance, we filter them out of the training process by simple trace metrics of their correlation matrix.\nHomogeneous Case. The experiment result of homogeneous federated learning is shown in Table 3. Since Fed-FoA focuses on feature correlation knowledge transferring, the benefits of FedFoA are orthogonal to other state-of-theart approaches. Hence, in this section, we use BYOL-based federated self-supervised learning approaches as our baselines and evaluate the performance improvement brought by   the regularization term of FedFoA.\nAs Table 3 shows, FedFoA significantly improves the performance of all baselines. The best method is the integration with FedEMA and FedFoA, reaching 78.82% in CIFAR-10 and 49.87% in CIFAR-100. FedFoA brings marginal accuracy improvements of 2.7% and 6.04% over two tasks respectively. As demonstrated in Figure 4, the integrated algorithm learns faster than FedEMA solo. Reaching 40% down stream accuracy costs the integrated algorithm 30 rounds of training. By contrast, it will take FedEMA for more than 50 training rounds.\nWe evaluate different trade-off settings between conventional contrastive loss and our regularization term by controlling the value of \u03bb. As Figure 5 shows, the most suitable value of \u03bb under CIFAR-100 task is 0.01.\n\n4.3. Visualization\nIn this section, we use visualization methods to show the intermediate effects and final feature representation of Fed-FoA.\nAs mentioned in Sec. 3.3, FedFoA is designed to enhance mutual understanding among clients. Though Fed-FoA does not require all clients to share a global feature correlation matrix, the regularization term in Eq. 7 still can facilitate clients to learn similar feature correlations. To further prove this, we track the distance of the R matrix among clients in the training process. We visualize the result using a heat map in Figure 6. The color and number inside each box represent the Euclidean distance of two feature correlation matrix. This intermediate result is recorded in the heterogeneous federated system. The number on the x axis and y axis represents the client id. Their model architectures are ResNet-18, VGG-9, AlexNet and ResNet-34 respectivelly.\nIn the first round, the feature correlation distance is high among clients. The distance between ResNet-18 and ResNet-34 is relatively shorter since they share the same basic block. Their semantic information tends to be similar. With the help of FedFoA, the feature correlation distance keeps decreasing in the subsequent training rounds. After convergence, though the most similar pair is still ResNet-18 and ResNet-34, their similarity is no longer significant among clients. This intermediate representation shows that FedFoA successfully narrows the semantic distance among   clients and facilitates mutual understanding.\nTo demonstrate the advantages of FedFoA on the final feature representation, we plot t-SNE distribution in Figure 7. We sample 2048 test samples of CIFAR-10 and feed them to the trained global model to extract the feature representations. After embedding the feature representations in a 2-dimensional space, we visualize the embedded data with a different color. Each color represents a category of the dataset. The Figure 7a is the learned result of FedEMA, and Figure 7b is the learned result of the integrated algorithm with FedEMA and FedFoA. The feature representation of integrated algorithm shows more clear class boundaries. For instance, in the figure of integrated method, the green points are clustered together at the right-up corner. In contrast, the green points of FedEMA are scattered and it is hard to find the a boundary that can distinguish green points from other colors.\n\n5. Conclusion\nIn this paper, we propose a novel method called Fed-FoA, to achieve a data-free and model-agnostic collaborative training framework in a communication-efficient and privacy-preserving manner. Specifically, we design a factorization-based method to extract shared feature correlation from local data and use it as a medium for knowledge transfer. Then, the relation matrix can be regarded as a\n\nFootnotes:\n\nReferences:\n\n- Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019. 3- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597-1607. PMLR, 2020. 3\n\n- Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750-15758, 2021. 2, 3\n\n- Don Kurian Dennis, Tian Li, and Virginia Smith. Hetero- geneity for the win: One-shot federated clustering. In In- ternational Conference on Machine Learning, pages 2611- 2620. PMLR, 2021. 3\n\n- Walter Gander. Algorithms for the qr decomposition. Res. Rep, 80(02):1251-1268, 1980. 4\n\n- Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. arXiv preprint arXiv:1803.07728, 2018. 3\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commu- nications of the ACM, 63(11):139-144, 2020. 3\n\n- Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271-21284, 2020. 3, 6\n\n- Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xing Xie, and Meeyoung Cha. Fedx: Unsuper- vised federated learning with cross knowledge distillation. arXiv preprint arXiv:2207.09158, 2022. 3\n\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 9729-9738, 2020. 3\n\n- Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard product for communication-efficient federated learning. arXiv preprint arXiv:2108.06098, 2021. 2\n\n- Wonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised learning with inter- client consistency & disjoint learning. arXiv preprint arXiv:2006.12097, 2020. 1\n\n- Yilun Jin, Xiguang Wei, Yang Liu, and Qiang Yang. Towards utilizing unlabeled data in federated learning: A survey and prospective. arXiv preprint arXiv:2002.11545, 2020. 3\n\n- Nikos Komodakis and Spyros Gidaris. Unsupervised repre- sentation learning by predicting image rotations. In Inter- national Conference on Learning Representations (ICLR), 2018. 2\n\n- Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581, 2019. 2, 6\n\n- Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In In- ternational Conference on Learning Representations, 2019. 2\n\n- Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in fed- erated learning. Advances in Neural Information Processing Systems, 33:2351-2363, 2020. 2, 6\n\n- Ekdeep Singh Lubana, Chi Ian Tang, Fahim Kawsar, Robert P Dick, and Akhil Mathur. Orchestra: Unsupervised federated learning via globally consistent clustering. arXiv preprint arXiv:2205.11506, 2022. 1, 3\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication- efficient learning of deep networks from decentralized data. In Proceedings of Artificial Intelligence and Statistics (AIS- TATS), 2017. 1, 2\n\n- Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International Conference on Machine Learning, pages 4615-4625. PMLR, 2019. 2\n\n- Dianwen Ng, Xiang Lan, Melissa Min-Szu Yao, Wing P Chan, and Mengling Feng. Federated learning: a collabo- rative effort to achieve better medical imaging models for in- dividual sites that have small labelled datasets. Quantitative Imaging in Medicine and Surgery, 11(2):852, 2021. 2\n\n- Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Euro- pean conference on computer vision, pages 69-84. Springer, 2016. 3\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3\n\n- Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2536-2544, 2016. 3\n\n- Alok Sharma, Kuldip K Paliwal, Seiya Imoto, and Satoru Miyano. Principal component analysis using qr decomposi- tion. International Journal of Machine Learning and Cyber- netics, 4(6):679-683, 2013. 4\n\n- Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated pro- totype learning over heterogeneous devices. arXiv preprint arXiv:2105.00243, 2021. 3\n\n- Bram van Berlo, Aaqib Saeed, and Tanir Ozcelebi. Towards federated unsupervised representation learning. In Proceed- ings of the third ACM international workshop on edge sys- tems, analytics and networking, pages 31-36, 2020. 2, 3\n\n- Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103, 2008. 3\n\n- Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Pa- pailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020. 2\n\n- Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang, and Xiaolin Li. Federated unsupervised representation learning. arXiv preprint arXiv:2010.08982, 2020. 1, 2, 3\n\n- Jie Zhang, Zhihao Qu, Chenxi Chen, Haozhao Wang, Yufeng Zhan, Baoliu Ye, and Song Guo. Edge learning: The en- abling technology for distributed big data analytics in the edge. ACM Computing Surveys (CSUR), 54(7):1-36, 2021. 1\n\n- Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on computer vision, pages 649-666. Springer, 2016. 3\n\n- Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual represen- tation learning from decentralized data. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 4912-4921, 2021. 2, 3, 6\n\n- Weiming Zhuang, Yonggang Wen, and Shuai Zhang. Divergence-aware federated self-supervised learning. In In- ternational Conference on Learning Representations, 2021. 2, 3, 6\n\n- Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin, Dongzhan Zhou, Shuai Zhang, and Shuai Yi. Performance optimization of federated person re- identification via benchmark analysis. In Proceedings of the 28th ACM International Conference on Multimedia, pages 955-963, 2020. 2, 3\n\n", "annotations": {"ReferenceToTable": [{"begin": 24644, "end": 24645, "target": "#tab_1", "idx": 0}, {"begin": 25111, "end": 25112, "target": "#tab_0", "idx": 1}, {"begin": 26654, "end": 26655, "target": "#tab_2", "idx": 2}, {"begin": 27004, "end": 27005, "target": "#tab_2", "idx": 3}], "SectionMain": [{"begin": 1845, "end": 30574, "idx": 0}], "SectionReference": [{"begin": 30588, "end": 37832, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1845, "idx": 0}], "Div": [{"begin": 96, "end": 1837, "idx": 0}, {"begin": 1848, "end": 6914, "idx": 1}, {"begin": 6916, "end": 7087, "idx": 2}, {"begin": 7089, "end": 8586, "idx": 3}, {"begin": 8588, "end": 9637, "idx": 4}, {"begin": 9639, "end": 10780, "idx": 5}, {"begin": 10782, "end": 11241, "idx": 6}, {"begin": 11243, "end": 13672, "idx": 7}, {"begin": 13674, "end": 16838, "idx": 8}, {"begin": 16840, "end": 18744, "idx": 9}, {"begin": 18746, "end": 20466, "idx": 10}, {"begin": 20468, "end": 22449, "idx": 11}, {"begin": 22451, "end": 22465, "idx": 12}, {"begin": 22467, "end": 24380, "idx": 13}, {"begin": 24382, "end": 27737, "idx": 14}, {"begin": 27739, "end": 30166, "idx": 15}, {"begin": 30168, "end": 30574, "idx": 16}], "Head": [{"begin": 6916, "end": 6931, "n": "2.", "idx": 0}, {"begin": 7089, "end": 7112, "n": "2.1.", "idx": 1}, {"begin": 8588, "end": 8617, "n": "2.2.", "idx": 2}, {"begin": 9639, "end": 9678, "n": "2.3.", "idx": 3}, {"begin": 10782, "end": 10796, "n": "3.", "idx": 4}, {"begin": 11243, "end": 11265, "n": "3.1.", "idx": 5}, {"begin": 13674, "end": 13718, "n": "3.2.", "idx": 6}, {"begin": 16840, "end": 16875, "n": "3.3.", "idx": 7}, {"begin": 18746, "end": 18775, "n": "3.4.", "idx": 8}, {"begin": 20468, "end": 20479, "n": "3.5.", "idx": 9}, {"begin": 22451, "end": 22464, "n": "4.", "idx": 10}, {"begin": 22467, "end": 22488, "n": "4.1.", "idx": 11}, {"begin": 24382, "end": 24409, "n": "4.2.", "idx": 12}, {"begin": 27739, "end": 27757, "n": "4.3.", "idx": 13}, {"begin": 30168, "end": 30181, "n": "5.", "idx": 14}], "Paragraph": [{"begin": 96, "end": 1837, "idx": 0}, {"begin": 1848, "end": 2135, "idx": 1}, {"begin": 2136, "end": 2827, "idx": 2}, {"begin": 2828, "end": 3835, "idx": 3}, {"begin": 3836, "end": 5027, "idx": 4}, {"begin": 5028, "end": 6197, "idx": 5}, {"begin": 6198, "end": 6439, "idx": 6}, {"begin": 6440, "end": 6581, "idx": 7}, {"begin": 6582, "end": 6737, "idx": 8}, {"begin": 6738, "end": 6914, "idx": 9}, {"begin": 6932, "end": 7087, "idx": 10}, {"begin": 7113, "end": 8586, "idx": 11}, {"begin": 8618, "end": 9637, "idx": 12}, {"begin": 9679, "end": 10780, "idx": 13}, {"begin": 10797, "end": 11241, "idx": 14}, {"begin": 11266, "end": 11656, "idx": 15}, {"begin": 11670, "end": 12035, "idx": 16}, {"begin": 12067, "end": 12072, "idx": 17}, {"begin": 12132, "end": 12223, "idx": 18}, {"begin": 12246, "end": 12626, "idx": 19}, {"begin": 12713, "end": 12874, "idx": 20}, {"begin": 12875, "end": 13496, "idx": 21}, {"begin": 13497, "end": 13672, "idx": 22}, {"begin": 13719, "end": 13824, "idx": 23}, {"begin": 13825, "end": 14121, "idx": 24}, {"begin": 14122, "end": 14827, "idx": 25}, {"begin": 14828, "end": 15245, "idx": 26}, {"begin": 15246, "end": 15401, "idx": 27}, {"begin": 15439, "end": 15558, "idx": 28}, {"begin": 15581, "end": 15690, "idx": 29}, {"begin": 15756, "end": 16511, "idx": 30}, {"begin": 16512, "end": 16838, "idx": 31}, {"begin": 16876, "end": 17264, "idx": 32}, {"begin": 17265, "end": 17551, "idx": 33}, {"begin": 17600, "end": 18052, "idx": 34}, {"begin": 18132, "end": 18284, "idx": 35}, {"begin": 18285, "end": 18466, "idx": 36}, {"begin": 18467, "end": 18589, "idx": 37}, {"begin": 18611, "end": 18708, "idx": 38}, {"begin": 18718, "end": 18744, "idx": 39}, {"begin": 18776, "end": 19256, "idx": 40}, {"begin": 19257, "end": 19456, "idx": 41}, {"begin": 19483, "end": 19956, "idx": 42}, {"begin": 20480, "end": 21780, "idx": 43}, {"begin": 21793, "end": 22449, "idx": 44}, {"begin": 22489, "end": 24380, "idx": 45}, {"begin": 24410, "end": 25081, "idx": 46}, {"begin": 25082, "end": 25407, "idx": 47}, {"begin": 25408, "end": 26036, "idx": 48}, {"begin": 26037, "end": 26561, "idx": 49}, {"begin": 26562, "end": 26994, "idx": 50}, {"begin": 26995, "end": 27521, "idx": 51}, {"begin": 27522, "end": 27737, "idx": 52}, {"begin": 27758, "end": 27881, "idx": 53}, {"begin": 27882, "end": 28646, "idx": 54}, {"begin": 28647, "end": 29273, "idx": 55}, {"begin": 29274, "end": 30166, "idx": 56}, {"begin": 30182, "end": 30574, "idx": 57}], "ReferenceToBib": [{"begin": 1984, "end": 1988, "target": "#b11", "idx": 0}, {"begin": 2000, "end": 2004, "target": "#b29", "idx": 1}, {"begin": 2026, "end": 2030, "target": "#b17", "idx": 2}, {"begin": 2822, "end": 2826, "target": "#b20", "idx": 3}, {"begin": 3222, "end": 3226, "target": "#b26", "idx": 4}, {"begin": 3419, "end": 3423, "target": "#b13", "idx": 5}, {"begin": 3432, "end": 3435, "target": "#b2", "idx": 6}, {"begin": 3575, "end": 3579, "target": "#b32", "idx": 7}, {"begin": 3589, "end": 3593, "target": "#b34", "idx": 8}, {"begin": 3606, "end": 3610, "target": "#b33", "idx": 9}, {"begin": 3716, "end": 3720, "target": "#b29", "idx": 10}, {"begin": 7337, "end": 7341, "target": "#b18", "idx": 11}, {"begin": 7587, "end": 7591, "target": "#b15", "idx": 12}, {"begin": 7592, "end": 7595, "target": "#b19", "idx": 13}, {"begin": 7657, "end": 7661, "target": "#b28", "idx": 14}, {"begin": 7769, "end": 7773, "target": "#b10", "idx": 15}, {"begin": 8122, "end": 8126, "target": "#b14", "idx": 16}, {"begin": 8137, "end": 8141, "target": "#b16", "idx": 17}, {"begin": 8347, "end": 8351, "target": "#b25", "idx": 18}, {"begin": 8940, "end": 8943, "target": "#b6", "idx": 19}, {"begin": 8944, "end": 8947, "target": "#b27", "idx": 20}, {"begin": 9036, "end": 9039, "target": "#b5", "idx": 21}, {"begin": 9040, "end": 9043, "target": "#b21", "idx": 22}, {"begin": 9044, "end": 9047, "target": "#b23", "idx": 23}, {"begin": 9048, "end": 9051, "target": "#b31", "idx": 24}, {"begin": 9113, "end": 9116, "target": "#b0", "idx": 25}, {"begin": 9117, "end": 9120, "target": "#b22", "idx": 26}, {"begin": 9392, "end": 9396, "target": "#b9", "idx": 27}, {"begin": 9458, "end": 9461, "target": "#b1", "idx": 28}, {"begin": 9512, "end": 9515, "target": "#b7", "idx": 29}, {"begin": 9528, "end": 9531, "target": "#b2", "idx": 30}, {"begin": 9967, "end": 9971, "target": "#b12", "idx": 31}, {"begin": 9972, "end": 9975, "target": "#b26", "idx": 32}, {"begin": 10104, "end": 10107, "target": "#b3", "idx": 33}, {"begin": 10108, "end": 10111, "target": "#b17", "idx": 34}, {"begin": 10140, "end": 10144, "target": "#b29", "idx": 35}, {"begin": 10200, "end": 10204, "target": "#b32", "idx": 36}, {"begin": 10231, "end": 10235, "target": "#b34", "idx": 37}, {"begin": 10242, "end": 10245, "target": "#b8", "idx": 38}, {"begin": 10257, "end": 10261, "target": "#b33", "idx": 39}, {"begin": 13403, "end": 13406, "target": "#b4", "idx": 40}, {"begin": 14297, "end": 14300, "target": "#b4", "idx": 41}, {"begin": 14301, "end": 14304, "target": "#b24", "idx": 42}, {"begin": 14768, "end": 14772, "target": "#b24", "idx": 43}, {"begin": 22896, "end": 22899, "target": "#b7", "idx": 44}, {"begin": 22907, "end": 22911, "target": "#b14", "idx": 45}, {"begin": 22923, "end": 22927, "target": "#b16", "idx": 46}, {"begin": 23460, "end": 23464, "target": "#b32", "idx": 47}, {"begin": 23476, "end": 23480, "target": "#b33", "idx": 48}], "Sentence": [{"begin": 96, "end": 358, "idx": 0}, {"begin": 359, "end": 668, "idx": 1}, {"begin": 669, "end": 901, "idx": 2}, {"begin": 902, "end": 1150, "idx": 3}, {"begin": 1151, "end": 1316, "idx": 4}, {"begin": 1317, "end": 1451, "idx": 5}, {"begin": 1452, "end": 1564, "idx": 6}, {"begin": 1565, "end": 1700, "idx": 7}, {"begin": 1701, "end": 1837, "idx": 8}, {"begin": 1848, "end": 2135, "idx": 9}, {"begin": 2136, "end": 2254, "idx": 10}, {"begin": 2255, "end": 2416, "idx": 11}, {"begin": 2417, "end": 2696, "idx": 12}, {"begin": 2697, "end": 2827, "idx": 13}, {"begin": 2828, "end": 2956, "idx": 14}, {"begin": 2957, "end": 3227, "idx": 15}, {"begin": 3228, "end": 3436, "idx": 16}, {"begin": 3437, "end": 3835, "idx": 17}, {"begin": 3836, "end": 4009, "idx": 18}, {"begin": 4010, "end": 4191, "idx": 19}, {"begin": 4192, "end": 4318, "idx": 20}, {"begin": 4319, "end": 4550, "idx": 21}, {"begin": 4551, "end": 4672, "idx": 22}, {"begin": 4673, "end": 4720, "idx": 23}, {"begin": 4721, "end": 4870, "idx": 24}, {"begin": 4871, "end": 5027, "idx": 25}, {"begin": 5028, "end": 5336, "idx": 26}, {"begin": 5337, "end": 5534, "idx": 27}, {"begin": 5535, "end": 5668, "idx": 28}, {"begin": 5669, "end": 5781, "idx": 29}, {"begin": 5782, "end": 5917, "idx": 30}, {"begin": 5918, "end": 6139, "idx": 31}, {"begin": 6140, "end": 6197, "idx": 32}, {"begin": 6198, "end": 6439, "idx": 33}, {"begin": 6440, "end": 6581, "idx": 34}, {"begin": 6582, "end": 6737, "idx": 35}, {"begin": 6738, "end": 6914, "idx": 36}, {"begin": 6932, "end": 7087, "idx": 37}, {"begin": 7113, "end": 7304, "idx": 38}, {"begin": 7305, "end": 7517, "idx": 39}, {"begin": 7518, "end": 7596, "idx": 40}, {"begin": 7597, "end": 7760, "idx": 41}, {"begin": 7761, "end": 7906, "idx": 42}, {"begin": 7907, "end": 8069, "idx": 43}, {"begin": 8070, "end": 8291, "idx": 44}, {"begin": 8292, "end": 8461, "idx": 45}, {"begin": 8462, "end": 8586, "idx": 46}, {"begin": 8618, "end": 8719, "idx": 47}, {"begin": 8720, "end": 8840, "idx": 48}, {"begin": 8841, "end": 8948, "idx": 49}, {"begin": 8949, "end": 9052, "idx": 50}, {"begin": 9053, "end": 9233, "idx": 51}, {"begin": 9234, "end": 9386, "idx": 52}, {"begin": 9387, "end": 9506, "idx": 53}, {"begin": 9507, "end": 9637, "idx": 54}, {"begin": 9679, "end": 9854, "idx": 55}, {"begin": 9855, "end": 9976, "idx": 56}, {"begin": 9977, "end": 10415, "idx": 57}, {"begin": 10416, "end": 10562, "idx": 58}, {"begin": 10563, "end": 10656, "idx": 59}, {"begin": 10657, "end": 10780, "idx": 60}, {"begin": 10797, "end": 10872, "idx": 61}, {"begin": 10873, "end": 11086, "idx": 62}, {"begin": 11087, "end": 11241, "idx": 63}, {"begin": 11266, "end": 11330, "idx": 64}, {"begin": 11331, "end": 11502, "idx": 65}, {"begin": 11503, "end": 11617, "idx": 66}, {"begin": 11618, "end": 11656, "idx": 67}, {"begin": 11670, "end": 11850, "idx": 68}, {"begin": 11851, "end": 11989, "idx": 69}, {"begin": 11990, "end": 12035, "idx": 70}, {"begin": 12067, "end": 12072, "idx": 71}, {"begin": 12132, "end": 12223, "idx": 72}, {"begin": 12246, "end": 12418, "idx": 73}, {"begin": 12419, "end": 12544, "idx": 74}, {"begin": 12545, "end": 12626, "idx": 75}, {"begin": 12713, "end": 12874, "idx": 76}, {"begin": 12875, "end": 12991, "idx": 77}, {"begin": 12992, "end": 13101, "idx": 78}, {"begin": 13102, "end": 13168, "idx": 79}, {"begin": 13169, "end": 13315, "idx": 80}, {"begin": 13316, "end": 13407, "idx": 81}, {"begin": 13408, "end": 13496, "idx": 82}, {"begin": 13497, "end": 13672, "idx": 83}, {"begin": 13719, "end": 13824, "idx": 84}, {"begin": 13825, "end": 14121, "idx": 85}, {"begin": 14122, "end": 14305, "idx": 86}, {"begin": 14306, "end": 14502, "idx": 87}, {"begin": 14503, "end": 14573, "idx": 88}, {"begin": 14574, "end": 14645, "idx": 89}, {"begin": 14646, "end": 14695, "idx": 90}, {"begin": 14696, "end": 14827, "idx": 91}, {"begin": 14828, "end": 14906, "idx": 92}, {"begin": 14907, "end": 15021, "idx": 93}, {"begin": 15022, "end": 15170, "idx": 94}, {"begin": 15171, "end": 15245, "idx": 95}, {"begin": 15246, "end": 15401, "idx": 96}, {"begin": 15439, "end": 15511, "idx": 97}, {"begin": 15512, "end": 15558, "idx": 98}, {"begin": 15581, "end": 15683, "idx": 99}, {"begin": 15684, "end": 15690, "idx": 100}, {"begin": 15756, "end": 15829, "idx": 101}, {"begin": 15830, "end": 15943, "idx": 102}, {"begin": 15944, "end": 16104, "idx": 103}, {"begin": 16105, "end": 16226, "idx": 104}, {"begin": 16227, "end": 16326, "idx": 105}, {"begin": 16327, "end": 16438, "idx": 106}, {"begin": 16439, "end": 16511, "idx": 107}, {"begin": 16512, "end": 16593, "idx": 108}, {"begin": 16594, "end": 16696, "idx": 109}, {"begin": 16697, "end": 16838, "idx": 110}, {"begin": 16876, "end": 16995, "idx": 111}, {"begin": 16996, "end": 17168, "idx": 112}, {"begin": 17169, "end": 17264, "idx": 113}, {"begin": 17265, "end": 17391, "idx": 114}, {"begin": 17392, "end": 17506, "idx": 115}, {"begin": 17507, "end": 17551, "idx": 116}, {"begin": 17600, "end": 17860, "idx": 117}, {"begin": 17861, "end": 17936, "idx": 118}, {"begin": 17937, "end": 18052, "idx": 119}, {"begin": 18132, "end": 18284, "idx": 120}, {"begin": 18285, "end": 18293, "idx": 121}, {"begin": 18294, "end": 18466, "idx": 122}, {"begin": 18467, "end": 18556, "idx": 123}, {"begin": 18557, "end": 18589, "idx": 124}, {"begin": 18611, "end": 18708, "idx": 125}, {"begin": 18718, "end": 18744, "idx": 126}, {"begin": 18776, "end": 18851, "idx": 127}, {"begin": 18852, "end": 18958, "idx": 128}, {"begin": 18959, "end": 19136, "idx": 129}, {"begin": 19137, "end": 19256, "idx": 130}, {"begin": 19257, "end": 19368, "idx": 131}, {"begin": 19369, "end": 19456, "idx": 132}, {"begin": 19483, "end": 19539, "idx": 133}, {"begin": 19540, "end": 19620, "idx": 134}, {"begin": 19621, "end": 19783, "idx": 135}, {"begin": 19784, "end": 19956, "idx": 136}, {"begin": 20480, "end": 20540, "idx": 137}, {"begin": 20541, "end": 20592, "idx": 138}, {"begin": 20593, "end": 20683, "idx": 139}, {"begin": 20684, "end": 20853, "idx": 140}, {"begin": 20854, "end": 20955, "idx": 141}, {"begin": 20956, "end": 21190, "idx": 142}, {"begin": 21191, "end": 21271, "idx": 143}, {"begin": 21272, "end": 21418, "idx": 144}, {"begin": 21419, "end": 21439, "idx": 145}, {"begin": 21440, "end": 21516, "idx": 146}, {"begin": 21517, "end": 21780, "idx": 147}, {"begin": 21793, "end": 21933, "idx": 148}, {"begin": 21934, "end": 22003, "idx": 149}, {"begin": 22004, "end": 22137, "idx": 150}, {"begin": 22138, "end": 22276, "idx": 151}, {"begin": 22277, "end": 22407, "idx": 152}, {"begin": 22408, "end": 22449, "idx": 153}, {"begin": 22489, "end": 22740, "idx": 154}, {"begin": 22741, "end": 22857, "idx": 155}, {"begin": 22858, "end": 22945, "idx": 156}, {"begin": 22946, "end": 23134, "idx": 157}, {"begin": 23135, "end": 23260, "idx": 158}, {"begin": 23261, "end": 23357, "idx": 159}, {"begin": 23358, "end": 23414, "idx": 160}, {"begin": 23415, "end": 23498, "idx": 161}, {"begin": 23499, "end": 23571, "idx": 162}, {"begin": 23572, "end": 23628, "idx": 163}, {"begin": 23629, "end": 23743, "idx": 164}, {"begin": 23744, "end": 23872, "idx": 165}, {"begin": 23873, "end": 23968, "idx": 166}, {"begin": 23969, "end": 24042, "idx": 167}, {"begin": 24043, "end": 24137, "idx": 168}, {"begin": 24138, "end": 24273, "idx": 169}, {"begin": 24274, "end": 24330, "idx": 170}, {"begin": 24331, "end": 24380, "idx": 171}, {"begin": 24410, "end": 24516, "idx": 172}, {"begin": 24517, "end": 24646, "idx": 173}, {"begin": 24647, "end": 24753, "idx": 174}, {"begin": 24754, "end": 24823, "idx": 175}, {"begin": 24824, "end": 24930, "idx": 176}, {"begin": 24931, "end": 24993, "idx": 177}, {"begin": 24994, "end": 25081, "idx": 178}, {"begin": 25082, "end": 25101, "idx": 179}, {"begin": 25102, "end": 25222, "idx": 180}, {"begin": 25223, "end": 25265, "idx": 181}, {"begin": 25266, "end": 25344, "idx": 182}, {"begin": 25345, "end": 25407, "idx": 183}, {"begin": 25408, "end": 25636, "idx": 184}, {"begin": 25637, "end": 25801, "idx": 185}, {"begin": 25802, "end": 25925, "idx": 186}, {"begin": 25926, "end": 26036, "idx": 187}, {"begin": 26037, "end": 26111, "idx": 188}, {"begin": 26112, "end": 26217, "idx": 189}, {"begin": 26218, "end": 26221, "idx": 190}, {"begin": 26222, "end": 26391, "idx": 191}, {"begin": 26392, "end": 26395, "idx": 192}, {"begin": 26396, "end": 26561, "idx": 193}, {"begin": 26562, "end": 26579, "idx": 194}, {"begin": 26580, "end": 26656, "idx": 195}, {"begin": 26657, "end": 26800, "idx": 196}, {"begin": 26801, "end": 26994, "idx": 197}, {"begin": 26995, "end": 27076, "idx": 198}, {"begin": 27077, "end": 27188, "idx": 199}, {"begin": 27189, "end": 27280, "idx": 200}, {"begin": 27281, "end": 27366, "idx": 201}, {"begin": 27367, "end": 27454, "idx": 202}, {"begin": 27455, "end": 27521, "idx": 203}, {"begin": 27522, "end": 27659, "idx": 204}, {"begin": 27660, "end": 27737, "idx": 205}, {"begin": 27758, "end": 27881, "idx": 206}, {"begin": 27882, "end": 27902, "idx": 207}, {"begin": 27903, "end": 27973, "idx": 208}, {"begin": 27974, "end": 28164, "idx": 209}, {"begin": 28165, "end": 28264, "idx": 210}, {"begin": 28265, "end": 28318, "idx": 211}, {"begin": 28319, "end": 28423, "idx": 212}, {"begin": 28424, "end": 28499, "idx": 213}, {"begin": 28500, "end": 28561, "idx": 214}, {"begin": 28562, "end": 28646, "idx": 215}, {"begin": 28647, "end": 28722, "idx": 216}, {"begin": 28723, "end": 28828, "idx": 217}, {"begin": 28829, "end": 28876, "idx": 218}, {"begin": 28877, "end": 28986, "idx": 219}, {"begin": 28987, "end": 29125, "idx": 220}, {"begin": 29126, "end": 29273, "idx": 221}, {"begin": 29274, "end": 29390, "idx": 222}, {"begin": 29391, "end": 29512, "idx": 223}, {"begin": 29513, "end": 29637, "idx": 224}, {"begin": 29638, "end": 29686, "idx": 225}, {"begin": 29687, "end": 29821, "idx": 226}, {"begin": 29822, "end": 29907, "idx": 227}, {"begin": 29908, "end": 30021, "idx": 228}, {"begin": 30022, "end": 30166, "idx": 229}, {"begin": 30182, "end": 30373, "idx": 230}, {"begin": 30374, "end": 30527, "idx": 231}, {"begin": 30528, "end": 30574, "idx": 232}], "ReferenceToFigure": [{"begin": 4453, "end": 4457, "target": "#fig_4", "idx": 0}, {"begin": 5528, "end": 5532, "target": "#fig_4", "idx": 1}, {"begin": 11083, "end": 11084, "idx": 2}, {"begin": 13335, "end": 13336, "idx": 3}, {"begin": 27307, "end": 27308, "target": "#fig_6", "idx": 4}, {"begin": 27670, "end": 27671, "target": "#fig_7", "idx": 5}, {"begin": 28316, "end": 28317, "target": "#fig_8", "idx": 6}, {"begin": 29388, "end": 29389, "target": "#fig_10", "idx": 7}, {"begin": 29698, "end": 29700, "target": "#fig_10", "idx": 8}, {"begin": 29745, "end": 29747, "target": "#fig_10", "idx": 9}], "Abstract": [{"begin": 86, "end": 1837, "idx": 0}], "SectionFootnote": [{"begin": 30576, "end": 30586, "idx": 0}], "ReferenceString": [{"begin": 30603, "end": 30792, "id": "b0", "idx": 0}, {"begin": 30794, "end": 31015, "id": "b1", "idx": 1}, {"begin": 31019, "end": 31216, "id": "b2", "idx": 2}, {"begin": 31220, "end": 31410, "id": "b3", "idx": 3}, {"begin": 31414, "end": 31501, "id": "b4", "idx": 4}, {"begin": 31505, "end": 31669, "id": "b5", "idx": 5}, {"begin": 31673, "end": 31887, "id": "b6", "idx": 6}, {"begin": 31891, "end": 32227, "id": "b7", "idx": 7}, {"begin": 32231, "end": 32438, "id": "b8", "idx": 8}, {"begin": 32442, "end": 32688, "id": "b9", "idx": 9}, {"begin": 32692, "end": 32860, "id": "b10", "idx": 10}, {"begin": 32864, "end": 33051, "id": "b11", "idx": 11}, {"begin": 33055, "end": 33227, "id": "b12", "idx": 12}, {"begin": 33231, "end": 33410, "id": "b13", "idx": 13}, {"begin": 33414, "end": 33547, "id": "b14", "idx": 14}, {"begin": 33551, "end": 33725, "id": "b15", "idx": 15}, {"begin": 33729, "end": 33936, "id": "b16", "idx": 16}, {"begin": 33940, "end": 34144, "id": "b17", "idx": 17}, {"begin": 34148, "end": 34390, "id": "b18", "idx": 18}, {"begin": 34394, "end": 34557, "id": "b19", "idx": 19}, {"begin": 34561, "end": 34845, "id": "b20", "idx": 20}, {"begin": 34849, "end": 35031, "id": "b21", "idx": 21}, {"begin": 35035, "end": 35186, "id": "b22", "idx": 22}, {"begin": 35190, "end": 35433, "id": "b23", "idx": 23}, {"begin": 35437, "end": 35637, "id": "b24", "idx": 24}, {"begin": 35641, "end": 35837, "id": "b25", "idx": 25}, {"begin": 35841, "end": 36071, "id": "b26", "idx": 26}, {"begin": 36075, "end": 36319, "id": "b27", "idx": 27}, {"begin": 36323, "end": 36499, "id": "b28", "idx": 28}, {"begin": 36503, "end": 36708, "id": "b29", "idx": 29}, {"begin": 36712, "end": 36937, "id": "b30", "idx": 30}, {"begin": 36941, "end": 37095, "id": "b31", "idx": 31}, {"begin": 37099, "end": 37358, "id": "b32", "idx": 32}, {"begin": 37362, "end": 37534, "id": "b33", "idx": 33}, {"begin": 37538, "end": 37830, "id": "b34", "idx": 34}]}}