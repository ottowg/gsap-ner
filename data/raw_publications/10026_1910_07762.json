{"text": "LEARNING ENERGY-BASED MODELS IN HIGH-DIMENSIONAL SPACES WITH MULTI-SCALE DENOISING SCORE MATCHING\n\nAbstract:\nEnergy-Based Models (EBMs) assign unnormalized log-probability to data samples. This functionality has a variety of applications, such as sample synthesis, data denoising, sample restoration, outlier detection, Bayesian reasoning, and many more. But training of EBMs using standard maximum likelihood is extremely slow because it requires sampling from the model distribution. Score matching potentially alleviates this problem. In particular, denoising score matching (Vincent, 2011) has been successfully used to train EBMs. Using noisy data samples with one fixed noise level, these models learn fast and yield good results in data denoising (Saremi and Hyvarinen, 2019). However, demonstrations of such models in high quality sample synthesis of high dimensional data were lacking. Recently, Song and Ermon (2019) have shown that a generative model trained by denoising score matching accomplishes excellent sample synthesis, when trained with data samples corrupted with multiple levels of noise. Here we provide analysis and empirical evidence showing that training with multiple noise levels is necessary when the data dimension is high. Leveraging this insight, we propose a novel EBM trained with multi-scale denoising score matching. Our model exhibits data generation performance comparable to state-of-the-art techniques such as GANs, and sets a new baseline for EBMs. The proposed model also provides density information and performs well in an image inpainting task.\n\nMain:\n\n\n\n1 INTRODUCTION AND MOTIVATION\nTreating data as stochastic samples from a probability distribution and developing models that can learn such distributions is at the core for solving a large variety of application problems, such as error correction/denoising (Vincent et al., 2010), outlier/novelty detection (Zhai et al., 2016; Choi and Jang, 2018), sample generation (Nijkamp et al., 2019; Du and Mordatch, 2019), invariant pattern recognition, Bayesian reasoning (Welling and Teh, 2011) which relies on good data priors, and many others.\nEnergy-Based Models (EBMs) (LeCun et al., 2006; Ngiam et al., 2011) assign an energy E(x x x) to each data point x x x which implicitly defines a probability by the Boltzmann distribution p m (x x x) = e \u2212E(x x x) /Z. Sampling from this distribution can be used as a generative process that yield plausible samples of x x x. Compared to other generative models, like GANs (Goodfellow et al., 2014), flow-Preprint based models (Dinh et al., 2015; Kingma and Dhariwal, 2018), or auto-regressive models (van den Oord et al., 2016; Ostrovski et al., 2018), energy-based models have significant advantages. First, they provide explicit (unnormalized) density information, compositionality (Hinton, 1999; Haarnoja et al., 2017), better mode coverage (Kumar et al., 2019) and flexibility (Du and Mordatch, 2019). Further, they do not require special model architecture, unlike auto-regressive and flow-based models. Recently, Energy-based models has been successfully trained with maximum likelihood (Nijkamp et al., 2019; Du and Mordatch, 2019), but training can be very computationally demanding due to the need of sampling model distribution. Variants with a truncated sampling procedure have been proposed, such as contrastive divergence (Hinton, 2002). Such models learn much faster with the draw back of not exploring the state space thoroughly (Tieleman, 2008).\n\n1.1 SCORE MATCHING, DENOISING SCORE MATCHING AND DEEP ENERGY ESTIMATORS\nScore matching (SM) (Hyv\u00e4rinen, 2005) circumvents the requirement of sampling the model distribution. In score matching, the score function is defined to be the gradient of log-density or the negative energy function. The expected L2 norm of difference between the model score function and the data score function are minimized. One convenient way of using score matching is learning the energy function corresponding to a Gaussian kernel Parzen density estimator (Parzen, 1962) of the data: p \u03c30 (x x x) = q \u03c30 (x x x|x x x)p(x x x)dx x x. Though hard to evaluate, the data score is well defined:s d (x x x) = \u2207 x\nx x log(p \u03c30 (x x x)), and the corresponding objective is:L SM (\u03b8) = E p\u03c30(x x x) \u2207 x x x log(p \u03c30 (x x x)) + \u2207 x x x E(x x x; \u03b8) 2\nVincent ( 2011) studied the connection between denoising auto-encoder and score matching, and proved the remarkable result that the following objective, named Denoising Score Matching (DSM), is equivalent to the objective above:\nL DSM (\u03b8) = E p\u03c3 0 (x x x,x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2\n(2)\nNote that in (2) the Parzen density score is replaced by the derivative of log density of the single noise kernel \u2207 x x x log(q \u03c30 (x x x|x x x)), which is much easier to evaluate. In the particular case of Gaussian noise, log(q \u03c30 (x x x|x x x)) = \u2212 (x x x\u2212x x x) 2 2\u03c3 2 0 + C, and therefore:L DSM (\u03b8) = E p\u03c30(x x x,x x x) x x x \u2212 x x x + \u03c3 0 2 \u2207 x x x E(x x x; \u03b8) 2\nThe interpretation of objective ( 3) is simple, it forces the energy gradient to align with the vector pointing from the noisy sample to the clean data sample. To optimize an objective involving the derivative of a function defined by a neural network, Kingma and LeCun (2010) proposed the use of double backpropagation (Drucker and Le Cun, 1991). Deep energy estimator networks (Saremi et al., 2018) first applied this technique to learn an energy function defined by a deep neural network.\nIn this work and similarly in Saremi and Hyvarinen (2019), an energy-based model was trained to match a Parzen density estimator of data with a certain noise magnitude. The previous models were able to perform denoising task, but they were unable to generate high-quality data samples from a random input initialization. Recently, Song and Ermon (2019) 2 A GEOMETRIC VIEW OF DENOISING SCORE MATCHING Song and Ermon (2019) used denoising score matching with a range of noise levels, achieving great empirical results. The authors explained that large noise perturbation are required to enable the learning of the score in low-data density regions. But it is still unclear why a series of different noise levels are necessary, rather than one single large noise level. Following Saremi and Hyvarinen (2019), we analyze the learning process in denoising score matching based on measure concentration properties of high-dimensional random vectors.\nWe adopt the common assumption that the data distribution to be learned is high-dimensional, but only has support around a relatively low-dimensional manifold (Tenenbaum et al., 2000; Roweis and Saul, 2000; Lawrence, 2005). If the assumption holds, it causes a problem for score matching:\nThe density, or the gradient of the density is then undefined outside the manifold, making it difficult to train a valid density model for the data distribution defined on the entire space.  Saremi and Hyvarinen (2019) and Song and Ermon (2019) discussed this problem and proposed to smooth the data distribution with a Gaussian kernel to alleviate the issue.\nTo further understand the learning in denoising score matching when the data lie on a manifold X and the data dimension is high, two elementary properties of random Gaussian vectors in highdimensional spaces are helpful: First, the length distribution of random vectors becomes concentrated at \u221a d\u03c3 (Vershynin, 2018), where \u03c3 2 is the variance of a single dimension. Second, a random vector is always close to orthogonal to a fixed vector (Tao, 2012). With these premises one can visualize the configuration of noisy and noiseless data points that enter the learning process: A data point x x x sampled from X and its noisy version x\nx x always lie on a line which is almost perpendicular to the tangent space T x x x X and intersects X at x x x. Further, the distance vectors between (x x x, x x x) pairs all have similar length \u221a d\u03c3. As a consequence, the set of noisy data points concentrate on a set X\u221a d\u03c3, that has a distance with ( \u221a d\u03c3 \u2212 , \u221a d\u03c3 + ) from the data manifold X , where \u221a d\u03c3.\nTherefore, performing denoising score matching learning with (x x x, x x x) pairs generated with a fixed noise level \u03c3, which is the approach taken previously except in Song and Ermon (2019), will match the score in the set X\u221a d\u03c3, and enable denoising of noisy points in the same set. However, the learning provides little information about the density outside this set, farther or closer to the data manifold, as noisy samples outside X\u221a d\u03c3, rarely appear in the training process. An illustration is presented in Figure 1A.Let X C \u221a d\u03c3, denote the complement of the set X\u221a d\u03c3, . Even if p \u03c30 (x x x \u2208 X C \u221a d\u03c3,\n) is very small in high-dimensional space, the score in X C \u221a d\u03c3, still plays a critical role in sampling from random initialization. This analysis may explain why models based on denoising score matching, trained with a single noise level encounter difficulties in generating data samples when initialized at random. For an empirical support of this explanation, see our experiments with models trained with single noise magnitudes (Appendix B). To remedy this problem, one has to apply a learning procedure of the sort proposed in Song and Ermon (2019), in which samples with different noise levels are used. Depending on the dimension of the data, the different noise levels have to be spaced narrowly enough to avoid empty regions in the data space. In the following, we will use Gaussian noise and employ a Gaussian scale mixture to produce the noisy data samples for the training (for details, See Section 3.1 and Appendix A).\nAnother interesting property of denoising score matching was suggested in the denoising autoencoder literature (Vincent et al., 2010; Karklin and Simoncelli, 2011). With increasing noise level, the learned features tend to have larger spatial scale. In our experiment we observe similar phenomenon when training model with denoising score matching with single noise scale. If one compare samples in Figure B.1, Appendix B, it is evident that noise level of 0.3 produced a model that learned short range correlation that spans only a few pixels, noise level of 0.6 learns longer stroke structure without coherent overall structure, and noise level of 1 learns more coherent long range structure without details such as stroke width variations. This suggests that training with single noise level in denoising score matching is not sufficient for learning a model capable of high-quality sample synthesis, as such a model have to capture data structure of all scales.\n\n3 LEARNING ENERGY-BASED MODEL WITH MULTISCALE DENOISING SCORE MATCHING\n\n\n3.1 MULTISCALE DENOISING SCORE MATCHING\nMotivated by the analysis in section 2, we strive to develop an EBM based on denoising score matching that can be trained with noisy samples in which the noise level is not fixed but drawn from a distribution. The model should approximate the Parzen density estimator of the data p \u03c30 (x x x) = q \u03c30 (x x x|x x x)p(x x x)dx. Specifically, the learning should minimize the difference between the derivative of the energy and the score of p \u03c30 under the expectation E p M (x x x) rather than E p\u03c3 0 (x x x) , the expectation taken in standard denoising score matching. Here p M (x x x) = q M (x x x|x x x)p(x x x)dx is chosen to cover the signal space more evenly to avoid the measure concentration issue described above. The resulting Multiscale Score Matching (MSM) objective is:L M SM (\u03b8) = E p M (x x x) \u2207 x x x log(p \u03c30 (x x x)) + \u2207 x x x E(x x x; \u03b8) 2 (4)\nCompared to the objective of denoising score matching (1), the only change in the new objective (4) is the expectation. Both objectives are consistent, if p M (x x x) and p \u03c30 (x x x) have the same support, as shown formally in Proposition 1 of Appendix A. In Proposition 2, we prove that Equation 4 is equivalent to the following denoising score matching objective:L M DSM * = E p M (x x x)q\u03c3 0 (x x x|x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2\nThe above results hold for any noise kernel q \u03c30 (x x x|x x x), but Equation 5 contains the reversed expectation, which is difficult to evaluate in general. To proceed, we choose q \u03c30 (x x x|x x x) to be Gaussian, and also choose q M (x x x|x x x) to be a Gaussian scale mixture:q M (x x x|x x x) = q \u03c3 (x x x|x x x)p(\u03c3)d\u03c3 and q \u03c3 (x x x|x x x) = N (x x x, \u03c3 2 I d ).\nAfter algebraic manipulation and one approximation (see the derivation following Proposition 2 in Appendix A), we can transform Equation 5 into a more convenient form, which we call Multiscale Denoising Score Matching (MDSM):L M DSM = E p(\u03c3)q\u03c3(x x x|x x x)p(x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 (6)\nThe square loss term evaluated at noisy points x x x at larger distances from the true data points x x x will have larger magnitude. Therefore, in practice it is convenient to add a monotonically decreasing term l(\u03c3) for balancing the different noise scales, e.g. l(\u03c3) = 1 \u03c3 2 . Ideally, we want our model to learn the correct gradient everywhere, so we would need to add noise of all levels. However, learning denoising score matching at very large or very small noise levels is useless. At very large noise levels the information of the original sample is completely lost. Conversely, in the limit of small noise, the noisy sample is virtually indistinguishable from real data. In neither case one can learn a gradient which is informative about the data structure. Thus, the noise range needs only to be broad enough to encourage learning of data features over all scales. Particularly, we do not sample \u03c3 but instead choose a series of fixed \u03c3 values \u03c3 4, we arrive at the final objective:1 \u2022 \u2022 \u2022 \u03c3 K . Further, substituting log(q \u03c30 (x x x|x x x)) = \u2212 (x x x\u2212x x x) 2 2\u03c3 2 0 + C into EquationL(\u03b8) = \u03c3\u2208{\u03c31\u2022\u2022\u2022\u03c3 K } E q\u03c3(x x x|x x x)p(x x x) l(\u03c3) x x x \u2212 x x x + \u03c3 2 0 \u2207 x x x E(x x x; \u03b8) 2 (7)\nIt may seem that \u03c3 0 is an important hyperparameter to our model, but after our approximation \u03c3 0 become just a scaling factor in front of the energy function, and can be simply set to one as long as the temperature range during sampling is scaled accordingly (See Section 3.2). Therefore the only hyper-parameter is the rang of noise levels used during training.\nOn the surface, objective (7) looks similar to the one in Song and Ermon (2019). The important difference is that Equation 7 approximates a single distribution, namely p \u03c30 (x x x), the data smoothed with one fixed kernel q \u03c30 (x x x|x x x). In contrast, Song and Ermon (2019) approximate the score of multiple distributions, the family of distributions {p \u03c3i (x x x) : i = 1, ..., n}, resulting from the data smoothed by kernels of different widths \u03c3 i . Because our model learns only a single target distribution, it does not require noise magnitude as input.\n\n3.2 SAMPLING BY ANNEALED LANGEVIN DYNAMICS\nLangevin dynamics has been used to sample from neural network energy functions (Du and Mordatch, 2019; Nijkamp et al., 2019). However, the studies described difficulties with mode exploration unless very large number of sampling steps is used. To improve mode exploration, we propose incorporating simulated annealing in the Langevin dynamics. Simulated annealing (Kirkpatrick et al., 1983; Neal, 2001) improves mode exploration by sampling first at high temperature and then cooling down gradually. This has been successfully applied to challenging computational problems, such as combinatorial optimization.\nTo apply simulated annealing to Langevin dynamics. Note that in a model of Brownian motion of a physical particle, the temperature in the Langevin equation enters as a factor \u221a T in front of the noise term, some literature uses \u03b2 \u22121 where \u03b2 = 1/T (Jordan et al., 1998). Adopting the \u221a T convention, the Langevin sampling process (Bellec et al., 2017) is given by:x x x t+1 = x x x t \u2212 2 2 \u2207 x x x E(x x x t ; \u03b8) + T t N (0, I d )\nwhere T t follows some annealing schedule, and denotes step length, which is fixed. During sampling, samples behave very much like physical particles under Brownian motion in a potential field.\nBecause the particles have average energies close to the their current thermic energy, they explore the state space at different distances from data manifold depending on temperature. Eventually, they settle somewhere on the data manifold. The behavior of the particle's energy value during a typical annealing process is depicted in Appendix Figure F.1B.\nIf the obtained sample is still slightly noisy, we can apply a single step gradient denoising jump (Saremi and Hyvarinen, 2019) to improve sample quality:x x x clean = x x x noisy \u2212 \u03c3 2 0 \u2207 x x x E(x x x noisy ; \u03b8)\nThis denoising procedure can be applied to noisy sample with any level of Gaussian noise because in our model the gradient automatically has the right magnitude to denoise the sample. This process is justified by the Empirical Bayes interpretation of this denoising process, as studied in Saremi and Hyvarinen (2019).\nSong and Ermon (2019) also call their sample generation process annealed Langevin dynamics. It should be noted that their sampling process does not coincide with Equation 8. Their sampling procedure is best understood as sequentially sampling a series of distributions corresponding to data distribution corrupted by different levels of noise.\nPreprint No normalization layer was used in any of the networks. We designed the output layer of all networks to take a generalized quadratic form (Fan et al., 2018).\nBecause the energy function is anticipated to be approximately quadratic with respect to the noise level, this modification was able to boost the performance significantly. For more detail on training and model architecture, see Appendix D. One notable result is that since our training method does not involve sampling, we achieved a speed up of roughly an order of magnitude compared to the maximum-likelihood training using Langevin dynamics 1. Our method thus enables the training of energy-based models even when limited computational resources prohibit maximum likelihood methods.\nWe found that the choice of the maximum noise level has little effect on learning as long as it is large enough to encourage learning of the longest range features in the data. However, as expected, learning with too small or too large noise levels is not beneficial and can even destabilize the training process. Further, our method appeared to be relatively insensitive to how the noise levels are distributed over a chosen range. Geometrically spaced noise as in (Song and Ermon, 2019) and linearly spaced noise both work, although in our case learning with linearly spaced noise was somewhat more robust.\nFor sampling the learned energy function we used annealed Langevin dynamics with an empirically optimized annealing schedule,see Figure F.1B for the particular shape of annealing schedule we used. In contrast, annealing schedules with theoretical guaranteed convergence property takes extremely long (Geman and Geman, 1984). The range of temperatures to use in the sampling process depends on the choice of \u03c3 0 , as the equilibrium distribution is roughly images with Gaussian noise of magnitude \u221a T \u03c3 0 added on top. To ease traveling between modes far apart and ensure even sampling, the initial temperature needs to be high enough to inject noise of sufficient magnitude. A choice of T = 100, which corresponds to added noise of magnitude \u221a 100 * 0.1 = 1, seems to be sufficient starting point. For step length we generally used 0.02, although any value within the range [0.015, 0.05] seemed to work fine. After the annealing process we performed a single step denoising to further enhance sample quality.\nUnconditional Image Generation. We demonstrate the generative ability of our model by displaying samples obtained by annealed Langevin sampling and single step denoising jump. We evaluated Preprint 50k sampled images after training on CIFAR-10 with two performance scores, Inception (Salimans et al., 2016) and FID (Heusel et al., 2017). We achieved Inception Score of 8.31 and FID of 31.7, comparable to modern GAN approaches. Scores for CelebA dataset are not reported here as they are not commonly reported and may depend on the specific pre-processing used. More samples and training images are provided in Appendix for visual inspection. We believe that visual assessment is still essential because of the possible issues with the Inception score (Barratt and Sharma, 2018). Indeed, we also found that the visually impressive samples were not necessarily the one achieving the highest Inception Score.\nAlthough overfitting is not a common concern for generative models, we still tested our model for overfitting. We found no indication for overfitting by comparing model samples with their nearest neighbors in the data set, see  We repeated with our model the 3 channel MNIST mode coverage experiment similar to the one in Kumar et al. (2019). An energy-based model was trained on 3-channel data where each channel is a random MNIST digit. Then 8000 samples were taken from the model and each channel was classified using a small MNIST classifier network. We obtained results of the 966 modes, comparable to GAN approaches. Training was successful and our model assigned low energy to all the learned modes, but some modes were not accessed during sampling, likely due to the Langevin Dynamics failing to explore these modes. A better sampling technique such as HMC Neal et al. (2011) or a Maximum Entropy Generator (Kumar et al., 2019) could improve this result.\nImage Inpainting. Image impainting can be achieved with our model by clamping a part of the image to ground truth and performing the same annealed Langevin and Jump sampling procedure on the missing part of the image. Noise appropriate to the sampling temperature need to be added to the clamped inputs. The quality of inpainting results of our model trained on CelebA and CIFAR-10 can be assessed in Figure 3. For CIFAR-10 inpainting results we used the test set.\nLog likelihood estimation. For energy-based models, the log density can be obtained after estimating the partition function with Annealed Importance Sampling (AIS) (Salakhutdinov and Murray, 2008) or Reverse AIS (Burda et al., 2015). In our experiment on CIFAR-10 model, similar to reports in Du and Mordatch (2019), there is still a substantial gap between AIS and Reverse AIS estimation, even after very substantial computational effort. In Table 1, we report result from Reverse AIS, as it tends to over-estimate the partition function thus underestimate the density. Note that although density values and likelihood values are not directly comparable, we list them together due to the sheer lack of a density model for CIFAR-10.\nWe also report a density of 1.21 bits/dim on MNIST dataset, and we refer readers to Du and Mordatch (2019) for comparison to other models on this dataset. Note that this number follows a different convention than the number for CIFAR-10, which consider images pixal values to lie between [0, 255]. More details on this experiment is provided in the Appendix.  2019) have reported intriguing behavior of high dimensional density models on out of distribution samples. Specifically, they showed that a lot of models assign higher likelihood to out of distribution samples than real data samples.\nWe investigated whether our model behaves similarly.\nOur energy function is only trained outside the data manifold where samples are noisy, so the energy value at clean data points may not always be well behaved. Therefore, we added noise with magnitude \u03c3 0 before measuring the energy value. We find that our network behaves similarly to previous likelihood models, it assigns lower energy, thus higher density, to some OOD samples. We show one example of this phenomenon in Appendix Figure F.1A.\nWe also attempted to use the denoising performance, or the objective function to perform outlier detection. Intriguingly, the results are similar as using the energy value. Denoising performance seems to correlate more with the variance of the original image than the content of the image.\n\n5 DISCUSSION\nIn this work we provided analyses and empirical results for understanding the limitations of learning the structure of high-dimensional data with denoising score matching. We found that the objective function confines learning to a small set due to the measure concentration phenomenon in random vectors. Therefore, sampling the learned distribution outside the set where the gradient is learned does not produce good result. One remedy to learn meaningful gradients in the entire space is to use samples during learning that are corrupted by different amounts of noise. Indeed, Song and Ermon (2019) applied this strategy very successfully.\nThe central contribution of our paper is to investigate how to use a similar learning strategy in EBMs. Specifically, we proposed a novel EBM model, the Multiscale Denoising Score Matching (MDSM) model. The new model is capable of denoising, producing high-quality samples from random noise, and performing image inpainting. While also providing density information, our model learns an order of magnitude faster than models based on maximum likelihood.\nOur approach is conceptually similar to the idea of combining denoising autoencoder and annealing (Geras and Sutton, 2015; Chandra and Sharma, 2014; Zhang and Zhang, 2018)L M SM (\u03b8) = E p M (x x x) \u2207 x x x E(x x x; \u03b8) 2 +2S(\u03b8) + C = E p M (x x x)q\u03c3 0 (x x x|x x x) \u2207 x x x E(x x x; \u03b8) 2 +2E p M (x x x)q\u03c3 0 (x x x|x x x) \u2207 x x x log q \u03c30 (x x x|x x x), \u2207 x x x E(x x x; \u03b8) + C = E p M (x x x)q\u03c30(x x x|x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 +C So L M SM (\u03b8) L M DSM * .\nThe above analysis applies to any noise distribution, not limited to Gaussian. but L M DSM * has a reversed expectation form that is not easy to work with. To proceed further we study the case where q \u03c30 (x x x|x x x) is Gaussian and choose q M (x x x|x x x) as a Gaussian scale mixture (Wainwright and Simoncelli, 2000) and p M (x x x) = q M (x x x|x x x)p(x x x)dx. By Proposition 1 and Proposition 2, we have the following form to optimize:L M DSM * (\u03b8) = x x x x x x p M (x x x)q \u03c30 (x x x|x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 dx x xdx x x = x x x x x x q \u03c30 (x x x|x x x) q M (x x x|x x x) p M (x x x)q M (x x x|x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 dx x xdx x x = x x x x x x q \u03c30 (x x x|x x x) q M (x x x|x x x) p M (x x x, x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 dx x xdx x x = x x x x x x q \u03c30 (x x x|x x x) q M (x x x|x x x) q M (x x x|x x x)p(x x x) \u2207 x x x log(q \u03c30 (x x x|x x x)) + \u2207 x x x E(x x x; \u03b8) 2 dx x xdx x x (*) \u2248 L M DSM (\u03b8)\nTo minimize Equation (*), we can use the following importance sampling procedure (Russell and Norvig, 2016): we can sample from the empirical distribution p(x x x), then sample the Gaussian scale mixture q M (x x x|x x x) and finally weight the sample by q\u03c3 0 (x x x|x x x) q M (x x x|x x x) . We expect the ratio to be close to 1 for the following reasons: Using Bayes rule, q \u03c30 (x x x|x x x) = p(x x x)q\u03c3 0 (x x x|x x x) p\u03c3 0 (x x x)\nwe can see that q \u03c30 (x x x|x x x) only has support on discret data points x x x, same thing holds for q M (x x x|x x x). because in x\nx x is generated by adding Gaussian noise to real data sample, both estimators should give results highly concentrated on the original sample point x x x. Therefore, in practice, we ignore the weighting factor and use Equation 6. Improving upon this approximation is left for future work.\n\nB PROBLEM WITH SINGLE NOISE DENOISING SCORE MATCHING\nTo compare with previous method, we trained energy-based model with denoising score matching using one noise level on MNIST, initialized the sampling with Gaussian noise of the same level, and sampled with Langevin dynamics at T = 1 for 1000 steps and perform one denoise jump to recover the model's best estimate of the clean sample, see  we used a 18-layer ResNet with 128 filters on the first layer. All network used the ELU activation function. We did not use any normalization in the ResBlocks and the filer number is doubled at each downsampling block. Details about the structure of our networks used can be found in our code release. All mentioned models can be trained on 2 GPUs within 2 days.\nSince the gradient of our energy model scales linearly with the noise, we expected our energy function to scale quadratically with noise magnitude. Therefore, we modified the standard energy-based network output layer to take a flexible quadratic form (Fan et al., 2018):E out = ( i a i h i + b 1 )( i c i h i + b 2 ) + i d i h 2 i + b 3 (12)\nwhere a i , c i , d i and b 1 , b 2 , b 3 are learnable parameters, and h i is the (flattened) output of last residual block. We found this modification to significantly improve performance compared to using a simple linear last layer.\nFor CIFAR and CelebA results we trained for 300k weight updates, saving a checkpoint every 5000 updates. We then took 1000 samples from each saved networks and used the network with the lowest Preprint FID score. For MNIST and fashion MNIST we simply trained for 100k updates and used the last checkpoint. During training we pad MNIST and Fashion MNIST to 32*32 for convenience and randomly flipped CelebA images. No other modification was performed. We only constrained the gradient of the energy function, the energy value itself could in principle be unbounded. However, we observed that they naturally stabilize so we did not explicitly regularize them. The annealing sampling schedule is optimized to improve sample quality for CIFAR-10 dataset, and consist of a total of 2700 steps. For other datasets the shape has less effect on sample quality, see Figure F.1 B for the shape of annealing schedule used.\nFor the Log likelihood estimation we initialized reverse chain on test images, then sample 10000 intermediate distribution using 10 steps HMC updates each. Temperature schedule is roughly exponential shaped and the reference distribution is an isotropic Gaussian. The variance of estimation was generally less than 10% on the log scale. Due to the high variance of results, and to avoid getting dominated by a single outlier, we report average of the log density instead of log of average density.\n\nE EXTENDED SAMPLES AND INPAINTING RESULTS\nWe provide more inpainting examples and further demonstrate the mixing during sampling process in\n\nFootnotes:\n1: For example, on a single GPU, training MNIST with a 12-layer Resnet takes 0.3s per batch with our method, while maximum likelihood training with a modest 30 Langevin step per weight update takes 3s per batch. Both methods need similar number of weight updates to train.\n2: Author reported difficulties evaluating Likelihood\n3: Upper Bound obtained by Reverse AIS\n\nReferences:\n\n- Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.- Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Invertible residual networks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 573-582, 2019.\n\n- Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136, 2017.\n\n- Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf log-likelihood using reverse annealing. In Artificial Intelligence and Statistics, pages 102-110, 2015. B Chandra and Rajesh Kumar Sharma. Adaptive noise schedule for denoising autoencoder. In International conference on neural information processing, pages 535-542. Springer, 2014.\n\n- Ricky TQ Chen, Jens Behrmann, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Residual flows for invertible generative modeling. arXiv preprint arXiv:1906.02735, 2019.\n\n- Hyunsun Choi and Eric Jang. Generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n\n- Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components es- timation. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015.\n\n- Harris Drucker and Yann Le Cun. Double backpropagation increasing generalization performance. In IJCNN-91-Seattle International Joint Conference on Neural Networks, volume 2, pages 145- 150. IEEE, 1991.\n\n- Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.\n\n- Fenglei Fan, Wenxiang Cong, and Ge Wang. A new type of neurons for machine learning. Interna- tional journal for numerical methods in biomedical engineering, 34(2):e2920, 2018.\n\n- Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, (6): 721-741, 1984.\n\n- Preprint Krzysztof J. Geras and Charles A. Sutton. Scheduled denoising autoencoders. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Con- ference Track Proceedings, 2015.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pages 2672-2680, 2014.\n\n- Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1352-1361. JMLR. org, 2017.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016a.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630-645. Springer, 2016b.\n\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626-6637, 2017.\n\n- Geoffrey E Hinton. Products of experts. 1999.\n\n- Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800, 2002.\n\n- Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(Apr):695-709, 2005.\n\n- Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker- planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.\n\n- Yan Karklin and Eero P Simoncelli. Efficient coding of natural images with a population of noisy linear-nonlinear neurons. In Advances in neural information processing systems, pages 999-1007, 2011.\n\n- Diederik P. Kingma and Yann LeCun. Regularized estimation of image statistics by score matching. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Van- couver, British Columbia, Canada., pages 1126-1134, 2010.\n\n- Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pages 10215-10224, 2018.\n\n- Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing. science, 220(4598):671-680, 1983.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n\n- Rithesh Kumar, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. Maximum entropy generators for energy-based models. arXiv preprint arXiv:1901.08508, 2019.\n\n- Neil Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent variable models. Journal of machine learning research, 6(Nov):1783-1816, 2005.\n\n- Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.\n\n- Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730-3738, 2015.\n\n- Preprint Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings, 2018.\n\n- Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan G\u00f6r\u00fcr, and Balaji Lakshminarayanan. Do deep generative models know what they don't know? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\n- Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125-139, 2001.\n\n- Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011.\n\n- Jiquan Ngiam, Zhenghao Chen, Pang W Koh, and Andrew Y Ng. Learning deep energy models. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 1105- 1112, 2011.\n\n- Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of mcmc- based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370, 2019.\n\n- Georg Ostrovski, Will Dabney, and R\u00e9mi Munos. Autoregressive quantile networks for generative modeling. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 3933-3942, 2018.\n\n- Emanuel Parzen. On estimation of a probability density function and mode. The Annals of Mathe- matical Statistics, 33(3):1065-1076, 1962.\n\n- Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed- ding. science, 290(5500):2323-2326, 2000.\n\n- Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach. Malaysia; Pearson Education Limited,, 2016.\n\n- Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning, pages 872-879. ACM, 2008. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 2234-2242, 2016.\n\n- Saeed Saremi and Aapo Hyvarinen. Neural empirical bayes. arXiv preprint arXiv:1903.02334, 2019.\n\n- Saeed Saremi, Arash Mehrjou, Bernhard Sch\u00f6lkopf, and Aapo Hyv\u00e4rinen. Deep energy estimator networks. arXiv preprint arXiv:1805.08306, 2018.\n\n- Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv preprint arXiv:1907.05600, 2019.\n\n- Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, page 204, 2019.\n\n- Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.\n\n- Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.\n\n- Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pages 1064- 1071. ACM, 2008.\n\n- Preprint A\u00e4ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1747-1756, 2016.\n\n- Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge University Press, 2018.\n\n- Pascal Vincent. A connection between score matching and denoising autoencoders. Neural compu- tation, 23(7):1661-1674, 2011.\n\n- Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(Dec):3371-3408, 2010.\n\n- Martin J Wainwright and Eero P Simoncelli. Scale mixtures of gaussians and the statistics of natural images. In Advances in neural information processing systems, pages 855-861, 2000.\n\n- Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681- 688, 2011.\n\n- Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models for anomaly detection. In Proceedings of the 33nd International Conference on Machine Learn- ing, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1100-1109, 2016.\n\n- Qianjun Zhang and Lei Zhang. Convolutional adaptive denoising autoencoders for hierarchical feature extraction. Frontiers of Computer Science, 12(6):1140-1148, 2018.\n\n", "annotations": {"ReferenceToTable": [{"begin": 22525, "end": 22526, "target": "#tab_1", "idx": 0}], "ReferenceToFootnote": [{"begin": 17981, "end": 17982, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 1597, "end": 30564, "idx": 0}], "ReferenceToFormula": [{"begin": 4331, "end": 4335, "idx": 0}, {"begin": 5045, "end": 5046, "target": "#formula_2", "idx": 1}, {"begin": 13692, "end": 13693, "idx": 2}, {"begin": 17196, "end": 17197, "target": "#formula_10", "idx": 3}, {"begin": 23169, "end": 23173, "idx": 4}, {"begin": 27616, "end": 27617, "idx": 5}], "SectionReference": [{"begin": 30944, "end": 41636, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1597, "idx": 0}], "Div": [{"begin": 109, "end": 1589, "idx": 0}, {"begin": 1600, "end": 3500, "idx": 1}, {"begin": 3502, "end": 10601, "idx": 2}, {"begin": 10603, "end": 10674, "idx": 3}, {"begin": 10676, "end": 14857, "idx": 4}, {"begin": 14859, "end": 24190, "idx": 5}, {"begin": 24192, "end": 27677, "idx": 6}, {"begin": 27679, "end": 30423, "idx": 7}, {"begin": 30425, "end": 30564, "idx": 8}], "Head": [{"begin": 1600, "end": 1629, "n": "1", "idx": 0}, {"begin": 3502, "end": 3573, "n": "1.1", "idx": 1}, {"begin": 10603, "end": 10673, "n": "3", "idx": 2}, {"begin": 10676, "end": 10715, "n": "3.1", "idx": 3}, {"begin": 14859, "end": 14901, "n": "3.2", "idx": 4}, {"begin": 24192, "end": 24204, "n": "5", "idx": 5}, {"begin": 27679, "end": 27731, "idx": 6}, {"begin": 30425, "end": 30466, "idx": 7}], "Paragraph": [{"begin": 109, "end": 1589, "idx": 0}, {"begin": 1630, "end": 2138, "idx": 1}, {"begin": 2139, "end": 3500, "idx": 2}, {"begin": 3574, "end": 4171, "idx": 3}, {"begin": 4189, "end": 4247, "idx": 4}, {"begin": 4321, "end": 4549, "idx": 5}, {"begin": 4550, "end": 4638, "idx": 6}, {"begin": 4639, "end": 4642, "idx": 7}, {"begin": 4643, "end": 4936, "idx": 8}, {"begin": 5011, "end": 5502, "idx": 9}, {"begin": 5503, "end": 6446, "idx": 10}, {"begin": 6447, "end": 6735, "idx": 11}, {"begin": 6736, "end": 7095, "idx": 12}, {"begin": 7096, "end": 7729, "idx": 13}, {"begin": 7730, "end": 8090, "idx": 14}, {"begin": 8091, "end": 8615, "idx": 15}, {"begin": 8703, "end": 9635, "idx": 16}, {"begin": 9636, "end": 10601, "idx": 17}, {"begin": 10716, "end": 11495, "idx": 18}, {"begin": 11576, "end": 11942, "idx": 19}, {"begin": 12042, "end": 12321, "idx": 20}, {"begin": 12410, "end": 12635, "idx": 21}, {"begin": 12735, "end": 13728, "idx": 22}, {"begin": 13932, "end": 14295, "idx": 23}, {"begin": 14296, "end": 14857, "idx": 24}, {"begin": 14902, "end": 15511, "idx": 25}, {"begin": 15512, "end": 15875, "idx": 26}, {"begin": 15942, "end": 16135, "idx": 27}, {"begin": 16136, "end": 16491, "idx": 28}, {"begin": 16492, "end": 16646, "idx": 29}, {"begin": 16707, "end": 17024, "idx": 30}, {"begin": 17025, "end": 17368, "idx": 31}, {"begin": 17369, "end": 17535, "idx": 32}, {"begin": 17536, "end": 18122, "idx": 33}, {"begin": 18123, "end": 18731, "idx": 34}, {"begin": 18732, "end": 19740, "idx": 35}, {"begin": 19741, "end": 20647, "idx": 36}, {"begin": 20648, "end": 21610, "idx": 37}, {"begin": 21611, "end": 22075, "idx": 38}, {"begin": 22076, "end": 22808, "idx": 39}, {"begin": 22809, "end": 23402, "idx": 40}, {"begin": 23403, "end": 23455, "idx": 41}, {"begin": 23456, "end": 23900, "idx": 42}, {"begin": 23901, "end": 24190, "idx": 43}, {"begin": 24205, "end": 24846, "idx": 44}, {"begin": 24847, "end": 25300, "idx": 45}, {"begin": 25301, "end": 25472, "idx": 46}, {"begin": 25795, "end": 26238, "idx": 47}, {"begin": 26817, "end": 27253, "idx": 48}, {"begin": 27254, "end": 27388, "idx": 49}, {"begin": 27389, "end": 27677, "idx": 50}, {"begin": 27732, "end": 28434, "idx": 51}, {"begin": 28435, "end": 28706, "idx": 52}, {"begin": 28778, "end": 29013, "idx": 53}, {"begin": 29014, "end": 29925, "idx": 54}, {"begin": 29926, "end": 30423, "idx": 55}, {"begin": 30467, "end": 30564, "idx": 56}], "ReferenceToBib": [{"begin": 1857, "end": 1879, "target": "#b51", "idx": 0}, {"begin": 1907, "end": 1926, "target": "#b54", "idx": 1}, {"begin": 1927, "end": 1947, "target": "#b5", "idx": 2}, {"begin": 1967, "end": 1989, "target": "#b35", "idx": 3}, {"begin": 1990, "end": 2012, "target": "#b8", "idx": 4}, {"begin": 2064, "end": 2087, "target": "#b53", "idx": 5}, {"begin": 2166, "end": 2186, "target": "#b28", "idx": 6}, {"begin": 2187, "end": 2206, "target": "#b34", "idx": 7}, {"begin": 2511, "end": 2536, "target": "#b12", "idx": 8}, {"begin": 2565, "end": 2584, "target": "#b6", "idx": 9}, {"begin": 2585, "end": 2611, "target": "#b23", "idx": 10}, {"begin": 2639, "end": 2666, "target": "#b48", "idx": 11}, {"begin": 2667, "end": 2690, "target": "#b36", "idx": 12}, {"begin": 2823, "end": 2837, "target": "#b17", "idx": 13}, {"begin": 2838, "end": 2860, "target": "#b13", "idx": 14}, {"begin": 2883, "end": 2903, "target": "#b26", "idx": 15}, {"begin": 2920, "end": 2943, "target": "#b8", "idx": 16}, {"begin": 3132, "end": 3154, "target": "#b35", "idx": 17}, {"begin": 3155, "end": 3177, "target": "#b8", "idx": 18}, {"begin": 3374, "end": 3388, "target": "#b18", "idx": 19}, {"begin": 3483, "end": 3499, "target": "#b47", "idx": 20}, {"begin": 3594, "end": 3611, "target": "#b19", "idx": 21}, {"begin": 4038, "end": 4052, "target": "#b37", "idx": 22}, {"begin": 5264, "end": 5287, "target": "#b22", "idx": 23}, {"begin": 5331, "end": 5357, "target": "#b7", "idx": 24}, {"begin": 5390, "end": 5411, "target": "#b42", "idx": 25}, {"begin": 5533, "end": 5560, "target": "#b41", "idx": 26}, {"begin": 5834, "end": 5855, "idx": 27}, {"begin": 5903, "end": 5924, "idx": 28}, {"begin": 6280, "end": 6307, "target": "#b41", "idx": 29}, {"begin": 6606, "end": 6630, "target": "#b46", "idx": 30}, {"begin": 6631, "end": 6653, "target": "#b38", "idx": 31}, {"begin": 6654, "end": 6669, "target": "#b27", "idx": 32}, {"begin": 6927, "end": 6954, "target": "#b41", "idx": 33}, {"begin": 6959, "end": 6980, "idx": 34}, {"begin": 7395, "end": 7412, "target": "#b49", "idx": 35}, {"begin": 7535, "end": 7546, "target": "#b45", "idx": 36}, {"begin": 8260, "end": 8281, "idx": 37}, {"begin": 9236, "end": 9257, "idx": 38}, {"begin": 9747, "end": 9769, "target": "#b51", "idx": 39}, {"begin": 9770, "end": 9799, "target": "#b21", "idx": 40}, {"begin": 14354, "end": 14375, "idx": 41}, {"begin": 14551, "end": 14572, "idx": 42}, {"begin": 14981, "end": 15004, "target": "#b8", "idx": 43}, {"begin": 15005, "end": 15026, "target": "#b35", "idx": 44}, {"begin": 15266, "end": 15292, "target": "#b24", "idx": 45}, {"begin": 15293, "end": 15304, "target": "#b32", "idx": 46}, {"begin": 15759, "end": 15780, "target": "#b20", "idx": 47}, {"begin": 15841, "end": 15862, "target": "#b2", "idx": 48}, {"begin": 16591, "end": 16619, "target": "#b41", "idx": 49}, {"begin": 16996, "end": 17023, "target": "#b41", "idx": 50}, {"begin": 17516, "end": 17534, "target": "#b9", "idx": 51}, {"begin": 18589, "end": 18611, "idx": 52}, {"begin": 19032, "end": 19055, "target": "#b10", "idx": 53}, {"begin": 20024, "end": 20047, "target": "#b40", "idx": 54}, {"begin": 20056, "end": 20077, "target": "#b16", "idx": 55}, {"begin": 20493, "end": 20519, "target": "#b0", "idx": 56}, {"begin": 20970, "end": 20989, "target": "#b26", "idx": 57}, {"begin": 21513, "end": 21531, "target": "#b33", "idx": 58}, {"begin": 21563, "end": 21583, "target": "#b26", "idx": 59}, {"begin": 22240, "end": 22272, "target": "#b40", "idx": 60}, {"begin": 22288, "end": 22308, "target": "#b3", "idx": 61}, {"begin": 22369, "end": 22391, "target": "#b8", "idx": 62}, {"begin": 22893, "end": 22915, "target": "#b8", "idx": 63}, {"begin": 24784, "end": 24805, "idx": 64}, {"begin": 25399, "end": 25423, "target": "#b11", "idx": 65}, {"begin": 25424, "end": 25449, "target": "#b3", "idx": 66}, {"begin": 25450, "end": 25472, "target": "#b55", "idx": 67}, {"begin": 26082, "end": 26115, "target": "#b52", "idx": 68}, {"begin": 28687, "end": 28704, "target": "#b9", "idx": 69}], "ReferenceString": [{"begin": 30959, "end": 31060, "id": "b0", "idx": 0}, {"begin": 31062, "end": 31332, "id": "b1", "idx": 1}, {"begin": 31336, "end": 31496, "id": "b2", "idx": 2}, {"begin": 31500, "end": 31877, "id": "b3", "idx": 3}, {"begin": 31881, "end": 32042, "id": "b4", "idx": 4}, {"begin": 32046, "end": 32163, "id": "b5", "idx": 5}, {"begin": 32167, "end": 32409, "id": "b6", "idx": 6}, {"begin": 32413, "end": 32615, "id": "b7", "idx": 7}, {"begin": 32619, "end": 32748, "id": "b8", "idx": 8}, {"begin": 32752, "end": 32928, "id": "b9", "idx": 9}, {"begin": 32932, "end": 33130, "id": "b10", "idx": 10}, {"begin": 33134, "end": 33363, "id": "b11", "idx": 11}, {"begin": 33367, "end": 33603, "id": "b12", "idx": 12}, {"begin": 33607, "end": 33843, "id": "b13", "idx": 13}, {"begin": 33847, "end": 34053, "id": "b14", "idx": 14}, {"begin": 34057, "end": 34231, "id": "b15", "idx": 15}, {"begin": 34235, "end": 34485, "id": "b16", "idx": 16}, {"begin": 34489, "end": 34534, "id": "b17", "idx": 17}, {"begin": 34538, "end": 34666, "id": "b18", "idx": 18}, {"begin": 34670, "end": 34812, "id": "b19", "idx": 19}, {"begin": 34816, "end": 34984, "id": "b20", "idx": 20}, {"begin": 34988, "end": 35186, "id": "b21", "idx": 21}, {"begin": 35190, "end": 35525, "id": "b22", "idx": 22}, {"begin": 35529, "end": 35700, "id": "b23", "idx": 23}, {"begin": 35704, "end": 35830, "id": "b24", "idx": 24}, {"begin": 35834, "end": 35963, "id": "b25", "idx": 25}, {"begin": 35967, "end": 36123, "id": "b26", "idx": 26}, {"begin": 36127, "end": 36303, "id": "b27", "idx": 27}, {"begin": 36307, "end": 36447, "id": "b28", "idx": 28}, {"begin": 36451, "end": 36641, "id": "b29", "idx": 29}, {"begin": 36645, "end": 36936, "id": "b30", "idx": 30}, {"begin": 36940, "end": 37201, "id": "b31", "idx": 31}, {"begin": 37205, "end": 37297, "id": "b32", "idx": 32}, {"begin": 37301, "end": 37408, "id": "b33", "idx": 33}, {"begin": 37412, "end": 37605, "id": "b34", "idx": 34}, {"begin": 37609, "end": 37798, "id": "b35", "idx": 35}, {"begin": 37802, "end": 38067, "id": "b36", "idx": 36}, {"begin": 38071, "end": 38208, "id": "b37", "idx": 37}, {"begin": 38212, "end": 38347, "id": "b38", "idx": 38}, {"begin": 38351, "end": 38473, "id": "b39", "idx": 39}, {"begin": 38477, "end": 38872, "id": "b40", "idx": 40}, {"begin": 38876, "end": 38971, "id": "b41", "idx": 41}, {"begin": 38975, "end": 39114, "id": "b42", "idx": 42}, {"begin": 39118, "end": 39255, "id": "b43", "idx": 43}, {"begin": 39259, "end": 39539, "id": "b44", "idx": 44}, {"begin": 39543, "end": 39633, "id": "b45", "idx": 45}, {"begin": 39637, "end": 39796, "id": "b46", "idx": 46}, {"begin": 39800, "end": 40006, "id": "b47", "idx": 47}, {"begin": 40010, "end": 40261, "id": "b48", "idx": 48}, {"begin": 40265, "end": 40407, "id": "b49", "idx": 49}, {"begin": 40411, "end": 40535, "id": "b50", "idx": 50}, {"begin": 40539, "end": 40811, "id": "b51", "idx": 51}, {"begin": 40815, "end": 40998, "id": "b52", "idx": 52}, {"begin": 41002, "end": 41194, "id": "b53", "idx": 53}, {"begin": 41198, "end": 41465, "id": "b54", "idx": 54}, {"begin": 41469, "end": 41634, "id": "b55", "idx": 55}], "Sentence": [{"begin": 109, "end": 188, "idx": 0}, {"begin": 189, "end": 354, "idx": 1}, {"begin": 355, "end": 485, "idx": 2}, {"begin": 486, "end": 537, "idx": 3}, {"begin": 538, "end": 635, "idx": 4}, {"begin": 636, "end": 783, "idx": 5}, {"begin": 784, "end": 894, "idx": 6}, {"begin": 895, "end": 1110, "idx": 7}, {"begin": 1111, "end": 1253, "idx": 8}, {"begin": 1254, "end": 1352, "idx": 9}, {"begin": 1353, "end": 1489, "idx": 10}, {"begin": 1490, "end": 1589, "idx": 11}, {"begin": 1630, "end": 2138, "idx": 12}, {"begin": 2139, "end": 2356, "idx": 13}, {"begin": 2357, "end": 2463, "idx": 14}, {"begin": 2464, "end": 2740, "idx": 15}, {"begin": 2741, "end": 2944, "idx": 16}, {"begin": 2945, "end": 3047, "idx": 17}, {"begin": 3048, "end": 3277, "idx": 18}, {"begin": 3278, "end": 3389, "idx": 19}, {"begin": 3390, "end": 3500, "idx": 20}, {"begin": 3574, "end": 3675, "idx": 21}, {"begin": 3676, "end": 3791, "idx": 22}, {"begin": 3792, "end": 3902, "idx": 23}, {"begin": 3903, "end": 4114, "idx": 24}, {"begin": 4115, "end": 4171, "idx": 25}, {"begin": 4189, "end": 4247, "idx": 26}, {"begin": 4321, "end": 4549, "idx": 27}, {"begin": 4550, "end": 4638, "idx": 28}, {"begin": 4639, "end": 4642, "idx": 29}, {"begin": 4643, "end": 4823, "idx": 30}, {"begin": 4824, "end": 4936, "idx": 31}, {"begin": 5011, "end": 5170, "idx": 32}, {"begin": 5171, "end": 5358, "idx": 33}, {"begin": 5359, "end": 5502, "idx": 34}, {"begin": 5503, "end": 5671, "idx": 35}, {"begin": 5672, "end": 5823, "idx": 36}, {"begin": 5824, "end": 6019, "idx": 37}, {"begin": 6020, "end": 6149, "idx": 38}, {"begin": 6150, "end": 6269, "idx": 39}, {"begin": 6270, "end": 6446, "idx": 40}, {"begin": 6447, "end": 6670, "idx": 41}, {"begin": 6671, "end": 6735, "idx": 42}, {"begin": 6736, "end": 6925, "idx": 43}, {"begin": 6926, "end": 7095, "idx": 44}, {"begin": 7096, "end": 7462, "idx": 45}, {"begin": 7463, "end": 7547, "idx": 46}, {"begin": 7548, "end": 7729, "idx": 47}, {"begin": 7730, "end": 7842, "idx": 48}, {"begin": 7843, "end": 7931, "idx": 49}, {"begin": 7932, "end": 8090, "idx": 50}, {"begin": 8091, "end": 8375, "idx": 51}, {"begin": 8376, "end": 8572, "idx": 52}, {"begin": 8573, "end": 8615, "idx": 53}, {"begin": 8703, "end": 8836, "idx": 54}, {"begin": 8837, "end": 9020, "idx": 55}, {"begin": 9021, "end": 9149, "idx": 56}, {"begin": 9150, "end": 9313, "idx": 57}, {"begin": 9314, "end": 9456, "idx": 58}, {"begin": 9457, "end": 9635, "idx": 59}, {"begin": 9636, "end": 9800, "idx": 60}, {"begin": 9801, "end": 9885, "idx": 61}, {"begin": 9886, "end": 10008, "idx": 62}, {"begin": 10009, "end": 10378, "idx": 63}, {"begin": 10379, "end": 10601, "idx": 64}, {"begin": 10716, "end": 10925, "idx": 65}, {"begin": 10926, "end": 11040, "idx": 66}, {"begin": 11041, "end": 11282, "idx": 67}, {"begin": 11283, "end": 11435, "idx": 68}, {"begin": 11436, "end": 11495, "idx": 69}, {"begin": 11576, "end": 11695, "idx": 70}, {"begin": 11696, "end": 11942, "idx": 71}, {"begin": 12042, "end": 12198, "idx": 72}, {"begin": 12199, "end": 12321, "idx": 73}, {"begin": 12410, "end": 12635, "idx": 74}, {"begin": 12735, "end": 12867, "idx": 75}, {"begin": 12868, "end": 12998, "idx": 76}, {"begin": 12999, "end": 13013, "idx": 77}, {"begin": 13014, "end": 13127, "idx": 78}, {"begin": 13128, "end": 13223, "idx": 79}, {"begin": 13224, "end": 13309, "idx": 80}, {"begin": 13310, "end": 13414, "idx": 81}, {"begin": 13415, "end": 13502, "idx": 82}, {"begin": 13503, "end": 13610, "idx": 83}, {"begin": 13611, "end": 13728, "idx": 84}, {"begin": 13932, "end": 14210, "idx": 85}, {"begin": 14211, "end": 14295, "idx": 86}, {"begin": 14296, "end": 14376, "idx": 87}, {"begin": 14377, "end": 14537, "idx": 88}, {"begin": 14538, "end": 14751, "idx": 89}, {"begin": 14752, "end": 14857, "idx": 90}, {"begin": 14902, "end": 15027, "idx": 91}, {"begin": 15028, "end": 15145, "idx": 92}, {"begin": 15146, "end": 15245, "idx": 93}, {"begin": 15246, "end": 15401, "idx": 94}, {"begin": 15402, "end": 15511, "idx": 95}, {"begin": 15512, "end": 15562, "idx": 96}, {"begin": 15563, "end": 15781, "idx": 97}, {"begin": 15782, "end": 15875, "idx": 98}, {"begin": 15942, "end": 16025, "idx": 99}, {"begin": 16026, "end": 16135, "idx": 100}, {"begin": 16136, "end": 16319, "idx": 101}, {"begin": 16320, "end": 16375, "idx": 102}, {"begin": 16376, "end": 16491, "idx": 103}, {"begin": 16492, "end": 16646, "idx": 104}, {"begin": 16707, "end": 16890, "idx": 105}, {"begin": 16891, "end": 17024, "idx": 106}, {"begin": 17025, "end": 17116, "idx": 107}, {"begin": 17117, "end": 17368, "idx": 108}, {"begin": 17369, "end": 17433, "idx": 109}, {"begin": 17434, "end": 17535, "idx": 110}, {"begin": 17536, "end": 17708, "idx": 111}, {"begin": 17709, "end": 17983, "idx": 112}, {"begin": 17984, "end": 18122, "idx": 113}, {"begin": 18123, "end": 18299, "idx": 114}, {"begin": 18300, "end": 18436, "idx": 115}, {"begin": 18437, "end": 18555, "idx": 116}, {"begin": 18556, "end": 18731, "idx": 117}, {"begin": 18732, "end": 18928, "idx": 118}, {"begin": 18929, "end": 19056, "idx": 119}, {"begin": 19057, "end": 19249, "idx": 120}, {"begin": 19250, "end": 19406, "idx": 121}, {"begin": 19407, "end": 19529, "idx": 122}, {"begin": 19530, "end": 19640, "idx": 123}, {"begin": 19641, "end": 19740, "idx": 124}, {"begin": 19741, "end": 19772, "idx": 125}, {"begin": 19773, "end": 19916, "idx": 126}, {"begin": 19917, "end": 20078, "idx": 127}, {"begin": 20079, "end": 20131, "idx": 128}, {"begin": 20132, "end": 20168, "idx": 129}, {"begin": 20169, "end": 20302, "idx": 130}, {"begin": 20303, "end": 20383, "idx": 131}, {"begin": 20384, "end": 20520, "idx": 132}, {"begin": 20521, "end": 20647, "idx": 133}, {"begin": 20648, "end": 20758, "idx": 134}, {"begin": 20759, "end": 20990, "idx": 135}, {"begin": 20991, "end": 21086, "idx": 136}, {"begin": 21087, "end": 21202, "idx": 137}, {"begin": 21203, "end": 21270, "idx": 138}, {"begin": 21271, "end": 21472, "idx": 139}, {"begin": 21473, "end": 21610, "idx": 140}, {"begin": 21611, "end": 21628, "idx": 141}, {"begin": 21629, "end": 21828, "idx": 142}, {"begin": 21829, "end": 21914, "idx": 143}, {"begin": 21915, "end": 22021, "idx": 144}, {"begin": 22022, "end": 22075, "idx": 145}, {"begin": 22076, "end": 22102, "idx": 146}, {"begin": 22103, "end": 22309, "idx": 147}, {"begin": 22310, "end": 22515, "idx": 148}, {"begin": 22516, "end": 22646, "idx": 149}, {"begin": 22647, "end": 22808, "idx": 150}, {"begin": 22809, "end": 22963, "idx": 151}, {"begin": 22964, "end": 23106, "idx": 152}, {"begin": 23107, "end": 23167, "idx": 153}, {"begin": 23168, "end": 23275, "idx": 154}, {"begin": 23276, "end": 23402, "idx": 155}, {"begin": 23403, "end": 23455, "idx": 156}, {"begin": 23456, "end": 23615, "idx": 157}, {"begin": 23616, "end": 23695, "idx": 158}, {"begin": 23696, "end": 23836, "idx": 159}, {"begin": 23837, "end": 23900, "idx": 160}, {"begin": 23901, "end": 24008, "idx": 161}, {"begin": 24009, "end": 24073, "idx": 162}, {"begin": 24074, "end": 24190, "idx": 163}, {"begin": 24205, "end": 24376, "idx": 164}, {"begin": 24377, "end": 24509, "idx": 165}, {"begin": 24510, "end": 24630, "idx": 166}, {"begin": 24631, "end": 24775, "idx": 167}, {"begin": 24776, "end": 24846, "idx": 168}, {"begin": 24847, "end": 24950, "idx": 169}, {"begin": 24951, "end": 25049, "idx": 170}, {"begin": 25050, "end": 25171, "idx": 171}, {"begin": 25172, "end": 25300, "idx": 172}, {"begin": 25301, "end": 25472, "idx": 173}, {"begin": 25795, "end": 25873, "idx": 174}, {"begin": 25874, "end": 25950, "idx": 175}, {"begin": 25951, "end": 26162, "idx": 176}, {"begin": 26163, "end": 26238, "idx": 177}, {"begin": 26817, "end": 27110, "idx": 178}, {"begin": 27111, "end": 27253, "idx": 179}, {"begin": 27254, "end": 27388, "idx": 180}, {"begin": 27389, "end": 27543, "idx": 181}, {"begin": 27544, "end": 27677, "idx": 182}, {"begin": 27732, "end": 28134, "idx": 183}, {"begin": 28135, "end": 28180, "idx": 184}, {"begin": 28181, "end": 28290, "idx": 185}, {"begin": 28291, "end": 28373, "idx": 186}, {"begin": 28374, "end": 28434, "idx": 187}, {"begin": 28435, "end": 28582, "idx": 188}, {"begin": 28583, "end": 28706, "idx": 189}, {"begin": 28778, "end": 28903, "idx": 190}, {"begin": 28904, "end": 29013, "idx": 191}, {"begin": 29014, "end": 29118, "idx": 192}, {"begin": 29119, "end": 29226, "idx": 193}, {"begin": 29227, "end": 29319, "idx": 194}, {"begin": 29320, "end": 29427, "idx": 195}, {"begin": 29428, "end": 29464, "idx": 196}, {"begin": 29465, "end": 29578, "idx": 197}, {"begin": 29579, "end": 29671, "idx": 198}, {"begin": 29672, "end": 29802, "idx": 199}, {"begin": 29803, "end": 29925, "idx": 200}, {"begin": 29926, "end": 30081, "idx": 201}, {"begin": 30082, "end": 30189, "idx": 202}, {"begin": 30190, "end": 30262, "idx": 203}, {"begin": 30263, "end": 30423, "idx": 204}, {"begin": 30467, "end": 30564, "idx": 205}], "ReferenceToFigure": [{"begin": 8612, "end": 8614, "target": "#fig_8", "idx": 0}, {"begin": 10042, "end": 10043, "idx": 1}, {"begin": 22019, "end": 22020, "target": "#fig_4", "idx": 2}], "Abstract": [{"begin": 99, "end": 1589, "idx": 0}], "SectionFootnote": [{"begin": 30566, "end": 30942, "idx": 0}], "Footnote": [{"begin": 30577, "end": 30849, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 30850, "end": 30903, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 30904, "end": 30942, "id": "foot_2", "n": "3", "idx": 2}]}}