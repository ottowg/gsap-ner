{"text": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\nAbstract:\nWe challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.\n\nMain:\n\n\n\n1 Introduction\nFrom CNNs [57] to Transformers [90], most of supervised deep learning relies on parametric modeling: models learn parameters \u03b8 from a set of training data D train = {(x 1 , y 1 ), . . . , (x n , y n )} to maximize training likelihoods p(y | x; \u03b8) mapping from features x \u2208 X to target values y \u2208 Y. At test time, they then make a prediction p(y * | x * ; \u03b8) that depends only on those parameters \u03b8 and the test input x * . That is, parametric models do not consider direct dependencies between datapoints. This paper challenges parametric modeling as the dominant paradigm in deep learning. Based on the same end-to-end learning motivations that underpin deep learning itself, we consider giving models the additional flexibility of using training data directly when making predictions p(y * | x * , D train ; \u03b8).\nConcretely, we introduce Non-Parametric Transformers (NPTs): a general deep learning architecture that takes the entire dataset as input and predicts by explicitly learning interactions between datapoints (Fig. 1). NPTs leverage both parametric and non-parametric predictive mechanisms, with the use of end-to-end training allowing the model to naturally learn from the data how to balance the two. Namely, instead of just learning predictive functions from the features to the targets of independent datapoints, NPTs can also learn to reason about general relationships between inputs. We use multi-head self-attention [4, 59, 90] to model relationships between datapoints and construct a training objective for NPTs with a stochastic masking mechanism inspired by self-supervised reconstruction tasks in natural language processing [24]. We show that these models learn to look up information from other datapoints and capture the causal mechanism generating the data in semi-synthetic settings. However, unlike conventional non-parametric models, NPTs are not forced to only make predictions in this way: they can also use the power of ordinary parametric deep learning.\nBackground. While questioning parametric modeling assumptions is unconventional in deep learning, in statistics, so-called non-parametric models are a well-known and long-established field of study. Non-parametric models make predictions in explicit dependence of the training data p(y * | x * , D train ). The most popular example of such models in the machine learning community are perhaps Gaussian Processes [74]. Non-parametric models typically do not require any training of parameters, and instead often directly interpolate between training points according to a fixed procedure, e.g., [74, p.17]. The interactions between inputs are fully defined by architectural choices and a small set of hyperparameters that must be carefully chosen. Conventional non-parametric models cannot learn -in the sense familiar to deep learning practitioners -interactions from the data, limiting the flexibility these models have in adapting to the data at hand. Approaches such as Deep Gaussian Processes [22], Deep Kernel Learning [95], and Neural Processes [36, 37, 49] have all sought to apply ideas from deep neural networks to non-parametrics. Compared to NPTs, these approaches rely heavily on motivations from stochastic processes. This leads to them being either less flexible than NPTs or requiring strong assumptions on the data, making them inapplicable to the practical scenarios considered in this paper (cf. \u00a73). Unlike previous work, NPTs explicitly learn to predict from interactions between datapoints, and they can be applied to general supervised machine learning tasks. We refer to \u00a73 for an overview of these and other related approaches.\nA key contribution of this paper is opening the door to a more general treatment of how deep learning models can make use of dependencies between datapoints for predictions. Our results demonstrate that NPTs make use of interactions between datapoints in practice, and we show highly competitive performance on several established tabular datasets as well as early image classification results. Additionally, we show that NPTs can solve complex reasoning tasks by combining representation learning and cross-datapoint lookup; something that is impossible for conventional deep learning or non-parametric models due to their inability to learn relations between datapoints.\nWe next discuss the specifics of our model ( \u00a72), before moving on to related work ( \u00a73), empirical results ( \u00a74), and finally, limitations, future work, and conclusions ( \u00a75).\n\n2 Non-Parametric Transformers\nNon-Parametric Transformers (NPTs) explicitly learn relationships between datapoints to improve predictions. To accomplish this, they rely on three main ingredients: (1) We provide the model with the entire dataset -all datapoints -as input. We approximate this with minibatches where necessary for large data. At test time, both training and test data are input to the model; during training, the model learns to predict targets from the training data ( \u00a72.6).  (2) We use self-attention between datapoints to explicitly model relationships between datapoints. For example, at test time, the attention mechanism models relationships amongst training points, amongst test points, and between the two. (3) NPT's training objective is to reconstruct a corrupted version of the input dataset. Similar to BERT [24], we apply stochastic masking to inputs and minimize a loss on predictions at entries masked out in the input. Next, we introduce the three components in detail. Between Attributes (ABA, \u00a72.5) then attends between the attributes for each datapoint independently. We repeat steps (c) and (d) and obtain a final prediction from a separate linear projection (not shown).\n\n2.1 Datasets as Inputs\nNPTs take as input the entire dataset X \u2208 R n\u00d7d . The datapoints are stacked as the rows of this matrix {X i,: \u2208 R d | i \u2208 1 . . . n}, and we refer to the columns as attributes {X :,j \u2208 R n | j \u2208 1 . . . d}. Each attribute is assumed to share a semantic meaning among all datapoints. In single-target classification and regression, we assume that the targets (labels) are the final attribute X :,d , and the other attributes {X :,j | j = d} are input features, e.g., the pixels of an image. Each X i,j is an entry or value. In addition to tabular data, many modalities such as images, graphs, or timeseries can be reshaped to fit this format. Note that this is a departure from common notation for supervised learning as introduced in \u00a71, as the input X now includes both features and targets (collectively, attributes).\nIn masked language modeling [24], mask tokens denote which words in a sentence are unknown and where, at training time, model predictions will have a loss backpropagated. Analogously, we use a binary matrix M \u2208 R n\u00d7d to specify which entries are masked in the input X. This matrix is also passed to NPT as input. The task is to predict the masked values X M = {X i,j | M i,j = 1} from the observed values X O = {X i,j | M i,j = 0}, i.e., to predict p(X M | X O ).\nIn summary, NPT takes as input the entire dataset and masking matrix (X, M ), and makes predictions X \u2208 R n\u00d7d for values masked at input. This general setup accommodates many machine learning settings simply by adjusting the placement of the binary masks in M . We focus on single-target classification and regression -corresponding to a masking matrix M with 1s at all entries of the label column X :,d -but outline multi-target settings, imputation, self-supervision using input features, and semi-supervision in Appendix C. 4. Next, we describe the NPT architecture.\n\n2.2 NPT Architecture\nAn overview of the Non-Parametric Transformer (NPT) is depicted in Fig. 2. NPT receives the dataset and masking matrix (X, M ) as input (Fig. 2a). We stack these and apply an identical linear embedding to each of n datapoints, obtaining an input representation H (0) \u2208 R n\u00d7d\u00d7e (Fig. 2b). Next, we apply a sequence of multi-head self-attention layers [4, 24, 90]. Crucially, we alternatingly apply attention between datapoints and attention between attributes of individual datapoints (Figs.  2c-d).\nThese operations allow our model to learn both relationships between datapoints as well as transformations of individual datapoints. Finally, an output embedding gives the prediction X \u2208 R n\u00d7d , which now has predicted values at entries that were masked at input. We refer to Appendix C. 3 for details, such as treatment of categorical and continuous variables. Importantly: Property 1. NPTs are equivariant to a permutation of the datapoints. (cf. Appendix A for proof.)\nIn other words, if the set of input datapoints is shuffled, NPTs produce the same prediction but shuffled in an analogous manner. This explicitly encodes the assumption that the learned relations between datapoints should not depend on their ordering. At a high level, permutation-equivariance holds because all components of NPTs are permutation-equivariant, and the composition of permutationequivariant functions is itself permutation-equivariant. We now briefly recap multi-head self-attention which plays an important role throughout the NPT architecture.\n\n2.3 Multi-Head Self-Attention\nMulti-head self-attention (MHSA) is a powerful mechanism for learning complex interactions between elements in an input sequence. Popularized in natural language processing [4, 24, 90], MHSA-based models have since been successfully applied to many areas of machine learning (cf. \u00a73).\n\nDot-product attention computes attention weights by comparing queries {Q\ni \u2208 R 1\u00d7h k | i \u2208 1 . . . n} with keys {K i \u2208 R 1\u00d7h k | i \u2208 1 .. . m}, ultimately updating the representation of the queries by aggregating over values {V i \u2208 R 1\u00d7hv | i \u2208 1 . . . m} via the attention weights. We stack the queries, keys, and values into matrices Q \u2208 R n\u00d7h k , K \u2208 R m\u00d7h k , and V \u2208 R m\u00d7hv and, as is commonly done for convenience, assume h k = h v = h. Then, we compute dot-product attention asAtt(Q, K, V ) = softmax(QK T / \u221a h)V .\nMulti-head dot-product attention concatenates a series of k independent attention headsMHAtt(Q, K, V ) = concat axis=h (O 1 , . . . , O k )W O , whereO j = Att(QW Q j , KW K j , V W V j ).\nWe learn embedding matrices W Q j , W K j , W V j \u2208 R h\u00d7h/k , j \u2208 {1, . . . , k} for each head j, and W O \u2208 R h\u00d7h mixes outputs from different heads. Here, we focus on multi-head self -attention, MHSelfAtt(H) = MHAtt(Q = H, K = H, V = H), which uses the same inputs for queries, keys, and values. Following Transformer best practices to improve performance [16, 24, 59, 66, 90], we first add a residual branch and apply Layer Normalization (LN) [3] followed by MHSelfAtt(\u2022),Res(H) = HW res + MHSelfAtt(LN(H)),\nwith learnable weight matrix W res \u2208 R h\u00d7h . Then, we add another residual branch with LN and a row-wise feed-forward network (rFF), finally giving the full multi-head self-attention layer asMHSA(H) = Res(H) + rFF(LN(Res(H)) \u2208 R n\u00d7h .\n\n2.4 Attention Between Datapoints (ABD)\nThe Attention Between Datapoints (ABD) layer is a key operation for NPT. It explicitly transforms data by reasoning about pairwise relationships between all datapoints, see Fig. 2c. As input to ABD, we flatten the output of the previous layer H ( ) from R n\u00d7d\u00d7e to R n\u00d7h with h = d \u2022 e. Then, we apply MHSA(\u2022) between the intermediate datapoint representations {H( ) i \u2208 R 1\u00d7h | i \u2208 1 . . . n} as ABD(H ( ) ) = MHSA(H ( ) ) = H ( +1) \u2208 R n\u00d7h .\nAt the first ABD layer, we input H (0) \u2208 R n\u00d7d\u00d7e , the linearly embedded input data. After applying ABD, we reshape the output again, from R n\u00d7h to R n\u00d7d\u00d7e . Here, the rFF of each ABD layer is an MLP that is applied independently to each of the n datapoints.\nNote that this is distinct from how MHSA(\u2022) is usually applied in the literature, as we compute attention between different datapoints and not between the features of a single datapoint [24, 25, 46, 90]. For example, in natural language processing, attention is usually applied between the tokens (attributes) of a sentence (datapoint) but not between different sentences. For example, NPT could learn to attend between two datapoints with indices i and i by embedding Q i and K i in close proximity. Following (1), datapoint i will then attend more closely to i because Q i K T i will be large. By stacking many ABD layers, NPT can learn higher-order interactions between datapoints [24, 90].\n\n2.5 Attention Between Attributes (ABA)\nWe now introduce Attention Between Attributes (ABA), which we by default perform after each ABD layer. ABA layers can help the model learn better per-datapoint representations for the between-datapoint interactions, see Fig. 2d. For ABA, we apply MHSA(\u2022) independently to each row (corresponding to a single datapoint) in the input H( ) i \u2208 R d\u00d7e , i \u2208 {1, . . . , n}, giving ABA(H ( ) ) = stack axis=n (MHSA(H ( ) 1 ), . . . , MHSA(H ( ) n )) = H ( +1) \u2208 R n\u00d7d\u00d7e .\nJust like in standard Transformers [24, 25, 46, 90], ABA is used to transform attribute representations of single datapoints independently. We batch over the n dimension to compute ABA efficiently. By alternating between attention between datapoints (ABD) and attributes (ABA), NPTs can model both complex dependencies between points as well as learn suitable transformations of datapoints individually. Next, we describe the use of masking mechanisms during NPT training and evaluation.\n\n2.6 Masking and Optimization\nMasking. Much like in masked language modeling [24], we use masks to indicate which values NPT is expected to predict, and to prevent the model from accessing ground truth values. Recall that NPT needs to predict p(X M | X O ), with masked valuesX M = {X i,j | M i,j = 1} and observed values X O = {X i,j | M i,j = 0}.\nMasked values can be either features or targets. Canonically, masked language modeling is used to perform self-supervised learning on a sequence of tokens in a sentence [24]. We use such stochastic feature masking to mask feature values X i,j , j = d, with probability p feature during training. We also apply stochastic masking to the targets of the training set X :,d with probability p target . We call this stochastic target masking. Note that we take great care to avoid test set leakage and never reveal targets of the test set to NPT. We refer to Appendix C. 4 for full details of our masking procedure in a variety of settings.\nNPT Objective. During training, we compute the negative log-likelihood loss at training targets L Targets as well as the auxiliary loss from masked-out features L Features . We write the NPT training objective as L NPT = (1 \u2212 \u03bb)L Targets + \u03bbL Features , where \u03bb is a hyperparameter. At test time, we only mask and compute a loss over the targets of test points. See Appendix C.5 for optimization details.\nThis objective has a few notable elements. Feature masking requires NPTs to make predictions over all attributes, encouraging the models to learn a representation of the entire dataset. This increases the difficulty of the task and adds more supervision, which we find tends to have a beneficial regularizing effect. Interestingly, stochastic target masking means that many training targets are unmasked to the model at training time. This allows NPTs to learn to predict the masked targets of certain training datapoints using the targets of other training datapoints in addition to all input features. 2 NPTs no longer have to memorize a mapping between training inputs and outputs in their parameters \u03b8, and can instead use their representational capacity to learn functions using other training features and targets as input. For example, NPTs could learn to assign test datapoints to clusters of training datapoints, and predict on those points using interpolation of the training targets in their respective cluster. We explore the ability of NPTs to solve such tasks in \u00a74.2. Further, we study more complex extensions to these tasks, which cannot be solved by simple interpolative models, in Appendix B.1.2.\nHandling Large Datasets. Due to the poor O(n 2 ) time and space complexity of self-attention, we resort to approximations once the data grows too large. For example, we reach 24 GB of GPU memory for standard NPT model sizes at about 8000 datapoints. We find that processing the data in random subsets for model training and prediction, i.e., minibatching, is a simple and effective solution. We construct minibatches such that, at test time, training and test data are both present in the same batch, to allow NPTs to attend to training datapoints. In \u00a74.3, we show that NPTs make use of attention between datapoints with minibatching enabled. See \u00a75 for further discussion and ideas for future work.\n\n3 Related Work\nDeep Non-Parametric Models. Deep Gaussian Processes [22] and Deep Kernel Learning (DKL) [95] extend ideas from Gaussian Processes [74] to representation learning. Deep GPs stack standard GPs with the aim to learn more expressive relationships between input points, sharing motivation with NPTs. However, unlike NPTs, deep GPs are difficult to work with in practice, requiring complex approximate inference schemes [13, 21, 77]. DKL applies a neural network to each datapoint independently before passing points on to a standard Gaussian Process, making predictions based directly on similarity in embedding space instead of learning the interactions themselves.\nNeural Processes. Similar to GPs, Neural Processes (NPs) [36, 37] define a distribution over functions. They use a latent variable model parametrized by neural networks, fulfilling specific architectural constraints to approximately preserve consistency of finite-dimensional marginals. Attentive Neural Processes (ANPs) [49] extend Neural Processes to allow for direct attention between a context set and targets. However, as the authors themselves stress, \"NPs and GPs have different training regimes\" [49]. While a GP can be trained on a single dataset, (A)NPs require multiple realizations of the dataset. The authors further note that \"a direct comparison between the two is usually not plausible\" [49], which is why we cannot compare (A)NPs to NPTs on our standard tasks.\nAttention. NPTs are part of a line of recent work that explores the use of Transformer-based architectures outside of natural language processing, e.g., Transformers in computer vision [25, 46, 67] or architectures exploiting desirable invariances or equivariances [33, 44, 59, 61]. Like NPTs, Set Transformer [59] attends to a set of input points. However, unlike NPTs, Set Transformer relies on the existence of multiple independent sets for training and makes only a single prediction for each set. Like NPTs, Axial Transformers [42] and MSA Transformers [73] attend to multiple dimensions of matrix-shaped input. However, Axial Transformers process single images as input, i.e., no attention across datapoints is performed. MSA Transformers use attention within individual protein sequences and across an aligned protein family for contact prediction, but do not consider a more general setting. Recent works have improved neural network performance on tabular data using attention.\nAutoInt [80] is a direct application of multi-head attention to tabular data, and TabNet [2] sequentially attends to sparse subsets of the features inspired by tree-based models. Both approaches do not reason about interactions between datapoints, a key contribution that we introduce with NPT in this work.\nFew-Shot Learning, Meta-Learning, and Prompting. In \u00a74.2, we apply NPTs to tasks that require learning of relational structure between datapoints on training data to achieve good generalization performance on novel test inputs. This setup shares motivations with meta-learning [6, 8, 29, 56], in which a model is pre-trained on a variety of tasks, such that it can then learn new tasks using only a small number of additional training points from the new task. However, we consider evaluation without any additional gradient updates, unlike recent meta-learning methods [29, 97] which are therefore inapplicable to this setting. Recent works on few-shot learning with text prompting [12, 72] provide a trained Transformer-based language model with a few examples of a novel relationship in a prompt at prediction time, where they observe strong generalization on the task. Similarly, we consider attention between a \"context\" of datapoints. While ground-truth input-output pairs are provided for prompting, we consider settings in which no ground-truth is given at prediction time (cf. Appendix B.1.2), but the model can solve the task if it has learned the underlying relational structure.\nSemi-Supervised Learning and Graph Neural Networks. NPTs relate to work on semi-supervised learning [15, 27, 51] and transductive learning [89], which both make use of unlabeled inputs during training. NPTs natively support this by simply including any unlabeled datapoints with masked-out targets in the input matrix at training time. This body of related work includes semi-supervised and transductive learning on graphs using graph neural networks (GNNs), e.g., [34, 52, 53, 91, 96]. NPTs can be seen as a generalization of GNNs in which a set of dependencies (edges) between datapoints is not known a priori and is instead learned from data using self-attention. Like NPTs, Neural Relational Inference (NRI) [53] attempts to discover relations amongst datapoints. However, NRI lacks scalability because it requires that embeddings be stored for each potential graph edge.\nMetric Learning. (Deep) Metric Learning aims to learn distance functions such that the (semantic) similarity and dissimilarity between input points is meaningfully captured, e.g., [65, 76, 79, [92] [93] [94].\nSimilarly, retrieval models in NLP learn to look up relevant training instances for prediction [38, 39, 41]. The attention between datapoints in NPTs can be seen as implicitly learning exactly such (dis-)similarity. Usually, metric learning embeds inputs by applying the same embedding function independently to each datapoint. This is in contrast to NPTs, which leverage a learned self-attention mechanism between test inputs and training datapoints (including their labels) at prediction time.\n\n4 Experiments\nWe seek to answer the following set of questions in our evaluation 3 of NPTs: (Q1) How do NPTs perform on standard benchmarks for supervised machine learning? (Q2) Can NPTs successfully model interactions between datapoints in idealized settings? (Q3) Do NPTs actually learn to rely on interactions between datapoints for prediction on real-world datasets? (Q4) If so, what is the nature of these interactions, e.g., which other datapoints are relevant for prediction? To answer (Q1), we evaluate NPTs on tabular data from the UCI Repository [26] as well as the CIFAR-10 [55] and MNIST [58] image classification datasets. Tabular data is ubiquitous in real-world machine learning [20] but notoriously challenging for general purpose deep neural networks, which are rarely used in practice here because they are consistently outperformed by boosting models [78]. 4 abular Datasets, Setup, and Baselines. We evaluate NPTs over 10 datasets varying across the number of datapoints, number of features, composition (categorical or continuous) of features, and task. 4 of the 10 are binary classification, 2 are multi-class classification, and 4 are regression. We compare NPT against a wide set of standard or state-of-the-art baselines: Random Forests [10], Gradient Boosting Trees [32], XGBoost [17], CatBoost [71], LightGBM [48], MLPs, k-NN [1, 30], and TabNet [2]. For additional background on tree-based models, see Appendix D.1. We tune the parameters of all models on validation sets and use 10-fold cross-validation whenever computationally feasible. Note that while we perform an extensive grid search for the baselines, we only search over a small set of configurations for NPTs. We refer the reader to Appendix E for further details on the setup for datasets and baselines, and Appendix C.1 for NPT hyperparameters.\nTabular Data Results. We report the average rank order for NPT and various tree-based and deep learning baselines in Table 1. NPT achieves the highest average ranking on binary and multi-class classification tasks, outperforming CatBoost and XGBoost, two popular state-of-the-art boosting methods designed specifically for tabular data. On regression tasks, NPT ties in average rank with XGBoost, and is outperformed only by CatBoost. In addition to its strong rank-wise performance, NPT achieves best performance on 4 of the 10 benchmark datasets -more than any other method. We find that these are remarkable results for a general purpose model that does not include tabular-specific design, supporting our hypothesis that attention between datapoints is a useful architectural inductive bias for prediction. For all metrics across all datasets, i.e., NLL for classification, AUROC/accuracy for binary/multi-class classification, and (R)MSE for regression, we refer the reader to Appendix B.7.\nIn the appendix, we present ablations which suggest that the performance of NPT is robust across a wide range of hyperparameter choices (Appendix B.4) and that both the introduction of the ABA layer and the stochastic feature masking contribute positively to the performance of NPTs (Appendix B.5).\nImage Data Results. On CIFAR-10, we replace our linear encoder with a CNN followed by ABD layers on the CNN encodings, achieving a test accuracy of 93.7%. We achieve 98.3% accuracy on MNIST using linear patching [25]. Crucially, we show in \u00a74.3 that NPTs learn to make use of interactions between images on both the CIFAR-10 and MNIST datasets, supporting the claim that attention between datapoints is useful beyond tabular data. We also explore linear patching on CIFAR-10. See Appendix B.8 for these results along with setup details and further discussion.\n\n4.2 NPTs Can Learn to Predict Using Attention Between Datapoints\nTo determine if NPTs can successfully learn to exploit interactions between datapoints (Q2), we introduce a task with strong input correlations for which we know ground-truth interactions. Concretely, we use the UCI Protein regression dataset (cf. \u00a74.1) to construct the following semi-synthetic task: for each batch, we input the original data with masked target values as well as a copy of the original data where all target values have been revealed, i.e., no masking is applied (Fig. 3a). NPTs can use attention between datapoints to achieve arbitrarily good performance by learning to look up the target values in the matching duplicate row. At test time, we input novel semi-synthetic test data to ensure that NPT has learned the correct relational mechanism and not just memorized target values.\nNPTs successfully learn to perform this lookup between original and duplicate datapoints. The ABD attention weights, visualized for the first three datapoints in Fig. 3b, clearly show the model correctly attending to the duplicates. As a result, NPT predictions are Pearson-correlated with the duplicate targets at r = 99.9% (Fig. 3c). This equals an RMSE of only 0.44, about a magnitude lower than the error on the original Protein dataset (Table 11). We conclude that NPTs learn to predict by looking up the target values from matching points. Further discussion and attention maps are in Appendix B.1.1.\nPurely parametric models cannot exploit information from other datapoints, limiting their performance. For example, MLPs achieve an RMSE of 3.62 on this task. Non-parametric approaches also cannot solve this task in its original form, because unlike NPTs they must be told which datapoints are the originals (training data) and which the duplicates (test data) as well as which columns contain features and which target values. We demonstrate in Appendix B.1.2 that even when we make these concessions, we can easily adapt the task such that both k-Nearest Neighbors and Deep Kernel Learning fail to solve it. In fact, we are not aware of any other model that can solve the adapted task.\nAdditionally, we perform an interventional experiment to investigate the extent to which NPTs have actually learned the causal mechanism underlying the lookup task. As illustrated in Fig. 3d, we now intervene on individual duplicate datapoints at test time by varying their target value across a wide range. We stress that we perform these experiments without retraining the model, using exactly the same NPT from Figs. 3a-c. The model is now confronted with target values associated with features1.2 \u22121.1 \u22121.1 \u22120.5 \u22120.4 \u22120.1 \u22120.1 0.0 \u2206RMSE /RMSE (%) Yacht Protein Boston Concrete \u221252% \u221221% \u221220% \u22127%\nthat are highly unlikely under the training data. This label distribution shift [35] is a challenging setting for neural networks. However, NPT predictions follow the intervened target values with near-perfect correlation, Fig. 3e, continuing to predict by correctly looking up targets.\nWe now confidently conclude that NPTs robustly learn the causal data-generating mechanism underlying the semi-synthetic dataset. This requires NPTs to learn a non-trivial sequence of compuational steps. They must learn to match rows based on similarity of relevant features; to look up the target value of the duplicated datapoint; and, to copy that value into the target of the masked datapoint.\n\n4.3 NPTs Learn to Use Attention Between Datapoints on Real Data\nWe next consider (Q3): do NPTs actually learn to use attention between datapoints for prediction on real data? We design a test that allows us to quantify the extent to which the predictions of an NPT trained in standard fashion on one of our benchmark datasets depend on relationships between datapoints at test time. Concretely, for each target value in the input we randomize the data for all other datapoints by independently shuffling each of their attributes across the rows. We then evaluate the loss on the prediction at the target entry and repeat this procedure for all test datapoints. This completely corrupts the information from all datapoints except the one for which we evaluate. Hence, a model that relies meaningfully on attention between datapoints will show deteriorating performance.\nWe give an algorithm for the corruption procedure as well as further discussion in Appendix B.2.1.\nWe report the resulting change in performance after corruption in Table 2 for all datasets from \u00a74.1. We find that for most datasets, the corruption of other rows at test time significantly decreases the performance of the trained NPT models. This indicates that the NPTs have successfully learned to make predictions supported by attention between datapoints. For some datasets, the corruption experiment deteriorates performance completely. For example, for the Protein regression dataset NPT achieves state-of-the-art performance, but corrupting the input at test time leads to NPT performing worse than all of the baselines considered in \u00a74.1. We note that minor differences in performance are often still significant, as differences between competing models in \u00a74.1 are often likewise small.\nInterestingly, on certain datasets such as Forest Cover, Kick, and Breast Cancer, corrupted inputs do not significantly affect performance. It appears that when NPTs do not find it advantageous to rely on attention between datapoints during training, they can learn to completely ignore other inputs, essentially collapsing into a standard parametric model. This supports our earlier claims that NPTs can learn end-to-end from data the extent to which they rely on other datapoints for prediction. We think this is extremely interesting behavior and are unaware of prior work reporting similar results. However, we stress that these results reflect inductive biases of the NPT architecture and do not lend themselves to general statements about the performance of parametric versus non-parametric models.\n\n4.4 NPTs Rely on Similar Datapoints for Predictions on Real Data\nSo far, we have presented convincing evidence that NPTs (sometimes strongly) depend on attention between datapoints. However, we do not know what kind of interactions are learned in practice on real data (Q4). As an initial step towards understanding this, we now present two experiments investigating to which other datapoints NPT attends.\nQualitative Evidence. Figure 4 shows an attention map for attention between datapoints (ABD) of NPT on a batch of the Protein regression dataset. We sort the input data with respect to their input space distance such that similar datapoints are now close to each other. The diagonal pattern in Fig. 4 indicates that NPT attends more strongly to datapoints that are similar in feature space. Appendix B.3.1 discusses this further and gives additional attention maps. Quantitative Evidence. Seeking a quantitative measure for this hypothesis, the data deletion experiment repeats the following procedure for all test set points: iteratively delete other datapoints from the input if they do not significantly affect the prediction. We stop if less than 2% of the original datapoints remain, or if the total change in prediction for the target (relative to the original prediction with all data) exceeds 10%. We investigate the average input feature space distances between the test point and the kept datapoints, as well as the distances between the test point and the deleted datapoints. \"Input features\" here refer to all attributes of the input datapoints that are not labels.\nWe find that kept datapoints have a significantly lower average feature space distance to the test point than those deleted. This indicates that two datapoints i, i that are similar in input feature space, such that j<d (X i,j \u2212 X i ,j ) 2 is low, have a larger effect on the predictions of one another. A Wilcoxon signed-rank test is significant at p \u2248 8.77 \u2022 10 \u2212130 . We give full details on this in Appendix B.3.2.\nBoth experiments support the hypothesis that NPTs rely on similar datapoints for prediction in real data settings. One possible explanation is that similar datapoints might have different realizations of observation noise which NPTs could learn to average out. Altogether, we conclude that NPTs can and do learn representations which rely on interactions between datapoints for prediction.\n\n5 Limitations, Future Work, and Conclusions\nLimitations. NPTs share scaling limitations with all na\u00efvely non-parametric approaches [74] and GNNs [52]. We demonstrate this in a preliminary analysis of the computational cost of NPTs and the baseline methods -including training time and CPU/GPU memory requirements -in Appendix B.6. While we have seen success with random minibatching ( \u00a72.6), future work might consider applying principled attention approximations, such as learning representative input points [59], kernelization [19, 47], or other sparsity-inducing methods [5, 18, 84], to improve the scalability of NPTs.\nFuture Work. We believe that the unique predictive mechanism of NPTs makes them an interesting object of study for other tasks including continual learning, multi-task learning, few-shot generalization, and domain adaptation. For example, when predicting under distribution shift, general relations between datapoints and attributes may remain valid and allow NPTs to accommodate such scenarios better. Additionally, future work could explore the connections to stochastic processes, e.g., by extending NPTs to be approximately consistent, similar to Neural Processes [36, 37, 49].\nConclusions. We have introduced Non-Parametric Transformers (NPTs), a novel deep learning architecture that takes the entire dataset as input and uses self-attention to model complex relationships between datapoints. NPTs challenge and naturally extend parametric modeling as the dominant paradigm of deep learning. They have the additional flexibility to learn to predict by directly attending to other datapoints. Notably, NPTs learn this end-to-end from the data at hand. Empirically, NPTs achieve highly competitive performance on a variety of benchmarks, and additional experiments demonstrate their ability to solve complex reasoning tasks over datapoints. Further, we show that on real data, NPTs learn to rely on attention between datapoints for prediction. We believe that the characteristics of NPTs will make them an exciting object of further study. We here provide proof that NPT is equivariant to a permutation of the datapoints. This requires, among other things, showing that multi-head self-attention is equivariant. We were unable to find this proof in the existing literature, e.g., Set Transformer [59] relies heavily on equivariance of self-attention but does not provide proof. In the following, we will refer to datapoints as the rows of our input, see e.g., Fig. 1.\n\nSelf-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\n\nAppendix\nDefinition 1. A function f : X n \u2192 X n is row-equivariant if for any permutation \u03c3 : [1, . . . , n] \u2192 [1, . . . , n] applied to the dimensions of X n , we have for all i, f (X 1 , . . . , X n )[i] = f (X \u03c3 \u22121 (1) , . . . , X \u03c3 \u22121 (n) )[\u03c3(i)].Lemma 1. Any function of the form f (X 1 , . . . , X n ) = (g(X 1 ), . . . , g(X n )) for some g is rowequivariant. These functions are denoted as 'row-wise operations', as they consist of the same function applied to each of the rows of the input.\nProof. Follows immediately from the structure of f . Lemma 2. The composition of row-equivariant functions is row-equivariant.\nProof. This result is widely known, but a proof here is included for completeness. Let f and g be row-equivariant.f \u2022 g(\u03c3X) = f (g(\u03c3X)) = f (\u03c3g(X)) = \u03c3f (g(X)). () Lemma 3. Let W \u2208 R n\u00d7m1 and X \u2208 R m2\u00d7n . The function X \u2192 XW is row-equivariant.\nProof. Let \u03c3X be a permutation of the rows of X. Then we have(\u03c3X)W [i, j] = \u03c3X[i, k]W [k, j] (9) = X[\u03c3 \u22121 (i), k]W [k, j] = XW [\u03c3 \u22121 (i), j] = \u03c3(XW )[i, j]. () Lemma 4. The function X \u2192 Att(XW Q , XW K , XW V ) is row-equivariant.\nProof. Let the row-wise softmax function be denoted \u03c9(\u2022). Then we haveAtt(XW Q , XW K , XW V ) = \u03c9(XW Q (XW K ) / \u221a h)XW V ,\nwhere\u03c3XW Q (\u03c3XW K ) [i, j] = \u03c3(XW Q )\u03c3(XW K ) [i, j]= \u03c3(XW Q )[i, k]\u03c3(XW K )[j, k]= XW Q [\u03c3 \u22121 (i), k]XW K [\u03c3 \u22121 (j), k]= XW Q (XW K ) [\u03c3 \u22121 (i), \u03c3 \u22121 (j)]=: A. ()\nNote that the above result states that the function XW Q (XW K ) is not row-equivariant because of the additional permutation of the columns. Let \u03c3 denote a permutation operator on matrices. Then straightforwardly we have the following:\u03c9(\u03c3A/ \u221a h)) = \u03c3\u03c9(A/ \u221a h) .\nFinally, it remains to show that the final matrix multiplication step restores the row-equivariance property we seek.\u03c3 \u03c9(XW Q (XW K ) / \u221a h) =:M (\u03c3XW V )[i, j] = \u03c3(M )(\u03c3XW V )[i, j] (18) = \u03c3(M )\u03c3(XW V )[i, j] (19) = M [\u03c3 \u22121 (i), \u03c3 \u22121 (k)](XW V )[\u03c3 \u22121 (k), j] (20) = M (XW V )[\u03c3 \u22121 (i), j].\nWhich shows that self-attention is row-equivariant.\nLemma 5. The following hold: 1. Multihead self-attention is equivariant.\n2. If f and g are row-equivariant, then the functionx \u2192 g(x) + f (x) is also row-equivariant. 3. Res(H) is row-equivariant. 4. MHSA(H) is row-equivariant. 5. ABD is row-equivariant. 6. ABA is row-equivariant.\nProof. We show each item.\n1. We know that X \u2192 O i is equivariant from the previous lemma, and this trivially implies that X \u2192 concat(O 1 , . . . , O k ) will also be row-equivariant. Finally, because \u03c3AB = \u03c3(AB), get that MHSelfAtt(H) is row-equivariant. 2. Straightforward.\n\n3. Because LayerNorm is row-equivariant (being a function applied row-wise to the matrix),\nRes(H) is a sum of two row-equivariant functions and so by a previous result will also be row-equivariant. 4. Because rFF is again a row-wise operation and so trivially row-equivariant, the previous results on sums and compositions of row-equivariant functions directly yield row-equivariance of MHSA. 5. ABD is by definition an application of MHSA(H), and therefore is row-equivariant by the above result. 6. ABA is a row-wise operation and is therefore trivially row-equivariant.\nProperty A.0.1. NPT is row-equivariant.\nProof. Each layer of NPT has been shown to be row-equivariant. Because NPT is a composition of such row-equivariant functions, it is therefore row-equivariant.\n\nB Additional Results\n\n\nB.1 Semi-Synthetic Experiments\n\n\nB.1.1 Attention Maps for the Semi-Synthetic Experiments\nWe here display additional results for the semi-synthetic experiments of Section 4.2. In Fig. B.1, we display attention weights for Attention Between Datapoints (ABD) for all depths and a subset of heads of the architecture. We see that some, but not all, attention heads display the desired diagonal lookup pattern. Note that, in this case, one head would suffice to implement lookup and perfectly solve the task.\nA brief comment on the attention maps with the \"double diagonal\" structure (e.g., depth 4, head 0): we see that (a) original datapoints attend to the duplicate points and (b) duplicates also attend to duplicate datapoints. Behavior (a) makes sense: NPT needs to attend to the duplicates from the originals to look up the target values. This behavior in turn minimizes loss. Behavior (b) is irrelevant to loss, because NPT does not need to predict anything for the duplicates, and no loss is computed. However, (b) suggests that the query embeddings learned by the self-attention ignore the masked\n\nB.1.2 Modified Semi-Synthetic Experiments\nSetup. In Section 4.2, we mention that with some concessions the original lookup task can also be solved by standard non-parametric models. However, we also mention that simple modifications to the task make it, again, unsolvable for any model of which we are aware other than NPT. We here demonstrate these hypotheses for two non-parametric models: k-Nearest Neighbors (k-NN) and Deep Kernel Learning (DKL). First, we apply k-NN and DKL to the original duplication tasks. As mentioned in the main text, this already requires us to make some concessions: we now need to explicitly split the input data into a global training set (all duplicated datapoints) as well as a test set (all original datapoints). That is, if all duplicate datapoints make up the training set, then non-parametric models are able to predict perfectly on the original datapoints, because most non-parametric models rely on distances in some manner, and here, distances in input feature space are sufficient to successfully match entries. This is trivially true for k-NN but also for DKL, where the RBF kernel of the GP will lead to the desired \"matching behavior\" as long as the learned neural network embedding does not collapse distances.\nIn other words, NPTs would ideally learn a k-NN-style prediction for the semi-synthetic dataset. Crucially, while non-parametric models predict based on distances because of fixed design choices, NPTs learn this behavior and can just as well learn other more complicated relations between datapoints.\nWe now present two modifications to the semi-synthetic dataset; NPT can accommodate them because the model learns the nature of interactions, but they significantly affect the performance of the fixed kernel methods.\n\u2022 Random Features: A subset of the features are randomized across both original and duplicate datapoints independently. Specifically, we overwrite the entries of the last three features with noise drawn independently from a Gaussian distribution N (1, 1). To solve the task, matches between datapoints must now be computed using the subset of non-randomized features only. \u2022 Add One: We add 1 to all target regression values only for the duplicate datapoints. Matches can still be made based on all features, but now a 1 must be subtracted from the lookup value to solve the task.\nAs in the original setting, we train the models on the modified semi-synthetic datasets and check with novel test data whether they have learnt the correct relational mechanism underlying the experiment.\nNote that the Random Features and Add One settings also distinguish our setup from prompting in natural language processing literature [12, 72] because the original datapoints are no longer \"correct\" input-output pairs; the model must use an underlying relational structure instead of memorization to solve the task.\nResults. Table 3 presents RMSE values obtained by the models when trained on the original duplication task, the two modifications separately, as well as both modifications applied.\nEvidently, for NPTs, the different scenarios do not lead to a large difference in performance; in all instances, they achieve near-perfect loss because their predictions leverage attention between datapoints. Careful optimization of NPT training convergence would likely lead to a further reduction in loss. Nevertheless, the achieved losses by NPT are more than a magnitude lower than those on the original data and correspond to a near-perfect Pearson-correlation with the target values of r > 99.9%. We conclude that NPTs successfully learn to attend to the correct subset of features, to subtract 1 from the lookup target values, or to do both at the same time.\nNext, we consider the non-parametric models. First, we confirm in Original Synthetic that the non-parametric models can indeed solve the original lookup task. However, we find that neither DKL nor k-NN can accommodate any of the modifications, reverting to an RMSE that is worse than the performance of all baselines on the original Protein dataset, see Table 11. 5 or k-Nearest Neighbor, k = 1 is clearly optimal in the original semi-synthetic setup. However, k-NN cannot learn to ignore certain attributes (Random Features) and or to modify looked-up values.\nSetting k > 1 actually improves prediction because it considers other matching points in addition to the (now misleading) duplicates for prediction. However, even with k > 1, k-NN does not achieve much better than guessing performance on the modified tasks.\nDKL also fails to accommodate any of the presented task modifications. We suspect that DKL, in theory, should be able to solve the Random Features task. That is, DKL should be able to use the neural network to learn a representation that discards any information from the randomized columns.\nWe were unable to achieve this, but it may be possible with additional adaptations to the model. Ideally, we would condition the GP on new \"test data\" (the duplicates) in each minibatch during training. This was not easily possible with the GPyTorch codebase. 6 At test time however, we did directly reconstruct an exact GP using embedded inputs and RBF scale parameters learned during training.\nIn any case, DKL can never solve the Add One scenario because, after independently transforming features with a neural network, DKL simply applies a GP in embedding space. This means that it will always naively interpolate target values between training data (duplicates) and test data (features) in embedding space, and cannot learn interactions between points, such as subtracting 1 from all duplicate targets.\nEven further, there is another easy option of how to construct this experiment such that only NPT will be able to solve it: we could randomly sample the attribute for which we mask out the entry, i.e., all columns can now be target columns. All non-parametric models presented here rely on a fixed set of features as input to predict for a fixed target column. They are not compatible with this style of \"imputation\" problem, i.e., there is no way to even take as input data like this in such models. NPTs, however, take both features and targets as input, only using the masking mechanism to distinguish between features and targets as well as train and test data. Hence, they can easily adapt to this scenario.\nThe bad results for the non-parametric models also highlight that these models must predict nonparametrically, unlike NPT, which could always fall back to parametric prediction if it cannot learn the interactions required for a task.\n(k)-NN Hyperparameter details. We use the scikit-learn [69]Input: list of masked minibatches B = [X (b) \u2208 R K\u00d7d | b \u2208 1 . . . B], unmasked label column X :,d , trained model f : X (b) \u2192 X (b)\n, batch size K, loss function L, number of attributes (including features and target) d Returns: test loss under data corruptionL corr L corr \u2190 0 for X (b) in B do for k in 1 . . . K do X (b,k) \u2190 X (b) // initialize batch to be corrupted for j in 1 . . . d do X (b,k) i =k,j \u2190 permute axis=i (X (b,k) i =k,j ) // permute each attr. column indep. end L corr += L(f (X (b,k) ) k,d , X k,d ) // compute loss w/ unmasked label column end end return L corr\nAlternatively, we could also input datapoints individually, i.e., decrease the minibatch size to 1, to test if NPT depends on attention between datapoints. Indeed, we find that performance also deteriorates in this scenario. However, we believe that the Data Corruption experiment provides stronger evidence because it preserves batch statistics across attributes. This makes sure that performance deterioration is not caused by spurious factors, such as a decreased batch size that was not encountered in training. While NPT is generally compatible with varying batch sizes, we leave a thorough investigation of this for future work.\n\nB.3 Real\n\n\nB.3.2 Data Deletion Experiment\nWe here give full details on the Data Deletion experiment presented in Section 4.4. To recap, we consider the prediction of NPT for a single test sample i * . We then iteratively delete other datapoints from the input if they do not significantly change the prediction of NPT on i * . Algorithm 2 describes this in detail. We are then interested in differences between the deleted and the kept datapoints. Specifically, we compare the average feature space distance in input space between the active datapoint i * and either the kept datapoints R or deleted datapoints {1, . . . , n} \\ ({i * } \u222a R), obtaining average distances D i * ,kept , D i * ,deleted . We break out of the deletion algorithm if less than % of the original points remain, to reduce variance in our estimates of the kept statistic. We repeat Algorithm 2 for all 5567 test points i * \u2208 D test in the Protein regression dataset.\nWe perform a Wilcoxon signed-rank test on the pairs {D i * ,kept , D i * ,deleted } i * \u2208Dtest to determine if the median of the kept datapoints is less than the median of the deleted ones. The test is highly significant at p \u2248 0, i.e., smaller than the floating point precision of SciPy Stats allows. The raw Wilcoxon statistic is 3125889.5.\nTo make sure the difference is not an effect of sample size, we also construct a set of average differences to a set of randomly drawn datapoints. 7 That is, instead of using Algorithm 2 for targeted deletion, we randomly construct R, essentially only applying lines 10 and 15 of Algorithm 2. For Displayed are average feature space differences and their standard errors between the active datapoint and the sets of kept, random, and deleted datapoints for a single batch. each active test row i * , we randomly delete as many datapoints as were deleted in targeted fashion. A Wilcoxon signed-rank test between the distances for the random and kept subset is likewise significant at p \u2248 8.77 \u2022 10 \u2212130 . This is the value we report in the main body.\nWe also run a computationally more demanding version of the algorithm with \u2206 it \u2190 0.005, \u2190 0.01 to see how many points we can successfully delete. This version of algorithm requires more computation which is why we limit execution to the test datapoints of a single batch. The results are statistically significant at 5.26 \u2022 10 \u221249 for kept < deleted and 8.38 \u2022 10 \u221239 for kept < random for a Wilcoxon signed-rank test. We illustrate the differences between the distances in Fig. B.3. We further note that using Algorithm 2, we are able to reduce the set of datapoints present in the input to 1% of the original n for 79.5% of active test datapoints and to 10% in 99.5% of cases. Percentages refer to n = 2048 datapoints in total, of which 398 were test datapoints. All in all, these experiments strongly suggest that NPT relies on interactions between similar datapoints for prediction.\n\nB.4 Ablation Study 1: NPT Hyperparameters\nWe conduct an ablation study on the Protein and Boston Housing datasets (Table 4). For Protein, the same 0.7/0.1/0.2 train/validation/test split is used for all model configurations. Boston Housing uses a 0.7/0.2/0.1 train/validation/test split with 10-fold cross-validation.\nDespite the significant difference in dataset sizes between Boston Housing (n = 506) and Protein (n = 45730), and the fact that Boston Housing includes both categorical and continuous variables, the base models used for each dataset are nearly identical.\nOn both datasets, we use an NPT model with 8 layers, 8 heads, per-attribute hidden dimension e = 128, feature and target masking with p = 0.15 for each, a cosine annealing schedule for the loss tradeoff \u03bb, the LAMB [98] optimizer with Lookahead [99], a flat-then-anneal learning rate schedule with cosine decay and base learning rate 0.001, dropout with rate 0.1 on the attention weights and after linear layers, and gradient clipping at 1. This configuration is essentially the same as the NPT-Base configuration described in Appendix C.1, which we use with minimal per-dataset modifications for all other results in this work.\nDifferent in our base models between the two datasets are the following settings. The Boston Housing model takes as input the full dataset (i.e., batch size = 507) and Protein uses minibatching with batch size = 2048. Boston Housing trains for 20 000 steps, and Protein for 400 000. The learning rate is constant for the first 70 % of steps for Protein, but only for the first 50 % of steps for Boston, starting the learning rate annealing earlier to defend against overfitting on the small dataset.These changes directly result from the different dataset sizes.\nAs Table 4 shows, the performance of NPT is robust to a variety of significant hyperparameter choices. This illustrates that practitioners will likely not need to spend much time tuning hyperparameters Protein Dataset. See Table 4 for results and performed ablations. It is computationally too expensive for us to perform full cross-validation over all ablations for the Protein regression dataset. Instead, we report the results of a single 5-fold cross-validation for the Base NPT configuration on Protein (also varying the model random state). This results in an RMSE of 3.40 \u00b1 0.05 (\u03c3). The standard deviation of the 5-fold cross-validation allows us to roughly gauge which ablations have significant effect. Given the results in Table 4, we find that the majority of ablations do not lead to meaningful changes in performance. Only the somewhat dramatic changes to the optimization of NPT result in its performance falling from the top rank on the Protein Dataset (second rank CatBoost has RMSE = 3.51): removing stochastic feature masking (p feature = 0), removing both stochastic feature masking (p feature = 0) and stochastic target masking (p target = 1, training targets are always masked out at training time and NPT therefore cannot learn to attend to training targets at test time), or changing p feature to 0.5 (meaning that 50% of all input features are masked out). NPT appears to be particularly robust to changes in model complexity, e.g., depth and number of heads, although the results suggest that we could have further increased the size of Base NPT to achieve slightly higher performance.\nBoston Dataset. See Table 4 for results and performed ablations. For the Boston dataset, we repeat ablations over all 10 CV splits. Similarly, ablations on the Boston dataset are largely inconsequential; none of them result in a statistically significant change in performance from the base model. The second rank performer on Boston is MLP, at RMSE = 3.32. Only ablation of semi-supervision or changing p feature to 0.5 result in a change in the top ranking of NPT among the baselines.\nAltogether, the ablation study supports the claim that NPT can be applied successfully with very little tuning to datasets of vastly different sizes and feature types. Changes in model depth and number of heads do not appear significant, but using a reasonably low feature masking probability (e.g., 15%, as has been commonly used in the literature [24]) may be important to stable training.\nSupported by these ablations, we sweep over only a small selection of configurations for our main benchmark comparison in Section 4.1. And indeed, it seems that NPT is robust to hyperparameter changes, given that these configurations perform well across vastly different settings (binary and multi-class classification, datasets with millions of datapoints, etc.) than those explored in the ablations. See Appendix E for details.\nWe speculate that NPT's robustness stems from (a) being a relatively overparametrized architecture that is powerful enough to model a wide variety of datasets and (b) from the effective regularization introduced by the feature masking mechanism. Finally, we emphasize that the aim of this work is to introduce the NPT architecture and examine its properties, not to spend significant effort and compute resources on achieving top performance across all benchmarks. We next present an additional ablation study targeting two core components of NPTs across all datasets: the Attention Between Attributes (ABA) layer and the stochastic feature masking.\nABA Layer. First, we perform an ablation to test if ABA layers are beneficial in practice. For this, we simply leave out the ABA layers, such that the MLP at the end of the ABD layers (see \"rFF\" in Eq. ( 5)) is now the only way for the model to independently transform the features of input datapoints.\nOur results, given in Table 5, show that, generally, ABA is a useful component of the NPT architecture.\nLeaving out ABA increases performance only for 3/10 datasets. Interestingly, all three of these datasets are regression tasks, which may warrant further investigation. We observe the largest difference for the Poker Hands dataset, which requires complex reasoning between input features: in the same number of training steps, the ablation only achieves 57.4% accuracy compared to 99.3% for full NPT. These results support our hypothesis that ABA is useful when the dataset requires complex transformations of the features. Our most general recommendation would be to default to using NPTs with ABA layers, as they boost performance on the majority of datasets we examine. However, if practitioners can spend the extra compute, exploring NPTs without ABA can be worthwhile.\nStochastic Feature Masking. We perform an ablation to test if the stochastic feature masking objective (cf. \u00a72.6) is beneficial in practice. For this, we simply disable all stochastic masking of input features by setting p features = 0.\nOur results, also in Table 5, show that for 9/10 datasets, enabling feature masking yields at least a small improvement in performance. Disabling feature masking is detrimental to the performance on the Poker Hands dataset, leading to a 30% drop in accuracy. Again, our general recommendation would be to use NPTs with feature masking by default, as it rarely seems to decrease performance and sometimes helps significantly, but to explore NPTs without feature masking if feasible.\n\nB.6 Computational Cost of Non-Parametric Transformers\nWe next compare the computational requirements of NPT against the various baselines. More specifically, we compare experiment runtimes and maximum memory usage on the Protein and Higgs datasets. We choose these datasets because they are representative of medium and large datasets in terms of computational requirements, with 45 730 and 11 000 000 datapoints respectively. Note that, while we re-use hyperparameter configurations across datasets for NPTs, the baselines require a novel hyperparameter search to be performed for each dataset (cf. Appendices C and E). Below, we include the cost of hyperparameter optimization for the baselines.\nNote that these numbers only provide a rough ordering of the compute and memory costs of the various methods. We did not optimize the baselines or NPT for memory usage, training time, or prediction speed. Additionally, while NPTs rely on GPU-accelerated PyTorch code, many of the baselines are CPU-only: therefore, the results depend on our particular CPU and GPU choices.\nWe also give the number of CPUs used in each experiment for each baseline. Here, we maximize the number of CPUs used in parallel execution in order to speed up training. This is mainly limited by the memory used per process: e.g., if we list # CPUs as 1, this does not mean that we used a machine with only 1 CPU, but rather that each process used a significant amount of the total available memory and hence we could not increase the number of CPUs used in parallel. Note that, additionally, for the CPU baselines, we made use of high-memory instances when this was necessary to avoid out-of-memory issues.\nIn summary, the numbers we give are a rough indication of the computational cost that a practitioner should expect to require in order to reproduce our results. It is likely that by tuning aspects of our setup, both for NPTs and the baselines, memory usage and/or runtimes could be improved.\nWe display the observed computational costs in Tables 6 and 7 for the Protein and Higgs datasets. As of now, NPTs do generally require longer training times than the non-neural baselines. For example, for the Protein dataset, the selected hyperparameter configuration of NPT trains in 11 hours, while all boosting methods finish their runs in less than 1 hour, including the hyperparameter tuning. The exception to this rule is given by some of the baselines, e.g., Random Forests, which do not scale well to large datasets such as Higgs. On Higgs, the NPT run takes 5d 22h compared to 13d 13h for Random Forests.\nWith NPTs, we want to store as much data as possible in addition to the network weights; recall that this is done to improve the quality of the minibatch approximation of the full dataset. Therefore, as expected, NPT is much more GPU-memory intensive during training than TabNet, the only other baseline with a GPU-based implementation, for which maximizing minibatch size is not desirable. In particular, the peak GPU memory usage on Higgs for NPTs is 19.18 GB and 1.18 GB for TabNet. However, we note that other methods are often also memory-intensive on larger datasets. For example, Random Forest with 1 process uses 189.18 GB peak CPU memory.\nWe next give a rough indication of prediction time behavior of NPT and the baselines. For the same reason as above, NPT is expected to have high memory usage at prediction time. In terms of prediction speed, we suspect that our ability to scale NPT to large batch sizes, e.g., 4096 on the Higgs dataset, might give us an advantage in comparison to those baselines that cannot be parallelized well and/or lack GPU support. We leave a detailed investigation of prediction time behavior to future work.\nFinally, as discussed in \u00a75, we note that by incorporating recent tools for sparse and efficient attention [5, 18, 19, 47, 84], future research could significantly improve the scalability of NPTs.\n\nB.7 Extended Results for Tabular Data Benchmarks\nSee Table 8 (Table 9) for test accuracies (negative log-likelihood scores) on the UCI classification datasets and additionally Table 10 for AUROC results on the binary classification datasets. For the regression datasets, see Table 11 for RMSE scores and Table 12 for MSE scores. 8 Out-of-memory on the Higgs Boson dataset when attempting approximate 3-NN on an Azure D64 v3 instance with 256 GB RAM. 9 TabNet had notably lower accuracy in our setup on the Poker Hand dataset (which has a fixed test set) than that the 99.2% reported in the original work [2]. We are in communication with the authors, attempting to improve these results. However, our results on Higgs Boson match the reported performance more closely (78.44% (theirs) vs 77.1% (ours)). Further, we note that our other baselines achieve significantly better performance on the same datasets than those reported in [2]; e.g., our MLP achieves 99.5% accuracy on Poker Hand dataset while they report 50.0%; our XGBoost achieves 97.1% on Forest Cover while they report 89.34%. However, we note that some of the datasets -such as Forest Cover -do not have fixed test sets. Therefore, we cannot exclude the possibility that the performance differences are due to differently chosen train-test splits. 10 See above note on out-of-memory. 11 See above note on out-of-memory. We explore two different setups for applying NPTs to high-dimensional image data: (1) using a CNN encoder based on the ResNet-18 architecture, followed by ABD layers, and (2) using a linear patching encoder that is then followed by ABD and ABA layers. We present results using (1) for CIFAR-10 and (2) for MNIST in the main paper, and additionally provide results using (2) on CIFAR-10 below.\nNote that the aim of our image classification experiments is not to match the performance of a pretrained Transformer image classifier. Rather, we hope to demonstrate that NPTs can readily learn interactions between datapoints on a wide variety of data modalities and tasks, including image classification, while achieving reasonable performance.\n(1) CNN Encoder. In this setup, we replace our linear encoder with a CNN, which is then folowed by several rounds of Attention Between Datapoints (ABD) on the CNN encodings. We apply this setup to CIFAR-10.\nIn detail, we use a ResNet-18 encoder followed by 4 blocks of ABD (as we have in a default 8 layer NPT, cf. Appendix C.1.1) with 8 heads. Because we do not use Attention Between Attributes (ABA) the output of the encoder corresponds to the dimensions h = d \u2022 e = 128. We train in a supervised manner (without test inputs available at training time) with a training batch size of 128 and evaluation batch size of 480, for a fixed 100 epochs.\nAs reported in the main text, we achieve a test accuracy of 93.7% on CIFAR-10 with this architecture. We find that the data corruption test (cf. Section 4.3) decreases accuracy by 1.2%, which suggests that NPT meaningfully relies on other datapoints for prediction on CIFAR-10. The ResNet-18 alone achieves a test accuracy of 93.9%.\nWe further note that with a ResNet-18 encoder pretrained on ImageNet, our ResNet + NPT architecture achieves a test accuracy of 94.7% on CIFAR-10, and loses 0.7% in the data corruption experiment, whereas the pretrained ResNet-18 alone achieves a lower 94.2% accuracy. We believe that an exploration of how pretraining might affect the performance of NPT and the extent to which predictions rely on other datapoints is interesting future work.\n(2) Linear Patching Encoder. We additionally consider an image classification setup using a linear patching encoder, which we apply to both MNIST and CIFAR-10.\nIn detail, we append the mask dimension as an extra channel and apply image patching with linear embeddings as in [25]. Further following [25], we use a learned position embedding for each patch and the class token. We use 7 \u00d7 7 = 49 patches on MNIST and 8 \u00d7 8 = 64 patches on CIFAR-10. On both datasets, for this linear patching setup, we begin with the NPT-Base architecture described in C.1.1. On MNIST, we use batch size 512, train for 500,000 steps, use hidden dimensions e = 16, p target = 0.15, and use 7 \u00d7 7 = 49 patches. On CIFAR-10, we use batch size 512, train for 1,000,000 steps, use random crops and horizontal flips for data augmentation, use 8 \u00d7 8 = 64 patches of each image, and do not use target masking due to constraints on compute time.\nWith this setup, NPT achieves 98.3% accuracy on MNIST and 68.2% accuracy on CIFAR-10. We additionally find in the data corruption experiment (detailed in Section 4.3) that after destroying information from other datapoints, the change in accuracy is -0.4% on MNIST and -5.1% on CIFAR-10, demonstrating that NPTs learn to make use of interactions between images.\nHowever, we did not find that this sufficiently demonstrated that NPTs make use of datapoint interactions in achieving reasonable performance on CIFAR-10, and hence conducted the experiment on CIFAR-10 using the CNN encoder setup above.\nWe expect that the relatively low performance in the linear patching setup on CIFAR-10 was due to a number of differences between our setup and other works, which report state-of-the-art results on image classification using Transformers and linear patching. Most importantly, previous works [25, 46] either consider only, or pretrain on, large or huge datasets; for example, ImageNet [23, 46, 86], ImageNet-21k [75], or JFT-300M, with over 375 million labeled datapoints [25, 83]. We perform no pretraining, and therefore a direct comparison of these results to this line of work is inappropriate. Additionally, previous works use significantly more patches (e.g., 256 in [25]) and use higher resolutions, including during fine-tuning by upscaling from 32 \u00d7 32 to 224 \u00d7 224 resolution [25, 46, 54, 85]. Below, we outline the NPT-Base model configuration. The final configurations used for each dataset are essentially the same as NPT-Base, with minor alterations in parameters such as hidden dimension size, learning rate warmup, batch size, and number of training steps. Given our limited memory and compute time budget, these changes directly result from differences in number of datapoints/attributes between the datasets. We divide the NPT-Base configuration into architectural details and optimization details.\n\nC Additional\nNPT-Base Architecture \u2022 8 layers, alternating Attention Between Datapoints and Attention Between Attributes.\n\u2022 8 heads.\n\u2022 Row-wise feed-forward (rFF) networks with one hidden layer, 4x expansion factor, and GeLU activation (standard in Transformer literature [66, 90]). \u2022 Attention weight and hidden layer dropout with p = 0.1 (cf. Appendix C.2.1).\n\u2022 Per-attribute hidden dimension e = 64.\n\nNPT-Base Optimization\n\u2022 LAMB [98] optimizer with \u03b2 = (0.9, 0.999) and = 1e \u2212 6, and a Lookahead [99] wrapper with slow update rate \u03b1 = 0.5 and k = 6 steps between updates. \u2022 Stochastic feature masking probability p feature = 0.15.\n\u2022 Anneal the tradeoff \u03bb between feature and target loss with a cosine schedule, starting at 1 (all feature loss) to 0 (all target loss) over the course of training. \u2022 Flat-then-anneal learning rate schedule: flat at the base learning rate for 70% of steps, and then anneals following a cosine schedule to 0 by the end of training. \u2022 Base learning rate 1e-3.\n\u2022 Gradient clipping at 1. On all datasets with minibatching, we approximately maintain relative train, validation, and test datapoint proportions in each batch. We train NPT in semi-supervised mode (cf. Appendix C.4.2) but have found that this does not consistently improve performance compared to conventional training because the amount of unlabeled test data is usually comparatively small.\n\nC.1.2 NPT Training on Small Data\nHere we describe the hyperparameter sweep details for small datasets -Breast Cancer, Boston, Concrete, and Yacht.\nBase Hyperparameter Configurations. Across these small datasets, we make a few minor adjustments to the NPT-Base architecture and optimization to obtain the NPT-Small configuration: we increase the default number of hidden dimensions to e = 128, fix the flat-then-anneal schedule to be flat for 50% instead of 70% of steps, and train with the entire dataset as input, i.e., no minibatching. We set stochastic target masking probability to p target = 1 by default, i.e., deterministically mask out train labels as would be done in a normal supervised setting, and then introduce modifications in our sweep.\nNote that the vast majority of hyperparameters such as the number of layers and heads, optimizer, p feature , tradeoff annealing schedule, learning rate schedule, and gradient clipping are exactly the same between NPT-Base and NPT-Small.\nWe would like to keep the base configuration for each of the small datasets exactly the same. However, we need to slightly vary the learning rate and number of epochs per dataset to optimize loss convergence across datasets. We use a base learning rate 5e-4 on Breast Cancer and 1e-3 on the other small datasets. We train for 2000 epochs on Breast Cancer and Boston, and 10 000 epochs on Yacht and Concrete. On Breast Cancer, we additionally drop e = 32 due to memory constraints (it has more attributes than other small datasets).\nSmall Data Sweep. Based on these configurations, we sweep over the following 8 configurations of the model on each dataset. 12 Vanilla NPT-Small model for given dataset.\n\u2022 Increase number of layers 8 \u2192 16.\n\u2022 Increase number of heads 8 \u2192 16.\n\u2022 Increase number of layers 8 \u2192 16, and number of heads 8 \u2192 16.\n\u2022 Stochastic target masking with probability p target = 0.1.\n\u2022 Stochastic target masking with probability p target = 0.5.\n\u2022 Increase stochastic feature masking probability from 0.15 to p feature = 0.2.\n\u2022 Use a cosine cyclic learning rate scheduler with two cycles, initial learning rate 1e-7, final learning rate 1e-7, and max learning rate given by the base model learning rate.\nFor the stochastic target masking variants, we proportionally increase the number of epochs (e.g., with p target = 0.5, half as many targets are observed in a given epoch, so we double the total number of epochs).\nSmall Data Variant Rank Orders. We report the rank order (\u00b1 standard error) of these variants in Table 13. A notable trend is that the target masking configurations perform particularly well.\nOne of the two configurations with target masking is the top performer on each of the four datasets. This could be attributed to some combination of the representational advantage of label masking (cf. Section 2.6), an additional regularization effect akin to dropout, or stabler convergence over a greater number of epochs.\nOther configurations did not display similarly obvious trends in performance. This is in concordance with the ablation study (Appendix B.4) and supports the claim that NPT is robust to changes in hyperparameters.\n\nC.1.3 NPT Training on Medium and Large Data\nFor the medium and large datasets, we again adopt the NPT-Base architecture and optimization hyperparameters, and make minor manual changes on a per-dataset basis to account for differences in number of datapoints and attributes across the datasets. No more than 3 manual iterations are performed to find these adaptations. We generally attempt to maximize batch size given a fixed memory budget. Given the rank order results on small data (cf. Table 13) we use target masking on the medium and large datasets whenever computationally feasible. 13 . These per-dataset alterations are reported below.\nTable 13 : Average rank order of variants of NPT-Small (\u00b1 standard error) across 10 cross-validation splits on each small dataset. We determine rank using negative log-likelihood and sort methods by ascending rank for each metric.\nDataset Boston UCI Datasets. We report results for Protein using the Base NPT configuration in the ablation study (cf. Table 4). On Kick, we use batch size 4096, train for 250 000 steps, and use p target = 0.5. On Income, we use batch size 2048, train for 2 000 000 steps, use no feature masking (and correspondingly fix the tradeoff parameter \u03bb = 0), and use p target = 0.15. On Poker Hand, we use batch size 4096, train for 200 000 steps, use p target = 0.5, and stratify by class (i.e., compose training datapoints in each minibatch proportionally to the empirical label distribution of the training set to account for significant class imbalance). On Forest Cover, we use batch size 1800, train for 800 000 steps, use a polynomial decay learning rate scheduler with warmup over the first 1% of steps, use base learning rate 0.005, p target = 0.5, and class balancing as above. The changes to learning rate scheduling were made to speed up training and hence save compute resources. On Higgs, we use batch size 4096, train for 500 000 steps, and do not use target masking due to constraints on compute time.p target = 0.5\nImage Data (CIFAR-10 and MNIST). See Appendix B.8 for details on the image data architecture and setup.\nAgain, we stress that the vast majority of hyperparameters used on all datasets (small, medium, and large benchmarks from UCI as well as the image benchmarks) are identical; configurations follow NPT-Base (cf. Appendix C.1.1) very closely and changes usually affect NPT optimization rather than architecture.\n\nC.2 Further Details on ABD and ABA Layers\n\n\nC.2.1 Dropout\nIn practice, we apply elementwise dropout on the attention scores exp(QK / \u221a h), as well as on the input/output embeddings and the output of the MHSelfAtt(\u2022) function (often referred to as attention and hidden dropout).\n\nC.3 Input and Output Embdedings\n\n\nC.3.1 Input Embedding\nAt a high-level, we embed inputs by encoding categorical attributes as one-hot vectors and standardizing continuous attributes, followed by a learned linear embedding for each attribute to obtain InputEmbed(X) = H (0) \u2208 R n\u00d7d\u00d7e .\nMore specifically, we perform the following sequence of steps: Attributes X :,j , j \u2208 {1, . . . , d} of the input matrix can be either continuous or categorical. We first apply a function Encode(\u2022) to each attribute X :,j . This \"encodes\" categorical attributes with a one-hot representation and standardizes continuous attributes to zero mean and unit standard deviation. Each encoded attribute j has (potentially unique) dimensions n \u00d7 e j . Then, we concatenate this encoded attribute with its respective column of the masking matrix M :,j along the second dimension to produce a column encoding of dimensions n \u00d7 (e j + 1). We learn separate embedding weights for each attribute W in j \u2208 R (ej +1)\u00d7e that embed all attributes to a common hidden dimension e. Altogether, we can state the embedding of a single attribute column X :,j asH (0) :,j = concat axis=e (Encode(X :,j ), M :,j )W in j + H Index :,j + H Type :,j ,\nwhere H Index :,j \u2208 R n\u00d7e is a learnt embedding for the index and H Type :,j \u2208 R n\u00d7e for the type (either continuous or categorical) of attribute j.\nFinally, we write the full NPT input embedding layer asInputEmbed(X) = stack axis=d (H (0) :,1 , . . . , H:,d ) = H (0) \u2208 R n\u00d7d\u00d7e .\nThe stack operation constructs H (0) \u2208 R n\u00d7d\u00d7e from d attribute embeddings H\n:,j \u2208 R n\u00d7e , j \u2208 {1, . . . d}.\n\nC.3.2 Output Embedding\nFor an NPT with L layers, we obtain an output prediction by applying a learnt linear output embedding (that closely mirrors the process of the input embedding) to the output of the last attention layer H (L) . We write the output embedding layer asOutputEmbed(H (L) ) = [Z :,1 , . . . , Z :,d ] = Z,\nwhere Z :,j = H (L) :,j,: W out j .\nOur prediction Z is a list of d attribute predictions Z j \u2208 R n\u00d7ej . We learn output embedding weights W out j \u2208 R e\u00d7ej which are applied on attribute slices H (L) :,j,: \u2208 R n\u00d7e of the output of the Lth layer H (L) \u2208 R n\u00d7d\u00d7e . Note that the second dimension of each attribute prediction Z j is determined by the encoding size (i.e., e j = 1 for continuous attributes, e j is the number of categories for a categorical attribute) as in the input embedding. Note also that we do not predict a mask value (i.e., we do not predict to dimensions n \u00d7 (e j + 1) for each attribute). To obtain the final prediction matrix X \u2208 R n\u00d7d we take the arg max over the categorical predictions.\n\nC.4 NPT Masking\n\n\nC.4.1 Handling Missing Values\nReal-world data -particularly tabular data -often contains missing entries. Many popular models for supervised prediction on tabular data cannot accommodate missing values as input. Instead they require that missing features are imputed, i.e., an additional model predicts a surrogate value for what the missing values could have been, such that the supervised model then receives a \"clean\" dataset as input which no longer overtly contains missing values.\nFor example, all scikit-learn [69] predictors, including Gradient Boosting and Random Forests, require an explicit imputation step before training. Often, extremely simple imputation methods are used in practice. For example, TabNet [2] drops datapoints with >10% missing entries and otherwise applies univariate mean imputation as part of a Google AI Platform pipeline [70]; and CatBoost [71] treats a missing continuous entry as the minimum or maximum of that feature (univariate min/max imputation), or raises an error. While more complex imputation methods could in theory be applied as pre-processing [43, 50, 81, 82, 88], there will always remain a separation between the imputation step and the prediction model. Additionally, more complex imputation methods often require training and hyperparameter selection, such that the combined imputation and prediction process becomes cumbersome. Both for practical as well as performance reasons, it is desirable to have a single model that can directly handle missing data, learn complex internal imputation operations from the data, and at the same time learn the desired predictive function from features to target. This is exactly what NPTs achieve. They are able to accommodate inputs with missing values gracefully without requiring any imputation pre-processing steps, therefore modeling data with missing values end-to-end. We can explicitly indicate that a value X i,j is missing by simply setting the mask token M i,j = 1. Already in standard NPTs, the stochastic feature masking during training teaches NPTs to predict values for which M i,j = 1 while ignoring the value of their entry X i,j at input. Further, no choice of fixed imputation algorithm has to be made with NPTs. Instead, NPTs learn directly from the data how to make predictions given missing values. Attention between datapoints might be particularly useful for learning a general mechanism of how to impute missing values by attending to other datapoints. We therefore suspect that NPTs could be a strong contender for predicting on data with missing values. Further, unlike common imputation pre-processing, NPTs do not discard the information of which attributes were missing. Future work could also explore the ability of NPT to model arbitrary correlations underlying the pattern of which data is missing, i.e., datasets where values are not missing at random.\n\nC.4.2 Masking Encompasses Many Common Machine Learning Settings\nThe flexible masking mechanism of NPTs can be used to accommodate a variety of common machine learning settings.\nMulti-Target Prediction. In multi-target classification or regression, more than one column of the dataset contains targets. Standard supervised models often do not support multi-output settings and must resort to training multiple models, one for each target. NPTs can accommodate multi-target prediction trivially, since they learn to make predictions at any masked input entry. For prediction in a multi-target setting, we simply apply target masking on all columns with targets.\nSelf-Supervision. In self-supervised learning, we are often interested in learning a generative model or useful encoding from unlabeled data. The reconstruction of corrupted input features as part of stochastic feature masking can already be seen as self-supervised learning. The stochastic masking mechanism allows NPTs to learn to predict masked out values anywhere in the input. In theory, NPTs should be able to learn a fully generative model of the dataset in this manner.\nSemi-Supervision. In semi-supervised learning, we hope to use large quantities of unlabeled data to aid in learning a predictive function on a small set of labeled data. Often, this involves a two-step process, such as learning a powerful autoencoder from all data and then training a predictor using the learnt encoder and the small set of labeled data. NPTs can accommodate semi-supervised learning without changes to the architecture. Specifically, we can include large amounts of unlabeled data by simply appending those feature values to the labeled input dataset. We indicate that no labels are available for all unlabeled datapoints i by setting their mask token at the target column X i ,d = 1. NPTs can use attention between datapoints to make use of information from the features of the unlabeled datapoints.\nImputation. With imputation, we refer to scenarios where the main task is to predict missing values for arbitrary attributes and datapoints. Similar to self-supervision, NPTs already learn how to do this from the stochastic masking mechanism that is enabled by default. (Unlike for the self-supervision category, the imputation scenario assumes that there are actually some missing values that we would like to predict.)\n\nC.4.3 Stochastic Masking: Details\nFor stochastic masking, a specified proportion of training entries (we default to 15% following [24]) are selected for masking at the start of each epoch. Among those entries chosen, we mask out the value with 90% probability and randomize it with 10% probability. \"Masking out\" means that the original value X i,j is overwritten with zeros and the mask token is set to 1. Randomization is done for categorical targets by sampling a new class uniformly at random. Continuous targets are sampled from a standard Normal N (0, 1). This sampling scheme is applied for both stochastic feature masking and stochastic target masking, where we allow for different masking proportions between the two (p feature and p target ). During training, a loss is backpropagated on the masked entries.\n\nC.5 NPT Optimization\nEach of the losses L Features (feature loss) and L Targets (target loss) is normalized by the number of entries on which it is evaluated.\nAs described in Appendix C.1.1: we anneal the \u03bb parameter in the NPT objective using a cosine schedule, i.e., starting with full weight on the feature loss term at epoch 0 and annealing to full weight on the target loss term by the end of training. We use LAMB [98] with Lookahead [99] for optimization, which we find to perform well with large minibatches. We use a flat-then-anneal learning rate schedule with cosine decay, notable as Transformer works [24, 90] often report that a linear learning rate warmup is necessary for training stability. Our placement of Layer Normalization before self-attention (\"pre-LayerNorm\" [3, 16]) may contribute to our not needing this.\n\nD Related Work -Continued D.1 Tree-Based Baselines\nTree-based approaches in machine learning have been popular for over half a century [11, 62, 64]. Each node of a tree splits the data into smaller subsets, and predictions are made at each of the leaves. The splits are learned from a set of training data by minimizing some objective function. Many established methods combine predictions of multiple trees through bagging [9] and/or boosting [78]. Bagging uses an ensemble of trees, each learned by training on a random subsample of the data. This approach is most popularly used in Random Forests [10]. Boosting learns a sequence of trees, conditioning the learning of each additional model on the predictions of previous models, with the aim of reducing overall prediction error.\nPopular examples of tree-based boosting models include AdaBoost [31], XGBoost [17], CatBoost [71], and LightGBM [48]. To date, boosting arguably comprises the most popular approach for tabular data prediction. These models often rely on careful tuning of a large variety of hyperparameters. However, training cost is often cheap compared to neural network architectures, and therefore, so is hyperparameter optimization. This balance is slightly offset for NPTs, which seem largely robust to hyperparameter tuning. Hence, the training of a single NPT is often competitive to a grid search over hyperparameters for a tree-based model.\n\nE Classification and Regression Benchmark Details E.1 General Setup\nFor certain datasets we use a canonical fixed test set. Otherwise, we default to 10-fold cross validation with 0.7/0.2/0.1 splits on smaller datasets and a single 0.7/0.1/0.2 split on larger datasets, where the exact split indices are always consistent across baselines. The full details on all UCI benchmark datasets are given in Tables 14 and 15. Note the variety of the datasets across number of instances, number of features, composition (categorical or continuous) of features, and task (multi-class classification, binary classification, and regression).\n\nE.2 Hyperparameter Tuning\n\n\nE.2.1 Overview\nTable 16 lists the number of unique hyperparameter configurations swept over for each baseline and classification/regression dataset. All details on the NPT hyperparameter setup are given in Appendix C.1. Note that for any given dataset, NPT is tuned over fewer configurations than the baselines: we fix a base model configuration with minimal data-dependent tuning of hyperparameters such as learning rate, scheduler, number of steps, and target masking percentage p feature , and choose the largest batch size viable for our hardware. On small datasets, we then sweep over 8 variants, and on medium and large datasets (including image data) use only the fixed variant with minor modifications.\nIn the case of TabNet, the configurations used for Poker Hand, Forest Cover, and Higgs Boson are those reported by the original authors for these datasets [2]; for Income, we performed a sweep over configurations including one reported for that dataset in the original publication. All deep learning approaches (MLP, TabNet, and NPT) use early stopping on the validation target loss.\n\nE.2.2 Baseline Sweep Details\nWe report hyperparameter sweep details for baselines below. The associated tables for each baseline give the bounds of the search space for numerical hyperparameters and all values for categorical hyperparameters. We clarify specific hyperparameters and provide context where helpful.\nRandom Forest (Tables 17, 18). criterion refers to the split criterion. max_features is the number of features to consider when looking for the best split.\nGradient Boosting, XGBoost, LightGBM, and CatBoost (Table 19). See D.1 for background on tree-based baselines.  Tables 20, 21, 22). The invscaling learning_rate scheduler scales with \u03b1 t = \u03b1 0 /t 0.5 where t is the step, \u03b1 0 the initial learning rate, and \u03b1 t the learning rate at step t. The adaptive learning_rate divides the current learning rate by 5 when two consecutive epochs fail to decrease training or validation log loss by a tolerance 1e-4. Due to compute constraints, we decreased the size of the search space as the dataset size increased by focusing on 3-layer networks, lower L2 penalties, and higher batch sizes. k-NN (Tables 23, 24, 25). weights describes the weight function applied to the neighborhood, i.e., \"distance\" means that closer neighbors of a query point have greater influence than those further away. algorithm specifies the underlying k-NN algorithm, where KD Tree [7] and Ball Tree [60] are approximations of brute-force search. The \"auto\" setting determines an appropriate algorithm based on the input data [69]. leaf_size is a hyperparameter of KD Tree and Ball Tree. p is the power parameter for the distance metric, i.e., p = 1 yields Manhattan and p = 2 Euclidean distance. It was computationally infeasible for us to obtain reasonable results on the 11M instance Higgs Boson dataset. Even when attempting approximate 3-NN on an Azure D64 v3 instance with 256 GB RAM, we encountered an out-of-memory error.\n\nMLP (\n\n\nF Societal Impacts of NPT\nWe have introduced Non-Parametric Transformers, a novel deep learning architecture that predicts by including learned interactions between points of the dataset. In this work, we take first steps towards exploring NPTs and their properties. We do not recommend that NPTs are carelessly applied in production settings, because we do not yet know enough about them. We now list common concerns in applying machine learning models, discuss how they may apply to NPTs, and how to potentially mitigate them.\nMany countries of the world, such as the US, UK, and the countries of the EU, are implementing \"Right to Explanation\"-schemes that grant those affected by autonomous decisionmaking the right to an explanation of why and how decisions were made. In general, Transformer-based architectures such as NPT have been shown to be amenable to explanations, see e.g., [90]. One could argue that our experiments in \u00a74.4 move in an explanatory direction. However, we have not sufficiently investigated the explanations of individual NPT decisions, and believe this to be exciting future work.\nMachine learning models are increasingly used in autonomous decision making that affects human beings in some capacity, e.g., clinical diagnosis, autonomous driving, and detection of toxic comments online. 14 It is of great importance that those decisions are fair, i.e., that they do not discriminate against underrepresented groups in some manner. We have not yet investigated how NPTs respond to common techniques of calibrating machine learning models to fulfil some definition of fairness. We believe that their special predictive behavior from similar datapoints likely poses both challenges and opportunities in this domain. For example, instead of needing to retrain the model to elicit changes in prediction -which could be infeasible in a real-world deployment -NPT could be \"prompted\" with a different set of context datapoints to modify its predictive behavior towards a more socially desirable response.\nIn large architectures based on Transformers, the memorization of training data is a common concern.\nIf the model memorizes training data, adversarial attacks can be used to extract training data from the model weights, see e.g., [14]. This can lead to violations of privacy if, for example, a publicly available model was trained on data that must remain private. This can also cause more subtle problems; for example, if training data \"lives on\" in the model but must be deleted at some point in time to comply with privacy regulations. As NPT directly relies on training data as input for prediction, NPT is not a \"private\" model per definition. However, we can imagine future work tackling this question; for example, by learning to predict from a set of anonymous representative points instead of the training data directly.\nAt the model sizes presented in the paper, the environmental impact of training and using NPT is relatively small compared to some of the large architectures currently in fashion, see e.g., [12]. However, NPT could be scaled up to larger sizes at which point the energy used for training and prediction would become a serious concern. When considering tabular data, training a single NPT model is expensive compared to training a single one of our tree-based baselines such as XGBoost. However, we find that such baselines are often more sensitive to correctly tuned hyperparameters than NPT, such that the total compute including hyperparameter tuning of NPT and the baselines\n\nFootnotes:\n2: A concern here could be that the model will memorize training targets and fail to generalize. In practice, we do not observe generalization issues, likely because (i) a loss is never backpropagated on an unmasked value, and (ii) BERT-style masking [24] uses token randomization to prevent memorization. See Appendix C.4.\n3: We release code for NPTs at github.com/OATML/Non-Parametric-Transformers.\n4: We conduct an informal survey of all Kaggle [45] competitions using tabular data completed in 2020 with a public leaderboard. In 11 out of a total of 13 cases, the winning entries relied on some form of boosting.\n5: In fact, the RMSEs are about equal to the standard deviations of the target values in the Protein dataset,\n6: .11, such that the values obtained by the models on the modified setups amount to random guessing. We further note that we apply all modifications to the standardized input data, such that the Add One setting adds a full standard deviation for the final evaluation in Table3. 6 Gardner, Jacob R., et al. \"Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.\" NeurIPS 2018.\n7: There are many fewer kept than deleted datapoints. Further, there are outliers in the dataset, and these affect the deleted datapoints more often than the kept datapoints. We find that the average distance between a random subset and the deleted (not the kept!) datapoints also becomes statistically significantly smaller at large sample sizes. Hence, we compare the deleted datapoints to a random subset to control for size effects.\n12: Note that we do not search a 2 8 grid over these modifications. We only try out these 8 distinct models.\n13: Training is slower as only a ptarget proportion of training labels are used for backpropagation in each epoch. Therefore, target masking may increase training time beyond our budget.\n14: For example, see [28, 63, 87].\n\nReferences:\n\n- Naomi S Altman. An introduction to kernel and nearest-neighbor nonparametric regression. The American Statistician, 46, 1992.- Sercan O Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. arXiv:1908.07442, 2019.\n\n- Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450, 2016.\n\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.\n\n- Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020.\n\n- Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. In International Joint Conference on Neural Networks, volume 2, 1991.\n\n- J. L. Bentley. Multidimensional binary search trees used for associative searching. In Communications of the ACM, volume 18, 1975.\n\n- John B Biggs. The role of metalearning in study processes. British journal of educational psychology, 55, 1985.\n\n- Leo Breiman. Bagging predictors. Machine learning, 24, 1996.\n\n- Leo Breiman. Random forests. Machine learning, 45, 2001.\n\n- Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classification and regression trees. CRC press, 1984.\n\n- Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv:2005.14165, 2020.\n\n- Thang Bui, Daniel Hern\u00e1ndez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner. Deep gaussian processes for regression using approximate expectation propagation. In International Conference on Machine Learning, 2016.\n\n- Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. arXiv:2012.07805, 2020.\n\n- Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. IEEE Transactions on Neural Networks, 20(3):542-542, 2009.\n\n- Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining recent advances in neural machine translation. In Annual Meeting of the Association for Computational Linguistics, volume 56, 2018.\n\n- Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Knowledge Discovery and Data Mining, volume 22, 2016.\n\n- Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv:1904.10509, 2019.\n\n- Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021.\n\n- Michael Chui, James Manyika, Mehdi Miremadi, Nicolaus Henke, Rita Chung, Pieter Nel, and Sankalp Malhotra. Notes from the AI frontier: Insights from hundreds of use cases, 2018.\n\n- Zhenwen Dai, Andreas Damianou, Javier Gonz\u00e1lez, and Neil Lawrence. Variational auto-encoded deep gaussian processes. In International Conference on Learning Representations, 2016.\n\n- Andreas Damianou and Neil D Lawrence. Deep gaussian processes. In International Conference on Artificial Intelligence and Statistics, volume 16, 2013.\n\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv:1810.04805, 2018.\n\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\n- Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci. edu/ml.\n\n- Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In International Conference on Artificial Intelligence and Statistics, volume 13, pages 201-208, 2010.\n\n- Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim GJ Rudner, Zachary Kenton, Lewis Smith, Milad Alizadeh, Arnoud De Kroon, and Yarin Gal. A systematic comparison of bayesian deep learning robustness in diabetic retinopathy tasks. In NeurIPS Workshop on Bayesian Deep Learning, 2019.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, volume 34, 2017.\n\n- Evelyn Fix. Discriminatory analysis: nonparametric discrimination, consistency properties, volume 1. USAF school of Aviation Medicine, 1985.\n\n- Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55, 1997.\n\n- Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, 2001.\n\n- Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\n- Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In International Conference on Learning Representations, 2018.\n\n- Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\n- Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, volume 35, 2018.\n\n- Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv:1807.01622, 2018.\n\n- Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437-450, 2018.\n\n- Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval- augmented language model pre-training. arXiv:2002.08909, 2020.\n\n- Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585, 2020.\n\n- Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit framework for predicting structured outputs. In Advances in neural information processing systems, 2018.\n\n- Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv:1912.12180, 2019.\n\n- James Honaker and Gary King. What to do about missing values in time series cross-section data. American Journal of Political Science, 2010.\n\n- Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. Lietransformer: Equivariant self-attention for lie groups. arXiv:2012.10885, 2020.\n\n- Google Inc. Kaggle. https://www.kaggle.com/, 2021.\n\n- Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention. arXiv:2103.03206, 2021.\n\n- Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, volume 37, 2020.\n\n- Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in neural information processing systems, volume 30, 2017.\n\n- Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning Representations, 2019.\n\n- Gary King, James Honaker, Anne Joseph, and Kenneth Scheve. Analyzing incomplete political science data: An alternative algorithm for multiple imputation. American Political Science Review, 2001.\n\n- Diederik P Kingma, Danilo J Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning with deep generative models. arXiv:1406.5298, 2014.\n\n- Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.\n\n- Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, volume 35, 2018.\n\n- Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European Conference on Computer Vision, 2020.\n\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\n\n- Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350, 2015.\n\n- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86, 1998.\n\n- Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online], 2, 2010.\n\n- Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, volume 36, 2019.\n\n- T. Liu, A. Moore, and A. Gray. New algorithms for efficient high-dimensional nonparametric classification. In Journal of Machine Learning Research, volume 7, 2006.\n\n- Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\n- Wei-Yin Loh. Fifty years of classification and regression trees. International Statistical Review, 82, 2014.\n\n- Rhiannon Michelmore, Marta Kwiatkowska, and Yarin Gal. Evaluating uncertainty quantification in end-to-end autonomous driving control. arXiv:1811.06817, 2018.\n\n- James N Morgan and John A Sonquist. Problems in the analysis of survey data, and a proposal. Journal of the American statistical association, 58, 1963.\n\n- Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In International Conference on Computer Vision, pages 360-368, 2017.\n\n- Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault F\u00e9vry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across implementations and applications? arXiv:2102.11972, 2021.\n\n- Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, volume 35, 2018.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019.\n\n- Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2011.\n\n- Google Cloud AI Platform. Getting started with the built-in tabnet algorithm, 2021. URL cloud.google. com/ai-platform/training/docs/algorithms/tab-net-start.\n\n- Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. In S. Bengio, H. Wallach, H. Larochelle, K. Grau- man, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, 2018.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019.\n\n- Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. bioRxiv, 2021.\n\n- Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine learning, 2003.\n\n- Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv:2104.10972, 2021.\n\n- Sam Roweis, Geoffrey Hinton, and Ruslan Salakhutdinov. Neighbourhood component analysis. In Advances in Neural Information Processing Systems, volume 17, page 4, 2004.\n\n- Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, 2017.\n\n- Robert E Schapire. The strength of weak learnability. Machine learning, 5, 1990.\n\n- Jenny Seidenschwarz, Ismail Elezi, and Laura Leal-Taix\u00e9. Learning intra-batch connections for deep metric learning. In International Conference on Machine Learning, 2021.\n\n- Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019.\n\n- D.J. Stekhoven and P. Buehlmann. Missforest -nonparametric missing value imputation for mixed-type data. Bioinformatics, 2012.\n\n- Yu-Sung Su, Andrew E. Gelman, Jennifer Hill, and Masanao Yajima. Multiple imputation with diagnostics (mi) in R: Opening windows into the black box. Journal of Statistical Software, 2012.\n\n- C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In 2017 IEEE International Conference on Computer Vision (ICCV), 2017.\n\n- Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv:2009.06732, 2020.\n\n- Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\n- Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv:2012.12877, 2020.\n\n- Betty Van Aken, Julian Risch, Ralf Krestel, and Alexander L\u00f6ser. Challenges for toxic comment classifica- tion: An in-depth error analysis. arXiv preprint arXiv:1809.07572, 2018.\n\n- Stef van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 2011.\n\n- Vladimir Vapnik. Estimation of dependences based on empirical data. Springer Science & Business Media, 2006.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.\n\n- Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\n\n- Sethu Vijayakumar and Stefan Schaal. Local dimensionality reduction for locally weighted learning. In International Symposium on Computational Intelligence in Robotics and Automation, pages 220-225. IEEE, 1997.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, volume 29, pages 3630-3638, 2016.\n\n- Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, and Neil M Robertson. Ranked list loss for deep metric learning. In Conference on Computer Vision and Pattern Recognition, pages 5207-5216, 2019.\n\n- Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In International Conference on Artificial Intelligence and Statistics, volume 19, 2016.\n\n- Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.\n\n- Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, 2018.\n\n- Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020.\n\n- Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\n", "annotations": {"ReferenceToTable": [{"begin": 25218, "end": 25219, "target": "#tab_0", "idx": 0}, {"begin": 28267, "end": 28269, "target": "#tab_13", "idx": 1}, {"begin": 31439, "end": 31440, "target": "#tab_1", "idx": 2}, {"begin": 45304, "end": 45305, "target": "#tab_3", "idx": 3}, {"begin": 46496, "end": 46498, "target": "#tab_13", "idx": 4}, {"begin": 53326, "end": 53327, "target": "#tab_6", "idx": 5}, {"begin": 54979, "end": 54980, "target": "#tab_6", "idx": 6}, {"begin": 55199, "end": 55200, "target": "#tab_6", "idx": 7}, {"begin": 55710, "end": 55711, "target": "#tab_6", "idx": 8}, {"begin": 56608, "end": 56609, "target": "#tab_6", "idx": 9}, {"begin": 58872, "end": 58873, "target": "#tab_7", "idx": 10}, {"begin": 59985, "end": 59986, "target": "#tab_7", "idx": 11}, {"begin": 62466, "end": 62473, "target": "#tab_9", "idx": 12}, {"begin": 64431, "end": 64432, "target": "#tab_10", "idx": 13}, {"begin": 64440, "end": 64441, "target": "#tab_11", "idx": 14}, {"begin": 64554, "end": 64556, "target": "#tab_12", "idx": 15}, {"begin": 64653, "end": 64655, "target": "#tab_13", "idx": 16}, {"begin": 64682, "end": 64684, "target": "#tab_14", "idx": 17}, {"begin": 74668, "end": 74670, "target": "#tab_3", "idx": 18}, {"begin": 75791, "end": 75793, "target": "#tab_3", "idx": 19}, {"begin": 75946, "end": 75948, "target": "#tab_3", "idx": 20}, {"begin": 76296, "end": 76297, "target": "#tab_6", "idx": 21}, {"begin": 89384, "end": 89393, "target": "#tab_16", "idx": 22}, {"begin": 89657, "end": 89659, "target": "#tab_8", "idx": 23}, {"begin": 91068, "end": 91074, "target": "#tab_10", "idx": 24}, {"begin": 91260, "end": 91262, "target": "#tab_11", "idx": 25}, {"begin": 91845, "end": 91851, "target": "#tab_17", "idx": 26}], "ReferenceToFootnote": [{"begin": 16423, "end": 16424, "target": "#foot_0", "idx": 0}, {"begin": 23340, "end": 23341, "target": "#foot_1", "idx": 1}, {"begin": 24135, "end": 24136, "target": "#foot_2", "idx": 2}, {"begin": 46500, "end": 46501, "target": "#foot_3", "idx": 3}, {"begin": 47507, "end": 47508, "target": "#foot_4", "idx": 4}, {"begin": 51713, "end": 51714, "target": "#foot_5", "idx": 5}, {"begin": 73790, "end": 73792, "target": "#foot_6", "idx": 6}, {"begin": 75885, "end": 75887, "target": "#foot_7", "idx": 7}, {"begin": 93974, "end": 93976, "target": "#foot_8", "idx": 8}], "SectionMain": [{"begin": 1074, "end": 96192, "idx": 0}], "ReferenceToFormula": [{"begin": 58745, "end": 58746, "target": "#formula_5", "idx": 0}], "SectionReference": [{"begin": 98107, "end": 116870, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1074, "idx": 0}], "Div": [{"begin": 106, "end": 1066, "idx": 0}, {"begin": 1077, "end": 5581, "idx": 1}, {"begin": 5583, "end": 6790, "idx": 2}, {"begin": 6792, "end": 8669, "idx": 3}, {"begin": 8671, "end": 10223, "idx": 4}, {"begin": 10225, "end": 10539, "idx": 5}, {"begin": 10541, "end": 11997, "idx": 6}, {"begin": 11999, "end": 13434, "idx": 7}, {"begin": 13436, "end": 14428, "idx": 8}, {"begin": 14430, "end": 17734, "idx": 9}, {"begin": 17736, "end": 23257, "idx": 10}, {"begin": 23259, "end": 26949, "idx": 11}, {"begin": 26951, "end": 30397, "idx": 12}, {"begin": 30399, "end": 32968, "idx": 13}, {"begin": 32970, "end": 35362, "idx": 14}, {"begin": 35364, "end": 37859, "idx": 15}, {"begin": 37861, "end": 37956, "idx": 16}, {"begin": 37958, "end": 40511, "idx": 17}, {"begin": 40513, "end": 41285, "idx": 18}, {"begin": 41287, "end": 41308, "idx": 19}, {"begin": 41310, "end": 41341, "idx": 20}, {"begin": 41343, "end": 42410, "idx": 21}, {"begin": 42412, "end": 50281, "idx": 22}, {"begin": 50283, "end": 50292, "idx": 23}, {"begin": 50294, "end": 53203, "idx": 24}, {"begin": 53205, "end": 60439, "idx": 25}, {"begin": 60441, "end": 64370, "idx": 26}, {"begin": 64372, "end": 70753, "idx": 27}, {"begin": 70755, "end": 71157, "idx": 28}, {"begin": 71159, "end": 72141, "idx": 29}, {"begin": 72143, "end": 75294, "idx": 30}, {"begin": 75296, "end": 77708, "idx": 31}, {"begin": 77710, "end": 77752, "idx": 32}, {"begin": 77754, "end": 77987, "idx": 33}, {"begin": 77989, "end": 78021, "idx": 34}, {"begin": 78023, "end": 79588, "idx": 35}, {"begin": 79590, "end": 80626, "idx": 36}, {"begin": 80628, "end": 80644, "idx": 37}, {"begin": 80646, "end": 83525, "idx": 38}, {"begin": 83527, "end": 85904, "idx": 39}, {"begin": 85906, "end": 86723, "idx": 40}, {"begin": 86725, "end": 87557, "idx": 41}, {"begin": 87559, "end": 88976, "idx": 42}, {"begin": 88978, "end": 89606, "idx": 43}, {"begin": 89608, "end": 89634, "idx": 44}, {"begin": 89636, "end": 90730, "idx": 45}, {"begin": 90732, "end": 92647, "idx": 46}, {"begin": 92649, "end": 92655, "idx": 47}, {"begin": 92657, "end": 96192, "idx": 48}], "Head": [{"begin": 1077, "end": 1091, "n": "1", "idx": 0}, {"begin": 5583, "end": 5612, "n": "2", "idx": 1}, {"begin": 6792, "end": 6814, "n": "2.1", "idx": 2}, {"begin": 8671, "end": 8691, "n": "2.2", "idx": 3}, {"begin": 10225, "end": 10254, "n": "2.3", "idx": 4}, {"begin": 10541, "end": 10613, "idx": 5}, {"begin": 11999, "end": 12037, "n": "2.4", "idx": 6}, {"begin": 13436, "end": 13474, "n": "2.5", "idx": 7}, {"begin": 14430, "end": 14458, "n": "2.6", "idx": 8}, {"begin": 17736, "end": 17750, "n": "3", "idx": 9}, {"begin": 23259, "end": 23272, "n": "4", "idx": 10}, {"begin": 26951, "end": 27015, "n": "4.2", "idx": 11}, {"begin": 30399, "end": 30462, "n": "4.3", "idx": 12}, {"begin": 32970, "end": 33034, "n": "4.4", "idx": 13}, {"begin": 35364, "end": 35407, "n": "5", "idx": 14}, {"begin": 37861, "end": 37955, "idx": 15}, {"begin": 37958, "end": 37966, "idx": 16}, {"begin": 40513, "end": 40603, "n": "3.", "idx": 17}, {"begin": 41287, "end": 41307, "idx": 18}, {"begin": 41310, "end": 41340, "idx": 19}, {"begin": 41343, "end": 41398, "idx": 20}, {"begin": 42412, "end": 42453, "idx": 21}, {"begin": 50283, "end": 50291, "idx": 22}, {"begin": 50294, "end": 50324, "idx": 23}, {"begin": 53205, "end": 53246, "idx": 24}, {"begin": 60441, "end": 60494, "idx": 25}, {"begin": 64372, "end": 64420, "idx": 26}, {"begin": 70755, "end": 70767, "idx": 27}, {"begin": 71159, "end": 71180, "idx": 28}, {"begin": 72143, "end": 72175, "idx": 29}, {"begin": 75296, "end": 75339, "idx": 30}, {"begin": 77710, "end": 77751, "idx": 31}, {"begin": 77754, "end": 77767, "idx": 32}, {"begin": 77989, "end": 78020, "idx": 33}, {"begin": 78023, "end": 78044, "idx": 34}, {"begin": 79590, "end": 79612, "idx": 35}, {"begin": 80628, "end": 80643, "idx": 36}, {"begin": 80646, "end": 80675, "idx": 37}, {"begin": 83527, "end": 83590, "idx": 38}, {"begin": 85906, "end": 85939, "idx": 39}, {"begin": 86725, "end": 86745, "idx": 40}, {"begin": 87559, "end": 87609, "idx": 41}, {"begin": 88978, "end": 89045, "idx": 42}, {"begin": 89608, "end": 89633, "idx": 43}, {"begin": 89636, "end": 89650, "idx": 44}, {"begin": 90732, "end": 90760, "idx": 45}, {"begin": 92649, "end": 92654, "idx": 46}, {"begin": 92657, "end": 92682, "idx": 47}], "Paragraph": [{"begin": 106, "end": 1066, "idx": 0}, {"begin": 1092, "end": 1905, "idx": 1}, {"begin": 1906, "end": 3079, "idx": 2}, {"begin": 3080, "end": 4731, "idx": 3}, {"begin": 4732, "end": 5404, "idx": 4}, {"begin": 5405, "end": 5581, "idx": 5}, {"begin": 5613, "end": 6790, "idx": 6}, {"begin": 6815, "end": 7635, "idx": 7}, {"begin": 7636, "end": 8099, "idx": 8}, {"begin": 8100, "end": 8669, "idx": 9}, {"begin": 8692, "end": 9190, "idx": 10}, {"begin": 9191, "end": 9662, "idx": 11}, {"begin": 9663, "end": 10223, "idx": 12}, {"begin": 10255, "end": 10539, "idx": 13}, {"begin": 10677, "end": 11025, "idx": 14}, {"begin": 11064, "end": 11151, "idx": 15}, {"begin": 11253, "end": 11727, "idx": 16}, {"begin": 11763, "end": 11954, "idx": 17}, {"begin": 12038, "end": 12401, "idx": 18}, {"begin": 12482, "end": 12740, "idx": 19}, {"begin": 12741, "end": 13434, "idx": 20}, {"begin": 13475, "end": 13808, "idx": 21}, {"begin": 13941, "end": 14428, "idx": 22}, {"begin": 14459, "end": 14705, "idx": 23}, {"begin": 14778, "end": 15413, "idx": 24}, {"begin": 15414, "end": 15818, "idx": 25}, {"begin": 15819, "end": 17033, "idx": 26}, {"begin": 17034, "end": 17734, "idx": 27}, {"begin": 17751, "end": 18412, "idx": 28}, {"begin": 18413, "end": 19190, "idx": 29}, {"begin": 19191, "end": 20177, "idx": 30}, {"begin": 20178, "end": 20485, "idx": 31}, {"begin": 20486, "end": 21676, "idx": 32}, {"begin": 21677, "end": 22552, "idx": 33}, {"begin": 22553, "end": 22761, "idx": 34}, {"begin": 22762, "end": 23257, "idx": 35}, {"begin": 23273, "end": 25094, "idx": 36}, {"begin": 25095, "end": 26090, "idx": 37}, {"begin": 26091, "end": 26389, "idx": 38}, {"begin": 26390, "end": 26949, "idx": 39}, {"begin": 27016, "end": 27818, "idx": 40}, {"begin": 27819, "end": 28425, "idx": 41}, {"begin": 28426, "end": 29113, "idx": 42}, {"begin": 29114, "end": 29611, "idx": 43}, {"begin": 29714, "end": 30000, "idx": 44}, {"begin": 30001, "end": 30397, "idx": 45}, {"begin": 30463, "end": 31267, "idx": 46}, {"begin": 31268, "end": 31366, "idx": 47}, {"begin": 31367, "end": 32163, "idx": 48}, {"begin": 32164, "end": 32968, "idx": 49}, {"begin": 33035, "end": 33375, "idx": 50}, {"begin": 33376, "end": 34553, "idx": 51}, {"begin": 34554, "end": 34972, "idx": 52}, {"begin": 34973, "end": 35362, "idx": 53}, {"begin": 35408, "end": 35987, "idx": 54}, {"begin": 35988, "end": 36569, "idx": 55}, {"begin": 36570, "end": 37859, "idx": 56}, {"begin": 38209, "end": 38457, "idx": 57}, {"begin": 38458, "end": 38584, "idx": 58}, {"begin": 38585, "end": 38699, "idx": 59}, {"begin": 38830, "end": 38891, "idx": 60}, {"begin": 39061, "end": 39131, "idx": 61}, {"begin": 39186, "end": 39191, "idx": 62}, {"begin": 39350, "end": 39586, "idx": 63}, {"begin": 39613, "end": 39730, "idx": 64}, {"begin": 39903, "end": 39954, "idx": 65}, {"begin": 39955, "end": 40027, "idx": 66}, {"begin": 40028, "end": 40080, "idx": 67}, {"begin": 40237, "end": 40262, "idx": 68}, {"begin": 40263, "end": 40511, "idx": 69}, {"begin": 40604, "end": 41085, "idx": 70}, {"begin": 41086, "end": 41125, "idx": 71}, {"begin": 41126, "end": 41285, "idx": 72}, {"begin": 41399, "end": 41813, "idx": 73}, {"begin": 41814, "end": 42410, "idx": 74}, {"begin": 42454, "end": 43668, "idx": 75}, {"begin": 43669, "end": 43969, "idx": 76}, {"begin": 43970, "end": 44186, "idx": 77}, {"begin": 44187, "end": 44767, "idx": 78}, {"begin": 44768, "end": 44971, "idx": 79}, {"begin": 44972, "end": 45288, "idx": 80}, {"begin": 45289, "end": 45469, "idx": 81}, {"begin": 45470, "end": 46135, "idx": 82}, {"begin": 46136, "end": 46696, "idx": 83}, {"begin": 46697, "end": 46954, "idx": 84}, {"begin": 46955, "end": 47246, "idx": 85}, {"begin": 47247, "end": 47642, "idx": 86}, {"begin": 47643, "end": 48055, "idx": 87}, {"begin": 48056, "end": 48768, "idx": 88}, {"begin": 48769, "end": 49002, "idx": 89}, {"begin": 49003, "end": 49062, "idx": 90}, {"begin": 49195, "end": 49323, "idx": 91}, {"begin": 49647, "end": 50281, "idx": 92}, {"begin": 50325, "end": 51222, "idx": 93}, {"begin": 51223, "end": 51565, "idx": 94}, {"begin": 51566, "end": 52315, "idx": 95}, {"begin": 52316, "end": 53203, "idx": 96}, {"begin": 53247, "end": 53522, "idx": 97}, {"begin": 53523, "end": 53777, "idx": 98}, {"begin": 53778, "end": 54406, "idx": 99}, {"begin": 54407, "end": 54969, "idx": 100}, {"begin": 54970, "end": 56581, "idx": 101}, {"begin": 56582, "end": 57068, "idx": 102}, {"begin": 57069, "end": 57460, "idx": 103}, {"begin": 57461, "end": 57890, "idx": 104}, {"begin": 57891, "end": 58540, "idx": 105}, {"begin": 58541, "end": 58843, "idx": 106}, {"begin": 58844, "end": 58947, "idx": 107}, {"begin": 58948, "end": 59720, "idx": 108}, {"begin": 59721, "end": 59957, "idx": 109}, {"begin": 59958, "end": 60439, "idx": 110}, {"begin": 60495, "end": 61138, "idx": 111}, {"begin": 61139, "end": 61511, "idx": 112}, {"begin": 61512, "end": 62119, "idx": 113}, {"begin": 62120, "end": 62411, "idx": 114}, {"begin": 62412, "end": 63025, "idx": 115}, {"begin": 63026, "end": 63673, "idx": 116}, {"begin": 63674, "end": 64173, "idx": 117}, {"begin": 64174, "end": 64370, "idx": 118}, {"begin": 64421, "end": 66147, "idx": 119}, {"begin": 66148, "end": 66494, "idx": 120}, {"begin": 66495, "end": 66701, "idx": 121}, {"begin": 66702, "end": 67142, "idx": 122}, {"begin": 67143, "end": 67475, "idx": 123}, {"begin": 67476, "end": 67919, "idx": 124}, {"begin": 67920, "end": 68079, "idx": 125}, {"begin": 68080, "end": 68837, "idx": 126}, {"begin": 68838, "end": 69199, "idx": 127}, {"begin": 69200, "end": 69436, "idx": 128}, {"begin": 69437, "end": 70753, "idx": 129}, {"begin": 70768, "end": 70876, "idx": 130}, {"begin": 70877, "end": 70887, "idx": 131}, {"begin": 70888, "end": 71116, "idx": 132}, {"begin": 71117, "end": 71157, "idx": 133}, {"begin": 71181, "end": 71389, "idx": 134}, {"begin": 71390, "end": 71747, "idx": 135}, {"begin": 71748, "end": 72141, "idx": 136}, {"begin": 72176, "end": 72289, "idx": 137}, {"begin": 72290, "end": 72895, "idx": 138}, {"begin": 72896, "end": 73133, "idx": 139}, {"begin": 73134, "end": 73665, "idx": 140}, {"begin": 73666, "end": 73835, "idx": 141}, {"begin": 73836, "end": 73871, "idx": 142}, {"begin": 73872, "end": 73906, "idx": 143}, {"begin": 73907, "end": 73970, "idx": 144}, {"begin": 73971, "end": 74031, "idx": 145}, {"begin": 74032, "end": 74092, "idx": 146}, {"begin": 74093, "end": 74172, "idx": 147}, {"begin": 74173, "end": 74350, "idx": 148}, {"begin": 74351, "end": 74564, "idx": 149}, {"begin": 74565, "end": 74756, "idx": 150}, {"begin": 74757, "end": 75081, "idx": 151}, {"begin": 75082, "end": 75294, "idx": 152}, {"begin": 75340, "end": 75939, "idx": 153}, {"begin": 75940, "end": 76170, "idx": 154}, {"begin": 76171, "end": 77281, "idx": 155}, {"begin": 77296, "end": 77399, "idx": 156}, {"begin": 77400, "end": 77708, "idx": 157}, {"begin": 77768, "end": 77987, "idx": 158}, {"begin": 78045, "end": 78274, "idx": 159}, {"begin": 78275, "end": 79113, "idx": 160}, {"begin": 79199, "end": 79347, "idx": 161}, {"begin": 79348, "end": 79403, "idx": 162}, {"begin": 79480, "end": 79556, "idx": 163}, {"begin": 79557, "end": 79588, "idx": 164}, {"begin": 79613, "end": 79861, "idx": 165}, {"begin": 79913, "end": 79948, "idx": 166}, {"begin": 79949, "end": 80626, "idx": 167}, {"begin": 80676, "end": 81132, "idx": 168}, {"begin": 81133, "end": 83525, "idx": 169}, {"begin": 83591, "end": 83703, "idx": 170}, {"begin": 83704, "end": 84186, "idx": 171}, {"begin": 84187, "end": 84664, "idx": 172}, {"begin": 84665, "end": 85483, "idx": 173}, {"begin": 85484, "end": 85904, "idx": 174}, {"begin": 85940, "end": 86723, "idx": 175}, {"begin": 86746, "end": 86883, "idx": 176}, {"begin": 86884, "end": 87557, "idx": 177}, {"begin": 87610, "end": 88342, "idx": 178}, {"begin": 88343, "end": 88976, "idx": 179}, {"begin": 89046, "end": 89606, "idx": 180}, {"begin": 89651, "end": 90346, "idx": 181}, {"begin": 90347, "end": 90730, "idx": 182}, {"begin": 90761, "end": 91045, "idx": 183}, {"begin": 91046, "end": 91201, "idx": 184}, {"begin": 91202, "end": 92647, "idx": 185}, {"begin": 92683, "end": 93185, "idx": 186}, {"begin": 93186, "end": 93767, "idx": 187}, {"begin": 93768, "end": 94684, "idx": 188}, {"begin": 94685, "end": 94785, "idx": 189}, {"begin": 94786, "end": 95514, "idx": 190}, {"begin": 95515, "end": 96192, "idx": 191}], "ReferenceToBib": [{"begin": 1102, "end": 1106, "target": "#b56", "idx": 0}, {"begin": 1123, "end": 1127, "target": "#b89", "idx": 1}, {"begin": 2526, "end": 2529, "target": "#b3", "idx": 2}, {"begin": 2530, "end": 2533, "target": "#b58", "idx": 3}, {"begin": 2534, "end": 2537, "target": "#b89", "idx": 4}, {"begin": 2740, "end": 2744, "target": "#b23", "idx": 5}, {"begin": 3492, "end": 3496, "target": "#b73", "idx": 6}, {"begin": 3674, "end": 3684, "idx": 7}, {"begin": 4077, "end": 4081, "target": "#b21", "idx": 8}, {"begin": 4104, "end": 4108, "target": "#b94", "idx": 9}, {"begin": 4131, "end": 4135, "target": "#b35", "idx": 10}, {"begin": 4136, "end": 4139, "target": "#b36", "idx": 11}, {"begin": 4140, "end": 4143, "target": "#b48", "idx": 12}, {"begin": 5779, "end": 5782, "target": "#b0", "idx": 13}, {"begin": 6076, "end": 6079, "target": "#b1", "idx": 14}, {"begin": 6419, "end": 6423, "target": "#b23", "idx": 15}, {"begin": 7664, "end": 7668, "target": "#b23", "idx": 16}, {"begin": 8627, "end": 8628, "target": "#b3", "idx": 17}, {"begin": 9042, "end": 9045, "target": "#b3", "idx": 18}, {"begin": 9046, "end": 9049, "target": "#b23", "idx": 19}, {"begin": 9050, "end": 9053, "target": "#b89", "idx": 20}, {"begin": 9479, "end": 9480, "target": "#b2", "idx": 21}, {"begin": 10428, "end": 10431, "target": "#b3", "idx": 22}, {"begin": 10432, "end": 10435, "target": "#b23", "idx": 23}, {"begin": 10436, "end": 10439, "target": "#b89", "idx": 24}, {"begin": 11610, "end": 11614, "target": "#b15", "idx": 25}, {"begin": 11615, "end": 11618, "target": "#b23", "idx": 26}, {"begin": 11619, "end": 11622, "target": "#b58", "idx": 27}, {"begin": 11623, "end": 11626, "target": "#b65", "idx": 28}, {"begin": 11627, "end": 11630, "target": "#b89", "idx": 29}, {"begin": 11698, "end": 11701, "target": "#b2", "idx": 30}, {"begin": 12927, "end": 12931, "target": "#b23", "idx": 31}, {"begin": 12932, "end": 12935, "target": "#b24", "idx": 32}, {"begin": 12936, "end": 12939, "target": "#b45", "idx": 33}, {"begin": 12940, "end": 12943, "target": "#b89", "idx": 34}, {"begin": 13425, "end": 13429, "target": "#b23", "idx": 35}, {"begin": 13430, "end": 13433, "target": "#b89", "idx": 36}, {"begin": 13976, "end": 13980, "target": "#b23", "idx": 37}, {"begin": 13981, "end": 13984, "target": "#b24", "idx": 38}, {"begin": 13985, "end": 13988, "target": "#b45", "idx": 39}, {"begin": 13989, "end": 13992, "target": "#b89", "idx": 40}, {"begin": 14506, "end": 14510, "target": "#b23", "idx": 41}, {"begin": 14947, "end": 14951, "target": "#b23", "idx": 42}, {"begin": 15344, "end": 15345, "target": "#b3", "idx": 43}, {"begin": 17803, "end": 17807, "target": "#b21", "idx": 44}, {"begin": 17839, "end": 17843, "target": "#b94", "idx": 45}, {"begin": 17881, "end": 17885, "target": "#b73", "idx": 46}, {"begin": 18165, "end": 18169, "target": "#b12", "idx": 47}, {"begin": 18170, "end": 18173, "target": "#b20", "idx": 48}, {"begin": 18174, "end": 18177, "target": "#b76", "idx": 49}, {"begin": 18470, "end": 18474, "target": "#b35", "idx": 50}, {"begin": 18475, "end": 18478, "target": "#b36", "idx": 51}, {"begin": 18734, "end": 18738, "target": "#b48", "idx": 52}, {"begin": 18917, "end": 18921, "target": "#b48", "idx": 53}, {"begin": 19116, "end": 19120, "target": "#b48", "idx": 54}, {"begin": 19376, "end": 19380, "target": "#b24", "idx": 55}, {"begin": 19381, "end": 19384, "target": "#b45", "idx": 56}, {"begin": 19385, "end": 19388, "target": "#b66", "idx": 57}, {"begin": 19456, "end": 19460, "target": "#b32", "idx": 58}, {"begin": 19461, "end": 19464, "target": "#b43", "idx": 59}, {"begin": 19465, "end": 19468, "target": "#b58", "idx": 60}, {"begin": 19469, "end": 19472, "target": "#b60", "idx": 61}, {"begin": 19501, "end": 19505, "target": "#b58", "idx": 62}, {"begin": 19723, "end": 19727, "target": "#b41", "idx": 63}, {"begin": 19749, "end": 19753, "target": "#b72", "idx": 64}, {"begin": 20186, "end": 20190, "target": "#b79", "idx": 65}, {"begin": 20267, "end": 20270, "target": "#b1", "idx": 66}, {"begin": 20763, "end": 20766, "target": "#b5", "idx": 67}, {"begin": 20767, "end": 20769, "target": "#b7", "idx": 68}, {"begin": 20770, "end": 20773, "target": "#b28", "idx": 69}, {"begin": 20774, "end": 20777, "target": "#b55", "idx": 70}, {"begin": 21056, "end": 21060, "target": "#b28", "idx": 71}, {"begin": 21061, "end": 21064, "target": "#b96", "idx": 72}, {"begin": 21169, "end": 21173, "target": "#b11", "idx": 73}, {"begin": 21174, "end": 21177, "target": "#b71", "idx": 74}, {"begin": 21777, "end": 21781, "target": "#b14", "idx": 75}, {"begin": 21782, "end": 21785, "target": "#b26", "idx": 76}, {"begin": 21786, "end": 21789, "target": "#b50", "idx": 77}, {"begin": 21816, "end": 21820, "target": "#b88", "idx": 78}, {"begin": 22142, "end": 22146, "target": "#b33", "idx": 79}, {"begin": 22147, "end": 22150, "target": "#b51", "idx": 80}, {"begin": 22151, "end": 22154, "target": "#b52", "idx": 81}, {"begin": 22155, "end": 22158, "target": "#b90", "idx": 82}, {"begin": 22159, "end": 22162, "target": "#b95", "idx": 83}, {"begin": 22389, "end": 22393, "target": "#b52", "idx": 84}, {"begin": 22733, "end": 22737, "target": "#b64", "idx": 85}, {"begin": 22738, "end": 22741, "target": "#b75", "idx": 86}, {"begin": 22742, "end": 22745, "target": "#b78", "idx": 87}, {"begin": 22746, "end": 22750, "target": "#b91", "idx": 88}, {"begin": 22751, "end": 22755, "target": "#b92", "idx": 89}, {"begin": 22756, "end": 22760, "target": "#b93", "idx": 90}, {"begin": 22857, "end": 22861, "target": "#b37", "idx": 91}, {"begin": 22862, "end": 22865, "target": "#b38", "idx": 92}, {"begin": 22866, "end": 22869, "target": "#b40", "idx": 93}, {"begin": 23815, "end": 23819, "target": "#b25", "idx": 94}, {"begin": 23844, "end": 23848, "target": "#b54", "idx": 95}, {"begin": 23859, "end": 23863, "target": "#b57", "idx": 96}, {"begin": 23953, "end": 23957, "target": "#b19", "idx": 97}, {"begin": 24129, "end": 24133, "target": "#b77", "idx": 98}, {"begin": 24521, "end": 24525, "target": "#b9", "idx": 99}, {"begin": 24551, "end": 24555, "target": "#b31", "idx": 100}, {"begin": 24565, "end": 24569, "target": "#b16", "idx": 101}, {"begin": 24580, "end": 24584, "target": "#b70", "idx": 102}, {"begin": 24595, "end": 24599, "target": "#b47", "idx": 103}, {"begin": 24612, "end": 24615, "target": "#b0", "idx": 104}, {"begin": 24616, "end": 24619, "target": "#b29", "idx": 105}, {"begin": 24632, "end": 24635, "target": "#b1", "idx": 106}, {"begin": 26602, "end": 26606, "target": "#b24", "idx": 107}, {"begin": 29794, "end": 29798, "target": "#b34", "idx": 108}, {"begin": 35495, "end": 35499, "target": "#b73", "idx": 109}, {"begin": 35509, "end": 35513, "target": "#b51", "idx": 110}, {"begin": 35874, "end": 35878, "target": "#b58", "idx": 111}, {"begin": 35894, "end": 35898, "target": "#b18", "idx": 112}, {"begin": 35899, "end": 35902, "target": "#b46", "idx": 113}, {"begin": 35939, "end": 35942, "target": "#b4", "idx": 114}, {"begin": 35943, "end": 35946, "target": "#b17", "idx": 115}, {"begin": 35947, "end": 35950, "target": "#b83", "idx": 116}, {"begin": 36556, "end": 36560, "target": "#b35", "idx": 117}, {"begin": 36561, "end": 36564, "target": "#b36", "idx": 118}, {"begin": 36565, "end": 36568, "target": "#b48", "idx": 119}, {"begin": 37688, "end": 37692, "target": "#b58", "idx": 120}, {"begin": 45107, "end": 45111, "target": "#b11", "idx": 121}, {"begin": 45112, "end": 45115, "target": "#b71", "idx": 122}, {"begin": 49058, "end": 49062, "target": "#b68", "idx": 123}, {"begin": 53993, "end": 53997, "target": "#b97", "idx": 124}, {"begin": 54023, "end": 54027, "target": "#b98", "idx": 125}, {"begin": 57418, "end": 57422, "target": "#b23", "idx": 126}, {"begin": 64281, "end": 64284, "target": "#b4", "idx": 127}, {"begin": 64285, "end": 64288, "target": "#b17", "idx": 128}, {"begin": 64289, "end": 64292, "target": "#b18", "idx": 129}, {"begin": 64293, "end": 64296, "target": "#b46", "idx": 130}, {"begin": 64297, "end": 64300, "target": "#b83", "idx": 131}, {"begin": 64976, "end": 64979, "target": "#b1", "idx": 132}, {"begin": 65302, "end": 65305, "target": "#b1", "idx": 133}, {"begin": 68194, "end": 68198, "target": "#b24", "idx": 134}, {"begin": 68218, "end": 68222, "target": "#b24", "idx": 135}, {"begin": 69729, "end": 69733, "target": "#b24", "idx": 136}, {"begin": 69734, "end": 69737, "target": "#b45", "idx": 137}, {"begin": 69822, "end": 69826, "target": "#b22", "idx": 138}, {"begin": 69827, "end": 69830, "target": "#b45", "idx": 139}, {"begin": 69831, "end": 69834, "target": "#b85", "idx": 140}, {"begin": 69849, "end": 69853, "target": "#b74", "idx": 141}, {"begin": 69909, "end": 69913, "target": "#b24", "idx": 142}, {"begin": 69914, "end": 69917, "target": "#b82", "idx": 143}, {"begin": 70110, "end": 70114, "target": "#b24", "idx": 144}, {"begin": 70223, "end": 70227, "target": "#b24", "idx": 145}, {"begin": 70228, "end": 70231, "target": "#b45", "idx": 146}, {"begin": 70232, "end": 70235, "target": "#b53", "idx": 147}, {"begin": 70236, "end": 70239, "target": "#b84", "idx": 148}, {"begin": 71027, "end": 71031, "target": "#b65", "idx": 149}, {"begin": 71032, "end": 71035, "target": "#b89", "idx": 150}, {"begin": 71188, "end": 71192, "target": "#b97", "idx": 151}, {"begin": 71255, "end": 71259, "target": "#b98", "idx": 152}, {"begin": 81163, "end": 81167, "target": "#b68", "idx": 153}, {"begin": 81366, "end": 81369, "target": "#b1", "idx": 154}, {"begin": 81503, "end": 81507, "target": "#b69", "idx": 155}, {"begin": 81522, "end": 81526, "target": "#b70", "idx": 156}, {"begin": 81739, "end": 81743, "target": "#b42", "idx": 157}, {"begin": 81744, "end": 81747, "target": "#b49", "idx": 158}, {"begin": 81748, "end": 81751, "target": "#b80", "idx": 159}, {"begin": 81752, "end": 81755, "target": "#b81", "idx": 160}, {"begin": 81756, "end": 81759, "target": "#b87", "idx": 161}, {"begin": 86036, "end": 86040, "target": "#b23", "idx": 162}, {"begin": 87145, "end": 87149, "target": "#b97", "idx": 163}, {"begin": 87165, "end": 87169, "target": "#b98", "idx": 164}, {"begin": 87339, "end": 87343, "target": "#b23", "idx": 165}, {"begin": 87344, "end": 87347, "target": "#b89", "idx": 166}, {"begin": 87509, "end": 87512, "target": "#b2", "idx": 167}, {"begin": 87513, "end": 87516, "target": "#b15", "idx": 168}, {"begin": 87694, "end": 87698, "target": "#b10", "idx": 169}, {"begin": 87699, "end": 87702, "target": "#b61", "idx": 170}, {"begin": 87703, "end": 87706, "target": "#b63", "idx": 171}, {"begin": 87983, "end": 87986, "target": "#b8", "idx": 172}, {"begin": 88003, "end": 88007, "target": "#b77", "idx": 173}, {"begin": 88159, "end": 88163, "target": "#b9", "idx": 174}, {"begin": 88407, "end": 88411, "target": "#b30", "idx": 175}, {"begin": 88421, "end": 88425, "target": "#b16", "idx": 176}, {"begin": 88436, "end": 88440, "target": "#b70", "idx": 177}, {"begin": 88455, "end": 88459, "target": "#b47", "idx": 178}, {"begin": 90502, "end": 90505, "target": "#b1", "idx": 179}, {"begin": 91314, "end": 91324, "idx": 180}, {"begin": 91325, "end": 91328, "target": "#b20", "idx": 181}, {"begin": 91329, "end": 91332, "target": "#b21", "idx": 182}, {"begin": 92100, "end": 92103, "target": "#b6", "idx": 183}, {"begin": 92118, "end": 92122, "target": "#b59", "idx": 184}, {"begin": 92244, "end": 92248, "target": "#b68", "idx": 185}, {"begin": 93545, "end": 93549, "target": "#b89", "idx": 186}, {"begin": 94915, "end": 94919, "target": "#b13", "idx": 187}, {"begin": 95705, "end": 95709, "target": "#b11", "idx": 188}, {"begin": 96456, "end": 96460, "target": "#b23", "idx": 189}, {"begin": 96653, "end": 96657, "target": "#b44", "idx": 190}, {"begin": 97211, "end": 97212, "target": "#b5", "idx": 191}, {"begin": 98092, "end": 98096, "target": "#b27", "idx": 192}, {"begin": 98097, "end": 98100, "target": "#b62", "idx": 193}, {"begin": 98101, "end": 98104, "target": "#b86", "idx": 194}], "ReferenceString": [{"begin": 98122, "end": 98247, "id": "b0", "idx": 0}, {"begin": 98249, "end": 98355, "id": "b1", "idx": 1}, {"begin": 98359, "end": 98458, "id": "b2", "idx": 2}, {"begin": 98462, "end": 98647, "id": "b3", "idx": 3}, {"begin": 98651, "end": 98765, "id": "b4", "idx": 4}, {"begin": 98769, "end": 98912, "id": "b5", "idx": 5}, {"begin": 98916, "end": 99046, "id": "b6", "idx": 6}, {"begin": 99050, "end": 99161, "id": "b7", "idx": 7}, {"begin": 99165, "end": 99225, "id": "b8", "idx": 8}, {"begin": 99229, "end": 99285, "id": "b9", "idx": 9}, {"begin": 99289, "end": 99411, "id": "b10", "idx": 10}, {"begin": 99415, "end": 99638, "id": "b11", "idx": 11}, {"begin": 99642, "end": 99870, "id": "b12", "idx": 12}, {"begin": 99874, "end": 100113, "id": "b13", "idx": 13}, {"begin": 100117, "end": 100259, "id": "b14", "idx": 14}, {"begin": 100263, "end": 100667, "id": "b15", "idx": 15}, {"begin": 100671, "end": 100802, "id": "b16", "idx": 16}, {"begin": 100806, "end": 100940, "id": "b17", "idx": 17}, {"begin": 100944, "end": 101279, "id": "b18", "idx": 18}, {"begin": 101283, "end": 101460, "id": "b19", "idx": 19}, {"begin": 101464, "end": 101643, "id": "b20", "idx": 20}, {"begin": 101647, "end": 101797, "id": "b21", "idx": 21}, {"begin": 101801, "end": 101988, "id": "b22", "idx": 22}, {"begin": 101992, "end": 102165, "id": "b23", "idx": 23}, {"begin": 102169, "end": 102516, "id": "b24", "idx": 24}, {"begin": 102520, "end": 102622, "id": "b25", "idx": 25}, {"begin": 102626, "end": 102850, "id": "b26", "idx": 26}, {"begin": 102854, "end": 103140, "id": "b27", "idx": 27}, {"begin": 103144, "end": 103324, "id": "b28", "idx": 28}, {"begin": 103328, "end": 103468, "id": "b29", "idx": 29}, {"begin": 103472, "end": 103645, "id": "b30", "idx": 30}, {"begin": 103649, "end": 103755, "id": "b31", "idx": 31}, {"begin": 103759, "end": 103964, "id": "b32", "idx": 32}, {"begin": 103968, "end": 104106, "id": "b33", "idx": 33}, {"begin": 104110, "end": 104290, "id": "b34", "idx": 34}, {"begin": 104294, "end": 104536, "id": "b35", "idx": 35}, {"begin": 104540, "end": 104689, "id": "b36", "idx": 36}, {"begin": 104693, "end": 104882, "id": "b37", "idx": 37}, {"begin": 104886, "end": 105039, "id": "b38", "idx": 38}, {"begin": 105043, "end": 105541, "id": "b39", "idx": 39}, {"begin": 105545, "end": 105735, "id": "b40", "idx": 40}, {"begin": 105739, "end": 105879, "id": "b41", "idx": 41}, {"begin": 105883, "end": 106023, "id": "b42", "idx": 42}, {"begin": 106027, "end": 106210, "id": "b43", "idx": 43}, {"begin": 106214, "end": 106264, "id": "b44", "idx": 44}, {"begin": 106268, "end": 106442, "id": "b45", "idx": 45}, {"begin": 106446, "end": 106665, "id": "b46", "idx": 46}, {"begin": 106669, "end": 106899, "id": "b47", "idx": 47}, {"begin": 106903, "end": 107112, "id": "b48", "idx": 48}, {"begin": 107116, "end": 107310, "id": "b49", "idx": 49}, {"begin": 107314, "end": 107460, "id": "b50", "idx": 50}, {"begin": 107464, "end": 107621, "id": "b51", "idx": 51}, {"begin": 107625, "end": 107819, "id": "b52", "idx": 52}, {"begin": 107823, "end": 108044, "id": "b53", "idx": 53}, {"begin": 108048, "end": 108149, "id": "b54", "idx": 54}, {"begin": 108153, "end": 108306, "id": "b55", "idx": 55}, {"begin": 108310, "end": 108449, "id": "b56", "idx": 56}, {"begin": 108453, "end": 108557, "id": "b57", "idx": 57}, {"begin": 108561, "end": 108798, "id": "b58", "idx": 58}, {"begin": 108802, "end": 108965, "id": "b59", "idx": 59}, {"begin": 108969, "end": 109232, "id": "b60", "idx": 60}, {"begin": 109236, "end": 109344, "id": "b61", "idx": 61}, {"begin": 109348, "end": 109506, "id": "b62", "idx": 62}, {"begin": 109510, "end": 109661, "id": "b63", "idx": 63}, {"begin": 109665, "end": 109872, "id": "b64", "idx": 64}, {"begin": 109876, "end": 110199, "id": "b65", "idx": 65}, {"begin": 110203, "end": 110393, "id": "b66", "idx": 66}, {"begin": 110397, "end": 110946, "id": "b67", "idx": 67}, {"begin": 110950, "end": 111215, "id": "b68", "idx": 68}, {"begin": 111219, "end": 111376, "id": "b69", "idx": 69}, {"begin": 111380, "end": 111696, "id": "b70", "idx": 70}, {"begin": 111700, "end": 111856, "id": "b71", "idx": 71}, {"begin": 111860, "end": 112005, "id": "b72", "idx": 72}, {"begin": 112009, "end": 112115, "id": "b73", "idx": 73}, {"begin": 112119, "end": 112248, "id": "b74", "idx": 74}, {"begin": 112252, "end": 112419, "id": "b75", "idx": 75}, {"begin": 112423, "end": 112699, "id": "b76", "idx": 76}, {"begin": 112703, "end": 112783, "id": "b77", "idx": 77}, {"begin": 112787, "end": 112957, "id": "b78", "idx": 78}, {"begin": 112961, "end": 113238, "id": "b79", "idx": 79}, {"begin": 113242, "end": 113368, "id": "b80", "idx": 80}, {"begin": 113372, "end": 113559, "id": "b81", "idx": 81}, {"begin": 113563, "end": 113749, "id": "b82", "idx": 82}, {"begin": 113753, "end": 113868, "id": "b83", "idx": 83}, {"begin": 113872, "end": 114051, "id": "b84", "idx": 84}, {"begin": 114055, "end": 114258, "id": "b85", "idx": 85}, {"begin": 114262, "end": 114440, "id": "b86", "idx": 86}, {"begin": 114444, "end": 114586, "id": "b87", "idx": 87}, {"begin": 114590, "end": 114698, "id": "b88", "idx": 88}, {"begin": 114702, "end": 114925, "id": "b89", "idx": 89}, {"begin": 114929, "end": 115118, "id": "b90", "idx": 90}, {"begin": 115122, "end": 115332, "id": "b91", "idx": 91}, {"begin": 115336, "end": 115538, "id": "b92", "idx": 92}, {"begin": 115542, "end": 115755, "id": "b93", "idx": 93}, {"begin": 115759, "end": 115942, "id": "b94", "idx": 94}, {"begin": 115946, "end": 116107, "id": "b95", "idx": 95}, {"begin": 116111, "end": 116397, "id": "b96", "idx": 96}, {"begin": 116401, "end": 116682, "id": "b97", "idx": 97}, {"begin": 116686, "end": 116868, "id": "b98", "idx": 98}], "Sentence": [{"begin": 106, "end": 281, "idx": 0}, {"begin": 282, "end": 438, "idx": 1}, {"begin": 439, "end": 624, "idx": 2}, {"begin": 625, "end": 776, "idx": 3}, {"begin": 777, "end": 905, "idx": 4}, {"begin": 906, "end": 1066, "idx": 5}, {"begin": 1092, "end": 1277, "idx": 6}, {"begin": 1278, "end": 1390, "idx": 7}, {"begin": 1391, "end": 1514, "idx": 8}, {"begin": 1515, "end": 1597, "idx": 9}, {"begin": 1598, "end": 1682, "idx": 10}, {"begin": 1683, "end": 1905, "idx": 11}, {"begin": 1906, "end": 2120, "idx": 12}, {"begin": 2121, "end": 2304, "idx": 13}, {"begin": 2305, "end": 2492, "idx": 14}, {"begin": 2493, "end": 2745, "idx": 15}, {"begin": 2746, "end": 2903, "idx": 16}, {"begin": 2904, "end": 3079, "idx": 17}, {"begin": 3080, "end": 3091, "idx": 18}, {"begin": 3092, "end": 3278, "idx": 19}, {"begin": 3279, "end": 3386, "idx": 20}, {"begin": 3387, "end": 3497, "idx": 21}, {"begin": 3498, "end": 3685, "idx": 22}, {"begin": 3686, "end": 3826, "idx": 23}, {"begin": 3827, "end": 4033, "idx": 24}, {"begin": 4034, "end": 4220, "idx": 25}, {"begin": 4221, "end": 4310, "idx": 26}, {"begin": 4311, "end": 4493, "idx": 27}, {"begin": 4494, "end": 4498, "idx": 28}, {"begin": 4499, "end": 4661, "idx": 29}, {"begin": 4662, "end": 4731, "idx": 30}, {"begin": 4732, "end": 4905, "idx": 31}, {"begin": 4906, "end": 5126, "idx": 32}, {"begin": 5127, "end": 5404, "idx": 33}, {"begin": 5405, "end": 5581, "idx": 34}, {"begin": 5613, "end": 5721, "idx": 35}, {"begin": 5722, "end": 5854, "idx": 36}, {"begin": 5855, "end": 5923, "idx": 37}, {"begin": 5924, "end": 6074, "idx": 38}, {"begin": 6075, "end": 6174, "idx": 39}, {"begin": 6175, "end": 6313, "idx": 40}, {"begin": 6314, "end": 6402, "idx": 41}, {"begin": 6403, "end": 6533, "idx": 42}, {"begin": 6534, "end": 6584, "idx": 43}, {"begin": 6585, "end": 6685, "idx": 44}, {"begin": 6686, "end": 6790, "idx": 45}, {"begin": 6815, "end": 6864, "idx": 46}, {"begin": 6865, "end": 6945, "idx": 47}, {"begin": 6946, "end": 7018, "idx": 48}, {"begin": 7019, "end": 7022, "idx": 49}, {"begin": 7023, "end": 7098, "idx": 50}, {"begin": 7099, "end": 7305, "idx": 51}, {"begin": 7306, "end": 7338, "idx": 52}, {"begin": 7339, "end": 7457, "idx": 53}, {"begin": 7458, "end": 7635, "idx": 54}, {"begin": 7636, "end": 7806, "idx": 55}, {"begin": 7807, "end": 7904, "idx": 56}, {"begin": 7905, "end": 7948, "idx": 57}, {"begin": 7949, "end": 8099, "idx": 58}, {"begin": 8100, "end": 8237, "idx": 59}, {"begin": 8238, "end": 8361, "idx": 60}, {"begin": 8362, "end": 8669, "idx": 61}, {"begin": 8692, "end": 8838, "idx": 62}, {"begin": 8839, "end": 8979, "idx": 63}, {"begin": 8980, "end": 9054, "idx": 64}, {"begin": 9055, "end": 9182, "idx": 65}, {"begin": 9183, "end": 9190, "idx": 66}, {"begin": 9191, "end": 9323, "idx": 67}, {"begin": 9324, "end": 9454, "idx": 68}, {"begin": 9455, "end": 9552, "idx": 69}, {"begin": 9553, "end": 9634, "idx": 70}, {"begin": 9635, "end": 9639, "idx": 71}, {"begin": 9640, "end": 9662, "idx": 72}, {"begin": 9663, "end": 9792, "idx": 73}, {"begin": 9793, "end": 9914, "idx": 74}, {"begin": 9915, "end": 10113, "idx": 75}, {"begin": 10114, "end": 10223, "idx": 76}, {"begin": 10255, "end": 10384, "idx": 77}, {"begin": 10385, "end": 10534, "idx": 78}, {"begin": 10535, "end": 10539, "idx": 79}, {"begin": 10677, "end": 10680, "idx": 80}, {"begin": 10681, "end": 10793, "idx": 81}, {"begin": 10794, "end": 10823, "idx": 82}, {"begin": 10824, "end": 10983, "idx": 83}, {"begin": 10984, "end": 11025, "idx": 84}, {"begin": 11064, "end": 11151, "idx": 85}, {"begin": 11253, "end": 11328, "idx": 86}, {"begin": 11329, "end": 11402, "idx": 87}, {"begin": 11403, "end": 11549, "idx": 88}, {"begin": 11550, "end": 11727, "idx": 89}, {"begin": 11763, "end": 11807, "idx": 90}, {"begin": 11808, "end": 11954, "idx": 91}, {"begin": 12038, "end": 12110, "idx": 92}, {"begin": 12111, "end": 12219, "idx": 93}, {"begin": 12220, "end": 12324, "idx": 94}, {"begin": 12325, "end": 12401, "idx": 95}, {"begin": 12482, "end": 12566, "idx": 96}, {"begin": 12567, "end": 12639, "idx": 97}, {"begin": 12640, "end": 12740, "idx": 98}, {"begin": 12741, "end": 12944, "idx": 99}, {"begin": 12945, "end": 13113, "idx": 100}, {"begin": 13114, "end": 13241, "idx": 101}, {"begin": 13242, "end": 13336, "idx": 102}, {"begin": 13337, "end": 13434, "idx": 103}, {"begin": 13475, "end": 13577, "idx": 104}, {"begin": 13578, "end": 13703, "idx": 105}, {"begin": 13704, "end": 13808, "idx": 106}, {"begin": 13941, "end": 14080, "idx": 107}, {"begin": 14081, "end": 14138, "idx": 108}, {"begin": 14139, "end": 14344, "idx": 109}, {"begin": 14345, "end": 14428, "idx": 110}, {"begin": 14459, "end": 14467, "idx": 111}, {"begin": 14468, "end": 14638, "idx": 112}, {"begin": 14639, "end": 14705, "idx": 113}, {"begin": 14778, "end": 14826, "idx": 114}, {"begin": 14827, "end": 14952, "idx": 115}, {"begin": 14953, "end": 15073, "idx": 116}, {"begin": 15074, "end": 15175, "idx": 117}, {"begin": 15176, "end": 15215, "idx": 118}, {"begin": 15216, "end": 15319, "idx": 119}, {"begin": 15320, "end": 15413, "idx": 120}, {"begin": 15414, "end": 15428, "idx": 121}, {"begin": 15429, "end": 15587, "idx": 122}, {"begin": 15588, "end": 15696, "idx": 123}, {"begin": 15697, "end": 15775, "idx": 124}, {"begin": 15776, "end": 15818, "idx": 125}, {"begin": 15819, "end": 15861, "idx": 126}, {"begin": 15862, "end": 16004, "idx": 127}, {"begin": 16005, "end": 16135, "idx": 128}, {"begin": 16136, "end": 16253, "idx": 129}, {"begin": 16254, "end": 16424, "idx": 130}, {"begin": 16425, "end": 16648, "idx": 131}, {"begin": 16649, "end": 16841, "idx": 132}, {"begin": 16842, "end": 16901, "idx": 133}, {"begin": 16902, "end": 17033, "idx": 134}, {"begin": 17034, "end": 17058, "idx": 135}, {"begin": 17059, "end": 17186, "idx": 136}, {"begin": 17187, "end": 17283, "idx": 137}, {"begin": 17284, "end": 17425, "idx": 138}, {"begin": 17426, "end": 17582, "idx": 139}, {"begin": 17583, "end": 17677, "idx": 140}, {"begin": 17678, "end": 17734, "idx": 141}, {"begin": 17751, "end": 17778, "idx": 142}, {"begin": 17779, "end": 17913, "idx": 143}, {"begin": 17914, "end": 18045, "idx": 144}, {"begin": 18046, "end": 18178, "idx": 145}, {"begin": 18179, "end": 18412, "idx": 146}, {"begin": 18413, "end": 18430, "idx": 147}, {"begin": 18431, "end": 18516, "idx": 148}, {"begin": 18517, "end": 18699, "idx": 149}, {"begin": 18700, "end": 18827, "idx": 150}, {"begin": 18828, "end": 18922, "idx": 151}, {"begin": 18923, "end": 19022, "idx": 152}, {"begin": 19023, "end": 19190, "idx": 153}, {"begin": 19191, "end": 19201, "idx": 154}, {"begin": 19202, "end": 19473, "idx": 155}, {"begin": 19474, "end": 19539, "idx": 156}, {"begin": 19540, "end": 19692, "idx": 157}, {"begin": 19693, "end": 19807, "idx": 158}, {"begin": 19808, "end": 19918, "idx": 159}, {"begin": 19919, "end": 20090, "idx": 160}, {"begin": 20091, "end": 20177, "idx": 161}, {"begin": 20178, "end": 20356, "idx": 162}, {"begin": 20357, "end": 20485, "idx": 163}, {"begin": 20486, "end": 20534, "idx": 164}, {"begin": 20535, "end": 20713, "idx": 165}, {"begin": 20714, "end": 20946, "idx": 166}, {"begin": 20947, "end": 21114, "idx": 167}, {"begin": 21115, "end": 21358, "idx": 168}, {"begin": 21359, "end": 21426, "idx": 169}, {"begin": 21427, "end": 21571, "idx": 170}, {"begin": 21572, "end": 21588, "idx": 171}, {"begin": 21589, "end": 21676, "idx": 172}, {"begin": 21677, "end": 21728, "idx": 173}, {"begin": 21729, "end": 21878, "idx": 174}, {"begin": 21879, "end": 22012, "idx": 175}, {"begin": 22013, "end": 22163, "idx": 176}, {"begin": 22164, "end": 22343, "idx": 177}, {"begin": 22344, "end": 22444, "idx": 178}, {"begin": 22445, "end": 22552, "idx": 179}, {"begin": 22553, "end": 22569, "idx": 180}, {"begin": 22570, "end": 22761, "idx": 181}, {"begin": 22762, "end": 22870, "idx": 182}, {"begin": 22871, "end": 22977, "idx": 183}, {"begin": 22978, "end": 23089, "idx": 184}, {"begin": 23090, "end": 23257, "idx": 185}, {"begin": 23273, "end": 23431, "idx": 186}, {"begin": 23432, "end": 23519, "idx": 187}, {"begin": 23520, "end": 23629, "idx": 188}, {"begin": 23630, "end": 23741, "idx": 189}, {"begin": 23742, "end": 23894, "idx": 190}, {"begin": 23895, "end": 24136, "idx": 191}, {"begin": 24137, "end": 24175, "idx": 192}, {"begin": 24176, "end": 24333, "idx": 193}, {"begin": 24334, "end": 24428, "idx": 194}, {"begin": 24429, "end": 24636, "idx": 195}, {"begin": 24637, "end": 24702, "idx": 196}, {"begin": 24703, "end": 24826, "idx": 197}, {"begin": 24827, "end": 24957, "idx": 198}, {"begin": 24958, "end": 25094, "idx": 199}, {"begin": 25095, "end": 25116, "idx": 200}, {"begin": 25117, "end": 25220, "idx": 201}, {"begin": 25221, "end": 25431, "idx": 202}, {"begin": 25432, "end": 25529, "idx": 203}, {"begin": 25530, "end": 25671, "idx": 204}, {"begin": 25672, "end": 25905, "idx": 205}, {"begin": 25906, "end": 26090, "idx": 206}, {"begin": 26091, "end": 26389, "idx": 207}, {"begin": 26390, "end": 26409, "idx": 208}, {"begin": 26410, "end": 26544, "idx": 209}, {"begin": 26545, "end": 26607, "idx": 210}, {"begin": 26608, "end": 26820, "idx": 211}, {"begin": 26821, "end": 26865, "idx": 212}, {"begin": 26866, "end": 26949, "idx": 213}, {"begin": 27016, "end": 27204, "idx": 214}, {"begin": 27205, "end": 27263, "idx": 215}, {"begin": 27264, "end": 27508, "idx": 216}, {"begin": 27509, "end": 27662, "idx": 217}, {"begin": 27663, "end": 27818, "idx": 218}, {"begin": 27819, "end": 27908, "idx": 219}, {"begin": 27909, "end": 28051, "idx": 220}, {"begin": 28052, "end": 28154, "idx": 221}, {"begin": 28155, "end": 28271, "idx": 222}, {"begin": 28272, "end": 28364, "idx": 223}, {"begin": 28365, "end": 28425, "idx": 224}, {"begin": 28426, "end": 28528, "idx": 225}, {"begin": 28529, "end": 28584, "idx": 226}, {"begin": 28585, "end": 28853, "idx": 227}, {"begin": 28854, "end": 29035, "idx": 228}, {"begin": 29036, "end": 29113, "idx": 229}, {"begin": 29114, "end": 29278, "idx": 230}, {"begin": 29279, "end": 29421, "idx": 231}, {"begin": 29422, "end": 29539, "idx": 232}, {"begin": 29540, "end": 29611, "idx": 233}, {"begin": 29714, "end": 29763, "idx": 234}, {"begin": 29764, "end": 29844, "idx": 235}, {"begin": 29845, "end": 30000, "idx": 236}, {"begin": 30001, "end": 30129, "idx": 237}, {"begin": 30130, "end": 30203, "idx": 238}, {"begin": 30204, "end": 30397, "idx": 239}, {"begin": 30463, "end": 30573, "idx": 240}, {"begin": 30574, "end": 30781, "idx": 241}, {"begin": 30782, "end": 30944, "idx": 242}, {"begin": 30945, "end": 31059, "idx": 243}, {"begin": 31060, "end": 31158, "idx": 244}, {"begin": 31159, "end": 31267, "idx": 245}, {"begin": 31268, "end": 31366, "idx": 246}, {"begin": 31367, "end": 31468, "idx": 247}, {"begin": 31469, "end": 31609, "idx": 248}, {"begin": 31610, "end": 31727, "idx": 249}, {"begin": 31728, "end": 31809, "idx": 250}, {"begin": 31810, "end": 32014, "idx": 251}, {"begin": 32015, "end": 32163, "idx": 252}, {"begin": 32164, "end": 32303, "idx": 253}, {"begin": 32304, "end": 32521, "idx": 254}, {"begin": 32522, "end": 32661, "idx": 255}, {"begin": 32662, "end": 32766, "idx": 256}, {"begin": 32767, "end": 32968, "idx": 257}, {"begin": 33035, "end": 33151, "idx": 258}, {"begin": 33152, "end": 33244, "idx": 259}, {"begin": 33245, "end": 33375, "idx": 260}, {"begin": 33376, "end": 33397, "idx": 261}, {"begin": 33398, "end": 33521, "idx": 262}, {"begin": 33522, "end": 33645, "idx": 263}, {"begin": 33646, "end": 33766, "idx": 264}, {"begin": 33767, "end": 33841, "idx": 265}, {"begin": 33842, "end": 33864, "idx": 266}, {"begin": 33865, "end": 34105, "idx": 267}, {"begin": 34106, "end": 34281, "idx": 268}, {"begin": 34282, "end": 34462, "idx": 269}, {"begin": 34463, "end": 34553, "idx": 270}, {"begin": 34554, "end": 34678, "idx": 271}, {"begin": 34679, "end": 34857, "idx": 272}, {"begin": 34858, "end": 34924, "idx": 273}, {"begin": 34925, "end": 34972, "idx": 274}, {"begin": 34973, "end": 35087, "idx": 275}, {"begin": 35088, "end": 35233, "idx": 276}, {"begin": 35234, "end": 35362, "idx": 277}, {"begin": 35408, "end": 35420, "idx": 278}, {"begin": 35421, "end": 35514, "idx": 279}, {"begin": 35515, "end": 35694, "idx": 280}, {"begin": 35695, "end": 35987, "idx": 281}, {"begin": 35988, "end": 36000, "idx": 282}, {"begin": 36001, "end": 36213, "idx": 283}, {"begin": 36214, "end": 36390, "idx": 284}, {"begin": 36391, "end": 36569, "idx": 285}, {"begin": 36570, "end": 36582, "idx": 286}, {"begin": 36583, "end": 36786, "idx": 287}, {"begin": 36787, "end": 36885, "idx": 288}, {"begin": 36886, "end": 36985, "idx": 289}, {"begin": 36986, "end": 37044, "idx": 290}, {"begin": 37045, "end": 37232, "idx": 291}, {"begin": 37233, "end": 37335, "idx": 292}, {"begin": 37336, "end": 37431, "idx": 293}, {"begin": 37432, "end": 37513, "idx": 294}, {"begin": 37514, "end": 37603, "idx": 295}, {"begin": 37604, "end": 37769, "idx": 296}, {"begin": 37770, "end": 37859, "idx": 297}, {"begin": 38209, "end": 38217, "idx": 298}, {"begin": 38218, "end": 38257, "idx": 299}, {"begin": 38258, "end": 38283, "idx": 300}, {"begin": 38284, "end": 38324, "idx": 301}, {"begin": 38325, "end": 38457, "idx": 302}, {"begin": 38458, "end": 38464, "idx": 303}, {"begin": 38465, "end": 38510, "idx": 304}, {"begin": 38511, "end": 38584, "idx": 305}, {"begin": 38585, "end": 38591, "idx": 306}, {"begin": 38592, "end": 38667, "idx": 307}, {"begin": 38668, "end": 38699, "idx": 308}, {"begin": 38830, "end": 38836, "idx": 309}, {"begin": 38837, "end": 38878, "idx": 310}, {"begin": 38879, "end": 38891, "idx": 311}, {"begin": 39061, "end": 39067, "idx": 312}, {"begin": 39068, "end": 39118, "idx": 313}, {"begin": 39119, "end": 39131, "idx": 314}, {"begin": 39186, "end": 39191, "idx": 315}, {"begin": 39350, "end": 39491, "idx": 316}, {"begin": 39492, "end": 39540, "idx": 317}, {"begin": 39541, "end": 39586, "idx": 318}, {"begin": 39613, "end": 39730, "idx": 319}, {"begin": 39903, "end": 39954, "idx": 320}, {"begin": 39955, "end": 39963, "idx": 321}, {"begin": 39964, "end": 40027, "idx": 322}, {"begin": 40028, "end": 40080, "idx": 323}, {"begin": 40237, "end": 40243, "idx": 324}, {"begin": 40244, "end": 40262, "idx": 325}, {"begin": 40263, "end": 40381, "idx": 326}, {"begin": 40382, "end": 40419, "idx": 327}, {"begin": 40420, "end": 40491, "idx": 328}, {"begin": 40492, "end": 40511, "idx": 329}, {"begin": 40604, "end": 40710, "idx": 330}, {"begin": 40711, "end": 41010, "idx": 331}, {"begin": 41011, "end": 41085, "idx": 332}, {"begin": 41086, "end": 41101, "idx": 333}, {"begin": 41102, "end": 41125, "idx": 334}, {"begin": 41126, "end": 41132, "idx": 335}, {"begin": 41133, "end": 41188, "idx": 336}, {"begin": 41189, "end": 41285, "idx": 337}, {"begin": 41399, "end": 41484, "idx": 338}, {"begin": 41485, "end": 41623, "idx": 339}, {"begin": 41624, "end": 41715, "idx": 340}, {"begin": 41716, "end": 41813, "idx": 341}, {"begin": 41814, "end": 42036, "idx": 342}, {"begin": 42037, "end": 42149, "idx": 343}, {"begin": 42150, "end": 42187, "idx": 344}, {"begin": 42188, "end": 42314, "idx": 345}, {"begin": 42315, "end": 42410, "idx": 346}, {"begin": 42454, "end": 42460, "idx": 347}, {"begin": 42461, "end": 42593, "idx": 348}, {"begin": 42594, "end": 42735, "idx": 349}, {"begin": 42736, "end": 42862, "idx": 350}, {"begin": 42863, "end": 42926, "idx": 351}, {"begin": 42927, "end": 43159, "idx": 352}, {"begin": 43160, "end": 43465, "idx": 353}, {"begin": 43466, "end": 43668, "idx": 354}, {"begin": 43669, "end": 43765, "idx": 355}, {"begin": 43766, "end": 43969, "idx": 356}, {"begin": 43970, "end": 44186, "idx": 357}, {"begin": 44187, "end": 44306, "idx": 358}, {"begin": 44307, "end": 44442, "idx": 359}, {"begin": 44443, "end": 44559, "idx": 360}, {"begin": 44560, "end": 44646, "idx": 361}, {"begin": 44647, "end": 44767, "idx": 362}, {"begin": 44768, "end": 44971, "idx": 363}, {"begin": 44972, "end": 45288, "idx": 364}, {"begin": 45289, "end": 45297, "idx": 365}, {"begin": 45298, "end": 45469, "idx": 366}, {"begin": 45470, "end": 45678, "idx": 367}, {"begin": 45679, "end": 45777, "idx": 368}, {"begin": 45778, "end": 45972, "idx": 369}, {"begin": 45973, "end": 46135, "idx": 370}, {"begin": 46136, "end": 46180, "idx": 371}, {"begin": 46181, "end": 46294, "idx": 372}, {"begin": 46295, "end": 46501, "idx": 373}, {"begin": 46502, "end": 46587, "idx": 374}, {"begin": 46588, "end": 46696, "idx": 375}, {"begin": 46697, "end": 46845, "idx": 376}, {"begin": 46846, "end": 46954, "idx": 377}, {"begin": 46955, "end": 47025, "idx": 378}, {"begin": 47026, "end": 47107, "idx": 379}, {"begin": 47108, "end": 47246, "idx": 380}, {"begin": 47247, "end": 47343, "idx": 381}, {"begin": 47344, "end": 47449, "idx": 382}, {"begin": 47450, "end": 47508, "idx": 383}, {"begin": 47509, "end": 47642, "idx": 384}, {"begin": 47643, "end": 47814, "idx": 385}, {"begin": 47815, "end": 48055, "idx": 386}, {"begin": 48056, "end": 48296, "idx": 387}, {"begin": 48297, "end": 48416, "idx": 388}, {"begin": 48417, "end": 48556, "idx": 389}, {"begin": 48557, "end": 48721, "idx": 390}, {"begin": 48722, "end": 48768, "idx": 391}, {"begin": 48769, "end": 49002, "idx": 392}, {"begin": 49003, "end": 49033, "idx": 393}, {"begin": 49034, "end": 49062, "idx": 394}, {"begin": 49195, "end": 49323, "idx": 395}, {"begin": 49647, "end": 49802, "idx": 396}, {"begin": 49803, "end": 49871, "idx": 397}, {"begin": 49872, "end": 50011, "idx": 398}, {"begin": 50012, "end": 50162, "idx": 399}, {"begin": 50163, "end": 50281, "idx": 400}, {"begin": 50325, "end": 50408, "idx": 401}, {"begin": 50409, "end": 50483, "idx": 402}, {"begin": 50484, "end": 50609, "idx": 403}, {"begin": 50610, "end": 50647, "idx": 404}, {"begin": 50648, "end": 50730, "idx": 405}, {"begin": 50731, "end": 50903, "idx": 406}, {"begin": 50904, "end": 50983, "idx": 407}, {"begin": 50984, "end": 51127, "idx": 408}, {"begin": 51128, "end": 51222, "idx": 409}, {"begin": 51223, "end": 51412, "idx": 410}, {"begin": 51413, "end": 51524, "idx": 411}, {"begin": 51525, "end": 51565, "idx": 412}, {"begin": 51566, "end": 51714, "idx": 413}, {"begin": 51715, "end": 52038, "idx": 414}, {"begin": 52039, "end": 52140, "idx": 415}, {"begin": 52141, "end": 52269, "idx": 416}, {"begin": 52270, "end": 52315, "idx": 417}, {"begin": 52316, "end": 52462, "idx": 418}, {"begin": 52463, "end": 52588, "idx": 419}, {"begin": 52589, "end": 52735, "idx": 420}, {"begin": 52736, "end": 52800, "idx": 421}, {"begin": 52801, "end": 52995, "idx": 422}, {"begin": 52996, "end": 53081, "idx": 423}, {"begin": 53082, "end": 53203, "idx": 424}, {"begin": 53247, "end": 53329, "idx": 425}, {"begin": 53330, "end": 53429, "idx": 426}, {"begin": 53430, "end": 53522, "idx": 427}, {"begin": 53523, "end": 53777, "idx": 428}, {"begin": 53778, "end": 54218, "idx": 429}, {"begin": 54219, "end": 54406, "idx": 430}, {"begin": 54407, "end": 54488, "idx": 431}, {"begin": 54489, "end": 54624, "idx": 432}, {"begin": 54625, "end": 54689, "idx": 433}, {"begin": 54690, "end": 54969, "idx": 434}, {"begin": 54970, "end": 55072, "idx": 435}, {"begin": 55073, "end": 55188, "idx": 436}, {"begin": 55189, "end": 55237, "idx": 437}, {"begin": 55238, "end": 55368, "idx": 438}, {"begin": 55369, "end": 55516, "idx": 439}, {"begin": 55517, "end": 55560, "idx": 440}, {"begin": 55561, "end": 55682, "idx": 441}, {"begin": 55683, "end": 55801, "idx": 442}, {"begin": 55802, "end": 56351, "idx": 443}, {"begin": 56352, "end": 56581, "idx": 444}, {"begin": 56582, "end": 56597, "idx": 445}, {"begin": 56598, "end": 56646, "idx": 446}, {"begin": 56647, "end": 56713, "idx": 447}, {"begin": 56714, "end": 56879, "idx": 448}, {"begin": 56880, "end": 56939, "idx": 449}, {"begin": 56940, "end": 57068, "idx": 450}, {"begin": 57069, "end": 57236, "idx": 451}, {"begin": 57237, "end": 57460, "idx": 452}, {"begin": 57461, "end": 57595, "idx": 453}, {"begin": 57596, "end": 57862, "idx": 454}, {"begin": 57863, "end": 57890, "idx": 455}, {"begin": 57891, "end": 58136, "idx": 456}, {"begin": 58137, "end": 58355, "idx": 457}, {"begin": 58356, "end": 58540, "idx": 458}, {"begin": 58541, "end": 58551, "idx": 459}, {"begin": 58552, "end": 58631, "idx": 460}, {"begin": 58632, "end": 58843, "idx": 461}, {"begin": 58844, "end": 58947, "idx": 462}, {"begin": 58948, "end": 59009, "idx": 463}, {"begin": 59010, "end": 59115, "idx": 464}, {"begin": 59116, "end": 59347, "idx": 465}, {"begin": 59348, "end": 59470, "idx": 466}, {"begin": 59471, "end": 59619, "idx": 467}, {"begin": 59620, "end": 59720, "idx": 468}, {"begin": 59721, "end": 59748, "idx": 469}, {"begin": 59749, "end": 59828, "idx": 470}, {"begin": 59829, "end": 59861, "idx": 471}, {"begin": 59862, "end": 59957, "idx": 472}, {"begin": 59958, "end": 60093, "idx": 473}, {"begin": 60094, "end": 60216, "idx": 474}, {"begin": 60217, "end": 60439, "idx": 475}, {"begin": 60495, "end": 60579, "idx": 476}, {"begin": 60580, "end": 60689, "idx": 477}, {"begin": 60690, "end": 60867, "idx": 478}, {"begin": 60868, "end": 61040, "idx": 479}, {"begin": 61041, "end": 61061, "idx": 480}, {"begin": 61062, "end": 61138, "idx": 481}, {"begin": 61139, "end": 61248, "idx": 482}, {"begin": 61249, "end": 61343, "idx": 483}, {"begin": 61344, "end": 61511, "idx": 484}, {"begin": 61512, "end": 61586, "idx": 485}, {"begin": 61587, "end": 61681, "idx": 486}, {"begin": 61682, "end": 61979, "idx": 487}, {"begin": 61980, "end": 62119, "idx": 488}, {"begin": 62120, "end": 62280, "idx": 489}, {"begin": 62281, "end": 62411, "idx": 490}, {"begin": 62412, "end": 62509, "idx": 491}, {"begin": 62510, "end": 62599, "idx": 492}, {"begin": 62600, "end": 62809, "idx": 493}, {"begin": 62810, "end": 62950, "idx": 494}, {"begin": 62951, "end": 63025, "idx": 495}, {"begin": 63026, "end": 63214, "idx": 496}, {"begin": 63215, "end": 63416, "idx": 497}, {"begin": 63417, "end": 63511, "idx": 498}, {"begin": 63512, "end": 63599, "idx": 499}, {"begin": 63600, "end": 63673, "idx": 500}, {"begin": 63674, "end": 63759, "idx": 501}, {"begin": 63760, "end": 63851, "idx": 502}, {"begin": 63852, "end": 64095, "idx": 503}, {"begin": 64096, "end": 64173, "idx": 504}, {"begin": 64174, "end": 64370, "idx": 505}, {"begin": 64421, "end": 64613, "idx": 506}, {"begin": 64614, "end": 64702, "idx": 507}, {"begin": 64703, "end": 64980, "idx": 508}, {"begin": 64981, "end": 65059, "idx": 509}, {"begin": 65060, "end": 65174, "idx": 510}, {"begin": 65175, "end": 65460, "idx": 511}, {"begin": 65461, "end": 65555, "idx": 512}, {"begin": 65556, "end": 65685, "idx": 513}, {"begin": 65686, "end": 65721, "idx": 514}, {"begin": 65722, "end": 65754, "idx": 515}, {"begin": 65755, "end": 66006, "idx": 516}, {"begin": 66007, "end": 66147, "idx": 517}, {"begin": 66148, "end": 66283, "idx": 518}, {"begin": 66284, "end": 66494, "idx": 519}, {"begin": 66495, "end": 66511, "idx": 520}, {"begin": 66512, "end": 66668, "idx": 521}, {"begin": 66669, "end": 66701, "idx": 522}, {"begin": 66702, "end": 66809, "idx": 523}, {"begin": 66810, "end": 66825, "idx": 524}, {"begin": 66826, "end": 66839, "idx": 525}, {"begin": 66840, "end": 66969, "idx": 526}, {"begin": 66970, "end": 67142, "idx": 527}, {"begin": 67143, "end": 67244, "idx": 528}, {"begin": 67245, "end": 67287, "idx": 529}, {"begin": 67288, "end": 67420, "idx": 530}, {"begin": 67421, "end": 67475, "idx": 531}, {"begin": 67476, "end": 67744, "idx": 532}, {"begin": 67745, "end": 67919, "idx": 533}, {"begin": 67920, "end": 67948, "idx": 534}, {"begin": 67949, "end": 68079, "idx": 535}, {"begin": 68080, "end": 68199, "idx": 536}, {"begin": 68200, "end": 68295, "idx": 537}, {"begin": 68296, "end": 68366, "idx": 538}, {"begin": 68367, "end": 68476, "idx": 539}, {"begin": 68477, "end": 68609, "idx": 540}, {"begin": 68610, "end": 68837, "idx": 541}, {"begin": 68838, "end": 68923, "idx": 542}, {"begin": 68924, "end": 69199, "idx": 543}, {"begin": 69200, "end": 69436, "idx": 544}, {"begin": 69437, "end": 69695, "idx": 545}, {"begin": 69696, "end": 69918, "idx": 546}, {"begin": 69919, "end": 70035, "idx": 547}, {"begin": 70036, "end": 70240, "idx": 548}, {"begin": 70241, "end": 70292, "idx": 549}, {"begin": 70293, "end": 70509, "idx": 550}, {"begin": 70510, "end": 70663, "idx": 551}, {"begin": 70664, "end": 70753, "idx": 552}, {"begin": 70768, "end": 70876, "idx": 553}, {"begin": 70877, "end": 70887, "idx": 554}, {"begin": 70888, "end": 71037, "idx": 555}, {"begin": 71038, "end": 71099, "idx": 556}, {"begin": 71100, "end": 71116, "idx": 557}, {"begin": 71117, "end": 71157, "idx": 558}, {"begin": 71181, "end": 71330, "idx": 559}, {"begin": 71331, "end": 71389, "idx": 560}, {"begin": 71390, "end": 71554, "idx": 561}, {"begin": 71555, "end": 71720, "idx": 562}, {"begin": 71721, "end": 71747, "idx": 563}, {"begin": 71748, "end": 71908, "idx": 564}, {"begin": 71909, "end": 71950, "idx": 565}, {"begin": 71951, "end": 72141, "idx": 566}, {"begin": 72176, "end": 72289, "idx": 567}, {"begin": 72290, "end": 72325, "idx": 568}, {"begin": 72326, "end": 72680, "idx": 569}, {"begin": 72681, "end": 72895, "idx": 570}, {"begin": 72896, "end": 73133, "idx": 571}, {"begin": 73134, "end": 73227, "idx": 572}, {"begin": 73228, "end": 73358, "idx": 573}, {"begin": 73359, "end": 73446, "idx": 574}, {"begin": 73447, "end": 73541, "idx": 575}, {"begin": 73542, "end": 73665, "idx": 576}, {"begin": 73666, "end": 73683, "idx": 577}, {"begin": 73684, "end": 73792, "idx": 578}, {"begin": 73793, "end": 73835, "idx": 579}, {"begin": 73836, "end": 73871, "idx": 580}, {"begin": 73872, "end": 73906, "idx": 581}, {"begin": 73907, "end": 73970, "idx": 582}, {"begin": 73971, "end": 74031, "idx": 583}, {"begin": 74032, "end": 74092, "idx": 584}, {"begin": 74093, "end": 74172, "idx": 585}, {"begin": 74173, "end": 74350, "idx": 586}, {"begin": 74351, "end": 74564, "idx": 587}, {"begin": 74565, "end": 74596, "idx": 588}, {"begin": 74597, "end": 74671, "idx": 589}, {"begin": 74672, "end": 74756, "idx": 590}, {"begin": 74757, "end": 74857, "idx": 591}, {"begin": 74858, "end": 74958, "idx": 592}, {"begin": 74959, "end": 75081, "idx": 593}, {"begin": 75082, "end": 75159, "idx": 594}, {"begin": 75160, "end": 75294, "idx": 595}, {"begin": 75340, "end": 75589, "idx": 596}, {"begin": 75590, "end": 75663, "idx": 597}, {"begin": 75664, "end": 75736, "idx": 598}, {"begin": 75737, "end": 75784, "idx": 599}, {"begin": 75785, "end": 75887, "idx": 600}, {"begin": 75888, "end": 75889, "idx": 601}, {"begin": 75890, "end": 75939, "idx": 602}, {"begin": 75940, "end": 76070, "idx": 603}, {"begin": 76071, "end": 76170, "idx": 604}, {"begin": 76171, "end": 76199, "idx": 605}, {"begin": 76200, "end": 76289, "idx": 606}, {"begin": 76290, "end": 76299, "idx": 607}, {"begin": 76300, "end": 76381, "idx": 608}, {"begin": 76382, "end": 76547, "idx": 609}, {"begin": 76548, "end": 76822, "idx": 610}, {"begin": 76823, "end": 77051, "idx": 611}, {"begin": 77052, "end": 77156, "idx": 612}, {"begin": 77157, "end": 77281, "idx": 613}, {"begin": 77296, "end": 77328, "idx": 614}, {"begin": 77329, "end": 77399, "idx": 615}, {"begin": 77400, "end": 77609, "idx": 616}, {"begin": 77610, "end": 77625, "idx": 617}, {"begin": 77626, "end": 77708, "idx": 618}, {"begin": 77768, "end": 77987, "idx": 619}, {"begin": 78045, "end": 78274, "idx": 620}, {"begin": 78275, "end": 78370, "idx": 621}, {"begin": 78371, "end": 78436, "idx": 622}, {"begin": 78437, "end": 78498, "idx": 623}, {"begin": 78499, "end": 78647, "idx": 624}, {"begin": 78648, "end": 78718, "idx": 625}, {"begin": 78719, "end": 78902, "idx": 626}, {"begin": 78903, "end": 79036, "idx": 627}, {"begin": 79037, "end": 79113, "idx": 628}, {"begin": 79199, "end": 79347, "idx": 629}, {"begin": 79348, "end": 79403, "idx": 630}, {"begin": 79480, "end": 79556, "idx": 631}, {"begin": 79557, "end": 79584, "idx": 632}, {"begin": 79585, "end": 79588, "idx": 633}, {"begin": 79613, "end": 79822, "idx": 634}, {"begin": 79823, "end": 79861, "idx": 635}, {"begin": 79913, "end": 79948, "idx": 636}, {"begin": 79949, "end": 80017, "idx": 637}, {"begin": 80018, "end": 80175, "idx": 638}, {"begin": 80176, "end": 80404, "idx": 639}, {"begin": 80405, "end": 80524, "idx": 640}, {"begin": 80525, "end": 80626, "idx": 641}, {"begin": 80676, "end": 80751, "idx": 642}, {"begin": 80752, "end": 80857, "idx": 643}, {"begin": 80858, "end": 81132, "idx": 644}, {"begin": 81133, "end": 81280, "idx": 645}, {"begin": 81281, "end": 81345, "idx": 646}, {"begin": 81346, "end": 81655, "idx": 647}, {"begin": 81656, "end": 81852, "idx": 648}, {"begin": 81853, "end": 82028, "idx": 649}, {"begin": 82029, "end": 82301, "idx": 650}, {"begin": 82302, "end": 82336, "idx": 651}, {"begin": 82337, "end": 82514, "idx": 652}, {"begin": 82515, "end": 82615, "idx": 653}, {"begin": 82616, "end": 82795, "idx": 654}, {"begin": 82796, "end": 82870, "idx": 655}, {"begin": 82871, "end": 82959, "idx": 656}, {"begin": 82960, "end": 83116, "idx": 657}, {"begin": 83117, "end": 83219, "idx": 658}, {"begin": 83220, "end": 83339, "idx": 659}, {"begin": 83340, "end": 83525, "idx": 660}, {"begin": 83591, "end": 83703, "idx": 661}, {"begin": 83704, "end": 83728, "idx": 662}, {"begin": 83729, "end": 83828, "idx": 663}, {"begin": 83829, "end": 83964, "idx": 664}, {"begin": 83965, "end": 84084, "idx": 665}, {"begin": 84085, "end": 84186, "idx": 666}, {"begin": 84187, "end": 84204, "idx": 667}, {"begin": 84205, "end": 84328, "idx": 668}, {"begin": 84329, "end": 84462, "idx": 669}, {"begin": 84463, "end": 84568, "idx": 670}, {"begin": 84569, "end": 84664, "idx": 671}, {"begin": 84665, "end": 84682, "idx": 672}, {"begin": 84683, "end": 84834, "idx": 673}, {"begin": 84835, "end": 85019, "idx": 674}, {"begin": 85020, "end": 85102, "idx": 675}, {"begin": 85103, "end": 85234, "idx": 676}, {"begin": 85235, "end": 85367, "idx": 677}, {"begin": 85368, "end": 85483, "idx": 678}, {"begin": 85484, "end": 85495, "idx": 679}, {"begin": 85496, "end": 85624, "idx": 680}, {"begin": 85625, "end": 85753, "idx": 681}, {"begin": 85754, "end": 85904, "idx": 682}, {"begin": 85940, "end": 86094, "idx": 683}, {"begin": 86095, "end": 86204, "idx": 684}, {"begin": 86205, "end": 86403, "idx": 685}, {"begin": 86404, "end": 86467, "idx": 686}, {"begin": 86468, "end": 86658, "idx": 687}, {"begin": 86659, "end": 86723, "idx": 688}, {"begin": 86746, "end": 86883, "idx": 689}, {"begin": 86884, "end": 86915, "idx": 690}, {"begin": 86916, "end": 87132, "idx": 691}, {"begin": 87133, "end": 87241, "idx": 692}, {"begin": 87242, "end": 87432, "idx": 693}, {"begin": 87433, "end": 87557, "idx": 694}, {"begin": 87610, "end": 87707, "idx": 695}, {"begin": 87708, "end": 87813, "idx": 696}, {"begin": 87814, "end": 87903, "idx": 697}, {"begin": 87904, "end": 88008, "idx": 698}, {"begin": 88009, "end": 88103, "idx": 699}, {"begin": 88104, "end": 88164, "idx": 700}, {"begin": 88165, "end": 88342, "idx": 701}, {"begin": 88343, "end": 88460, "idx": 702}, {"begin": 88461, "end": 88552, "idx": 703}, {"begin": 88553, "end": 88633, "idx": 704}, {"begin": 88634, "end": 88763, "idx": 705}, {"begin": 88764, "end": 88857, "idx": 706}, {"begin": 88858, "end": 88976, "idx": 707}, {"begin": 89046, "end": 89101, "idx": 708}, {"begin": 89102, "end": 89316, "idx": 709}, {"begin": 89317, "end": 89394, "idx": 710}, {"begin": 89395, "end": 89606, "idx": 711}, {"begin": 89651, "end": 89784, "idx": 712}, {"begin": 89785, "end": 89855, "idx": 713}, {"begin": 89856, "end": 90187, "idx": 714}, {"begin": 90188, "end": 90346, "idx": 715}, {"begin": 90347, "end": 90628, "idx": 716}, {"begin": 90629, "end": 90730, "idx": 717}, {"begin": 90761, "end": 90820, "idx": 718}, {"begin": 90821, "end": 90974, "idx": 719}, {"begin": 90975, "end": 91045, "idx": 720}, {"begin": 91046, "end": 91076, "idx": 721}, {"begin": 91077, "end": 91117, "idx": 722}, {"begin": 91118, "end": 91201, "idx": 723}, {"begin": 91202, "end": 91264, "idx": 724}, {"begin": 91265, "end": 91312, "idx": 725}, {"begin": 91313, "end": 91333, "idx": 726}, {"begin": 91334, "end": 91490, "idx": 727}, {"begin": 91491, "end": 91654, "idx": 728}, {"begin": 91655, "end": 91831, "idx": 729}, {"begin": 91832, "end": 91857, "idx": 730}, {"begin": 91858, "end": 92034, "idx": 731}, {"begin": 92035, "end": 92164, "idx": 732}, {"begin": 92165, "end": 92249, "idx": 733}, {"begin": 92250, "end": 92305, "idx": 734}, {"begin": 92306, "end": 92414, "idx": 735}, {"begin": 92415, "end": 92525, "idx": 736}, {"begin": 92526, "end": 92647, "idx": 737}, {"begin": 92683, "end": 92844, "idx": 738}, {"begin": 92845, "end": 92923, "idx": 739}, {"begin": 92924, "end": 93046, "idx": 740}, {"begin": 93047, "end": 93185, "idx": 741}, {"begin": 93186, "end": 93430, "idx": 742}, {"begin": 93431, "end": 93550, "idx": 743}, {"begin": 93551, "end": 93629, "idx": 744}, {"begin": 93630, "end": 93767, "idx": 745}, {"begin": 93768, "end": 93976, "idx": 746}, {"begin": 93977, "end": 94117, "idx": 747}, {"begin": 94118, "end": 94262, "idx": 748}, {"begin": 94263, "end": 94399, "idx": 749}, {"begin": 94400, "end": 94684, "idx": 750}, {"begin": 94685, "end": 94785, "idx": 751}, {"begin": 94786, "end": 94920, "idx": 752}, {"begin": 94921, "end": 95049, "idx": 753}, {"begin": 95050, "end": 95223, "idx": 754}, {"begin": 95224, "end": 95333, "idx": 755}, {"begin": 95334, "end": 95514, "idx": 756}, {"begin": 95515, "end": 95710, "idx": 757}, {"begin": 95711, "end": 95849, "idx": 758}, {"begin": 95850, "end": 96000, "idx": 759}, {"begin": 96001, "end": 96192, "idx": 760}], "ReferenceToFigure": [{"begin": 2117, "end": 2118, "target": "#fig_0", "idx": 0}, {"begin": 8764, "end": 8765, "target": "#fig_1", "idx": 1}, {"begin": 8834, "end": 8836, "target": "#fig_1", "idx": 2}, {"begin": 8975, "end": 8977, "target": "#fig_1", "idx": 3}, {"begin": 9184, "end": 9188, "target": "#fig_1", "idx": 4}, {"begin": 12216, "end": 12218, "target": "#fig_1", "idx": 5}, {"begin": 13700, "end": 13702, "target": "#fig_1", "idx": 6}, {"begin": 27504, "end": 27506, "target": "#fig_7", "idx": 7}, {"begin": 27986, "end": 27988, "target": "#fig_7", "idx": 8}, {"begin": 28150, "end": 28152, "target": "#fig_7", "idx": 9}, {"begin": 29302, "end": 29304, "target": "#fig_7", "idx": 10}, {"begin": 29534, "end": 29538, "target": "#fig_7", "idx": 11}, {"begin": 29942, "end": 29944, "target": "#fig_7", "idx": 12}, {"begin": 33405, "end": 33406, "target": "#fig_4", "idx": 13}, {"begin": 33675, "end": 33676, "target": "#fig_4", "idx": 14}, {"begin": 37857, "end": 37858, "target": "#fig_0", "idx": 15}, {"begin": 41493, "end": 41494, "idx": 16}, {"begin": 52796, "end": 52797, "idx": 17}], "Abstract": [{"begin": 96, "end": 1066, "idx": 0}], "SectionFootnote": [{"begin": 96194, "end": 98105, "idx": 0}], "Footnote": [{"begin": 96205, "end": 96528, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 96529, "end": 96605, "id": "foot_1", "n": "3", "idx": 1}, {"begin": 96606, "end": 96821, "id": "foot_2", "n": "4", "idx": 2}, {"begin": 96822, "end": 96931, "id": "foot_3", "n": "5", "idx": 3}, {"begin": 96932, "end": 97337, "id": "foot_4", "n": "6", "idx": 4}, {"begin": 97338, "end": 97774, "id": "foot_5", "n": "7", "idx": 5}, {"begin": 97775, "end": 97883, "id": "foot_6", "n": "12", "idx": 6}, {"begin": 97884, "end": 98070, "id": "foot_7", "n": "13", "idx": 7}, {"begin": 98071, "end": 98105, "id": "foot_8", "n": "14", "idx": 8}]}}