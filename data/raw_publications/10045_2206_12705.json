{"text": "p-Meta: Towards On-device Deep Model Adaptation\n\nAbstract:\nData collected by IoT devices are often private and have a large diversity across users. Therefore, learning requires pre-training a model with available representative data samples, deploying the pre-trained model on IoT devices, and adapting the deployed model on the device with local data. Such an on-device adaption for deep learning empowered applications demands data and memory efficiency. However, existing gradient-based meta learning schemes fail to support memory-efficient adaptation. To this end, we propose p-Meta, a new meta learning method that enforces structure-wise partial parameter updates while ensuring fast generalization to unseen tasks. Evaluations on few-shot image classification and reinforcement learning tasks show that p-Meta not only improves the accuracy but also substantially reduces the peak dynamic memory by a factor of 2.5 on average compared to state-of-the-art few-shot adaptation methods.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Neural networks.\n\nMain:\n\n\n\n1 INTRODUCTION\nAdaption to unseen environments, users, and tasks is crucial for deep learning empowered IoT applications to deliver consistent performance and customized services. Data collected by IoT devices are often private and have a large diversity across users. For instance, activity recognition with smartphone sensors should adapt to countless walking patterns and sensor orientation [13]. Human motion prediction with home robots needs fast learning of unseen poses for seamless human-robot interaction [19]. In these applications, the new data collected for model adaptation tend to relate to personal habits and lifestyle. Hence, on-device model adaptation is preferred over uploading the data to cloud servers for retraining.\n\nFSL on IoT Device Meta-Train on Cloud\nYet on-device adaption of a deep neural network (DNN) demands data efficiency and memory efficiency. The excellent accuracy of contemporary DNNs is attributed to training with high-performance computers on large-scale datasets [14]. For example, it takes 29 hours to complete a 90-epoch ResNet50 [21] training on ImageNet (1.2 million training images) [31] with 8 NVIDIA Tesla P100 GPUs [16]. For on-device adaptation, however, neither abundant data nor resources are available. A personal voice assistant, for example, may learn to adapt to users' accent and dialect within a few sentences, while a home robot should learn to recognize new object categories with few labelled images to navigate in new environments. Furthermore, such adaptation is expected to be conducted on low-resource platforms such as smart portable devices, home hubs, and other IoT devices, with only several  to  memory.\nFor data-efficient DNN adaptation, we resort to meta learning, a paradigm that learns to fast generalize to unseen tasks [22]. Of our particular interest is gradient-based meta learning [1, 10, 26, 28] for its wide applicability in classification, regression and reinforcement learning, as well as the availability of gradient-based training frameworks for low-resource devices, e.g., TensorFlow Lite [36]. Fig. 1 explains major terminologies in the context of on-device adaptation. Given a backbone, its weights are meta-trained on many tasks, to output a model that is expected to fast adapt to new unseen tasks. The process of adaptation is also known as few-shot learning, where the meta-trained model is further retrained by standard stochastic gradient decent (SGD) on few new samples only.\nHowever, existing gradient-based meta learning schemes [1, 10, 26, 28] fail to support memory-efficient adaptation. Although meta training is conducted in the cloud, few-shot learning (adaptation) of the meta-trained model is performed on IoT devices. Consider to retrain a common backbone ResNet12 in a 5-way (5 new classes) 5-shot (5 samples per class) scenario. One round of SGD consumes 370.44MB peak dynamic memory, since the inputs of all layers must be stored to compute the gradients of these layers' weights in the backward path. In comparison, inference only needs 3.61MB. The necessary dynamic memory is a key bottleneck for on-device adaptation due to cost and power constraints, even though the meta-trained model only needs to be retrained with a few data.\nPrior efficient DNN training solutions mainly focus on parallel and distributed training on data centers [3, 4, 17, 18, 29]. On-device training has been explored for vanilla supervised training [15, 24, 25], where training and testing are performed on the same task. A pioneer study [2] investigated on-device adaptation to new tasks via memory-efficient transfer learning. Yet transfer learning is prone to overfitting when only a few samples are available [10].\nIn this paper, we propose p-Meta, a new meta learning method for data-and memory-efficient DNN adaptation. The key idea is to enforce structured partial parameter updates while ensuring fast generalization to unseen tasks. The idea is inspired by recent advances in understanding gradient-based meta learning [26, 28]. Empirical evidence shows that only the head (the last output layer) of a DNN needs to be updated to achieve reasonable few-shot classification accuracy [28] whereas the body (the layers closed to the input) needs to be updated for cross-domain few-shot classification [26]. These studies imply that certain weights are more important than others when generalizing to unseen tasks. Hence, we propose to automatically identify these adaptation-critical weights to minimize the memory demand in few-shot learning.\nParticularly, the critical weights are determined in two structured dimensionalities as, (i) layer-wise: we meta-train a layer-by-layer learning rate that enables a static selection of critical layers for updating; (ii) channel-wise: we introduce meta attention modules in each layer to select critical channels dynamically, i.e., depending on samples from new tasks. Partial updating of weights means that (structurally) sparse gradients are generated, reducing memory requirements to those for computing nonzero gradients. In addition, the computation demand for calculating zero gradients can be also saved. To further reduce the memory, we utilize gradient accumulation in few-shot learning and group normalization in the backbone. Although weight importance metrics and SGD with sparse gradients have been explored in vanilla training [8, 15, 20, 29], it is unknown (i) how to identify adaptation-critical weights and (ii) whether meta learning is robust to sparse gradients, where the objective is fast adaptation to unseen tasks.\nOur main contributions are summarized as follows.\n\u2022 We design p-Meta, a new meta learning method for dataand memory-efficient DNN adaptation to unseen tasks. p-Meta automatically identifies adaptation-critical weights both layer-wise and channel-wise for low-memory adaptation. The hierarchical approach combines static identification of layers and dynamic identification of channels whose weights are critical for few-shot adaptation. To the best of our knowledge, p-Meta is the first meta learning method designed for on-device few-shot learning. \u2022 Evaluations on few-shot image classification and reinforcement learning show that, p-Meta not only improves the accuracy but also reduces the peak dynamic memory by a factor of 2.5 on average over the state-of-the-art few-shot adaptation methods. p-Meta can also simultaneously reduce the computation by a factor of 1.7 on average.\nIn the rest of this paper, we introduce the preliminaries and challenges in Sec. 2, elaborate on the design of p-Meta in Sec. 3, present its evaluations in Sec. 4, review related work in Sec. 5, and conclude in Sec. 6.\n\n2 PRELIMINARIES AND CHALLENGES\nIn this section, we provide the basics on meta learning for fast adaptation and highlight the challenges to enable on-device adaptation.\nMeta Learning for Fast Adaptation. Meta learning is a prevailing solution to adapt a DNN to unseen tasks with limited training samples, i.e., few-shot learning [22]. We ground our work on modelagnostic meta learning (MAML) [10], a generic meta learning framework which supports classification, regression and reinforcement learning. Given the dataset D = {S, Q} of an unseen few-shot task, where S (support set) and Q (query set) are for training and testing, MAML trains a model  () with weights  such that it yields high accuracy on Q even when S only contains a few samples. This is enabled by simulating the few-shot learning experiences over abundant few-shot tasks sampled from a task distribution  (T). Specifically, it meta-trains a backbone  over few-shot tasks T  \u223c  (T), where each T  has dataset D  = {S  , Q  }, and then generates  ( meta ), an initialization for the unseen few-shot task T new with dataset D new = {S new , Q new }. Training from  ( meta ) over S new is expected to achieve a high test accuracy on Q new .\nMAML achieves fast adaptation via two-tier optimization. In the inner loop, a task T  and its dataset D  are sampled. The weights  are updated to   on support dataset S  via  gradient descent steps, where  is usually small, compared to vanilla training:\ud835\udc98 \ud835\udc56,\ud835\udc58 = \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 \u2212 \ud835\udefc \u2207 \ud835\udc98 \u2113 \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 ; S \ud835\udc56 for \ud835\udc58 = 1 \u2022 \u2022 \u2022 \ud835\udc3e\nwhere  , are the weights at step  in the inner loop, and  is the inner step size. Note that  ,0 =  and   =  , . \u2113 (; D) is the loss function on dataset D. In the outer loop, the weights are optimized to minimize the sum of loss at   on query dataset Q  across tasks. The gradients to update weights in the outer loop are calculated w.r.t. the starting point  of the inner loop.\ud835\udc98 \u2190 \ud835\udc98 \u2212 \ud835\udefd \u2207 \ud835\udc98 \u2211\ufe01 \ud835\udc56 \u2113 \ud835\udc98 \ud835\udc56 ; Q \ud835\udc56 (2)\nwhere  is the outer step size. The meta-trained weights  meta are then used as initialization for few-shot learning into  new by  gradient descent steps over S new . Finally we assess the accuracy of  ( new ) on Q new . Table 1 : The static memory, the peak dynamic memory, and the total computation (GMACs = 10 9 MACs) of inference and adaptation for sample applications. For image classification we use batch size = 25. For robot locomotion we use rollouts = 20, horizon = 200; each sample corresponds to a rollouted episode, and the case for an observation is reported in brackets. The calculation is based on Appendix A.  Memory Bottleneck of On-device Adaptation. As mentioned above, the meta-trained model  ( meta ) can adapt to unseen tasks via  gradient descent steps. Each step is the same as the inner loop of meta-training Eq.( 1), but on dataset S new .\ud835\udc98 new,\ud835\udc58 = \ud835\udc98 new,\ud835\udc58\u22121 \u2212 \ud835\udefc \u2207 \ud835\udc98 new \u2113 \ud835\udc98 new,\ud835\udc58\u22121 ; S new\nwhere  new,0 =  meta . For brevity, we omit the superscripts of model adaption in Eq.( 3) and use (\u2022) as the loss gradients w.r.t. the given tensor. Hence, without ambiguity, we simplify the notations of Eq.( 3) as follows:\ud835\udc98 \u2190 \ud835\udc98 \u2212 \ud835\udefc\ud835\udc88 (\ud835\udc98)\nLet us now understand where the main memory cost for iterating Eq.(4) comes from. For the sake of clarity, we focus on a feed forward DNNs that consist of  convolutional (conv) layers or fullyconnected (fc) layers. A typical layer (see Fig. 2) consists of two operations: (i) a linear operation with trainable parameters, e.g., convolution or affine; (ii) a parameter-free non-linear operation (may not exist in certain layers), where we consider max-pooling or ReLU-styled (ReLU, LeakyReLU) activation functions in this paper.\nTake a network consisting of conv layers only as an example. The memory requirements for storing the activations   \u2208 R   \u00d7  \u00d7  as well as the convolution weights   \u2208 R   \u00d7  \u22121 \u00d7  \u00d7  of layer  in words can be determined as\ud835\udc5a (\ud835\udc99 \ud835\udc59 ) = \ud835\udc36 \ud835\udc59 \ud835\udc3b \ud835\udc59 \ud835\udc4a \ud835\udc59 , \ud835\udc5a (\ud835\udc98 \ud835\udc59 ) = \ud835\udc36 \ud835\udc59 \ud835\udc36 \ud835\udc59 \u22121 \ud835\udc46 2 \ud835\udc59\nwhere  \u22121 ,   ,   , and   stand for input channel number, output channel number, height and width of layer , respectively;   stands for the kernel size. The detailed memory and computation demand analysis as provided in Appendix A reveals that the by far largest memory requirement is neither attributed to determining the activations   in the forward path nor to determining the gradients of the activations (  ) in the backward path. Instead, the memory bottleneck lies in the computation of the weight gradients (  ), which requires the availability of the activations  \u22121 from the forward path. Following Eq.( 17) in Appendix A, the necessary memory in words is\u2211\ufe01 1\u2264\ud835\udc59 \u2264\ud835\udc3f \ud835\udc5a (\ud835\udc99 \ud835\udc59 \u22121 )\nTab. 1 summarizes the memory consumption and the total computation of the commonly used few-shot learning backbone models [10, 27]. The requirements are based on the detailed analysis in Appendix A. We can draw two intermediate conclusions.\n\u2022 The total computation of adaptation (training) is approximately 2.7\u00d7 to 3\u00d7 larger compared to inference. Yet the peak dynamic memory of training is far larger, 47\u00d7 to 103\u00d7 over inference. The peak dynamic memory consumption of training is also also significantly higher than the static memory consumption from the model and the training samples in few-shot learning. \u2022 To enable adaptation for memory-constrained IoT devices, we need to find some way of getting rid of the major dynamic memory contribution in Eq.( 5).\n\n3 METHOD\nThis section presents p-Meta, a new meta learning scheme that enables memory-efficient few-shot learning on unseen tasks.\n\n3.1 p-Meta Overview\nWe first provide an overview of p-Meta and introduce its main concepts, namely selecting critical gradients, using a hierarchical approach to determine adaption-critical layers and channels, and using a mixture of static and dynamic selection mechanisms.\nPrinciples. We impose structured sparsity on the gradients (  ) such that the corresponding tensor dimensions of   do not need to be saved. There are other options to reduce the dominant memory demand in Eq.( 5). They are inapplicable for the reasons below.\n\u2022 One may trade-off computation and memory by recomputing activations  \u22121 when needed for determining   , see for example [4, 18]. Due to the limited processing abilities of IoT devices, we exclude this option. \u2022 It is also possible to prune activations  \u22121 . Yet based on our experiments in Appendix B.3, imposing sparsity on  \u22121 hugely degrades few-shot learning accuracy as this causes error accumulation along the propagation, see also [29]. \u2022 Note that unstructured sparsity, as proposed in [11, 40], does not in general lead to memory savings, since there is a very small probability that all weight gradients for which an element of  \u22121 is necessary have been pruned.\nWe impose sparsity on the gradients in a hierarchical manner.\n\u2022 Selecting adaption-critical layers. We first impose layerby-layer sparsity on (  ). It is motivated by previous results showing that manual freezing of certain layers does no harm to few-shot learning accuracy [26, 28]. Layer-wise sparsity reduces the number of layers whose weights need to be updated. We determine the adaptation-critical layers from the meta-trained layer-wise sparse learning rates. \u2022 Selecting adaption-critical channels. We further reduce the memory demand by imposing sparsity on (  ) within each layer. Noting that calculating (  ) needs both the input channels  \u22121 and the output channels (  ), we enforce sparsity on both of them. Input channel sparsity decreases memory and computation overhead, whereas output channel sparsity improves few-shot learning accuracy and reduces computation. We design a novel meta attention mechanism to dynamically determine adaptation-critical channels. They take as inputs  \u22121 and (  ) and determine adaptationcritical channels during few-shot learning, based on the given few data samples from new unseen tasks. Dynamic channel-wise learning rates as determined by meta attention yield a significantly higher accuracy than a static channelwise learning rate (see Sec. 4.4).\nMemory Reduction. The reduced memory demand due to our hierarchical approach can be seen in Eq.( 17) in Appendix A:\u2211\ufe01 1\u2264\ud835\udc59 \u2264\ud835\udc3f \u03b1\ud835\udc59 \ud835\udf07 fw \ud835\udc59 \ud835\udc5a (\ud835\udc99 \ud835\udc59 \u22121 )\nwhere \u03b1 \u2208 {0, 1} is the mask from the static selection of critical layers and 0 \u2264  fw  \u2264 1 denotes the relative amount of dynamically chosen input channels.\nNext, we explain how p-Meta selects adaptation-critical layers (Sec. 3.2) and channels within layers (Sec. 3.3) as well as the deployment optimizations (Sec. 3.5) for memory-efficient adaptation.\n\n3.2 Selecting Adaption-Critical Layers by Learning Sparse Inner Step Sizes\nThis subsection introduces how p-Meta meta-learns adaptationcritical layers to reduce the number of updated layers during fewshot learning. Particularly, instead of manual configuration as in [26, 28], we propose to automate the layer selection process. During meta training, we identify adaptation-critical layers by learning layer-wise sparse inner step sizes (Sec. 3.2.1). Only these critical layers with nonzero step sizes will be updated during on-device adaptation to new tasks (Sec. 3.2.2).\n\n3.2.1 Learning Sparse Inner\nStep Sizes in Meta Training. Prior work [1] suggests that instead of a global fixed inner step size , learning the inner step sizes  for each layer and each gradient descent step improves the generalization of meta learning, where  =  1: 1: \u2ab0 0. We utilize such learned inner step sizes to infer layer importance for adaptation. We learn the inner step sizes  in the outer loop of meta-training while fixing them in the inner loop.\nLearning Layer-wise Inner Step Sizes. We change the inner loop of Eq.( 1) to incorporate the per-layer inner step sizes:\ud835\udc98 \ud835\udc56,\ud835\udc58 \ud835\udc59 = \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 \ud835\udc59 \u2212 \ud835\udefc \ud835\udc58 \ud835\udc59 \u2207 \ud835\udc98 \ud835\udc59 \u2113 \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 1:\ud835\udc3f ; S \ud835\udc56\nwhere  ,  is the weights of layer  at step  optimized on task  (dataset S  ). In the outer loop, weights  are still optimized as\ud835\udc98 \u2190 \ud835\udc98 \u2212 \ud835\udefd \u2207 \ud835\udc98 \u2211\ufe01 \ud835\udc56 \u2113 \ud835\udc98 \ud835\udc56 ; Q \ud835\udc56\nwhere   =  , =  , 1: , which is a function of  . The inner step sizes  are then optimized as\ud835\udf36 \u2190 \ud835\udf36 \u2212 \ud835\udefd \u2207 \ud835\udf36 \u2211\ufe01 \ud835\udc56 \u2113 \ud835\udc98 \ud835\udc56 ; Q \ud835\udc56 (8)\nImposing Sparsity on Inner Step Sizes. To facilitate layer selection, we enforce sparsity in  , i.e., encouraging a subset of layers to be selected for updating. Specifically, we add a Lasso regularization term in the loss function of Eq.( 8) when optimizing  . Hence, the final optimization of  in the outer loop is formulated as\ud835\udf36 \u2190 \ud835\udf36 \u2212 \ud835\udefd \u2207 \ud835\udf36 ( \u2211\ufe01 \ud835\udc56 \u2113 \ud835\udc98 \ud835\udc56 ; Q \ud835\udc56 + \ud835\udf06 \u2211\ufe01 \ud835\udc59,\ud835\udc58 \ud835\udc5a (\ud835\udc99 \ud835\udc59 \u22121 ) \u2022 |\ud835\udefc \ud835\udc58 \ud835\udc59 |) ()\nwhere  is a positive scalar to control the ratio between two terms in the loss function. We empirically set  = 0.001. |   | is re-weighted by ( \u22121 ), which denotes the necessary memory in Eq.( 5) if only updating the weights in layer .\n\n3.2.2 Exploiting Sparse Inner\nStep Sizes for On-device Adaptation. We now explain how to apply the learned  to save memory during on-device adaptation. After deploying the meta-trained model to IoT devices for adaptation, at updating step , for layers with    = 0, the activations (i.e., their inputs)  \u22121 need not be stored, see Eq.( 16) and Eq.( 17) in Appendix A. In addition, we do not need to calculate the corresponding weight gradients (  ), which saves computation, see Eq.( 18) in Appendix A.\n\n3.3 Selecting Adaption-Critical Channels within Layers via Sparse Meta Attention\nThis subsection explains how p-Meta learns a novel meta attention mechanism in each layer to dynamically select adaptation-critical channels for further memory saving in few-shot learning. Despite the widespread adoption of channel-wise attention for inference [5, 23], we make the first attempt to use attention for memory-efficient training (few-shot learning in our case). For each layer, its meta attention outputs a dynamic channel-wise sparse attention score based on the samples from new tasks. The sparse attention score is used to re-weight (also sparsify) the weight gradients. Therefore, by calculating only the nonzero gradients of critical weights within a layer, we can save both memory and computation. We first present our meta attention mechanism during meta training (Sec. 3.3.1) and then show its usage for on-device model adaptation (Sec. 3.3.2).\n\n3.3.1 Learning Sparse Meta Attention in Meta\nTraining. Since mainstream backbones in meta learning use small kernel sizes (1 or 3), we design the meta attention mechanism channel-wise. Fig. 3 illustrates the attention design during meta-training.\nLearning Meta Attention. The attention mechanism is as follows.\n\u2022 We assign an attention score to the weight gradients of layer  in the inner loop of meta training. The attention scores are expected to indicate which weights/channels are important and thus should be updated in layer . \u2022 The attention score is obtained from two attention modules: one taking  \u22121 as input in the forward pass, and the other taking (  ) as input during the backward pass. We use  \u22121 and (  ) to calculate the attention scores because they are used to compute the weight gradients (  ). Concretely, we define the forward and backward attention scores for a conv layer as,\ud835\udf38 fw \ud835\udc59 = \u210e (\ud835\udc98 fw \ud835\udc59 ; \ud835\udc99 \ud835\udc59 \u22121 ) \u2208 R \ud835\udc36 \ud835\udc59 \u22121 \u00d71\u00d71\ud835\udf38 bw \ud835\udc59 = \u210e (\ud835\udc98 bw \ud835\udc59 ; \ud835\udc88 (\ud835\udc9a \ud835\udc59 )) \u2208 R \ud835\udc36 \ud835\udc59 \u00d71\u00d71\nwhere \u210e(\u2022; \u2022) stands for the meta attention module, and  fw  and  bw  are the parameters of the meta attention modules. The overall (sparse) attention scores   \u2208 R   \u00d7  \u22121 \u00d71\u00d71 and is computed as,\ud835\udefe \ud835\udc59,\ud835\udc4f\ud835\udc4e11 = \ud835\udefe fw \ud835\udc59,\ud835\udc4e11 \u2022 \ud835\udefe bw \ud835\udc59,\ud835\udc4f11\nIn the inner loop, for layer , step  and task ,   is (broadcasting) multiplied with the dense weight gradients to get the sparse ones,\ud835\udf38 \ud835\udc56,\ud835\udc58 \ud835\udc59 \u2299 \u2207 \ud835\udc98 \ud835\udc59 \u2113 \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 1:\ud835\udc3f ; S \ud835\udc56\nThe weights are then updated by,\ud835\udc98 \ud835\udc56,\ud835\udc58 \ud835\udc59 = \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 \ud835\udc59 \u2212 \ud835\udefc \ud835\udc58 \ud835\udc59 (\ud835\udf38 \ud835\udc56,\ud835\udc58 \ud835\udc59 \u2299 \u2207 \ud835\udc98 \ud835\udc59 \u2113 \ud835\udc98 \ud835\udc56,\ud835\udc58\u22121 1:\ud835\udc3f ; S \ud835\udc56 )\nLet all attention parameters be  atten = { fw  ,  bw  }  =1 . The attention parameters  atten are optimized in the outer loop as,\ud835\udc98 atten \u2190 \ud835\udc98 atten \u2212 \ud835\udefd \u2207 \ud835\udc98 atten \u2211\ufe01 \ud835\udc56 \u2113 \ud835\udc98 \ud835\udc56 ; Q \ud835\udc56\nNote that we use a dense forward path and a dense backward path in both meta-training and on-device adaptation, as shown in Fig. 3.   Update attention parameters  atten with Eq.( 15); That is, the attention scores  fw  and  bw  are only calculated locally and will not affect   during forward and ( \u22121 ) during backward.\nMeta Attention Module Design. Fig. 3 (upper part) shows an example meta attention module. We adapt the inference attention modules used in [5, 23], yet with the following modifications.\n\u2022 Unlike inference attention that applies to a single sample, training may calculate the averaged loss gradients based on a batch of samples. Since (  ) does not have a batch dimension, the input to softmax function is first averaged over the batch data, see in Fig. 3. \u2022 We enforce sparsity on the meta attention scores such that they can be utilized to save memory and computation in few-shot learning. The original attention in [5, 23] outputs normalized scales in [0, 1] from softmax. We clip the output with a clip ratio  \u2208 [0, 1] to create zeros in  . This way, our meta attention modules yield batch-averaged sparse attention scores  fw  and  bw  . Alg. 1 shows this clipping and re-normalization process. Note that Alg. 1 is not differentiable. Hence we use the straight-through-estimator for its backward propagation in meta training.\n\n3.3.2 Exploiting Meta Attention for\nOn-device Adaptation. We now explain how to apply the meta attention to save memory during on-device few-shot learning. Note that the parameters in the meta attention modules are fixed during few-shot learning. Assume that at step , layer  has a nonzero step size    . In the forward pass, we only store a sparse tensor  fw  \u2022  \u22121 , i.e., its channels are stored only if they correspond to nonzero entries in  fw  . This reduces memory consumption as shown in Eq.( 17) in Appendix A. Similarly, in the backward pass, we get a channel-wise sparse tensor  bw  \u2022 (  ). Since both sparse tensors are used to calculate the corresponding nonzero gradients in (  ), the computation cost is also reduced, see Eq.( 18) in Appendix A. We plot the meta attention during ondevice adaptation in Fig. 4.\n\n3.4 Summary of p-Meta\nAlg. 2 shows the overall process of p-Meta during meta-training. The final meta-trained weights  from Alg. 2 are assigned to  meta , see Sec. 2. The meta-trained backbone model  ( meta ), the sparse inner step sizes  , and the meta attention modules will be then deployed on edge devices and used to conduct a memory-efficient few-shot learning.\n\n3.5 Deployment Optimization\nTo further reduce the memory during few-shot learning, we propose gradient accumulation during backpropagation and replace batch normalization in the backbone with group normalization.\n\n3.5.1 Gradient Accumulation.\nIn standard few-shot learning, all the new samples (e.g., 25 for 5-way 5-shot) are fed into the model as one batch. To reduce the peak memory due to large batch sizes, we conduct few-shot learning with gradient accumulation (GA).\nGA is a technique that (i) breaks a large batch into smaller partial batches; (ii) sequentially forward/backward propagates each partial batches through the model; (iii) accumulates the loss gradients of each partial batch and get the final averaged gradients of the full batch. Note that GA does not increase computation, which is desired for low-resource platforms. We evaluate the impact of different sample batch sizes in GA in Appendix B.2.\n\n3.5.2 Group Normalization.\nMainstream backbones in meta learning typically adopt batch normalization layers. Batch normalization layers compute the statistical information in each batch, which is dependent on the sample batch size. When using GA with different sample batch sizes, the inaccurate batch statistics can degrade the training performance (see Appendix B.1). As a remedy, we use group normalization [43], which does not rely on batch statistics (i.e., independent of the sample batch size). We also apply meta attention on group normalization layers when updating their weights.\nThe only difference w.r.t. conv and fc layers is that the stored input tensor (also the one used for the meta attention) is not  \u22121 , but its normalized version.\n\n4 EVALUATION\nThis section presents the evaluations of p-Meta on standard fewshot image classification and reinforcement learning benchmarks.\n\n4.1 General Experimental Settings\nCompared Methods. We test the meta learning algorithms below.\n\u2022 MAML [10] : the original model-agnostic meta learning.\n\u2022 ANIL [28] : update the last layer only in few-shot learning.\n\u2022 BOIL [26] : update the body except the last layer.\n\u2022 MAML++ [1] : learn a per-step per-layer step sizes  . Implementation. The experiments are conducted with tools provided by TorchMeta [6, 7]. Particularly, the backbone is meta-trained with full sample batch size (e.g., 25 for 5-way 5-shot) on meta training dataset. After each meta training epoch, the model is tested (i.e., few-shot learned) on meta validation dataset. The model with the highest validation performance is used to report the final few-shot learning results on meta test dataset. We follow the same process as TorchMeta [6, 7] to build the dataset. During few-shot learning, we adopt a sample batch size of 1 to verify the model performance under the most strict memory constraints.\nIn p-Meta, meta attention is applied to all conv, fc, and group normalization layers, except the last output layer, because (i) we find modifying the last layer's gradients may decrease accuracy; (ii) the final output is often rather small in size, resulting in little memory saving even if imposing sparsity on the last layer. Without further notations, we set  = 0.3 in forward attention, and  = 0 in backward attention across all layers, as the sparsity of  bw  almost has no effect on the memory saving.\nMetrics. We compare the peak memory and MACs of different algorithms. Note that the reported peak memory and MACs for p-Meta also include the consumption from meta attention, although they are rather small related to the backward propagation.\n\n4.2 Performance on Image Classification\nSettings. We test on standard few-shot image classification tasks (both in-domain and cross-domain). We adopt two common backbones, \"4Conv\" [10] which has 4 conv blocks with 32 channels in each block, and \"ResNet12\" [27] which contains 4 residual blocks with {64, 128, 256, 512} channels in each block respectively. We replace the batch normalization layers with group normalization layers, as discussed in Sec. 3.5.2. We experiment in both 5-way 1-shot and 5-way 5-shot settings. We train the model on MiniIma-geNet [39] (both meta training and meta validation dataset) with 100 meta epochs. In each meta epoch, 1000 random tasks are drawn from the task distribution. The task batch size is set to 4 in general, Table 2 : Few-shot image classification results on 4Conv and ResNet12. All methods are meta-trained on MiniImageNet, and are few-shot learned on the reported datasets: MiniImageNet, TieredImageNet, and CUB (denoted by Mini, Tiered, and CUB in the table). The total computation (# GMACs) and the peak memory (MB) during few-shot learning are reported based on the theoretical analysis in Appendix A.  The model is updated with 5 gradient steps (i.e.,  = 5) in both inner loop of meta-training and few-shot learning. We use Adam optimizer with cosine learning rate scheduling as [1] for all outer loop updating. The (initial) inner step size  is set to 0.01. The meta-trained model is then tested on three datasets MiniImageNet [39], TieredImageNet [30], and CUB [41] to verify both in-domain and cross-domain performance.\nResults. Tab. 2 shows the accuracy averaged over 5000 new unseen tasks randomly drawn from the meta test dataset. We also report the average number of GMACs and the average peak memory per task according to Appendix A. Clearly, p-Meta almost always yields the highest accuracy in all settings. Note that the comparison between \"p-Meta (3.2)\" and \"MAML++\" can be considered as the ablation studies on learning sparse layer-wise inner step sizes proposed in Sec. 3.2. Thanks to the imposed sparsity on  , \"p-Meta (3.2)\" significantly reduces the peak memory (2.5\u00d7 saving on average and up to 3.1\u00d7) and the computation burden (1.7\u00d7 saving on average and up to 2.4\u00d7) over \"MAML++\". Note that the imposed sparsity also cause a moderate accuracy drop. However, with the meta attention, \"p-Meta (3.2+3.3)\" not only notably improves the accuracy but also further reduces the peak memory (2.7\u00d7 saving on average and up to 3.4\u00d7) and computation (1.9\u00d7 saving on average and up to 2.6\u00d7) over \"MAML++\". \"ANIL\" only updates the last layer, and therefore consumes less memory but also yields a substantially lower accuracy.\n\n4.3 Performance on Reinforcement Learning\nSettings. To show the versatility of p-Meta, we experiment with two few-shot reinforcement learning problems: 2D navigation and Half-Cheetah robot locomotion simulated with MuJoCo library [37].\nFor all experiments, we mainly adopt the experimental setup in [10].\nWe use a neural network policy with two hidden fc layers of size 100 and ReLU activation function. We adopt vanilla policy gradient [42] for the inner loop and trust-region policy optimization [32] for the outer loop. During the inner loop as well as few-shot learning, the agents rollout 20 episodes with a horizon size of 200 and are updated for one gradient step. The policy model is trained for 500 meta epochs, and the model with the best average return during training is used for evaluation. The task batch size is set to 20 for 2D navigation, and 40 for robot locomotion. The (initial) inner step size  is set to 0.1. Each episode is considered as a data sample, and thus the gradients are accumulated 20 times for a gradient step.\nResults. Tab. 3 lists the average return averaged over 400 new unseen tasks randomly drawn from simulated environments. We also report the average number of GMACs and the average peak memory per task according to Appendix A. Note that the reported computation and peak memory do not include the estimations of the advantage [9], as they are relatively small and could be done during the rollout. p-Meta consumes a rather small amount of memory and computation, while often obtains the highest return in comparison to others. Therefore, p-Meta can fast adapt its policy to reach the new goal in the environment with less on-device resource demand.\n\n4.4 Ablation Studies on Meta Attention\nWe study the effectiveness of our meta attention via the following two ablation studies. The experiments are conducted on \"4Conv\" in both 5-way 1-shot and 5-way 5-shot as Sec. 4.2. Sparsity in Meta Attention. Tab. 4 shows the few-shot classification accuracy with different sparsity settings in the meta attention.\nWe first do not impose sparsity on  fw  and  bw  (i.e., set both 's as 0), and adopt forward attention and backward attention separately. In comparison to no meta attention at all, enabling either forward or backward attention improves accuracy. With both attention enabled, the model achieves the best performance.\nWe then test the effects when imposing sparsity on  fw  or  bw  (i.e., set  > 0). We use the same  for all layers. We observe a sparse  bw  often cause a larger accuracy drop than a sparse  fw  . Since a sparse  bw  does not bring substantial memory or computation saving (see Appendix A), we use  = 0 for backward attention and  = 0.3 for forward attention.\nAttention scores   introduce a dynamic channel-wise learning rate according to the new data samples. We further compare meta attention with a static channel-wise learning rate, where the channel-wise learning rate  Ch is meta-trained as the layer-wise inner step sizes in Sec. 3.2 while without imposing sparsity. By comparing \" Ch \" with \"0, 0\" in Tab. 4, we conclude that the dynamic channel-wise learning rate yields a significantly higher accuracy.\nLayer-wise Updating Ratios. To study the resulted updating ratios across layers, i.e., the layer-wise sparsity of weight gradients, we randomly select 100 new tasks and plot the layer-wise updating ratios, see Fig. 5 Left (1:5). The \"4Conv\" backbone has 9 layers ( = 9), i.e., 8 alternates of conv and group normalization layers, and an fc output layer. As mentioned in Sec. 4.1, we do not apply meta attention to the output layer, i.e.,  9 = 1. The used backbone is updated with 5 gradient steps ( = 5). We use  = 0.3 for forward attention, and  = 0 for backward. Note that Alg. 1 adaptively determines the sparsity of   , which also means different samples may result in different updating ratios even with the same  (see Fig. 5). The size of  \u22121 often decreases along the layers in current DNNs. As expected, the latter layers are preferred to be updated more, since they need a smaller amount of memory for updating. Interestingly, even if with a small (= 0.3), the ratio of updated weights is rather small, e.g., smaller than 0.2 in step 3 of 5-way 5-shot. It implies that the outputs of softmax have a large discrepancy, i.e., only a few channels are adaptation-critical for each sample, which in turn verifies the effectiveness of our meta attention mechanism.\nWe also randomly pair data samples and compute the cosine similarity between their attention scores   . We plot the cosine similarity of step 1 in Fig. 5 Right. The results show that there may exist a considerable variation on the adaptation-critical weights selected by different samples, which is consistent with our observation in Tab. 4, i.e., dynamic learning rate outperforms the static one.\n\n5 RELATED WORK\nMeta Learning for Few-Shot Learning. Meta learning is a prevailing solution to few-shot learning [22], where the meta-trained model can learn an unseen task from a few training samples, i.e., data-efficient adaptation. The majority of meta learning methods can be divided into two categories, (i) metric-based methods [34, 35, 39] that learn an embedded metric for classification tasks to map the query samples onto the classes of labeled support samples, (ii) gradient-based methods [1, 10, 26, 28, 38, 40] that learn an initial model (and/or optimizer parameters) such that it can be adapted with gradient information calculated on the new few samples. In comparison to metric-based methods, we focus on gradient-based meta learning methods for their wide applicability in various learning tasks (e.g., regression, classification, reinforcement learning) and the availability of gradient-based training frameworks for lowresource devices [36].\nParticularly, we aim at meta training a DNN that allows effective adaptation on memory-constrained devices. Most meta learning algorithms [1, 10, 40] optimize the backbone network for better generalization yet ignore the workload if the meta-trained backbone is deployed to low-resource platforms for model adaptation. Manually fixing certain layers during on-device few-shot learning [26, 28, 33] may also reduce memory and computation, but to a much lesser extent as shown in our evaluations.\nEfficient DNN Training. Existing efficient training schemes are mainly designed for high-throughput GPU training on large-scale datasets. A general strategy is to trade memory with computation [4, 18], which is unfit for IoT device with a limited computation capability. An alternative is to sparsify the computational graphs in backpropagation [29]. Yet it relies on massive training iterations on large-scale datasets. Other techniques include layer-wise local training [17] and reversible residual module [12], but they often incur notable accuracy drops.\nThere are a few studies on DNN training on low-resource platforms, such as updating the last several layers only [25], reducing batch sizes [24], and gradient approximation [15]. However, they are designed for vanilla supervised training, i.e., train and test on the same task. One recent study proposes to update the bias parameters only for memory-efficient transfer learning [2], yet transfer learning is prone to overfitting when trained with limited data [10].\n\n6 CONCLUSION\nIn this paper, we present p-Meta, a new meta learning scheme for data-and memory-efficient on-device DNN adaptation. It enables structured partial parameter updates for memory-efficient few-shot learning by automatically identifying adaptation-critical weights both layer-wise and channel-wise. Evaluations show a reduction in peak dynamic memory by 2.5\u00d7 on average over the state-of-the-art\n\nCosine Similarity\nSimilarity of \u03b3l (k = 1)\nFigure 5 : Left (1:5): Layer-wise updating ratios (mean \u00b1 standard deviation) in each updating step. Note that the ratio of updated weights is determined by both static layer-wise inner step sizes  1: 1: and the dynamic meta attention scores  1: . The layer with an updating ratio of 0 means its  = 0. Right: Cosine similarity (mean \u00b1 standard deviation) of  1: between random pair of data samples. The results are reported in step 1, because all samples are fed into the same initial model in step 1.\nfew-shot adaptation methods. We envision p-Meta as an early step towards adaptive and autonomous edge intelligence applications.\n\nFootnotes:\n\nReferences:\n\n- Antreas Antoniou, Harrison Edwards, and Amos Storkey. 2019. How to train your MAML. In ICLR.- Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. 2020. TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning. In NeurIPS.\n\n- Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. 2021. MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training. In ICLR.\n\n- Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv:1604.06174\n\n- Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. 2020. Dynamic Convolution: Attention Over Convolution Ker- nels. In CVPR.\n\n- Tristan Deleu. 2018. Model-Agnostic Meta-Learning for Reinforcement Learning in PyTorch. Available at: https://github.com/tristandeleu/pytorch-maml-rl.\n\n- Tristan Deleu, Tobias W\u00fcrfl, Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio. 2019. Torchmeta: A Meta-Learning library for PyTorch. https://arxiv. org/abs/1909.06576 Available at: https://github.com/tristandeleu/pytorch-meta.\n\n- Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. 2020. Model compression and hardware acceleration for neural networks: a comprehensive survey. Proc. IEEE 108, 4 (2020), 485-532.\n\n- Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. Benchmarking Deep Reinforcement Learning for Continuous Control. In ICML.\n\n- Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta- learning for fast adaptation of deep networks. In ICML.\n\n- Dawei Gao, Xiaoxi He, Zimu Zhou, Yongxin Tong, and Lothar Thiele. 2021. Pruning meta-trained networks for on-device adaptation. In CIKM.\n\n- Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. 2017. The Reversible Residual Network: Backpropagation Without Storing Activations. In NeurIPS.\n\n- Taesik Gong, Yeonsu Kim, Jinwoo Shin, and Sung-Ju Lee. 2019. Metasense: few-shot adaptation to untrained conditions in deep mobile sensing. In SenSys.\n\n- Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning. Vol. 1. MIT press Cambridge.\n\n- Mary Gooneratne, Khe Chai Sim, Petr Zadrazil, Andreas Kabel, Fran\u00e7oise Beau- fays, and Giovanni Motta. 2020. Low-rank Gradient Approximation For Memory- Efficient On-device Training of Deep Neural Network. In ICASSP.\n\n- Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv:1706.02677\n\n- Klaus Greff, Rupesh K. Srivastava, and J\u00fcrgen Schmidhuber. 2017. Highway and Residual Networks learn Unrolled Iterative Estimation. In NeurIPS.\n\n- Audr\u016bnas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. 2016. Memory-Efficient Backpropagation Through Time. In NeurIPS.\n\n- Liang-Yan Gui, Yu-Xiong Wang, Deva Ramanan, and Jos\u00e9 MF Moura. 2018. Few- shot human motion prediction via meta-learning. In ECCV.\n\n- Song Han, Huizi Mao, and William J Dally. 2016. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR.\n\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR.\n\n- Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. 2020. Meta-learning in neural networks: a survey. arXiv:2004.05439\n\n- Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-Excitation Networks. In CVPR.\n\n- Seulki Lee and Shahriar Nirjon. 2019. Neuro.ZERO: a zero-energy neural network accelerator for embedded sensing and inference systems. In SenSys.\n\n- Akhil Mathur, Daniel J. Beutel, Pedro Porto Buarque de Gusm\u00e3o, Javier Fernandez- Marques, Taner Topal, Xinchi Qiu, Titouan Parcollet, Yan Gao, and Nicholas D. Lane. 2021. On-device Federated Learning with Flower. In MLSys.\n\n- Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, and Se-Young Yun. 2021. BOIL: Towards Representation Change for Few-shot Learning. In ICLR.\n\n- Boris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste. 2018. TADAM: Task dependent adaptive metric for improved few-shot learning. In NeurIPS.\n\n- Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. 2020. Rapid learning or feature reuse? Towards understanding the effectiveness of MAML. In ICLR.\n\n- Md Aamir Raihan and Tor M. Aamodt. 2020. Sparse Weight Activation Training. In NeurIPS.\n\n- Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. 2018. Meta-Learning for Semi-Supervised Few-Shot Classification. In ICLR.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet large scale visual recognition challenge. International Journal of Computer Vision 115, 3 (2015), 211-252.\n\n- John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. 2015. Trust Region Policy Optimization. In ICML.\n\n- Zhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. 2021. Partial Is Better Than All: Revisiting Fine-tuning Strategy for Few-shot Learning. In AAAI.\n\n- Jake Snell, Kevin Swersky, and Richard S. Zemel. 2017. Prototypical Networks for Few-shot Learning. In NeurIPS.\n\n- Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. 2018. Learning to Compare: Relation Network for Few-Shot Learning. In CVPR.\n\n- TensorFlow. [n.d.].\n\n- On-Device Training with TensorFlow Lite. https://www. tensorflow.org/lite/examples/on_device_training/overview. Accessed: 2022-01- 15.\n\n- Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine for model-based control. In IROS.\n\n- Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Man- zagol, and Hugo Larochelle. 2020. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. In ICLR.\n\n- Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. Matching Networks for One Shot Learning. In NeurIPS.\n\n- Johannes Von Oswald, Dominic Zhao, Seijin Kobayashi, Simon Schug, Massimo Caccia, Nicolas Zucchet, and Jo\u00e3o Sacramento. 2021. Learning where to learn: Gradient sparsity in meta and continual learning. In NeurIPS.\n\n- P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. 2010. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001. California Institute of Technology.\n\n- Ronald J. Williams. 1992. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach. Learn. 8, 3-4 (may 1992). https://doi.org/10.1007/BF00992696\n\n- Yuxin Wu and Kaiming He. 2018. Group Normalization. In ECCV.\n\n", "annotations": {"ReferenceToTable": [{"begin": 9892, "end": 9893, "idx": 0}, {"begin": 28309, "end": 28310, "idx": 1}], "SectionMain": [{"begin": 1057, "end": 38637, "idx": 0}], "ReferenceToFormula": [{"begin": 10505, "end": 10506, "target": "#formula_0", "idx": 0}, {"begin": 10670, "end": 10671, "target": "#formula_2", "idx": 1}, {"begin": 10792, "end": 10793, "target": "#formula_2", "idx": 2}, {"begin": 12237, "end": 12239, "idx": 3}, {"begin": 13068, "end": 13069, "target": "#formula_5", "idx": 4}, {"begin": 13689, "end": 13690, "target": "#formula_5", "idx": 5}, {"begin": 15810, "end": 15812, "idx": 6}, {"begin": 17319, "end": 17320, "target": "#formula_0", "idx": 7}, {"begin": 17948, "end": 17949, "idx": 8}, {"begin": 18302, "end": 18303, "target": "#formula_5", "idx": 9}, {"begin": 18681, "end": 18683, "idx": 10}, {"begin": 18694, "end": 18696, "idx": 11}, {"begin": 18829, "end": 18831, "idx": 12}, {"begin": 21643, "end": 21645, "target": "#formula_17", "idx": 13}, {"begin": 23317, "end": 23319, "idx": 14}, {"begin": 23558, "end": 23560, "idx": 15}], "SectionReference": [{"begin": 38651, "end": 45614, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1057, "idx": 0}], "Div": [{"begin": 59, "end": 991, "idx": 0}, {"begin": 992, "end": 1049, "idx": 1}, {"begin": 1060, "end": 1799, "idx": 2}, {"begin": 1801, "end": 7736, "idx": 3}, {"begin": 7738, "end": 13071, "idx": 4}, {"begin": 13073, "end": 13203, "idx": 5}, {"begin": 13205, "end": 16212, "idx": 6}, {"begin": 16214, "end": 16786, "idx": 7}, {"begin": 16788, "end": 18344, "idx": 8}, {"begin": 18346, "end": 18847, "idx": 9}, {"begin": 18849, "end": 19796, "idx": 10}, {"begin": 19798, "end": 22814, "idx": 11}, {"begin": 22816, "end": 23641, "idx": 12}, {"begin": 23643, "end": 24010, "idx": 13}, {"begin": 24012, "end": 24224, "idx": 14}, {"begin": 24226, "end": 24930, "idx": 15}, {"begin": 24932, "end": 25683, "idx": 16}, {"begin": 25685, "end": 25825, "idx": 17}, {"begin": 25827, "end": 27548, "idx": 18}, {"begin": 27550, "end": 30232, "idx": 19}, {"begin": 30234, "end": 31925, "idx": 20}, {"begin": 31927, "end": 35074, "idx": 21}, {"begin": 35076, "end": 37556, "idx": 22}, {"begin": 37558, "end": 37962, "idx": 23}, {"begin": 37964, "end": 38637, "idx": 24}], "Head": [{"begin": 992, "end": 1004, "idx": 0}, {"begin": 1060, "end": 1074, "n": "1", "idx": 1}, {"begin": 1801, "end": 1838, "idx": 2}, {"begin": 7738, "end": 7768, "n": "2", "idx": 3}, {"begin": 13073, "end": 13081, "n": "3", "idx": 4}, {"begin": 13205, "end": 13224, "n": "3.1", "idx": 5}, {"begin": 16214, "end": 16288, "n": "3.2", "idx": 6}, {"begin": 16788, "end": 16815, "n": "3.2.1", "idx": 7}, {"begin": 18346, "end": 18375, "n": "3.2.2", "idx": 8}, {"begin": 18849, "end": 18929, "n": "3.3", "idx": 9}, {"begin": 19798, "end": 19842, "n": "3.3.1", "idx": 10}, {"begin": 22816, "end": 22851, "n": "3.3.2", "idx": 11}, {"begin": 23643, "end": 23664, "n": "3.4", "idx": 12}, {"begin": 24012, "end": 24039, "n": "3.5", "idx": 13}, {"begin": 24226, "end": 24254, "n": "3.5.1", "idx": 14}, {"begin": 24932, "end": 24958, "n": "3.5.2", "idx": 15}, {"begin": 25685, "end": 25697, "n": "4", "idx": 16}, {"begin": 25827, "end": 25860, "n": "4.1", "idx": 17}, {"begin": 27550, "end": 27589, "n": "4.2", "idx": 18}, {"begin": 30234, "end": 30275, "n": "4.3", "idx": 19}, {"begin": 31927, "end": 31965, "n": "4.4", "idx": 20}, {"begin": 35076, "end": 35090, "n": "5", "idx": 21}, {"begin": 37558, "end": 37570, "n": "6", "idx": 22}, {"begin": 37964, "end": 37981, "idx": 23}], "Paragraph": [{"begin": 59, "end": 991, "idx": 0}, {"begin": 1005, "end": 1049, "idx": 1}, {"begin": 1075, "end": 1799, "idx": 2}, {"begin": 1839, "end": 2735, "idx": 3}, {"begin": 2736, "end": 3532, "idx": 4}, {"begin": 3533, "end": 4303, "idx": 5}, {"begin": 4304, "end": 4767, "idx": 6}, {"begin": 4768, "end": 5597, "idx": 7}, {"begin": 5598, "end": 6634, "idx": 8}, {"begin": 6635, "end": 6684, "idx": 9}, {"begin": 6685, "end": 7517, "idx": 10}, {"begin": 7518, "end": 7736, "idx": 11}, {"begin": 7769, "end": 7905, "idx": 12}, {"begin": 7906, "end": 8942, "idx": 13}, {"begin": 8943, "end": 9196, "idx": 14}, {"begin": 9254, "end": 9631, "idx": 15}, {"begin": 9666, "end": 10531, "idx": 16}, {"begin": 10583, "end": 10806, "idx": 17}, {"begin": 10821, "end": 11348, "idx": 18}, {"begin": 11349, "end": 11570, "idx": 19}, {"begin": 11623, "end": 12288, "idx": 20}, {"begin": 12310, "end": 12550, "idx": 21}, {"begin": 12551, "end": 13071, "idx": 22}, {"begin": 13082, "end": 13203, "idx": 23}, {"begin": 13225, "end": 13479, "idx": 24}, {"begin": 13480, "end": 13737, "idx": 25}, {"begin": 13738, "end": 14412, "idx": 26}, {"begin": 14413, "end": 14474, "idx": 27}, {"begin": 14475, "end": 15712, "idx": 28}, {"begin": 15713, "end": 15828, "idx": 29}, {"begin": 15860, "end": 16016, "idx": 30}, {"begin": 16017, "end": 16212, "idx": 31}, {"begin": 16289, "end": 16786, "idx": 32}, {"begin": 16816, "end": 17247, "idx": 33}, {"begin": 17248, "end": 17368, "idx": 34}, {"begin": 17422, "end": 17550, "idx": 35}, {"begin": 17581, "end": 17673, "idx": 36}, {"begin": 17708, "end": 18038, "idx": 37}, {"begin": 18109, "end": 18344, "idx": 38}, {"begin": 18376, "end": 18847, "idx": 39}, {"begin": 18930, "end": 19796, "idx": 40}, {"begin": 19843, "end": 20044, "idx": 41}, {"begin": 20045, "end": 20108, "idx": 42}, {"begin": 20109, "end": 20697, "idx": 43}, {"begin": 20786, "end": 20982, "idx": 44}, {"begin": 21017, "end": 21151, "idx": 45}, {"begin": 21187, "end": 21219, "idx": 46}, {"begin": 21286, "end": 21415, "idx": 47}, {"begin": 21464, "end": 21784, "idx": 48}, {"begin": 21785, "end": 21970, "idx": 49}, {"begin": 21971, "end": 22814, "idx": 50}, {"begin": 22852, "end": 23641, "idx": 51}, {"begin": 23665, "end": 24010, "idx": 52}, {"begin": 24040, "end": 24224, "idx": 53}, {"begin": 24255, "end": 24484, "idx": 54}, {"begin": 24485, "end": 24930, "idx": 55}, {"begin": 24959, "end": 25521, "idx": 56}, {"begin": 25522, "end": 25683, "idx": 57}, {"begin": 25698, "end": 25825, "idx": 58}, {"begin": 25861, "end": 25922, "idx": 59}, {"begin": 25923, "end": 25979, "idx": 60}, {"begin": 25980, "end": 26042, "idx": 61}, {"begin": 26043, "end": 26095, "idx": 62}, {"begin": 26096, "end": 26797, "idx": 63}, {"begin": 26798, "end": 27305, "idx": 64}, {"begin": 27306, "end": 27548, "idx": 65}, {"begin": 27590, "end": 29123, "idx": 66}, {"begin": 29124, "end": 30232, "idx": 67}, {"begin": 30276, "end": 30469, "idx": 68}, {"begin": 30470, "end": 30538, "idx": 69}, {"begin": 30539, "end": 31278, "idx": 70}, {"begin": 31279, "end": 31925, "idx": 71}, {"begin": 31966, "end": 32280, "idx": 72}, {"begin": 32281, "end": 32596, "idx": 73}, {"begin": 32597, "end": 32955, "idx": 74}, {"begin": 32956, "end": 33408, "idx": 75}, {"begin": 33409, "end": 34676, "idx": 76}, {"begin": 34677, "end": 35074, "idx": 77}, {"begin": 35091, "end": 36036, "idx": 78}, {"begin": 36037, "end": 36531, "idx": 79}, {"begin": 36532, "end": 37090, "idx": 80}, {"begin": 37091, "end": 37556, "idx": 81}, {"begin": 37571, "end": 37962, "idx": 82}, {"begin": 37982, "end": 38006, "idx": 83}, {"begin": 38007, "end": 38508, "idx": 84}, {"begin": 38509, "end": 38637, "idx": 85}], "ReferenceToBib": [{"begin": 1454, "end": 1458, "target": "#b12", "idx": 0}, {"begin": 1574, "end": 1578, "target": "#b18", "idx": 1}, {"begin": 2066, "end": 2070, "target": "#b13", "idx": 2}, {"begin": 2135, "end": 2139, "target": "#b20", "idx": 3}, {"begin": 2191, "end": 2195, "target": "#b30", "idx": 4}, {"begin": 2226, "end": 2230, "target": "#b15", "idx": 5}, {"begin": 2857, "end": 2861, "target": "#b21", "idx": 6}, {"begin": 2922, "end": 2925, "target": "#b0", "idx": 7}, {"begin": 2926, "end": 2929, "target": "#b9", "idx": 8}, {"begin": 2930, "end": 2933, "target": "#b25", "idx": 9}, {"begin": 2934, "end": 2937, "target": "#b27", "idx": 10}, {"begin": 3137, "end": 3141, "target": "#b35", "idx": 11}, {"begin": 3588, "end": 3591, "target": "#b0", "idx": 12}, {"begin": 3592, "end": 3595, "target": "#b9", "idx": 13}, {"begin": 3596, "end": 3599, "target": "#b25", "idx": 14}, {"begin": 3600, "end": 3603, "target": "#b27", "idx": 15}, {"begin": 4409, "end": 4412, "target": "#b2", "idx": 16}, {"begin": 4413, "end": 4415, "target": "#b3", "idx": 17}, {"begin": 4416, "end": 4419, "target": "#b16", "idx": 18}, {"begin": 4420, "end": 4423, "target": "#b17", "idx": 19}, {"begin": 4424, "end": 4427, "target": "#b28", "idx": 20}, {"begin": 4498, "end": 4502, "target": "#b14", "idx": 21}, {"begin": 4503, "end": 4506, "target": "#b23", "idx": 22}, {"begin": 4507, "end": 4510, "target": "#b24", "idx": 23}, {"begin": 4587, "end": 4590, "target": "#b1", "idx": 24}, {"begin": 4762, "end": 4766, "target": "#b9", "idx": 25}, {"begin": 5077, "end": 5081, "target": "#b25", "idx": 26}, {"begin": 5082, "end": 5085, "target": "#b27", "idx": 27}, {"begin": 5239, "end": 5243, "target": "#b27", "idx": 28}, {"begin": 5355, "end": 5359, "target": "#b25", "idx": 29}, {"begin": 6438, "end": 6441, "target": "#b7", "idx": 30}, {"begin": 6442, "end": 6445, "target": "#b14", "idx": 31}, {"begin": 6446, "end": 6449, "target": "#b19", "idx": 32}, {"begin": 6450, "end": 6453, "target": "#b28", "idx": 33}, {"begin": 8066, "end": 8070, "target": "#b21", "idx": 34}, {"begin": 8129, "end": 8133, "target": "#b9", "idx": 35}, {"begin": 12432, "end": 12436, "target": "#b9", "idx": 36}, {"begin": 12437, "end": 12440, "target": "#b26", "idx": 37}, {"begin": 13860, "end": 13863, "target": "#b3", "idx": 38}, {"begin": 13864, "end": 13867, "target": "#b17", "idx": 39}, {"begin": 14178, "end": 14182, "target": "#b28", "idx": 40}, {"begin": 14234, "end": 14238, "target": "#b10", "idx": 41}, {"begin": 14239, "end": 14242, "target": "#b40", "idx": 42}, {"begin": 14687, "end": 14691, "target": "#b25", "idx": 43}, {"begin": 14692, "end": 14695, "target": "#b27", "idx": 44}, {"begin": 16481, "end": 16485, "target": "#b25", "idx": 45}, {"begin": 16486, "end": 16489, "target": "#b27", "idx": 46}, {"begin": 16856, "end": 16859, "target": "#b0", "idx": 47}, {"begin": 19191, "end": 19194, "target": "#b4", "idx": 48}, {"begin": 19195, "end": 19198, "target": "#b22", "idx": 49}, {"begin": 21924, "end": 21927, "target": "#b4", "idx": 50}, {"begin": 21928, "end": 21931, "target": "#b22", "idx": 51}, {"begin": 22402, "end": 22405, "target": "#b4", "idx": 52}, {"begin": 22406, "end": 22409, "target": "#b22", "idx": 53}, {"begin": 25342, "end": 25346, "target": "#b43", "idx": 54}, {"begin": 25930, "end": 25934, "target": "#b9", "idx": 55}, {"begin": 25987, "end": 25991, "target": "#b27", "idx": 56}, {"begin": 26050, "end": 26054, "target": "#b25", "idx": 57}, {"begin": 26105, "end": 26108, "target": "#b0", "idx": 58}, {"begin": 26231, "end": 26234, "target": "#b5", "idx": 59}, {"begin": 26235, "end": 26237, "target": "#b6", "idx": 60}, {"begin": 26635, "end": 26638, "target": "#b5", "idx": 61}, {"begin": 26639, "end": 26641, "target": "#b6", "idx": 62}, {"begin": 27730, "end": 27734, "target": "#b9", "idx": 63}, {"begin": 27806, "end": 27810, "target": "#b26", "idx": 64}, {"begin": 28107, "end": 28111, "target": "#b39", "idx": 65}, {"begin": 28880, "end": 28883, "target": "#b0", "idx": 66}, {"begin": 29029, "end": 29033, "target": "#b39", "idx": 67}, {"begin": 29050, "end": 29054, "target": "#b29", "idx": 68}, {"begin": 29064, "end": 29068, "target": "#b41", "idx": 69}, {"begin": 30464, "end": 30468, "target": "#b37", "idx": 70}, {"begin": 30533, "end": 30537, "target": "#b9", "idx": 71}, {"begin": 30671, "end": 30675, "target": "#b42", "idx": 72}, {"begin": 30732, "end": 30736, "target": "#b31", "idx": 73}, {"begin": 31603, "end": 31606, "target": "#b8", "idx": 74}, {"begin": 35188, "end": 35192, "target": "#b21", "idx": 75}, {"begin": 35409, "end": 35413, "target": "#b33", "idx": 76}, {"begin": 35414, "end": 35417, "target": "#b34", "idx": 77}, {"begin": 35418, "end": 35421, "target": "#b39", "idx": 78}, {"begin": 35575, "end": 35578, "target": "#b0", "idx": 79}, {"begin": 35579, "end": 35582, "target": "#b9", "idx": 80}, {"begin": 35583, "end": 35586, "target": "#b25", "idx": 81}, {"begin": 35587, "end": 35590, "target": "#b27", "idx": 82}, {"begin": 35591, "end": 35594, "target": "#b38", "idx": 83}, {"begin": 35595, "end": 35598, "target": "#b40", "idx": 84}, {"begin": 36031, "end": 36035, "target": "#b35", "idx": 85}, {"begin": 36175, "end": 36178, "target": "#b0", "idx": 86}, {"begin": 36179, "end": 36182, "target": "#b9", "idx": 87}, {"begin": 36183, "end": 36186, "target": "#b40", "idx": 88}, {"begin": 36422, "end": 36426, "target": "#b25", "idx": 89}, {"begin": 36427, "end": 36430, "target": "#b27", "idx": 90}, {"begin": 36431, "end": 36434, "target": "#b32", "idx": 91}, {"begin": 36725, "end": 36728, "target": "#b3", "idx": 92}, {"begin": 36729, "end": 36732, "target": "#b17", "idx": 93}, {"begin": 36877, "end": 36881, "target": "#b28", "idx": 94}, {"begin": 37004, "end": 37008, "target": "#b16", "idx": 95}, {"begin": 37040, "end": 37044, "target": "#b11", "idx": 96}, {"begin": 37204, "end": 37208, "target": "#b24", "idx": 97}, {"begin": 37231, "end": 37235, "target": "#b23", "idx": 98}, {"begin": 37264, "end": 37268, "target": "#b14", "idx": 99}, {"begin": 37469, "end": 37472, "target": "#b1", "idx": 100}, {"begin": 37551, "end": 37555, "target": "#b9", "idx": 101}], "Sentence": [{"begin": 59, "end": 147, "idx": 0}, {"begin": 148, "end": 352, "idx": 1}, {"begin": 353, "end": 456, "idx": 2}, {"begin": 457, "end": 556, "idx": 3}, {"begin": 557, "end": 722, "idx": 4}, {"begin": 723, "end": 991, "idx": 5}, {"begin": 1005, "end": 1049, "idx": 6}, {"begin": 1075, "end": 1239, "idx": 7}, {"begin": 1240, "end": 1328, "idx": 8}, {"begin": 1329, "end": 1459, "idx": 9}, {"begin": 1460, "end": 1579, "idx": 10}, {"begin": 1580, "end": 1695, "idx": 11}, {"begin": 1696, "end": 1799, "idx": 12}, {"begin": 1839, "end": 1939, "idx": 13}, {"begin": 1940, "end": 2071, "idx": 14}, {"begin": 2072, "end": 2231, "idx": 15}, {"begin": 2232, "end": 2317, "idx": 16}, {"begin": 2318, "end": 2555, "idx": 17}, {"begin": 2556, "end": 2735, "idx": 18}, {"begin": 2736, "end": 2862, "idx": 19}, {"begin": 2863, "end": 3142, "idx": 20}, {"begin": 3143, "end": 3218, "idx": 21}, {"begin": 3219, "end": 3350, "idx": 22}, {"begin": 3351, "end": 3532, "idx": 23}, {"begin": 3533, "end": 3648, "idx": 24}, {"begin": 3649, "end": 3784, "idx": 25}, {"begin": 3785, "end": 3897, "idx": 26}, {"begin": 3898, "end": 4071, "idx": 27}, {"begin": 4072, "end": 4115, "idx": 28}, {"begin": 4116, "end": 4303, "idx": 29}, {"begin": 4304, "end": 4428, "idx": 30}, {"begin": 4429, "end": 4570, "idx": 31}, {"begin": 4571, "end": 4677, "idx": 32}, {"begin": 4678, "end": 4767, "idx": 33}, {"begin": 4768, "end": 4874, "idx": 34}, {"begin": 4875, "end": 4990, "idx": 35}, {"begin": 4991, "end": 5086, "idx": 36}, {"begin": 5087, "end": 5360, "idx": 37}, {"begin": 5361, "end": 5467, "idx": 38}, {"begin": 5468, "end": 5597, "idx": 39}, {"begin": 5598, "end": 5965, "idx": 40}, {"begin": 5966, "end": 6122, "idx": 41}, {"begin": 6123, "end": 6208, "idx": 42}, {"begin": 6209, "end": 6333, "idx": 43}, {"begin": 6334, "end": 6634, "idx": 44}, {"begin": 6635, "end": 6684, "idx": 45}, {"begin": 6685, "end": 6792, "idx": 46}, {"begin": 6793, "end": 6912, "idx": 47}, {"begin": 6913, "end": 7070, "idx": 48}, {"begin": 7071, "end": 7183, "idx": 49}, {"begin": 7184, "end": 7432, "idx": 50}, {"begin": 7433, "end": 7517, "idx": 51}, {"begin": 7518, "end": 7736, "idx": 52}, {"begin": 7769, "end": 7905, "idx": 53}, {"begin": 7906, "end": 7940, "idx": 54}, {"begin": 7941, "end": 8071, "idx": 55}, {"begin": 8072, "end": 8238, "idx": 56}, {"begin": 8239, "end": 8483, "idx": 57}, {"begin": 8484, "end": 8615, "idx": 58}, {"begin": 8616, "end": 8852, "idx": 59}, {"begin": 8853, "end": 8942, "idx": 60}, {"begin": 8943, "end": 8999, "idx": 61}, {"begin": 9000, "end": 9060, "idx": 62}, {"begin": 9061, "end": 9196, "idx": 63}, {"begin": 9254, "end": 9335, "idx": 64}, {"begin": 9336, "end": 9365, "idx": 65}, {"begin": 9366, "end": 9520, "idx": 66}, {"begin": 9521, "end": 9631, "idx": 67}, {"begin": 9666, "end": 9696, "idx": 68}, {"begin": 9697, "end": 9831, "idx": 69}, {"begin": 9832, "end": 9885, "idx": 70}, {"begin": 9886, "end": 10038, "idx": 71}, {"begin": 10039, "end": 10087, "idx": 72}, {"begin": 10088, "end": 10250, "idx": 73}, {"begin": 10251, "end": 10334, "idx": 74}, {"begin": 10335, "end": 10442, "idx": 75}, {"begin": 10443, "end": 10531, "idx": 76}, {"begin": 10583, "end": 10605, "idx": 77}, {"begin": 10606, "end": 10731, "idx": 78}, {"begin": 10732, "end": 10806, "idx": 79}, {"begin": 10821, "end": 10902, "idx": 80}, {"begin": 10903, "end": 11035, "idx": 81}, {"begin": 11036, "end": 11348, "idx": 82}, {"begin": 11349, "end": 11409, "idx": 83}, {"begin": 11410, "end": 11570, "idx": 84}, {"begin": 11623, "end": 11775, "idx": 85}, {"begin": 11776, "end": 12058, "idx": 86}, {"begin": 12059, "end": 12221, "idx": 87}, {"begin": 12222, "end": 12288, "idx": 88}, {"begin": 12310, "end": 12441, "idx": 89}, {"begin": 12442, "end": 12550, "idx": 90}, {"begin": 12551, "end": 12657, "idx": 91}, {"begin": 12658, "end": 12740, "idx": 92}, {"begin": 12741, "end": 12919, "idx": 93}, {"begin": 12920, "end": 13071, "idx": 94}, {"begin": 13082, "end": 13203, "idx": 95}, {"begin": 13225, "end": 13479, "idx": 96}, {"begin": 13480, "end": 13491, "idx": 97}, {"begin": 13492, "end": 13619, "idx": 98}, {"begin": 13620, "end": 13692, "idx": 99}, {"begin": 13693, "end": 13737, "idx": 100}, {"begin": 13738, "end": 13868, "idx": 101}, {"begin": 13869, "end": 13948, "idx": 102}, {"begin": 13949, "end": 13997, "idx": 103}, {"begin": 13998, "end": 14183, "idx": 104}, {"begin": 14184, "end": 14412, "idx": 105}, {"begin": 14413, "end": 14474, "idx": 106}, {"begin": 14475, "end": 14512, "idx": 107}, {"begin": 14513, "end": 14560, "idx": 108}, {"begin": 14561, "end": 14696, "idx": 109}, {"begin": 14697, "end": 14779, "idx": 110}, {"begin": 14780, "end": 14879, "idx": 111}, {"begin": 14880, "end": 14919, "idx": 112}, {"begin": 14920, "end": 15003, "idx": 113}, {"begin": 15004, "end": 15133, "idx": 114}, {"begin": 15134, "end": 15292, "idx": 115}, {"begin": 15293, "end": 15390, "idx": 116}, {"begin": 15391, "end": 15550, "idx": 117}, {"begin": 15551, "end": 15712, "idx": 118}, {"begin": 15713, "end": 15730, "idx": 119}, {"begin": 15731, "end": 15828, "idx": 120}, {"begin": 15860, "end": 16016, "idx": 121}, {"begin": 16017, "end": 16085, "idx": 122}, {"begin": 16086, "end": 16123, "idx": 123}, {"begin": 16124, "end": 16174, "idx": 124}, {"begin": 16175, "end": 16212, "idx": 125}, {"begin": 16289, "end": 16428, "idx": 126}, {"begin": 16429, "end": 16542, "idx": 127}, {"begin": 16543, "end": 16656, "idx": 128}, {"begin": 16657, "end": 16664, "idx": 129}, {"begin": 16665, "end": 16778, "idx": 130}, {"begin": 16779, "end": 16786, "idx": 131}, {"begin": 16816, "end": 16844, "idx": 132}, {"begin": 16845, "end": 17144, "idx": 133}, {"begin": 17145, "end": 17247, "idx": 134}, {"begin": 17248, "end": 17285, "idx": 135}, {"begin": 17286, "end": 17368, "idx": 136}, {"begin": 17422, "end": 17499, "idx": 137}, {"begin": 17500, "end": 17550, "idx": 138}, {"begin": 17581, "end": 17629, "idx": 139}, {"begin": 17630, "end": 17673, "idx": 140}, {"begin": 17708, "end": 17746, "idx": 141}, {"begin": 17747, "end": 17869, "idx": 142}, {"begin": 17870, "end": 17969, "idx": 143}, {"begin": 17970, "end": 18038, "idx": 144}, {"begin": 18109, "end": 18197, "idx": 145}, {"begin": 18198, "end": 18226, "idx": 146}, {"begin": 18227, "end": 18344, "idx": 147}, {"begin": 18376, "end": 18412, "idx": 148}, {"begin": 18413, "end": 18497, "idx": 149}, {"begin": 18498, "end": 18847, "idx": 150}, {"begin": 18930, "end": 19118, "idx": 151}, {"begin": 19119, "end": 19305, "idx": 152}, {"begin": 19306, "end": 19431, "idx": 153}, {"begin": 19432, "end": 19517, "idx": 154}, {"begin": 19518, "end": 19647, "idx": 155}, {"begin": 19648, "end": 19720, "idx": 156}, {"begin": 19721, "end": 19727, "idx": 157}, {"begin": 19728, "end": 19788, "idx": 158}, {"begin": 19789, "end": 19796, "idx": 159}, {"begin": 19843, "end": 19852, "idx": 160}, {"begin": 19853, "end": 19982, "idx": 161}, {"begin": 19983, "end": 20044, "idx": 162}, {"begin": 20045, "end": 20069, "idx": 163}, {"begin": 20070, "end": 20108, "idx": 164}, {"begin": 20109, "end": 20209, "idx": 165}, {"begin": 20210, "end": 20330, "idx": 166}, {"begin": 20331, "end": 20498, "idx": 167}, {"begin": 20499, "end": 20612, "idx": 168}, {"begin": 20613, "end": 20697, "idx": 169}, {"begin": 20786, "end": 20905, "idx": 170}, {"begin": 20906, "end": 20982, "idx": 171}, {"begin": 21017, "end": 21151, "idx": 172}, {"begin": 21187, "end": 21219, "idx": 173}, {"begin": 21286, "end": 21347, "idx": 174}, {"begin": 21348, "end": 21415, "idx": 175}, {"begin": 21464, "end": 21784, "idx": 176}, {"begin": 21785, "end": 21814, "idx": 177}, {"begin": 21815, "end": 21874, "idx": 178}, {"begin": 21875, "end": 21970, "idx": 179}, {"begin": 21971, "end": 22112, "idx": 180}, {"begin": 22113, "end": 22375, "idx": 181}, {"begin": 22376, "end": 22459, "idx": 182}, {"begin": 22460, "end": 22528, "idx": 183}, {"begin": 22529, "end": 22626, "idx": 184}, {"begin": 22627, "end": 22683, "idx": 185}, {"begin": 22684, "end": 22723, "idx": 186}, {"begin": 22724, "end": 22814, "idx": 187}, {"begin": 22852, "end": 22873, "idx": 188}, {"begin": 22874, "end": 22971, "idx": 189}, {"begin": 22972, "end": 23062, "idx": 190}, {"begin": 23063, "end": 23120, "idx": 191}, {"begin": 23121, "end": 23267, "idx": 192}, {"begin": 23268, "end": 23417, "idx": 193}, {"begin": 23418, "end": 23641, "idx": 194}, {"begin": 23665, "end": 23729, "idx": 195}, {"begin": 23730, "end": 24010, "idx": 196}, {"begin": 24040, "end": 24224, "idx": 197}, {"begin": 24255, "end": 24370, "idx": 198}, {"begin": 24371, "end": 24484, "idx": 199}, {"begin": 24485, "end": 24763, "idx": 200}, {"begin": 24764, "end": 24852, "idx": 201}, {"begin": 24853, "end": 24930, "idx": 202}, {"begin": 24959, "end": 25040, "idx": 203}, {"begin": 25041, "end": 25163, "idx": 204}, {"begin": 25164, "end": 25301, "idx": 205}, {"begin": 25302, "end": 25433, "idx": 206}, {"begin": 25434, "end": 25521, "idx": 207}, {"begin": 25522, "end": 25548, "idx": 208}, {"begin": 25549, "end": 25683, "idx": 209}, {"begin": 25698, "end": 25825, "idx": 210}, {"begin": 25861, "end": 25878, "idx": 211}, {"begin": 25879, "end": 25922, "idx": 212}, {"begin": 25923, "end": 25979, "idx": 213}, {"begin": 25980, "end": 26042, "idx": 214}, {"begin": 26043, "end": 26095, "idx": 215}, {"begin": 26096, "end": 26151, "idx": 216}, {"begin": 26152, "end": 26167, "idx": 217}, {"begin": 26168, "end": 26238, "idx": 218}, {"begin": 26239, "end": 26363, "idx": 219}, {"begin": 26364, "end": 26468, "idx": 220}, {"begin": 26469, "end": 26594, "idx": 221}, {"begin": 26595, "end": 26663, "idx": 222}, {"begin": 26664, "end": 26797, "idx": 223}, {"begin": 26798, "end": 27125, "idx": 224}, {"begin": 27126, "end": 27305, "idx": 225}, {"begin": 27306, "end": 27314, "idx": 226}, {"begin": 27315, "end": 27375, "idx": 227}, {"begin": 27376, "end": 27548, "idx": 228}, {"begin": 27590, "end": 27599, "idx": 229}, {"begin": 27600, "end": 27690, "idx": 230}, {"begin": 27691, "end": 27905, "idx": 231}, {"begin": 27906, "end": 28001, "idx": 232}, {"begin": 28002, "end": 28008, "idx": 233}, {"begin": 28009, "end": 28070, "idx": 234}, {"begin": 28071, "end": 28182, "idx": 235}, {"begin": 28183, "end": 28258, "idx": 236}, {"begin": 28259, "end": 28373, "idx": 237}, {"begin": 28374, "end": 28557, "idx": 238}, {"begin": 28558, "end": 28817, "idx": 239}, {"begin": 28818, "end": 28912, "idx": 240}, {"begin": 28913, "end": 28959, "idx": 241}, {"begin": 28960, "end": 29123, "idx": 242}, {"begin": 29124, "end": 29132, "idx": 243}, {"begin": 29133, "end": 29237, "idx": 244}, {"begin": 29238, "end": 29417, "idx": 245}, {"begin": 29418, "end": 29584, "idx": 246}, {"begin": 29585, "end": 29589, "idx": 247}, {"begin": 29590, "end": 29801, "idx": 248}, {"begin": 29802, "end": 29869, "idx": 249}, {"begin": 29870, "end": 29922, "idx": 250}, {"begin": 29923, "end": 30113, "idx": 251}, {"begin": 30114, "end": 30232, "idx": 252}, {"begin": 30276, "end": 30285, "idx": 253}, {"begin": 30286, "end": 30469, "idx": 254}, {"begin": 30470, "end": 30538, "idx": 255}, {"begin": 30539, "end": 30637, "idx": 256}, {"begin": 30638, "end": 30756, "idx": 257}, {"begin": 30757, "end": 30905, "idx": 258}, {"begin": 30906, "end": 31037, "idx": 259}, {"begin": 31038, "end": 31118, "idx": 260}, {"begin": 31119, "end": 31164, "idx": 261}, {"begin": 31165, "end": 31278, "idx": 262}, {"begin": 31279, "end": 31287, "idx": 263}, {"begin": 31288, "end": 31398, "idx": 264}, {"begin": 31399, "end": 31674, "idx": 265}, {"begin": 31675, "end": 31803, "idx": 266}, {"begin": 31804, "end": 31925, "idx": 267}, {"begin": 31966, "end": 32054, "idx": 268}, {"begin": 32055, "end": 32141, "idx": 269}, {"begin": 32142, "end": 32146, "idx": 270}, {"begin": 32147, "end": 32174, "idx": 271}, {"begin": 32175, "end": 32280, "idx": 272}, {"begin": 32281, "end": 32418, "idx": 273}, {"begin": 32419, "end": 32526, "idx": 274}, {"begin": 32527, "end": 32596, "idx": 275}, {"begin": 32597, "end": 32678, "idx": 276}, {"begin": 32679, "end": 32711, "idx": 277}, {"begin": 32712, "end": 32792, "idx": 278}, {"begin": 32793, "end": 32955, "idx": 279}, {"begin": 32956, "end": 33056, "idx": 280}, {"begin": 33057, "end": 33232, "idx": 281}, {"begin": 33233, "end": 33269, "idx": 282}, {"begin": 33270, "end": 33408, "idx": 283}, {"begin": 33409, "end": 33436, "idx": 284}, {"begin": 33437, "end": 33637, "idx": 285}, {"begin": 33638, "end": 33762, "idx": 286}, {"begin": 33763, "end": 33783, "idx": 287}, {"begin": 33784, "end": 33854, "idx": 288}, {"begin": 33855, "end": 33913, "idx": 289}, {"begin": 33914, "end": 33973, "idx": 290}, {"begin": 33974, "end": 34141, "idx": 291}, {"begin": 34142, "end": 34207, "idx": 292}, {"begin": 34208, "end": 34329, "idx": 293}, {"begin": 34330, "end": 34470, "idx": 294}, {"begin": 34471, "end": 34676, "idx": 295}, {"begin": 34677, "end": 34780, "idx": 296}, {"begin": 34781, "end": 34837, "idx": 297}, {"begin": 34838, "end": 35074, "idx": 298}, {"begin": 35091, "end": 35127, "idx": 299}, {"begin": 35128, "end": 35309, "idx": 300}, {"begin": 35310, "end": 35745, "idx": 301}, {"begin": 35746, "end": 36036, "idx": 302}, {"begin": 36037, "end": 36144, "idx": 303}, {"begin": 36145, "end": 36355, "idx": 304}, {"begin": 36356, "end": 36531, "idx": 305}, {"begin": 36532, "end": 36555, "idx": 306}, {"begin": 36556, "end": 36669, "idx": 307}, {"begin": 36670, "end": 36802, "idx": 308}, {"begin": 36803, "end": 36882, "idx": 309}, {"begin": 36883, "end": 36952, "idx": 310}, {"begin": 36953, "end": 37090, "idx": 311}, {"begin": 37091, "end": 37269, "idx": 312}, {"begin": 37270, "end": 37368, "idx": 313}, {"begin": 37369, "end": 37556, "idx": 314}, {"begin": 37571, "end": 37687, "idx": 315}, {"begin": 37688, "end": 37865, "idx": 316}, {"begin": 37866, "end": 37962, "idx": 317}, {"begin": 37982, "end": 38006, "idx": 318}, {"begin": 38007, "end": 38107, "idx": 319}, {"begin": 38108, "end": 38254, "idx": 320}, {"begin": 38255, "end": 38405, "idx": 321}, {"begin": 38406, "end": 38508, "idx": 322}, {"begin": 38509, "end": 38537, "idx": 323}, {"begin": 38538, "end": 38637, "idx": 324}], "ReferenceToFigure": [{"begin": 3148, "end": 3149, "target": "#fig_0", "idx": 0}, {"begin": 11062, "end": 11063, "target": "#fig_1", "idx": 1}, {"begin": 19988, "end": 19989, "target": "#fig_2", "idx": 2}, {"begin": 21593, "end": 21594, "target": "#fig_2", "idx": 3}, {"begin": 21820, "end": 21821, "target": "#fig_2", "idx": 4}, {"begin": 22238, "end": 22239, "target": "#fig_2", "idx": 5}, {"begin": 23639, "end": 23640, "target": "#fig_6", "idx": 6}, {"begin": 33624, "end": 33625, "idx": 7}, {"begin": 34138, "end": 34139, "idx": 8}, {"begin": 34829, "end": 34830, "idx": 9}, {"begin": 38014, "end": 38015, "idx": 10}], "Abstract": [{"begin": 49, "end": 1049, "idx": 0}], "SectionFootnote": [{"begin": 38639, "end": 38649, "idx": 0}], "ReferenceString": [{"begin": 38666, "end": 38758, "id": "b0", "idx": 0}, {"begin": 38760, "end": 38896, "id": "b1", "idx": 1}, {"begin": 38900, "end": 39125, "id": "b2", "idx": 2}, {"begin": 39129, "end": 39256, "id": "b3", "idx": 3}, {"begin": 39260, "end": 39414, "id": "b4", "idx": 4}, {"begin": 39418, "end": 39569, "id": "b5", "idx": 5}, {"begin": 39573, "end": 39803, "id": "b6", "idx": 6}, {"begin": 39807, "end": 39992, "id": "b7", "idx": 7}, {"begin": 39996, "end": 40144, "id": "b8", "idx": 8}, {"begin": 40148, "end": 40278, "id": "b9", "idx": 9}, {"begin": 40282, "end": 40418, "id": "b10", "idx": 10}, {"begin": 40422, "end": 40580, "id": "b11", "idx": 11}, {"begin": 40584, "end": 40734, "id": "b12", "idx": 12}, {"begin": 40738, "end": 40854, "id": "b13", "idx": 13}, {"begin": 40858, "end": 41074, "id": "b14", "idx": 14}, {"begin": 41078, "end": 41298, "id": "b15", "idx": 15}, {"begin": 41302, "end": 41445, "id": "b16", "idx": 16}, {"begin": 41449, "end": 41589, "id": "b17", "idx": 17}, {"begin": 41593, "end": 41723, "id": "b18", "idx": 18}, {"begin": 41727, "end": 41889, "id": "b19", "idx": 19}, {"begin": 41893, "end": 42008, "id": "b20", "idx": 20}, {"begin": 42012, "end": 42149, "id": "b21", "idx": 21}, {"begin": 42153, "end": 42231, "id": "b22", "idx": 22}, {"begin": 42235, "end": 42380, "id": "b23", "idx": 23}, {"begin": 42384, "end": 42606, "id": "b24", "idx": 24}, {"begin": 42610, "end": 42742, "id": "b25", "idx": 25}, {"begin": 42746, "end": 42890, "id": "b26", "idx": 26}, {"begin": 42894, "end": 43053, "id": "b27", "idx": 27}, {"begin": 43057, "end": 43144, "id": "b28", "idx": 28}, {"begin": 43148, "end": 43354, "id": "b29", "idx": 29}, {"begin": 43358, "end": 43660, "id": "b30", "idx": 30}, {"begin": 43664, "end": 43796, "id": "b31", "idx": 31}, {"begin": 43800, "end": 43972, "id": "b32", "idx": 32}, {"begin": 43976, "end": 44087, "id": "b33", "idx": 33}, {"begin": 44091, "end": 44259, "id": "b34", "idx": 34}, {"begin": 44263, "end": 44282, "id": "b35", "idx": 35}, {"begin": 44286, "end": 44420, "id": "b36", "idx": 36}, {"begin": 44424, "end": 44532, "id": "b37", "idx": 37}, {"begin": 44536, "end": 44808, "id": "b38", "idx": 38}, {"begin": 44812, "end": 44960, "id": "b39", "idx": 39}, {"begin": 44964, "end": 45176, "id": "b40", "idx": 40}, {"begin": 45180, "end": 45361, "id": "b41", "idx": 41}, {"begin": 45365, "end": 45548, "id": "b42", "idx": 42}, {"begin": 45552, "end": 45612, "id": "b43", "idx": 43}]}}