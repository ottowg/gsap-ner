{"text": "TimeLMs: Diachronic Language Models from Twitter\n\nAbstract:\nDespite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models' capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at https://github. com/cardiffnlp/timelms.\n\n\n1 Introduction\nNeural language models (LMs) (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) are today a key enabler in NLP. They have contributed to a general uplift in downstream performance across many applications, even sometimes rivaling human judgement (Wang et al., 2018 (Wang et al., , 2019)), while also bringing about a new paradigm of knowledge acquisition through pre-training. However, currently, both from model development and evaluation standpoints, this paradigm is essentially static, which affects both the ability to generalize to future data and the reliability of experimental results, since it is not uncommon that evaluation benchmarks overlap with pre-training corpora (Lazaridou et al., 2021). As an example, neither the original versions of BERT and RoBERTa are up to date with the current coronavirus pandemic. This is clearly troublesome, as most of the communication in recent years has been affected by it, yet these models would barely know what we are referring to when we talk about COVID-19 or lockdown, to name just Authors marked with an asterisk (*) contributed equally.\na few examples. The lack of diachronic specialization is especially concerning in contexts such as social media, where topics of discussion change often and rapidly (Del Tredici et al., 2019).\nIn this paper, we address this issue by sharing with the community a series of time-specific LMs specialized to Twitter data (TimeLMs). Our initiative goes beyond the initial release, analysis and experimental results reported in this paper, as models will periodically continue to be trained, improved and released.\n\n2 Related Work\nThere exists a significant body of work on dealing with the time variable in NLP. For instance, by specializing language representations derived from word embedding models or neural networks (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Del Tredici et al., 2019; Hofmann et al., 2021). Concerning the particular case of LMs, exposing them to new data and updating their parameters accordingly, also known as continual learning, is a promising direction, with an established tradition in machine learning (Lopez-Paz and Ranzato, 2017; Lewis et al., 2020; Lazaridou et al., 2021; Jang et al., 2021). Other works, however, have proposed to enhance BERT-based topic models with the time variable (Grootendorst, 2020). With regards to in-domain specialization, there are numerous approaches that perform domain adaptation by pre-training a generic LM on specialized corpora. A well-known case is the biomedical domain, e.g., BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019) or PubMedBERT (Gu et al., 2021). In addition to these approaches to specialize language models, there have been similar temporal adaptation analyses to the one presented in our paper (Agarwal and Nenkova, 2021; Jin et al., 2021). In particular, these works showed that training language models in recent data can be beneficial, an improvement that was found to be marginal in Luu 2021) in a different setting. In terms of continual lifelong learning, which is tangential to our main goal, Biesialska et al. (2020) provide a detailed survey on the main techniques proposed in the NLP literature.\nMore relevant to this paper, on the other hand, are LMs specialized to social media data, specifically Twitter, with BERTweet (Nguyen et al., 2020), TweetEval (Barbieri et al., 2020) and XLM-T ( Barbieri et al., 2021) being, to the best of our knowledge, the most prominent examples. However, the above efforts barely address the diachronic nature of language. Crucially, they do not address the problem of specializing LMs to social media and putting the time variable at the core of the framework. Moreover, it is desirable that such timeaware models are released alongside usable software and a reliable infrastructure. Our TimeLMs initiative, detailed in Section 3, aims to address the above challenges.\n3 TimeLMs: Diachronic Language Models from Twitter\nIn this section, we present our approach to train language models for different time periods.\n\n3.1 Twitter corpus\nFor the training and evaluation of language models, we first collect a large corpus of tweets. In the following we explain both the data collection and cleaning processes. Data collection. We use the Twitter Academic API to obtain a large sample of tweets evenly distributed across time. In order to obtain a sample which is representative of general conversation on that social platform, we query the API using the most frequent stopwords 1, for a set number of tweets at timestamps distanced by 5 minutes -for every hour of every day constituting a particular yearly quarter. We also use specific flags supported by the API to retrieve only tweets in English and ignore retweets, quotes, links, media posts and ads.\nFor our initial base model (2019-90M henceforth), we used an evenly time-distributed corpus from the API, for the period between 2018 and 2019, supplemented with additional tweets from Archive.org which cover the same period but are not evenly distributed. Data cleaning. Before training any model, we filter each model's training set of tweets using the procedure detailed in this section. Starting with the assumption that bots are amongst the most active users, we remove tweets from the top one percent of users that have posted most frequently. Additionally, following the recommendation of Lee et al. (2021), we remove duplicates and near-duplicates. We find near-duplicates by hashing the texts of tweets after lowercasing and stripping punctuation. Hashing is performed using MinHash (Broder, 1997), with 16 permutations. Finally, user mentions are replaced with a generic placeholder (@user), except for verified users.\n\n3.2 Language model training\nOnce the Twitter corpus has been collected and cleaned, we proceed to the language model pretraining. This consists of two phases: (1) training of a base model consisting of data until the end of 2019; and (2) continual training of language models every three months since the date of the base model. Base model training. Our base model is trained with data until 2019 (included). Following Barbieri et al. (2020), we start from the original RoBERTabase model (Liu et al., 2019) and continue training the masked language model on Twitter data. The model is trained using the same settings as Barbieri et al. (2020), namely early stopping on the validation split and a learning rate of 1.0e \u22125 . This initial 2019-90M base model converged after around fifteen days on 8 NVIDIA V100 GPUs. Continuous training. After training our base model, our goal is to continue training this language model with recent Twitter corpora. At the time of writing, for practical and logistical reasons, the decision is to train a new version of each language model every three months. The process to train this updated language model is simple, as it follows the same training procedure as the initial pre-training of the language model explained above. Our commitment is to keep updating and releasing a new model every three months, effectively enabling the community to make use of an up-to-date language model at any period in time.\n\n3.3 TimeLMs release summary\nIn Table 1 we include a summary of the Twitter corpora collected and models trained until the date of writing. Models are split in four three-month quarters (Q1, Q2, Q3 and Q4). Our base 2019-90M model consists of 90 million tweets until the end of 2019. Then, every quarter (i.e., every three months) 4.2M additional tweets are added, and the model gets updated as described above. Our latest released models, which are 2021-Q4 and 2021-124M (the latter was re-trained only once with all the data from 2020 and 2021), are trained on 124M tweets on top of the original RoBERTa-base model (Liu et al., 2019). All models are currently available through the Hugging Face hub at https: //huggingface.co/cardiffnlp. In addition to these corpora for training language models, we set apart a number of tweets for each quarter (independent from the training set, with no overlap). These sets are used as test sets on our perplexity evaluation (see Section 4.2), and consist of 300K tweets per quarter, which were sampled and cleaned in the same way as the original corpus.\n\nModels\n\n\n4 Evaluation\nIn this section, we aim at evaluating the effectiveness of time-specific language models (see Section 3) on time-specific tasks. In other words, our goal is to test the possible degradation of older models over time and, accordingly, test if this can be mitigated by continuous training. Evaluation tasks. We evaluated the released language models in two tasks: (1) TweetEval (Barbieri et al., 2020), which consists of seven downstream tweet classification tasks; and (2) Pseudoperplexity on corpora sampled from different time periods. While the first evaluation is merely aimed at validating the training procedure of the base language model, the second evaluation is the core contribution of this paper in terms of evaluation, where different models can be tested in different time periods.\n\n4.1 TweetEval\nTweetEval (Barbieri et al., 2020) is a unified Twitter benchmark composed of seven heterogeneous tweet classification tasks. It is commonly used to evaluate the performance of language models (or task-agnostic models more generally) on Twitter data. With this evaluation, our goal is simply to show the general competitiveness of the models released with our package, irrespective of their time periods. Evaluation tasks. The seven tweet classification tasks in TweetEval are emoji prediction (Barbieri et al., 2018), emotion recognition (Mohammad et al., 2018), hate speech detection (Basile et al., 2019), irony detection (Van Hee et al., 2018), offensive language identification (Zampieri et al., 2019), sentiment analysis (Rosenthal et al., 2017) and stance detection (Mohammad et al., 2016). Experimental setting. Similarly to the TweetEval original baselines, only a moderate parameter search was conducted. The only hyper-parameter fine-tuned was the learning rate (1.0e \u22123 , 1.0e \u22124 , 1.0e \u22125 ). The number of epochs each model is trained is variable, as we used early stopping monitoring the validation loss. The validation loss is also used to select the best model in each task. Comparison systems. The comparison systems (SVM, FastText, BLSTM, RoBERTa-base and TweetEval) are those taken from the original TweetEval paper, as well as the state-of-the-art BERTweet model (Nguyen et al., 2020), which is trained over 900M tweets (posted between 2013 and 2019). All the language models compared are based on the RoBERTa-base architecture. Results. TweetEval results are summarized in Table 2. BERTweet, which was trained on substantially more data, attains the best averaged results. However, when looking at single tasks, BERTweet outperforms both our latest released models, i.e., TimeLM-19 and TimeLM-21, on the irony detection task 2 only. It is also important to highlight that TweetEval tasks include tweets dated until 2018 at the latest (with most tasks being considerably earlier). This suggests that our latest released model (i.e. TimeLM-21), even if trained up to 2021 tweets,  is generally competitive even on past tweets. Indeed, TimeLM-21 outperforms the most similar TweetEval model, which was trained following a similar strategy (in this case trained on fewer tweets until 2019), in most tasks.Metric M-F1 M-F1 M-F1 F (i) M-F1 M-Rec AVG (F1) TE\n\n4.2 Time-aware language model evaluation\nOnce the effectiveness of the base and subsequent models have been tested in downstream tasks, our goal is to measure to what extent the various models released are sensitive to a more time-aware evaluation. To this end, we rely on the pseudo perplexity measure (Salazar et al., 2020).\nEvaluation metric: Pseudo-perplexity (PPPL).\nThe pseudo log-likelihood (PLL) score introduced by Salazar et al. ( 2020) is computed by iteratively replacing each token in a sequence with a mask, and summing the corresponding conditional log probabilities. This approach is specially suited to masked language models, rather than traditional left-to-right models. Pseudo-perplexity (PPPL) follows analogously from the standard perplexity formula, using PLL for conditional probability.\nResults. Table 3 shows the pseudo-perplexity results in all test sets. As the main conclusion, the table shows how more recent models tend to outperform models trained when evaluated older data in most test sets (especially those contemporaneous). This can be appreciated by simply observing the decreasing values in the columns of the Table 3. There are a few interesting exceptions, however. For instance, the 2020-Q1 and 2020-Q2 test sets, which corresponding to the global start of the coronavirus pandemic, are generally better suited for models trained until that periods. Nonetheless, models trained on more contemporary data appear to converge to the optimal results.\nDegradation over time. How long does it take for a model to be outdated? Overall, PPPL scores tend to increase almost 10% after one year. In general, PPPL appears to decrease consistently every quarterly update. This result reinforces the need for updated language models even for short time periods such as three-month quarters. In most cases, degradation on future data is usually larger than on older data. This result is not completely unexpected since newer models are also trained on more data for more time periods. In Section 6.1 we expand on this by including a table detailing the relative performance degradation over language models over time.\n\n5 Python Interface\nIn this section we present an integrated Python interface that we release along with the data and language models presented in this paper. As mentioned in Section 3.3, all language models will be available from the Hugging Face hub and our code is designed to be used with this platform. Our interface, based on the Transformers package (Wolf et al., 2020), is focused on providing easy single-line access to language models trained for specific periods and related use cases. The choice of language models to be used with our interface is determined using one of four modes of operation:\n(1) 'latest': using our most recently trained Twitter model; (2) 'corresponding': using the model that was trained only until each tweet's date (i.e., its specific quarter); (3) custom: providing the preferred date or quarter (e.g., '2021-Q3'); and (4) 'quarterly': using all available models trained over time in quarterly intervals. Having specified the preferred language models, there are three main functionalities within the code, namely: (1) computing pseudo-perplexity scores, (2) evaluating lan-   guage models in our released or customized test sets, and (3) obtaining masked predictions.\nUsers can measure the extent to which the chosen pretrained language models are aligned (i.e., familiar) with a given list of tweets (or any text) using pseudo-perplexity (see Section 4.2 for more details), computed as shown in Code 1. For a more extensive evaluation of language models using pseudo-perplexity, we provide a random subset of our test data across 2020 and 2021. 3 o evaluate other models from the Transformers package, we provide the 'eval_model' method (tlms.eval_model()) to compute pseudoperplexity on any given set of tweets or texts (e.g., the subset we provide) using other language models supported by the Transformers package. Both scoring methods not only provide the pseudo-perplexity scores specific to each model (depending on specified model name, or TimeLMs specified mode), but also the PLL scores assigned to each tweet by the different models.\nFinally, predictions for masked tokens of any given tweet or text may be easily obtained as demonstrated in Code 2. Code 2: Obtaining masked predictions using model corresponding to the tweet's date. Requires tweets or texts with a <mask> token.\nNote that while the examples included in this paper are associated with specific dates (i.e., the created_at field), these are only required for the 'corresponding' mode.\n\n6 Analysis\nTo complement the evaluation in the previous section, we perform a more detailed analysis in three important aspects: (1) a quantitative analysis on the degradation suffered by language models over time; (2) the relation between time and size (Section 6.2); and (3) a qualitative analysis where we show the influence of time in language models for specific examples (Section 6.3).\n\n6.1 Degradation analysis\nTable 4 displays the relative performance degradation (or improvement) of TimeLMs language models with respect to the test sets whose time period is the latest where they have been trained on (diagonals in the table). The table shows how models tend to perform worse in newer data sets, with a degradation of performance up to 13.68% of the earlier 2020-Q1 model on the latest 2021-Q4 model (with data almost two years later than the latest data the language model was trained on).\nIn order to compare the effect of continuous   training with respect to single training, Figure 1 shows the PPPL performances of 2021-124M (trained on all 2020-2021 data at once) and the 2021-Q4 (updating 2021-Q3) models. Note how 2021-124M shows improved performance generally, with the largest differences being attained on the first two quarters of 2020, but not for the latest quarters where continuous training seems to work slightly better. While more analysis would be required, this result suggests that a single training is beneficial for earlier periods, while a quarterly training seems to be better adapted to the most recent data. However, there does not seem to be any meaningful catastrophic forgetting in the quarterlyupdated model, as the differences are relative small.\n\n6.2 Time and size control experiment\nGiven the results presented earlier, one may naturally wonder whether the improvement may be due to the increase in training size or the recency of additional data. While this question is not easy to answer (and probably the answer will be in-between these two reasons), we perform a simple control experiment as an initial attempt. To this end, we trained an additional language model with twice the training data of the third quarter of 2021 (2021-Q3). This way, the total number of training tweets is exactly the same as the model trained until the fourth quarter of 2021 (2021-Q4).\nConsidering the results on Table 5, we find that the model trained on twice the data for Q3 outperforms the model trained with the default Q3 data in all tested quarters. This confirms the assumption that increasing training data leads to improved language model performance. When comparing with the model trained until 2021-Q4, results show this 2021-Q3-2x model is only slightly better in the 2021-Q2 and 2021-Q3 test sets. However, as we  could expect, the model trained in more recent data (i.e., until 2021-Q4) gets the best overall results on the more recent test set (i.e., 2021-Q4).\n\n6.3 Qualitative analysis\nIn this section we illustrate, in practice, how models trained on different quarters perceive specific tweets. First, we use their masked language modeling head to predict a <mask> token in context. Table 6 shows three tweets and associated predictions from each of our quarterly models. The model belonging to the most pertinent quarter exhibits background knowledge more aligned to the trends of that period. In the two COVID-related examples, we observe increasing awareness of the general notion of being fully vaccinated (as opposed to not vaccinated, the top prediction from the 2020-Q1 model) in the former, and, in the latter, two instances where forgetting a mask is more likely than forgetting other apparel less related to a particular period, such as a charger, a lighter or a purse. Finally, note how, in the last example, \"Looking forward to watching <mask> Game tonight!\", it is only in 2021-Q4 that predictions change substantially, when the model has been exposed to reactions to the \"Squid Game\" show, overlapping in time with its global release.\nOur second piece of analysis involves the visualization of pseudo log-likehood (PLL) scores for tweets requiring awareness of a trend or event tied to a specific period (Figure 2). Indeed, more recent models are better at predicting tweets involving popular events, such as NFTs or, again, the show \"Squid Game\". Conversely, we observe a stagnation (or even degradation) of the PLL scores for a tweet about a contestant of an older reality show.\n\n7 Conclusion\nIn this paper we presented TimeLMs, language models trained on Twitter over different time periods. The initiative also includes the future training of language models every three months, thus providing free-to-use and up-to-date language models for NLP practitioners. These language models are released together with a simple Python interface which facilitates loading and working with these models, including time-aware evaluation. In our evaluation in this paper, we have shown how timeaware training is relevant, not only from the theoretical point of view, but also the practical one, as the results demonstrate a clear degradation in performance when models are used for future data, which is one of the most common settings in practice.\nAs future work, we are planning to explicitly in-tegrate the time span variable in the language models, i.e., introducing string prefixes, along the lines of Dhingra et al. (2022) and Rosin et al. (2022).\n\nFootnotes:\n1: We use the top 10 entries from: google-10000-english.txt\n2: We note that the irony dataset was created via distant supervision using the #irony hashtag, and there could be a \"labels\" leak since BERTweet was the only model trained on tweets of the time period (2014/15) of the irony dataset.\n3: Limited to 50K tweets, the maximum allowed by Twitter. IDs for all test tweets are available on the repository.\n\nReferences:\n\n- Oshin Agarwal and Ani Nenkova. 2021. Temporal ef- fects on pre-trained models for language processing tasks. arXiv preprint arXiv:2111.12790.- Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2021. XLM-T: A multilingual language model toolkit for twitter. arXiv preprint arXiv:2104.12250.\n\n- Francesco Barbieri, Jose Camacho-Collados, Luis Es- pinosa Anke, and Leonardo Neves. 2020. TweetE- val: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2020, pages 1644-1650, Online. Association for Computa- tional Linguistics.\n\n- Francesco Barbieri, Jose Camacho-Collados,\n\n- Francesco Ronzano, Luis Espinosa-Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion. 2018. SemEval 2018 task 2: Multilingual emoji prediction. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 24-33, New Orleans, Louisiana. Association for Computational Linguistics.\n\n- Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela San- guinetti. 2019. SemEval-2019 task 5: Multilin- gual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th Inter- national Workshop on Semantic Evaluation, pages 54-63, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\n- Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB- ERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3615- 3620, Hong Kong, China. Association for Computa- tional Linguistics.\n\n- Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-juss\u00e0. 2020. Continual lifelong learn- ing in natural language processing: A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6523-6541.\n\n- A.Z. Broder. 1997. On the resemblance and con- tainment of documents. In Proceedings. Compres- sion and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pages 21-29.\n\n- Marco Del Tredici, Raquel Fern\u00e1ndez, and Gemma Boleda. 2019. Short-term meaning shift: A distri- butional exploration. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2069-2075, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\n- Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-Aware Language Models as Temporal Knowledge Bases. Transac- tions of the Association for Computational Linguis- tics, 10:257-273.\n\n- Maarten Grootendorst. 2020. BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.\n\n- Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain- specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23.\n\n- William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic word embeddings reveal statisti- cal laws of semantic change. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1489-1501, Berlin, Germany. Association for Com- putational Linguistics.\n\n- Valentin Hofmann, Janet Pierrehumbert, and Hinrich Sch\u00fctze. 2021. Dynamic contextualized word em- beddings. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 6970-6984, Online. Association for Computa- tional Linguistics.\n\n- Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2021. Towards contin- ual knowledge learning of language models. arXiv preprint arXiv:2110.03215.\n\n- Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2021. Lifelong pretraining: Continu- ally adapting language models to emerging corpora. arXiv preprint arXiv:2110.08534.\n\n- Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gri- bovskaya, Devang Agrawal, Adam Liska, et al. 2021. Pitfalls of static language modelling. arXiv preprint arXiv:2102.01951.\n\n- Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomed- ical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240.\n\n- Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499.\n\n- Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\n- David Lopez-Paz and Marc' Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\n\n- Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar- ishma Mandyam, and Noah A Smith. 2021. Time waits for no one! analysis and challenges of tempo- ral misalignment. arXiv preprint arXiv:2111.07408.\n\n- Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. SemEval- 2018 task 1: Affect in tweets. In Proceedings of The 12th International Workshop on Semantic Eval- uation, pages 1-17, New Orleans, Louisiana. Asso- ciation for Computational Linguistics.\n\n- Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob- hani, Xiaodan Zhu, and Colin Cherry. 2016. SemEval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31- 41, San Diego, California. Association for Computa- tional Linguistics.\n\n- Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. 2020. BERTweet: A pre-trained language model for English tweets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing: System Demonstrations, pages 9- 14, Online. Association for Computational Linguis- tics.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\n- Alex Rosenfeld and Katrin Erk. 2018. Deep neural models of semantic shift. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 474-484, New Orleans, Louisiana. Associa- tion for Computational Linguistics.\n\n- Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment analysis in Twit- ter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502-518, Vancouver, Canada. Association for Computational Linguistics.\n\n- Guy D. Rosin, Ido Guy, and Kira Radinsky. 2022. Time masking for temporal language models. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, WSDM '22, page 833-841, New York, NY, USA. Association for Computing Machinery.\n\n- Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka- trin Kirchhoff. 2020. Masked language model scor- ing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699-2712, Online. Association for Compu- tational Linguistics.\n\n- Terrence Szymanski. 2017. Temporal word analogies: Identifying lexical replacement with diachronic word embeddings. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 448- 453, Vancouver, Canada. Association for Computa- tional Linguistics.\n\n- Cynthia Van Hee, Els Lefever, and V\u00e9ronique Hoste. 2018. SemEval-2018 task 3: Irony detection in En- glish tweets. In Proceedings of The 12th Interna- tional Workshop on Semantic Evaluation, pages 39- 50, New Orleans, Louisiana. Association for Com- putational Linguistics.\n\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. Super- glue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.\n\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\n- Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. SemEval-2019 task 6: Identifying and catego- rizing offensive language in social media (OffensE- val). In Proceedings of the 13th International Work- shop on Semantic Evaluation, pages 75-86, Min- neapolis, Minnesota, USA. Association for Compu- tational Linguistics.\n\n", "annotations": {"Abstract": [{"begin": 50, "end": 731, "idx": 0}], "Head": [{"begin": 734, "end": 748, "n": "1", "idx": 0}, {"begin": 2367, "end": 2381, "n": "2", "idx": 1}, {"begin": 4830, "end": 4848, "n": "3.1", "idx": 2}, {"begin": 6497, "end": 6524, "n": "3.2", "idx": 3}, {"begin": 7943, "end": 7970, "n": "3.3", "idx": 4}, {"begin": 9037, "end": 9043, "idx": 5}, {"begin": 9046, "end": 9058, "n": "4", "idx": 6}, {"begin": 9854, "end": 9867, "n": "4.1", "idx": 7}, {"begin": 12241, "end": 12281, "n": "4.2", "idx": 8}, {"begin": 14386, "end": 14404, "n": "5", "idx": 9}, {"begin": 16888, "end": 16898, "n": "6", "idx": 10}, {"begin": 17281, "end": 17305, "n": "6.1", "idx": 11}, {"begin": 18577, "end": 18613, "n": "6.2", "idx": 12}, {"begin": 19792, "end": 19816, "n": "6.3", "idx": 13}, {"begin": 21329, "end": 21341, "n": "7", "idx": 14}], "ReferenceToBib": [{"begin": 778, "end": 799, "target": "#b10", "idx": 0}, {"begin": 800, "end": 821, "target": "#b28", "idx": 1}, {"begin": 822, "end": 839, "target": "#b22", "idx": 2}, {"begin": 1006, "end": 1024, "target": "#b36", "idx": 3}, {"begin": 1025, "end": 1047, "target": "#b35", "idx": 4}, {"begin": 1441, "end": 1465, "target": "#b18", "idx": 5}, {"begin": 2021, "end": 2047, "target": "#b9", "idx": 6}, {"begin": 2573, "end": 2596, "target": "#b14", "idx": 7}, {"begin": 2597, "end": 2613, "target": "#b33", "idx": 8}, {"begin": 2614, "end": 2638, "target": "#b29", "idx": 9}, {"begin": 2639, "end": 2664, "target": "#b9", "idx": 10}, {"begin": 2665, "end": 2686, "target": "#b15", "idx": 11}, {"begin": 2906, "end": 2935, "target": "#b23", "idx": 12}, {"begin": 2936, "end": 2955, "target": "#b21", "idx": 13}, {"begin": 2956, "end": 2979, "target": "#b18", "idx": 14}, {"begin": 2980, "end": 2998, "target": "#b16", "idx": 15}, {"begin": 3094, "end": 3114, "target": "#b12", "idx": 16}, {"begin": 3330, "end": 3348, "target": "#b19", "idx": 17}, {"begin": 3358, "end": 3380, "target": "#b6", "idx": 18}, {"begin": 3395, "end": 3412, "target": "#b13", "idx": 19}, {"begin": 3564, "end": 3591, "target": "#b0", "idx": 20}, {"begin": 3592, "end": 3609, "target": "#b17", "idx": 21}, {"begin": 3870, "end": 3894, "target": "#b7", "idx": 22}, {"begin": 4093, "end": 4123, "idx": 23}, {"begin": 4135, "end": 4158, "target": "#b2", "idx": 24}, {"begin": 4171, "end": 4193, "target": "#b1", "idx": 25}, {"begin": 6163, "end": 6180, "target": "#b20", "idx": 26}, {"begin": 6359, "end": 6373, "target": "#b8", "idx": 27}, {"begin": 6916, "end": 6938, "target": "#b2", "idx": 28}, {"begin": 6985, "end": 7003, "target": "#b22", "idx": 29}, {"begin": 7117, "end": 7139, "target": "#b2", "idx": 30}, {"begin": 8559, "end": 8577, "target": "#b22", "idx": 31}, {"begin": 9435, "end": 9458, "target": "#b2", "idx": 32}, {"begin": 9878, "end": 9901, "target": "#b2", "idx": 33}, {"begin": 10361, "end": 10384, "idx": 34}, {"begin": 10406, "end": 10429, "target": "#b25", "idx": 35}, {"begin": 10453, "end": 10474, "target": "#b5", "idx": 36}, {"begin": 10492, "end": 10514, "target": "#b34", "idx": 37}, {"begin": 10550, "end": 10573, "target": "#b38", "idx": 38}, {"begin": 10594, "end": 10618, "target": "#b30", "idx": 39}, {"begin": 10640, "end": 10663, "target": "#b26", "idx": 40}, {"begin": 11250, "end": 11271, "idx": 41}, {"begin": 12544, "end": 12566, "target": "#b32", "idx": 42}, {"begin": 14742, "end": 14761, "target": "#b37", "idx": 43}, {"begin": 22244, "end": 22265, "target": "#b11", "idx": 44}, {"begin": 22270, "end": 22289, "target": "#b31", "idx": 45}], "ReferenceToFootnote": [{"begin": 5289, "end": 5290, "target": "#foot_0", "idx": 0}, {"begin": 11713, "end": 11714, "target": "#foot_1", "idx": 1}, {"begin": 15971, "end": 15972, "target": "#foot_2", "idx": 2}], "SectionFootnote": [{"begin": 22292, "end": 22711, "idx": 0}], "ReferenceString": [{"begin": 22728, "end": 22869, "id": "b0", "idx": 0}, {"begin": 22871, "end": 23034, "id": "b1", "idx": 1}, {"begin": 23038, "end": 23358, "id": "b2", "idx": 2}, {"begin": 23362, "end": 23404, "id": "b3", "idx": 3}, {"begin": 23408, "end": 23730, "id": "b4", "idx": 4}, {"begin": 23734, "end": 24154, "id": "b5", "idx": 5}, {"begin": 24158, "end": 24524, "id": "b6", "idx": 6}, {"begin": 24528, "end": 24773, "id": "b7", "idx": 7}, {"begin": 24777, "end": 24944, "id": "b8", "idx": 8}, {"begin": 24948, "end": 25331, "id": "b9", "idx": 9}, {"begin": 25335, "end": 25757, "id": "b10", "idx": 10}, {"begin": 25761, "end": 26016, "id": "b11", "idx": 11}, {"begin": 26020, "end": 26125, "id": "b12", "idx": 12}, {"begin": 26129, "end": 26412, "id": "b13", "idx": 13}, {"begin": 26416, "end": 26745, "id": "b14", "idx": 14}, {"begin": 26749, "end": 27121, "id": "b15", "idx": 15}, {"begin": 27125, "end": 27342, "id": "b16", "idx": 16}, {"begin": 27346, "end": 27571, "id": "b17", "idx": 17}, {"begin": 27575, "end": 27746, "id": "b18", "idx": 18}, {"begin": 27750, "end": 27978, "id": "b19", "idx": 19}, {"begin": 27982, "end": 28198, "id": "b20", "idx": 20}, {"begin": 28202, "end": 28378, "id": "b21", "idx": 21}, {"begin": 28382, "end": 28617, "id": "b22", "idx": 22}, {"begin": 28621, "end": 28807, "id": "b23", "idx": 23}, {"begin": 28811, "end": 29010, "id": "b24", "idx": 24}, {"begin": 29014, "end": 29297, "id": "b25", "idx": 25}, {"begin": 29301, "end": 29613, "id": "b26", "idx": 26}, {"begin": 29617, "end": 29911, "id": "b27", "idx": 27}, {"begin": 29915, "end": 30084, "id": "b28", "idx": 28}, {"begin": 30088, "end": 30417, "id": "b29", "idx": 29}, {"begin": 30421, "end": 30694, "id": "b30", "idx": 30}, {"begin": 30698, "end": 30960, "id": "b31", "idx": 31}, {"begin": 30964, "end": 31233, "id": "b32", "idx": 32}, {"begin": 31237, "end": 31551, "id": "b33", "idx": 33}, {"begin": 31555, "end": 31828, "id": "b34", "idx": 34}, {"begin": 31832, "end": 32079, "id": "b35", "idx": 35}, {"begin": 32083, "end": 32296, "id": "b36", "idx": 36}, {"begin": 32300, "end": 32879, "id": "b37", "idx": 37}, {"begin": 32883, "end": 33252, "id": "b38", "idx": 38}], "ReferenceToTable": [{"begin": 7980, "end": 7981, "idx": 0}, {"begin": 13068, "end": 13069, "target": "#tab_5", "idx": 1}, {"begin": 13395, "end": 13396, "target": "#tab_5", "idx": 2}, {"begin": 17312, "end": 17313, "target": "#tab_7", "idx": 3}, {"begin": 19233, "end": 19234, "idx": 4}, {"begin": 20022, "end": 20023, "target": "#tab_10", "idx": 5}], "Footnote": [{"begin": 22303, "end": 22362, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 22363, "end": 22596, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 22597, "end": 22711, "id": "foot_2", "n": "3", "idx": 2}], "ReferenceToFormula": [{"begin": 3761, "end": 3765, "idx": 0}, {"begin": 12682, "end": 12686, "idx": 1}], "Paragraph": [{"begin": 60, "end": 731, "idx": 0}, {"begin": 749, "end": 1855, "idx": 1}, {"begin": 1856, "end": 2048, "idx": 2}, {"begin": 2049, "end": 2365, "idx": 3}, {"begin": 2382, "end": 3975, "idx": 4}, {"begin": 3976, "end": 4683, "idx": 5}, {"begin": 4684, "end": 4734, "idx": 6}, {"begin": 4735, "end": 4828, "idx": 7}, {"begin": 4849, "end": 5566, "idx": 8}, {"begin": 5567, "end": 6495, "idx": 9}, {"begin": 6525, "end": 7941, "idx": 10}, {"begin": 7971, "end": 9035, "idx": 11}, {"begin": 9059, "end": 9852, "idx": 12}, {"begin": 9868, "end": 12189, "idx": 13}, {"begin": 12282, "end": 12567, "idx": 14}, {"begin": 12568, "end": 12612, "idx": 15}, {"begin": 12613, "end": 13052, "idx": 16}, {"begin": 13053, "end": 13728, "idx": 17}, {"begin": 13729, "end": 14384, "idx": 18}, {"begin": 14405, "end": 14993, "idx": 19}, {"begin": 14994, "end": 15592, "idx": 20}, {"begin": 15593, "end": 16469, "idx": 21}, {"begin": 16470, "end": 16715, "idx": 22}, {"begin": 16716, "end": 16886, "idx": 23}, {"begin": 16899, "end": 17279, "idx": 24}, {"begin": 17306, "end": 17787, "idx": 25}, {"begin": 17788, "end": 18575, "idx": 26}, {"begin": 18614, "end": 19199, "idx": 27}, {"begin": 19200, "end": 19790, "idx": 28}, {"begin": 19817, "end": 20881, "idx": 29}, {"begin": 20882, "end": 21327, "idx": 30}, {"begin": 21342, "end": 22085, "idx": 31}, {"begin": 22086, "end": 22290, "idx": 32}], "SectionHeader": [{"begin": 0, "end": 731, "idx": 0}], "SectionReference": [{"begin": 22713, "end": 33254, "idx": 0}], "Sentence": [{"begin": 60, "end": 170, "idx": 0}, {"begin": 171, "end": 270, "idx": 1}, {"begin": 271, "end": 510, "idx": 2}, {"begin": 511, "end": 667, "idx": 3}, {"begin": 668, "end": 707, "idx": 4}, {"begin": 708, "end": 731, "idx": 5}, {"begin": 749, "end": 871, "idx": 6}, {"begin": 872, "end": 1136, "idx": 7}, {"begin": 1137, "end": 1466, "idx": 8}, {"begin": 1467, "end": 1585, "idx": 9}, {"begin": 1586, "end": 1855, "idx": 10}, {"begin": 1856, "end": 1871, "idx": 11}, {"begin": 1872, "end": 2048, "idx": 12}, {"begin": 2049, "end": 2184, "idx": 13}, {"begin": 2185, "end": 2365, "idx": 14}, {"begin": 2382, "end": 2463, "idx": 15}, {"begin": 2464, "end": 2687, "idx": 16}, {"begin": 2688, "end": 2999, "idx": 17}, {"begin": 3000, "end": 3115, "idx": 18}, {"begin": 3116, "end": 3271, "idx": 19}, {"begin": 3272, "end": 3413, "idx": 20}, {"begin": 3414, "end": 3610, "idx": 21}, {"begin": 3611, "end": 3790, "idx": 22}, {"begin": 3791, "end": 3975, "idx": 23}, {"begin": 3976, "end": 4259, "idx": 24}, {"begin": 4260, "end": 4336, "idx": 25}, {"begin": 4337, "end": 4475, "idx": 26}, {"begin": 4476, "end": 4598, "idx": 27}, {"begin": 4599, "end": 4683, "idx": 28}, {"begin": 4684, "end": 4734, "idx": 29}, {"begin": 4735, "end": 4828, "idx": 30}, {"begin": 4849, "end": 4943, "idx": 31}, {"begin": 4944, "end": 5020, "idx": 32}, {"begin": 5021, "end": 5037, "idx": 33}, {"begin": 5038, "end": 5136, "idx": 34}, {"begin": 5137, "end": 5426, "idx": 35}, {"begin": 5427, "end": 5566, "idx": 36}, {"begin": 5567, "end": 5823, "idx": 37}, {"begin": 5824, "end": 5838, "idx": 38}, {"begin": 5839, "end": 5957, "idx": 39}, {"begin": 5958, "end": 6116, "idx": 40}, {"begin": 6117, "end": 6223, "idx": 41}, {"begin": 6224, "end": 6323, "idx": 42}, {"begin": 6324, "end": 6396, "idx": 43}, {"begin": 6397, "end": 6495, "idx": 44}, {"begin": 6525, "end": 6626, "idx": 45}, {"begin": 6627, "end": 6825, "idx": 46}, {"begin": 6826, "end": 6846, "idx": 47}, {"begin": 6847, "end": 6905, "idx": 48}, {"begin": 6906, "end": 7068, "idx": 49}, {"begin": 7069, "end": 7219, "idx": 50}, {"begin": 7220, "end": 7311, "idx": 51}, {"begin": 7312, "end": 7332, "idx": 52}, {"begin": 7333, "end": 7445, "idx": 53}, {"begin": 7446, "end": 7589, "idx": 54}, {"begin": 7590, "end": 7758, "idx": 55}, {"begin": 7759, "end": 7941, "idx": 56}, {"begin": 7971, "end": 8081, "idx": 57}, {"begin": 8082, "end": 8148, "idx": 58}, {"begin": 8149, "end": 8225, "idx": 59}, {"begin": 8226, "end": 8353, "idx": 60}, {"begin": 8354, "end": 8578, "idx": 61}, {"begin": 8579, "end": 8681, "idx": 62}, {"begin": 8682, "end": 8843, "idx": 63}, {"begin": 8844, "end": 9035, "idx": 64}, {"begin": 9059, "end": 9187, "idx": 65}, {"begin": 9188, "end": 9346, "idx": 66}, {"begin": 9347, "end": 9364, "idx": 67}, {"begin": 9365, "end": 9595, "idx": 68}, {"begin": 9596, "end": 9852, "idx": 69}, {"begin": 9868, "end": 9992, "idx": 70}, {"begin": 9993, "end": 10117, "idx": 71}, {"begin": 10118, "end": 10271, "idx": 72}, {"begin": 10272, "end": 10289, "idx": 73}, {"begin": 10290, "end": 10664, "idx": 74}, {"begin": 10665, "end": 10686, "idx": 75}, {"begin": 10687, "end": 10781, "idx": 76}, {"begin": 10782, "end": 10871, "idx": 77}, {"begin": 10872, "end": 10985, "idx": 78}, {"begin": 10986, "end": 11057, "idx": 79}, {"begin": 11058, "end": 11077, "idx": 80}, {"begin": 11078, "end": 11338, "idx": 81}, {"begin": 11339, "end": 11415, "idx": 82}, {"begin": 11416, "end": 11424, "idx": 83}, {"begin": 11425, "end": 11560, "idx": 84}, {"begin": 11561, "end": 11720, "idx": 85}, {"begin": 11721, "end": 11867, "idx": 86}, {"begin": 11868, "end": 11918, "idx": 87}, {"begin": 11919, "end": 12012, "idx": 88}, {"begin": 12013, "end": 12189, "idx": 89}, {"begin": 12282, "end": 12489, "idx": 90}, {"begin": 12490, "end": 12567, "idx": 91}, {"begin": 12568, "end": 12612, "idx": 92}, {"begin": 12613, "end": 12823, "idx": 93}, {"begin": 12824, "end": 12930, "idx": 94}, {"begin": 12931, "end": 13052, "idx": 95}, {"begin": 13053, "end": 13061, "idx": 96}, {"begin": 13062, "end": 13123, "idx": 97}, {"begin": 13124, "end": 13300, "idx": 98}, {"begin": 13301, "end": 13397, "idx": 99}, {"begin": 13398, "end": 13446, "idx": 100}, {"begin": 13447, "end": 13631, "idx": 101}, {"begin": 13632, "end": 13728, "idx": 102}, {"begin": 13729, "end": 13751, "idx": 103}, {"begin": 13752, "end": 13801, "idx": 104}, {"begin": 13802, "end": 13866, "idx": 105}, {"begin": 13867, "end": 13940, "idx": 106}, {"begin": 13941, "end": 14058, "idx": 107}, {"begin": 14059, "end": 14138, "idx": 108}, {"begin": 14139, "end": 14251, "idx": 109}, {"begin": 14252, "end": 14384, "idx": 110}, {"begin": 14405, "end": 14543, "idx": 111}, {"begin": 14544, "end": 14692, "idx": 112}, {"begin": 14693, "end": 14881, "idx": 113}, {"begin": 14882, "end": 14993, "idx": 114}, {"begin": 14994, "end": 15328, "idx": 115}, {"begin": 15329, "end": 15592, "idx": 116}, {"begin": 15593, "end": 15828, "idx": 117}, {"begin": 15829, "end": 15972, "idx": 118}, {"begin": 15973, "end": 16243, "idx": 119}, {"begin": 16244, "end": 16469, "idx": 120}, {"begin": 16470, "end": 16669, "idx": 121}, {"begin": 16670, "end": 16715, "idx": 122}, {"begin": 16716, "end": 16886, "idx": 123}, {"begin": 16899, "end": 17279, "idx": 124}, {"begin": 17306, "end": 17523, "idx": 125}, {"begin": 17524, "end": 17787, "idx": 126}, {"begin": 17788, "end": 18009, "idx": 127}, {"begin": 18010, "end": 18234, "idx": 128}, {"begin": 18235, "end": 18431, "idx": 129}, {"begin": 18432, "end": 18575, "idx": 130}, {"begin": 18614, "end": 18778, "idx": 131}, {"begin": 18779, "end": 18946, "idx": 132}, {"begin": 18947, "end": 19068, "idx": 133}, {"begin": 19069, "end": 19199, "idx": 134}, {"begin": 19200, "end": 19370, "idx": 135}, {"begin": 19371, "end": 19475, "idx": 136}, {"begin": 19476, "end": 19625, "idx": 137}, {"begin": 19626, "end": 19790, "idx": 138}, {"begin": 19817, "end": 19927, "idx": 139}, {"begin": 19928, "end": 20015, "idx": 140}, {"begin": 20016, "end": 20104, "idx": 141}, {"begin": 20105, "end": 20227, "idx": 142}, {"begin": 20228, "end": 20612, "idx": 143}, {"begin": 20613, "end": 20881, "idx": 144}, {"begin": 20882, "end": 21062, "idx": 145}, {"begin": 21063, "end": 21194, "idx": 146}, {"begin": 21195, "end": 21327, "idx": 147}, {"begin": 21342, "end": 21441, "idx": 148}, {"begin": 21442, "end": 21610, "idx": 149}, {"begin": 21611, "end": 21775, "idx": 150}, {"begin": 21776, "end": 22085, "idx": 151}, {"begin": 22086, "end": 22290, "idx": 152}], "ReferenceToFigure": [{"begin": 17884, "end": 17885, "target": "#fig_3", "idx": 0}, {"begin": 21059, "end": 21060, "target": "#fig_4", "idx": 1}], "Div": [{"begin": 60, "end": 731, "idx": 0}, {"begin": 734, "end": 2365, "idx": 1}, {"begin": 2367, "end": 4828, "idx": 2}, {"begin": 4830, "end": 6495, "idx": 3}, {"begin": 6497, "end": 7941, "idx": 4}, {"begin": 7943, "end": 9035, "idx": 5}, {"begin": 9037, "end": 9044, "idx": 6}, {"begin": 9046, "end": 9852, "idx": 7}, {"begin": 9854, "end": 12239, "idx": 8}, {"begin": 12241, "end": 14384, "idx": 9}, {"begin": 14386, "end": 16886, "idx": 10}, {"begin": 16888, "end": 17279, "idx": 11}, {"begin": 17281, "end": 18575, "idx": 12}, {"begin": 18577, "end": 19790, "idx": 13}, {"begin": 19792, "end": 21327, "idx": 14}, {"begin": 21329, "end": 22290, "idx": 15}], "SectionMain": [{"begin": 731, "end": 22290, "idx": 0}]}}