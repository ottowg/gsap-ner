{"text": "Recovering the Graph Underlying Networked Dynamical Systems under Partial Observability: A Deep Learning Approach\n\nAbstract:\nWe study the problem of graph structure identification, i.e., of recovering the graph of dependencies among time series. We model these time series data as components of the state of linear stochastic networked dynamical systems. We assume partial observability, where the state evolution of only a subset of nodes comprising the network is observed. We devise a new feature vector computed from the observed time series and prove that these features are linearly separable, i.e., there exists a hyperplane that separates the cluster of features associated with connected pairs of nodes from those associated with disconnected pairs. This renders the features amenable to train a variety of classifiers to perform causal inference. In particular, we use these features to train Convolutional Neural Networks (CNNs). The resulting causal inference mechanism outperforms state-of-the-art counterparts w.r.t. samplecomplexity. The trained CNNs generalize well over structurally distinct networks (dense or sparse) and noise-level profiles. Remarkably, they also generalize well to real-world networks while trained over a synthetic network (realization of a random graph). Finally, the proposed method consistently reconstructs the graph in a pairwise manner, that is, by deciding if an edge or arrow is present or absent in each pair of nodes, from the corresponding time series of each pair. This fits the framework of large-scale systems, where observation or processing of all nodes in the network is prohibitive.\n\nMain:\n\n\n\nIntroduction\nNetworked dynamical systems are characterized by a set of interconnected nodes or agents. The state of the nodes evolves over time according to their peer-to-peer interactions constrained by a support network of contacts (Barrat, Barth\u00e9lemy, and Vespignani 2012; Liggett 2005; Robert 2003; Porter and Gleeson 2016). More concretely, the state of a node i is only immediately affected by the state of nodes that directly link to i, i.e., nodes that bear a direct causal effect on node i. This causal network is captured by a graph, which is often a latent structure underlying these systems.\nExamples of networked dynamical systems include: i) Pandemics -the fraction of infections within each community of individuals is captured by a time series that is strongly influenced by contacts in neighboring communities. Knowledge of the contact network (which determines the main avenues of contagion) is critical for designing effective mitigation measures (Lahmanovich and James 1976; Ganesh, Massouli\u00e9, and Towsley 2005; Santos, Moura, and Xavier 2015; Braunstein et al. 2016; Ren et al. 2019). For example, a natural mitigation policy is network dismantling: aiming to quarantine a minimal set of nodes to promote a maximal disconnect of the underlying contagion network (Braunstein et al. 2016; Ren et al. 2019) -thus, hindering virus propagation across communities without disrupting the global function of the networked system; ii) Brain activity -based on temporal signals gathered from cranial probes, an important task is to infer the so-called Functional Connectivity Matrix, which represents the graph of interactions among the active regions of the brain (see, e.g., (Li\u00e9geois et al. 2020)). Recent evidence shows that the Functional Connectivity Matrix can be used to diagnose or predict the onset of motor activities or cognitive disorders (Douw et al. 2010; Ranasinghe et al. 2014; Oltra et al. 2021; Stam et al. 2007; Monajemi et al. 2016; van Mierlo et al. 2019; Lehnertz, Br\u00f6hl, and Rings 2020); iii) Finance -the dynamics of stock prices can be influenced by interactions between firms, and knowledge of this interaction network can inform government interventions, for instance (Fenn et al. 2009 (Fenn et al. , 2012;; Bazzi et al. 2016).\nIn most practical instances of the examples above, the node-level time series are readily accessible, but the underlying causal network -which is of fundamental importance in downstream tasks -is fully or partially unknown. To address this issue, a growing body of literature has developed methods for reconstructing the network from the observed node-level time series (Chen, Wang, and Shen 2022; Pereira, Ibrahimi, and Montanari 2010; Materassi and Salapaka 2015; Matta, Santos, and Sayed 2020). In this work, we focus on linear stochastic networked dynamical systems, which is arguably one of the most natural settings for network identification from time series since a great class of nonlinear networked dynamical systems can be addressed via linearization about the equilibria under smallnoise regimes (Ching and Tam 2017; Napoletani and Sauer 2008) or via appropriate embedding in higher dimensional spaces (Lim et al. 2015; Mauroy and Goncalves 2016). Moreover, since it is typically impractical to monitor all node-level signals in large-scale systems, we assume a partial observability setting, wherein we observe the time series corresponding to a small subset of nodes and aim to reconstruct the corresponding subgraph connecting them using a small number of samples. This task is much more challenging than the full observability case, since the time series of the observed nodes are also affected by the unobserved dynamics of the remainder of the network. Fig. 1 summarizes the structure identification framework considered. This work departs from the standard approach of reconstructing networks based on scalar measures between time series -i.e., measures that assign a real-value to the coupling-strength between nodes, e.g., correlation, Granger, Precision matrix, etc. -which dates back to (Chow and Liu 1968). We propose a novel feature vector based setting constructed for each pair of nodes from their time series. In this novel setting, structure identification leverages the separability properties of the proposed feature vectors in a higherdimensional space. We provide rigorous theoretical results proving that our features are linearly separable for any undirected network, once sufficiently many samples are taken. Thus, our features can be readily used as input to a variety of machine learning pipelines to perform causal inference. We demonstrate that Convolutional Neural Networks (CNN) trained with our features outperform state-of-the-art methods in terms of accuracy and sample complexity.\n\nRelated Work\nCausal inference relies on the nature of the samples, and it depends on whether the observed time series are drawn independently from multivariate distributions (often assumed i.i.d. within the scope of graphical models), or are time series stemming from some networked dynamical law (not i.i.d.). For the multivariate case, the Markov property establishes a one-to-one correspondence between certain probability distributions and the set of (possibly directed) graphs (Hammersley and Clifford 2021; Pearl 2009). If X is conditionally independent of Z given Y , then there is no arrow, or direct causal effect, from Z to X. The dependence relationships are thus captured by a graph. This Markov property can be extended to discrete-time networked dynamical systems: the state of a node at instant n + 1 depends only on the state of some nodes at time n (also known as neighbors or parents). The general goal of causal inference is to uncover the possible avenues of information flow: to recover from observation of the time series samples the underlying graph structure of dependencies defined by the Markov property. Typically, this is done by leveraging various forms of scalar measures between time series, e.g., transformations of the covariance matrix; regression (e.g., Granger estimator) (Geiger et al. 2015; Pereira, Ibrahimi, and Montanari 2010; Matta, Santos, and Sayed 2020); or other scalar graph metrics (Materassi and Salapaka 2015). The performance of the precise method ties strongly to the data generative process and to whether the system is fully or partially observed.\n\nFull-observability\nGraphical models. Classical algorithms (assuming i.i.d. samples) based on conditional independence tests, include the SGS (Spirtes, Glymour, and Scheines 2000), PC (Spirtes and Glymour 1991), GES (Chickering 2003), and FGS (Ramsey et al. 2016). The algorithms and sufficient conditions for consistency devised in these works rely on sparsity related structural constraints that hardly fit the connectivity pattern of general networks. The work (Anandkumar et al. 2012) offers an approach for the large scale setting under certain complying assumptions of sparsity. The independence tests are leveraged via conditional covariance tests. All structural constraints, revolving around sparsity, play a critical role to render the scaling of the independence tests amenable to computation, otherwise, the problem becomes quickly unfeasible (Bresler, Gamarnik, and Shah 2014; Bogdanov, Mossel, and Vadhan 2008).\nNetworked dynamical systems. For approaches in the signal processing literature, (Mateos et al. 2019) provides an overview highlighting regression plus regularization of the network sparsity methods for full-observability over distinct models -primarily promoting sparsity of the latent network, -including linear dynamical systems as vector autoregressive (VAR) models, as in (Mei and Moura 2017; Moneta et al. 2009; Kivits and Hof 2022). In this regard, (Pereira, Ibrahimi, and Montanari 2010) addresses the problem for linear stochastic differential equations (SDEs) via an optimization formulation that regularizes sparsity of the latent network. This problem is addressed by first converting the continuous-time SDE into a discrete-time linear dynamical system -a technique that yields the discrete-time model considered in this work. Other schemes exploit spectral-based methods (Granger 1969; Segarra, Schaub, and Jadbabaie 2017; Segarra et al. 2017). These leverage the spectral properties of the interaction matrix or support graph (Sandryhaila and Moura 2013) to characterize signatures that allow consistent estimation over certain sparse networks.\n\nPartial observability\nGraphical models. In general, the proposed approaches rely on conditional independence (CI) tests or measures thereof, e.g., conditional mutual information (CMI) or transfer entropy, and a causal link is declared whenever a test yields a positive CMI-based metric. Classical algorithms for causal inference under the presence of latent variables are the FCI (Spirtes, Meek, and Richardson 1995) and RFCI (Colombo et al. 2012). As in the full-observability setting, consistent tests scale combinatorially with the connectivity of the causal graph, rendering the CI-based approaches impractical for denser graphs. To control the curse of connectivity, CI-based methods often act at a microscopical level relying on several strong structural constraints including, directed acyclic graphs, long girth (Anandkumar et al. 2013; Anandkumar and Valluvan 2013) and other more technical local structural conditions, such as bottleneck and non-redundancy (Adams, Hansen, and Zhang 2021; Mastakouri, Sch\u00f6lkopf, and Janzing 2021).\nNetworked dynamical systems. In (Materassi and Salapaka 2015, 2012a,b), linear dynamical systems are addressed via certain pseudo-metrics, e.g. log-coherence distance, aiming to capture the true graph-distance between nodes. In (Geiger et al. 2015) some conditions on the network connectivity and interaction matrix of a linear networked dynamical system are proposed, in order to obtain uniqueness of the network connectivity given partially observed samples. It does not provide, however, an algorithm with consistency guarantees to retrieve the uniquely determined network. On the other hand, the work (Zhao and Wan 2022) uses an expectation-maximization based approach to address certain discrete-time discrete state-space networked dynamical systems, while (Chandrasekaran, Parrilo, and Willsky 2012; Jalali and Sanghavi 2012; Mei and Moura 2018) resort to convex optimization based methods for regularizing the sparsity of the network under partial observability. The works (Santos, Matta, and Sayed 2020; Matta, Santos, and Sayed 2020, 2022) establish structural consistency of the Granger (or regression) and other matrixvalued estimators over partially observed discrete-time linear stochastic networked dynamical systems with symmetric interaction matrices, for distinct regimes of network connectivity (including densely connected networks). Similar to (Anandkumar and Valluvan 2013), the structural consistency of these estimators is established in the thermodynamic limit, i.e., as the number of nodes scales to infinite, which fits the framework of large-scale networks. Recently, (Chen, Wang, and Shen 2022) proved that the underlying interaction matrix, up to a multiplicative constant related to the noise level, can be expressed as a linear combination of covariance matrices, with high probability, under the following regime: i) the interaction matrix A is symmetric; ii) the noise x is diagonal and homogeneous, i.e., its covariance matrix is a multiple of the identity matrix. Theorem 1 in (Chen, Wang, and Shen 2022) will be used in the present work to establish an important result regarding the proposed set of feature vectors, namely, consistent linear separability. This property will further yield a competitive performance for the trained CNNs in terms of sample-complexity.\n\nProblem Formulation\nWe consider the linear networked dynamical lawy(n + 1) = Ay(n) + x(n + 1),\nwherey(n) = [y 1 (n) y 2 (n) . . . y N (n)] \u2208 R N represents\nthe state-vector of the N -dimensional networked dynamical system at time n that collects the states y i (n) of each node i at time n; x(n) \u223c N 0, \u03c3 2 I N represents the excitation noise associated with the N nodes of the system with covariance matrix \u03c3 2 I N , and independent across time n; A \u2208 R N \u00d7N + refers to the non-negative interaction matrix whose support represents the underlying graph linking the nodes. The dynamical system is assumed to be stable, i.e., \u03c1(A) < 1, where \u03c1(A) stands for the spectral radius of A.\nThis work deals with the problem of recovering the support of the submatrix A S , i.e., the graph structure of connections among the observed nodes in the subset S from observation of the subvector[y(n)] S = y m1 (n) y m2 (n) . . . y m |S| (n) \u2208 R |S| over time n,\nwhere |S| is the cardinality of the subset S (see Fig. 1).\nNotation:S = m 1 , m 2 , . . . , m |S| \u2282 {1, 2, . . . , N } is a nonempty subset of indexes with m 1 < m 2 < . . . < m |S| and |S| \u2264 N ; given a vector y \u2208 R N , [y] S = y m1 (n) y m2 (n) . . . y m |S| (n)\nis the subvector obtained from y and indexed by S; accordingly, a similar notation is adopted for matrices, namely, given A \u2208 R N \u00d7N , the matrixA S \u2208 R |S|\u00d7|S| or [A] S \u2208 R |S|\u00d7|S| is defined as the subma- trix whose ij th entry is A mimj ; Supp (A) is the support of the matrix A, i.e., [Supp (A)] ij = 1 {Aij =0} ; ||y|| \u221e refers to the L \u221e -norm\nthat returns the maximal absolute value across the entries of the vector y \u2208 R N ; the set of natural numbers, including zero, is denoted by N = {0, 1, 2, . . .}.\n\nStructural Consistency\nConsider the following k th lag covariance matrixR k (n) \u2206 = E y(n + k)y(n) (2)\nassociated with the process (y(n)) n\u2208N . In addition, define the empirical counterpart of R k (n)R k (n) \u2206 = 1 n n\u22121 =0 y( + k)y( ) .\nWe refer to a matrix-valued estimator as any map whose input is given by the (observed) time series and the output is given by a matrix, namely,F (n) : R |S|\u00d7n \u2212\u2192 R |S|\u00d7|S| {[y( )] S } n\u22121 =0 \u2212\u2192 F (n) ,\nfor any given n \u2208 N. The idea is that the ij th entry of the output matrix F (n) estimates the strength of the link from i to j from n samples of the observed time series. For instance, the empirical covariance matrix R k (n), under full-observability, or R k (n)\n\nS\n, in the case of partial-observability, are examples of matrix-valued estimators. Definition 1 (structural consistency of a matrix). A matrixvalued estimator F (n) is structurally consistent with high probability, whenever there exists a threshold \u03c4 so that, i.e., i links to j if and only if the ij th entry of the estimator matrix F (n) lies above the threshold \u03c4 , provided that there is a large enough number of samples n.P F (n) ij > \u03c4 n\u2192\u221e \u2212\u2192 1 \u21d0\u21d2 i \u2192 j,\nIn other words, up to a proper threshold \u03c4 , the output matrix F (n) reflects the underlying structure of the graph in that[Supp(A S )] ij = 1 F (n) ij >\u03c4\n, for all pairs i = j w.h.p. An example of a structurally consistent w.h.p. matrixvalued estimator (under partial observability) is given by (Chen, Wang, and Shen 2022). Other examples of matrix-valued estimators that are provably structurally consistent under partial observability in-F (n) \u2206 = R 1 (n) \u2212 R 3 (n)clude: i) Granger R 1 (n) S R 0 (n) S \u22121 ; ii) One-lag R 1 (n) S ; iii) Residual R 1 (n) S \u2212 R 0 (n) S\n. These latter estimators are proven to be structurally consistent under a certain thermodynamic limit regime (Matta, Santos, and Sayed 2022), i.e., structural consistency is met in the limit N \u2212\u2192 \u221e with |S| /N \u2212\u2192 \u03be > 0 or with |S| /N \u2212\u2192 0 for certain sparse regimes (Santos, Matta, and Sayed 2020).\nRemark 1. Technically, one should formally refer to the sequence F (n)  n\u2208N of maps (estimators) as structurally consistent with high probability. However, hereby, for the sake of simplicity it will be simply referred to as \"the estimator F (n) is structurally consistent w.h.p.\".\nNext, we introduce a tensor-valued estimator which is, formally, any map whose input is given by the (observed) time series and the output is an order-3 tensor, as followsT (n) : R |S|\u00d7n \u2212\u2192 R |S|\u00d7|S|\u00d7M {[y( )] S } n\u22121 n=0 \u2212\u2192 T (n) ,\nwhere the ij th entry of the order-3 tensor T (n) is a vectorT (n) ij\n\u2208 R M that models a feature statistical descriptor corresponding to the pair ij in the network and that is built from n samples of the time series {[y( )] S } n\u22121 =0 . Definition 2 (structural consistency of a tensor). A tensorvalued estimator T (n) of order-3 is linearly structurally consistent with high probability, if there exists an affine map L : R M \u2192 R (or hyperplane) that separates the underlying features associated with connected pairs from those associated with disconnected pairs w.h.p., that is,P L(T (n) ij ) > 0 n\u2192\u221e \u2212\u2192 1, if ij is connected, P L(T (n) ij ) \u2264 0 n\u2192\u221e \u2212\u2192 1, if ij is disconnected . (7)\nAs an example, the estimator T (n) whose ij th entry of the tensor output T (n) is defined asT (n) ij \u2206 = R D (n) ij , R D+1 (n) ij , . . . , R L (n) ij\ncorresponds to an order-3 tensor-valued estimator. As we will show in the next section, if D \u2264 1 and L \u2265 3, then this estimator is linearly structurally consistent w.h.p.\n\nFeatures Separability\nThe results presented in this section motivate the CNNbased approach for graph learning considered in this work.Assumption 1. Let E (n) := E (n) 1 , E (n) 2 , . . . , E (n) M\nbe a family of matrix-valued estimators such that for some w := (w 1 , w 2 , . . . , w M ) \u2208 R M with w = 0, the linear combinationE (n) (w) = M =1 w E (n)\nis a structurally consistent w.h.p. matrix-valued estimator for the dynamics (1).\nLemma 1. For each pair ij, with i = j, define the associated feature vector as,T (n) ij := E (n) 1 ij , E (n) 2 ij , . . . , E (n) M ij \u2208 R M .\n(8) Then, under Assumption 1, the tensor-valued estimator T (n) is linearly structurally consistent w.h.p., or equivalently, the set of features n) is structurally consistent w.h.p. for some w \u2208 R M , then there exists a threshold \u03c4 w so that E (n) (w) ij > \u03c4 w across connected pairs ij and E (n) (w) ij < \u03c4 w , otherwise. Therefore, the affine map L w (x) = x \u2022 w \u2212 \u03c4 w consistently separates the set of featuresT (n) ij i =j \u2282 R M is consistently linearly separable w.h.p. Proof. Since E (n) (w) = M =1 w ET (n) ij ij w.h.p. Indeed, L w (T (n) ij ) = T (n) ij \u2022 w \u2212 \u03c4 w = [E(w)] ij \u2212 \u03c4 w > 0 (9) for a connected pair ij or L w (T (n) ij ) = E (n) (w) ij \u2212 \u03c4 w < 0,\notherwise. In other words, the hyperplane characterized by the linear map L w : R M \u2212\u2192 R separates consistently the pairs ij for all i = j, w.h.p.\nTheorem 1. For each pair ij, with i = j, define the associated feature vector as, with D \u2264 1 and L \u2265 3, and assume that the interaction matrix A underlying the dynamics (1) is symmetric and the covariance matrix of the noise process (x(n)) n\u2208N is given by \u03a3 x := \u03c3 2 I N , for some \u03c3 > 0. Then, the setT (n) ij := R D (n) ij , R D+1 (n) ij , . . . , R L (n) ij ,T (n) ij i =j\n\u2282 R M is consistently linearly separable w.h.p.\nProof. Define the vector w (Chen, Wang, and Shen 2022)\u2208 {\u22121, 0, 1} M so that E (n) (w) = R 1 (n) \u2212 R 3 (n), which is possible since D \u2264 1 and L \u2265 3. According to Theorem 1 in, E (n) (w) = R 1 (n) \u2212 R 3 (n)\nis structurally consistent w.h.p. and the result now follows from the previous Lemma 1.\nRemark 2 (Locality of the structural estimation). Note that, to compute the feature T (n) ij associated with each pair ij defined in Theorem 1, we need only the time series {y i ( ), y j ( )} n =0 associated with the pair ij asT (n) ij := 1 n n\u22121 =0 (y i ( + D)y j ( ), . . . , y i ( + M )y j ( )) ,\nwhich only involves information related to nodes i and j. As such, it is possible to reconstruct the connectivity pattern in a pairwise manner. This is a special property that results from the fact that each lag-moment, or covariance matrix, in the feature vector can be locally estimated. Observe that the majority of the matrix-valued estimators does not exhibit this locality property. For example, to reconstruct the ij th en-try of the Precision matrix R 0 (n) \u22121\n, one needs to know the whole matrix R 0 (n) (or a large portion around the pair ij thereof). This has the drawback of implying the observation of a large set of nodes (or of the whole network) just to estimate the corresponding entry ij of the Precision matrix.\nNow, given a matrix-valued estimator F (n) , define its identifiability gap as (Matta, Santos, and Sayed 2022)\u0393 F (n) \u2206 = min ij : Aij =0 F (n) ij \u2212 max ij : Aij =0 F (n) ij ,\ni.e., the gap between the smallest entry of F (n) ij across connected pairs and the largest entry of F (n) ij over disconnected pairs. An estimator F (n) is structurally consistent w.h.p. if and only if \u0393 F (n) > 0 w.h.p., or in other words, if and only if connected pairs are separated from disconnected pairs, in view of the entries of the matrix F (n) , for n large enough. This statistical metric is a relevant parameter regarding the hardness of the classification. The larger the identifiability gap, the easier the classification via thresholding of the entries of the matrix F (n) tends to be.\nSimilarly, define the identifiability gap \u0393 T (n) associated with a tensor-valued estimator T (n) as the maximum distance among all parallel hyperplanes that consistently separate the features, as in Fig. 2. For example, the SVM algorithm is designed to find these margins. More concretely,\u0393 T (n) \u2206 = max (w,\u03c41),(w,\u03c42)\u2208H |\u03c4 1 \u2212 \u03c4 2 | ||w|| ,\nwhere H indexes the set of linear maps that consistently separate the features: (w, \u03c4 ) \u2208 H if and only if the linear map L w,\u03c4 (x) := w \u2022 x \u2212 \u03c4 consistently separates the features.\nLemma 2. Let T (n) be a tensor-valued estimator whose underlying features at each pair ij are defined as n) and T (n) are (linearly) structurally consistent w.h.p., then the tensor-valued estimator T (n) defined via the augmented featuresT (n) ij := E (n) 1 ij , E (n) 2 ij , . . . , E (n) M ij \u2208 R M , (13) with identifiability gap \u0393 (n) E \u2206 = \u0393 T (n) . Let A (n) be a matrix-valued estimator with identifiability gap \u0393 (n) A \u2206 = \u0393 A (n) . If both A (T (n) ij := A (n) ij , E (n) 1 ij , . . . , E (n) M ij \u2208 R M , (14) exhibits an identifiability gap obeying \u0393 T (n) \u2265 \u0393 (n) 2 w.h.p., with \u0393 (n) \u2206 = \u0393 (n) A , \u0393\n.\nLemma 2 asserts that, if further matrix-valued structurally consistent estimators are incorporated into the feature vector, the identifiability gap increases.\nProof. Let Cv (S) denote the convex hull of a set S \u2282 R M , i.e., the smallest convex set containing S (Hiriart-Urruty and Lemar\u00e9chal 2001). DefineC \u2206 = T (n) ij ij : Aij =0\nas the set of augmented features associated with connected pairs andD \u2206 = T (n) ij ij : Aij =0\nassociated with disconnected pairs. Similarly, defineC \u2206 = T (n) ij ij : Aij =0 and D \u2206 = T (n) ij ij : Aij =0\n. Let R be the smallest entry of A (n)   across connected pairs and r be the greatest entry of A (n) across disconnected pairs and note that r < R w.h.p, since A (n) is structurally consistent w.h.p. We have that\u0393 T (n) 2 (a) = d Cv C , Cv D 2 (b) \u2265 d (Cv (C \u00d7[R, \u221e )) , Cv (D \u00d7(\u2212\u221e, r ])) 2 (c) \u2265 d (Cv (C) \u00d7[R, \u221e ) , Cv (D) \u00d7(\u2212\u221e, r ]) 2 (d) = d (Cv (C) , Cv (D)) 2 + (R \u2212 r) 2 = \u0393 (n) E 2 + \u0393 (n) A 2 = \u0393 (n) 2 2 where for two subsets X , Y \u2282 R M , d (X , Y) is the distance d (X , Y) \u2206 = inf\n\nMethodology\nIn order to stratify the pairs of nodes into connected or disconnected from the observed time series, we address the linear separability property of the covariance-based featuresT (n) ij ij\nestablished in Theorem 1, by studying the performance of trained classifiers, in particular, linear Support Vector Machines (SVMs) and Convolutional Neural Networks (CNNs). The training set is given byTr (n) \u2206 = T (n) ij , 1 {Aij =0} i =j\nwhere we have introduced the normalized feature vectorsT (n) ij := T (n) ij max i =j T (n) ij \u221e ,\nwith the unnormalized features given by,T (n) ij \u2206 = R \u2212100 (n) ij , R \u221299 (n) ij , . . . , R 100 (n) ij .\nIn other words, for training, we provide a normalized feature T\n(n) ij associated with the pair ij as input to a classifier and the output should be the ground truth1 {Aij =0} .\nThe normalization in the training set is motivated by the following observation. With infinitely many samples,T \u221e ij = \u03c3 2 R D ij , R D+1 ij , . . . , R M ij (18)\nwhere R k is the k-lag covariance matrix (equation ( 2)) of the normalized process (y(n)/\u03c3) n\u2208N , i.e., the process whose noise is normalized to unit variance. With the proposed normalization in equation ( 17), the multiplicative factor \u03c3 2 is cancelled out, which decreases the role played by the noiselevel in the performance of the trained CNNs. Furthermore, this normalization renders the generalization performance of the trained CNNs robust across structurally distinct graphs.\nTo generate the matrix A to obtain the time series data {y( )} n given a graph G, the following procedure was considered. Let G be a given graph without self-loops, i.e., G ii = 0 for all i. Define the interaction matrix A asA ij = \u03b1 1 Gij dmax(G) , for i = j A ii = \u03b1 \u2212 k =i A ik , for all i ,\nwhere d max (G) is the maximum in-flow degree of the underlying graph G and 0 < \u03b1 1 \u2264 \u03b1 < 1 are some constants. In other words, the rows of A sum to \u03b1 < 1 and its support is given by G. This is often cast as the Laplacian rule (Sayed 2014). This interaction matrix renders the networked dynamical system (1) stable and with a support graph of interactions given by G. To generate the support graph G, we considered the realization of random graph models as Erd\u0151s-R\u00e9nyi for undirected graphs, binomial random graphs for directed graphs, and real-world networks. We train the classifiers over one realization of a random graph model with p = 0.5 and N = 100 and apply them to distinct networks, including real-world ones, where p is the probability of edge or arrow drawing in the random graph model and N is the number of nodes. Throughout, we assume that we can only observe the time series data from |S| = 20 nodes, that is, we assume S = {1, 2, . . . , 20}.\n\nSimulation Results\nIn the numerical results considered, we define accuracy as the number of directed pairs correctly classified over the total number of directed pairs in the underlying graph. We consider 1000 Monte Carlo runs across all plots.\nFig. 3 (a) \u2212 (f ) depict the sample-complexity performance of the estimators across structurally distinct networks, considering: i) Granger under partial observabil-ity R 1 (n) S R 0 (n) S \u22121\nthat is provably structurally consistent (Santos, Matta, and Sayed 2020; Matta, Santos, and Sayed 2020) for distinct regimes of network connectivity; ii) The one-lag estimator R 1 (n), which is also consistent for several network connectivity regimes (Matta, Santos, and Sayed 2022); iii) the R 1 (n)\u2212 R 3 (n) that is structurally consistent (Chen, Wang, and Shen 2022); iv) the linear SVM; and v) the trained CNNs. For classification, we apply Gaussian mixture over the sorted entries of the matrix-valued estimators in order to stratify the connected and disconnected pairs. Our results show the overall superiority in performance for the CNN-based classifier. Figs. 3 ( Fig. 4a illustrates the robustness of both the trained CNNs and the linear SVMs against distinct noise-level regimes. The CNNs and SVMs are trained with a noise variance of \u03c3 2 = 0.5, but generalize well over an extended range of noise variance. Fig. 4a shows that the performance of these classifiers is not sensitive to the variance of the input noise in the dynamics (1). Fig. 4b shows the gain in performance when the Granger estimator is included in the feature vector. In particular, when we include in the feature vector that is the Granger under partial observability, with only |S| = 20 nodes observed. This is consistent with Lemma 2, motivating the search for feature vectors built on other matrix-valued structurally consistent estimators. It motivates the following causal inference paradigm: i) characterize matrix-valued structurally consistent estimators; ii) define feature vectors that collect these consistent estimators; iii) use these new features to train classifiers like a CNN.\n\nConcluding Remarks\nThis paper considered the problem of determining the graph that captures the fundamental dependencies among time series of data. These time series are indexed as nodes in linear stochastic networked dynamical systems. Only the time series of some nodes are observed (partial observability). We proposed a set of covariance-based features and proved they are consistently linearly separable. With this separability property, our features can be used as an input to a variety of machine learning pipelines in order to design new state-of-the-art algorithms for causal inference of linear networked dynamical systems. In particular, CNNs trained over this set of features exhibited remarkable sample-complexity performance, significantly reducing the number of samples required to reach a certain level of accuracy, as compared with other state-of-the-art estimators, which require a much larger number of samples. Simulation results show the superiority of the CNN-based approach. It was further shown that the inclusion of structurally consistent matrix-valued estimators in the feature vectors increases the performance of structure identification. This motivates further study of new structurally consistent matrix-valued estimators as building blocks for feature vectors or tensor-valued estimators.\n\nFootnotes:\n\nReferences:\n\n- Adams, J.; Hansen, N. R.; and Zhang, K. 2021. Identification of Partially Observed Causal Models: Graphical Conditions for the Linear Non-Gaussian and Heterogeneous Cases. In Advances in Neural Information Processing Systems 34 pre-proceedings (NeurIPS 2021), NeurIPS '21.- Anandkumar, A.; Hsu, D.; Javanmard, A.; and Kakade, S. 2013. Learning Linear Bayesian Networks with Latent Variables. In Pro- ceedings of the 30th International Conference on Machine Learn- ing, volume 28 of Proceedings of Machine Learning Research, 249-257. Atlanta, Georgia, USA: PMLR.\n\n- Anandkumar, A.; Tan, V. Y. F.; Huang, F.; and Willsky, A. S.\n\n- High-dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion. J. Mach. Learn. Res., 13(1): 2293-2337.\n\n- Anandkumar, A.; and Valluvan, R. 2013. Learning loopy graphi- cal models with latent variables: Efficient methods and guarantees. Ann. Statist., 41(2): 401-435.\n\n- Barrat, A.; Barth\u00e9lemy, M.; and Vespignani, A. 2012. Dynamical Processes on Complex Networks. London, UK: Cambridge Uni- versity Press. ISBN 9781107626256.\n\n- Bazzi, M.; Porter, M. A.; Williams, S.; McDonald, M.; Fenn, D. J.; and Howison, S. D. 2016. Community Detection in Temporal Mul- tilayer Networks, with an Application to Correlation Networks. Multiscale Modeling & Simulation, 14(1): 1-41.\n\n- Bogdanov, A.; Mossel, E.; and Vadhan, S. 2008. The Complex- ity of Distinguishing Markov Random Fields. In Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, 331-342. Berlin, Heidelberg: Springer Berlin Heidel- berg. ISBN 978-3-540-85363-3.\n\n- Braunstein, A.; Dall'Asta, L.; Semerjian, G.; and Zdeborov\u00e1, L.\n\n- Network dismantling. Proceedings of the National Academy of Sciences, 113(44): 12368-12373.\n\n- Bresler, G.; Gamarnik, D.; and Shah, D. 2014. Hardness Pa- rameter Estimation in Graphical Models. In Proceedings of the 27th International Conference on Neural Information Processing Systems -Volume 1, NIPS'14, 1062-1070. Cambridge, MA, USA: MIT Press.\n\n- Chandrasekaran, V.; Parrilo, P. A.; and Willsky, A. S. 2012. Latent variable graphical model selection via convex optimization. The Annals of Statistics, 40(4): 1935-1967.\n\n- Chen, Y.; Wang, Z.; and Shen, X. 2022. An Unbiased Symmetric Matrix Estimator for Topology Inference under Partial Observabil- ity. IEEE Signal Processing Letters, 29(02): 1257-1261.\n\n- Chickering, D. M. 2003. Optimal Structure Identification with Greedy Search. J. Mach. Learn. Res., 3(null): 507-554.\n\n- Ching, E. S. C.; and Tam, H. C. 2017. Reconstructing links in directed networks from noisy dynamics. Phys. Rev. E, 95: 010301.\n\n- Chow, C.; and Liu, C. 1968. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Infor- mation Theory, 14(3): 462-467.\n\n- Colombo, D.; Maathuis, M. H.; Kalisch, M.; and Richardson, T. S. 2012. Learning High-dimensional Directed Acyclic Graphs with Latent and Selection Variables. The Annals of Statistics, 40(1): 294-321.\n\n- Dax, A. 2006. The distance between two convex sets. Linear Alge- bra and its Applications, 416(1): 184-213. Special Issue devoted to the Haifa 2005 conference on matrix theory.\n\n- Douw, L.; De Groot, M.; Dellen, E.; Heimans, J.; Ronner, H.; Stam, C.; and Reijneveld, J. 2010. 'Functional Connectivity' is a Sensi- tive Predictor of Epilepsy Diagnosis after the First Seizure. PloS one, 5. Fenn, D. J.; Porter, M. A.; McDonald, M.; Williams, S.; Johnson, N. F.; and Jones, N. S. 2009. Dynamic communities in multichan- nel data: An application to the foreign exchange market during the 2007-2008 credit crisis. Chaos: An Interdisciplinary Journal of Nonlinear Science, 19(3): 033119. Fenn, D. J.; Porter, M. A.; Mucha, P. J.; McDonald, M.; Williams, S.; Johnson, N. F.; and Jones, N. S. 2012. Dynamical clustering of exchange rates. Quantitative Finance, 12(10): 1493-1520.\n\n- Ganesh, A.; Massouli\u00e9, L.; and Towsley, D. 2005. The effect of net- work topology on the spread of epidemics. In Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Commu- nications Societies., volume 2, 1455-1466.\n\n- Geiger, P.; Zhang, K.; Sch\u00f6lkopf, B.; Gong, M.; and Janzing, D. 2015. Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components. In Proc. International Con- ference on Machine Learning, volume 37, 1917-1925.\n\n- Granger, C. W. J. 1969. Investigating Causal Relations by Econo- metric Models and Cross-spectral Methods. Econometrica, 37(3): 424-438.\n\n- Hammersley, J. M.; and Clifford, P. 2021. Markov fields on finite graphs and lattices. University of Oxford. Hiriart-Urruty, J.-B.; and Lemar\u00e9chal, C. 2001. Fundamentals of Convex Analysis. Grundlehren Text Editions. Springer-Verlag Berlin Heidelberg. ISBN 978-3-540-42205-1.\n\n- Jalali, A.; and Sanghavi, S. 2012. Learning the Dependence Graph of Time Series with Latent Factors. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, 619-626. Madison, WI, USA: Omnipress. ISBN 9781450312851.\n\n- Kivits, E.; and Hof, P. M. V. d. 2022. Identification of diffusively coupled linear networks through structured polynomial models. IEEE Transactions on Automatic Control, 1-16.\n\n- Lahmanovich, A.; and James, A. Y. 1976. A Deterministic Model for Gonorrhea in a Nonhomogeneous Population. Mathematical Biosciences, 28: 221-236.\n\n- Lehnertz, K.; Br\u00f6hl, T.; and Rings, T. 2020. The Human Organ- ism as an Integrated Interaction Network: Recent Conceptual and Methodological Challenges. Frontiers in Physiology, 11. Li\u00e9geois, R.; Santos, A.; Matta, V.; Van De Ville, D.; and Sayed, A. H. 2020. Revisiting correlation-based functional connectivity and its relationship with structural connectivity. Network Neuro- science, 4(4): 1235-1251.\n\n- Liggett, T. 2005. Interacting Particle Systems. Springer-Verlag Berlin Heidelberg, first edition. ISBN 978-3-540-26962-5.\n\n- Lim, N.; d'Alch\u00e9 Buc, F.; Auliac, C.; and Michailidis, G. 2015. Operator-valued kernel-based vector autoregressive models for net- work inference. Machine Learning, 99(3): 489-513.\n\n- Mastakouri, A. A.; Sch\u00f6lkopf, B.; and Janzing, D. 2021. Necessary and sufficient conditions for causal feature selection in time series with latent common causes. In Proceedings of the 38th Interna- tional Conference on Machine Learning, volume 139 of Proceed- ings of Machine Learning Research, 7502-7511. PMLR.\n\n- Mateos, G.; Segarra, S.; Marques, A. G.; and Ribeiro, A. 2019. Connecting the Dots: Identifying Network Structure via Graph Sig- nal Processing. IEEE Signal Processing Magazine, 36(3): 16-43.\n\n- Materassi, D.; and Salapaka, M. V. 2012a. Network reconstruc- tion of dynamical polytrees with unobserved nodes. In Proc. IEEE Conference on Decision and Control (CDC), 4629-4634. Maui, Hawaii. Materassi, D.; and Salapaka, M. V. 2012b. On the problem of re- constructing an unknown topology via locality properties of the Wiener filter. IEEE Transactions on Automatic Control, 57(7): 1765-1777.\n\n- Materassi, D.; and Salapaka, M. V. 2015. Identification of net- work components in presence of unobserved nodes. In Proc. IEEE Conference on Decision and Control (CDC), 1563-1568. Osaka, Japan. Matta, V.; Santos, A.; and Sayed, A. H. 2020. Graph Learning under Partial Observability. Proceedings of the IEEE, 108: 2049 - 2066. Matta, V.; Santos, A.; and Sayed, A. H. 2022. Graph Learning over Partially Observed Diffusion Networks: Role of Degree Concen- tration. IEEE Open Journal of Signal Processing (Early Access), 1-34. Mauroy, A.; and Goncalves, J. 2016. Linear identification of non- linear systems: A lifting technique based on the Koopman operator. In 2016 IEEE 55th Conference on Decision and Control (CDC), 6500-6505. Las Vegas, USA.\n\n- Mei, J.; and Moura, J. M. F. 2017. Signal Processing on Graphs: Causal Modeling of Unstructured Data. IEEE Transactions on Sig- nal Processing, 65(8): 2077-2092.\n\n- Mei, J.; and Moura, J. M. F. 2018. SILVar: Single Index Latent Variable Models. IEEE Transactions on Signal Processing, 66(11): 2790-2803.\n\n- Monajemi, S.; Eftaxias, K.; Sanei, S.; and Ong, S. H. 2016. An In- formed Multitask Diffusion Adaptation Approach to Study Tremor in Parkinson's Disease. IEEE Journal of Selected Topics in Signal Processing, 10(7): 1306-1314.\n\n- Moneta, A.; Chla\u00df, N.; Entner, D.; and Hoyer, P. 2009. Causal Search in Structural Vector Autoregressive Models. In Proceed- ings of the 12th International Conference on Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series, 95-118. Vancouver, Canada. Napoletani, D.; and Sauer, T. D. 2008. Reconstructing the topology of sparsely connected dynamical networks. Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics, 77: 026103.\n\n- Oltra, J.; Campabadal Delgado, A.; Segura, B.; Uribe, C.; Marti, M.; Compta, Y.; Valldeoriola, F.; Bargall\u00f3, N.; Iranzo, A.; and Junqu\u00e9, C. 2021. Disrupted functional connectivity in PD with probable RBD and its cognitive correlates. Scientific Reports, 11. Pearl, J. 2009. Causality. Cambridge University Press, 2 edition. Pereira, J.; Ibrahimi, M.; and Montanari, A. 2010. Learning Net- works of Stochastic Differential Equations. In Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc. Porter, M.; and Gleeson, J. 2016. Dynamical Systems on Net- works: A Tutorial. Springer International Publishing. ISBN 9783319266411.\n\n- Ramsey, J.; Glymour, M.; Sanchez-Romero, R.; and Glymour, C. 2016. A million variables and more: the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance im- ages. International Journal of Data Science and Analytics, 3: 121- 129. Ranasinghe, K. G.; Hinkley, L. B.; Beagle, A. J.; Mizuiri, D.; Dowling, A. F.; Honma, S. M.; Finucane, M. M.; Scherling, C.; Miller, B. L.; Nagarajan, S. S.; and Vossel, K. A. 2014. Regional functional connectivity predicts distinct cognitive impairments in Alzheimer's disease spectrum. NeuroImage: Clinical, 5: 385-395.\n\n- Ren, X.-L.; Gleinig, N.; Helbing, D.; and Antulov-Fantulin, N. 2019. Generalized network dismantling. Proceedings of the Na- tional Academy of Sciences, 116(14): 6554-6559.\n\n- Robert, P. 2003. Stochastic Networks and Queues. Springer-Verlag. ISBN 978-3-540-00657-2.\n\n- Rossi, R. A.; and Ahmed, N. K. 2015. The Network Data Reposi- tory with Interactive Graph Analytics and Visualization. In AAAI. Sandryhaila, A.; and Moura, J. M. F. 2013. Discrete Signal Pro- cessing on Graphs. IEEE Transactions on Signal Processing, 61(7): 1644-1656.\n\n- Santos, A.; Matta, V.; and Sayed, A. H. 2020. Local Tomography of Large Networks under the Low-Observability Regime. IEEE Transactions on Information Theory, 66: 587 -613.\n\n- Santos, A.; Moura, J. M. F.; and Xavier, J. 2015. Bi-Virus SIS Epi- demics over Networks: Qualitative Analysis. IEEE Transactions on Network Science and Engineering, 2(1): 17-29.\n\n- Sayed, A. H. 2014. Adaptation, Learning, and Optimization over Networks. Found. Trends Mach. Learn., 7(4-5): 311-801.\n\n- Segarra, S.; Marques, A. G.; Mateos, G.; and Ribeiro, A. 2017. Network Topology Inference from Spectral Templates. IEEE Transactions on Signal and Information Processing over Networks, 3(3): 467-483.\n\n- Segarra, S.; Schaub, M. T.; and Jadbabaie, A. 2017. Network infer- ence from consensus dynamics. In 2017 IEEE 56th Annual Con- ference on Decision and Control (CDC), 3212-3217.\n\n- Spirtes, P.; and Glymour, C. 1991. An Algorithm for Fast Recovery of Sparse Causal Graphs. Social Science Computer Review, 9(1): 62-72.\n\n- Spirtes, P.; Glymour, C.; and Scheines, R. 2000. Causation, Pre- diction, and Search. MIT press, 2nd edition.\n\n- Spirtes, P.; Meek, C.; and Richardson, T. 1995. Causal Inference in the Presence of Latent Variables and Selection Bias. In Pro- ceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI'95, 499-506. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN 1558603859.\n\n- Stam, C.; Jones, B.; Nolte, G.; Breakspear, M.; and Scheltens, P. 2007. Small-World Networks and Functional Connectivity in Alzheimer's Disease. Cerebral Cortex, 17(1): 92-99.\n\n- Mierlo, P.; H\u00f6ller, Y.; Focke, N. K.; and Vulliemoz, S. 2019. Network Perspectives on Epilepsy Using EEG/MEG Source Con- nectivity. Frontiers in Neurology, 10.\n\n- Zhao, L.; and Wan, Y. 2022. Identifiability and Estimation of Partially-observed Influence Models. IEEE Control Systems Let- ters, 1-1.\n\n", "annotations": {"SectionMain": [{"begin": 1647, "end": 31027, "idx": 0}], "ReferenceToFormula": [{"begin": 25907, "end": 25908, "idx": 0}, {"begin": 26060, "end": 26062, "target": "#formula_37", "idx": 1}], "SectionReference": [{"begin": 31041, "end": 43583, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1647, "idx": 0}], "Div": [{"begin": 125, "end": 1639, "idx": 0}, {"begin": 1650, "end": 6443, "idx": 1}, {"begin": 6445, "end": 8046, "idx": 2}, {"begin": 8048, "end": 10132, "idx": 3}, {"begin": 10134, "end": 13478, "idx": 4}, {"begin": 13480, "end": 15205, "idx": 5}, {"begin": 15207, "end": 15910, "idx": 6}, {"begin": 15912, "end": 18769, "idx": 7}, {"begin": 18771, "end": 24865, "idx": 8}, {"begin": 24867, "end": 27592, "idx": 9}, {"begin": 27594, "end": 29705, "idx": 10}, {"begin": 29707, "end": 31027, "idx": 11}], "Head": [{"begin": 1650, "end": 1662, "idx": 0}, {"begin": 6445, "end": 6457, "idx": 1}, {"begin": 8048, "end": 8066, "idx": 2}, {"begin": 10134, "end": 10155, "idx": 3}, {"begin": 13480, "end": 13499, "idx": 4}, {"begin": 15207, "end": 15229, "idx": 5}, {"begin": 15912, "end": 15913, "idx": 6}, {"begin": 18771, "end": 18792, "idx": 7}, {"begin": 24867, "end": 24878, "idx": 8}, {"begin": 27594, "end": 27612, "idx": 9}, {"begin": 29707, "end": 29725, "idx": 10}], "Paragraph": [{"begin": 125, "end": 1639, "idx": 0}, {"begin": 1663, "end": 2253, "idx": 1}, {"begin": 2254, "end": 3916, "idx": 2}, {"begin": 3917, "end": 6443, "idx": 3}, {"begin": 6458, "end": 8046, "idx": 4}, {"begin": 8067, "end": 8972, "idx": 5}, {"begin": 8973, "end": 10132, "idx": 6}, {"begin": 10156, "end": 11174, "idx": 7}, {"begin": 11175, "end": 13478, "idx": 8}, {"begin": 13500, "end": 13546, "idx": 9}, {"begin": 13575, "end": 13580, "idx": 10}, {"begin": 13636, "end": 14162, "idx": 11}, {"begin": 14163, "end": 14360, "idx": 12}, {"begin": 14428, "end": 14486, "idx": 13}, {"begin": 14487, "end": 14496, "idx": 14}, {"begin": 14693, "end": 14838, "idx": 15}, {"begin": 15043, "end": 15205, "idx": 16}, {"begin": 15230, "end": 15279, "idx": 17}, {"begin": 15310, "end": 15407, "idx": 18}, {"begin": 15444, "end": 15588, "idx": 19}, {"begin": 15647, "end": 15910, "idx": 20}, {"begin": 15914, "end": 16340, "idx": 21}, {"begin": 16374, "end": 16497, "idx": 22}, {"begin": 16529, "end": 16815, "idx": 23}, {"begin": 16945, "end": 17244, "idx": 24}, {"begin": 17245, "end": 17525, "idx": 25}, {"begin": 17526, "end": 17697, "idx": 26}, {"begin": 17759, "end": 17820, "idx": 27}, {"begin": 17829, "end": 18340, "idx": 28}, {"begin": 18446, "end": 18539, "idx": 29}, {"begin": 18599, "end": 18769, "idx": 30}, {"begin": 18793, "end": 18905, "idx": 31}, {"begin": 18968, "end": 19099, "idx": 32}, {"begin": 19124, "end": 19205, "idx": 33}, {"begin": 19206, "end": 19285, "idx": 34}, {"begin": 19350, "end": 19764, "idx": 35}, {"begin": 20018, "end": 20164, "idx": 36}, {"begin": 20165, "end": 20467, "idx": 37}, {"begin": 20541, "end": 20588, "idx": 38}, {"begin": 20589, "end": 20643, "idx": 39}, {"begin": 20795, "end": 20882, "idx": 40}, {"begin": 20883, "end": 21110, "idx": 41}, {"begin": 21183, "end": 21613, "idx": 42}, {"begin": 21652, "end": 21914, "idx": 43}, {"begin": 21915, "end": 22025, "idx": 44}, {"begin": 22091, "end": 22692, "idx": 45}, {"begin": 22693, "end": 22983, "idx": 46}, {"begin": 23036, "end": 23217, "idx": 47}, {"begin": 23218, "end": 23456, "idx": 48}, {"begin": 23831, "end": 23832, "idx": 49}, {"begin": 23833, "end": 23991, "idx": 50}, {"begin": 23992, "end": 24139, "idx": 51}, {"begin": 24166, "end": 24234, "idx": 52}, {"begin": 24261, "end": 24314, "idx": 53}, {"begin": 24372, "end": 24584, "idx": 54}, {"begin": 24879, "end": 25057, "idx": 55}, {"begin": 25069, "end": 25270, "idx": 56}, {"begin": 25308, "end": 25363, "idx": 57}, {"begin": 25406, "end": 25446, "idx": 58}, {"begin": 25513, "end": 25576, "idx": 59}, {"begin": 25577, "end": 25678, "idx": 60}, {"begin": 25691, "end": 25801, "idx": 61}, {"begin": 25854, "end": 26337, "idx": 62}, {"begin": 26338, "end": 26563, "idx": 63}, {"begin": 26633, "end": 27592, "idx": 64}, {"begin": 27613, "end": 27838, "idx": 65}, {"begin": 27839, "end": 28004, "idx": 66}, {"begin": 28031, "end": 29705, "idx": 67}, {"begin": 29726, "end": 31027, "idx": 68}], "ReferenceToBib": [{"begin": 1884, "end": 1925, "target": "#b5", "idx": 0}, {"begin": 1926, "end": 1939, "target": "#b27", "idx": 1}, {"begin": 1940, "end": 1952, "target": "#b40", "idx": 2}, {"begin": 1953, "end": 1977, "idx": 3}, {"begin": 2616, "end": 2644, "target": "#b25", "idx": 4}, {"begin": 2645, "end": 2681, "target": "#b19", "idx": 5}, {"begin": 2682, "end": 2713, "target": "#b43", "idx": 6}, {"begin": 2714, "end": 2737, "idx": 7}, {"begin": 2738, "end": 2754, "target": "#b39", "idx": 8}, {"begin": 2933, "end": 2957, "idx": 9}, {"begin": 2958, "end": 2973, "target": "#b39", "idx": 10}, {"begin": 3338, "end": 3360, "target": "#b26", "idx": 11}, {"begin": 3513, "end": 3531, "target": "#b18", "idx": 12}, {"begin": 3532, "end": 3555, "target": "#b38", "idx": 13}, {"begin": 3556, "end": 3574, "target": "#b37", "idx": 14}, {"begin": 3575, "end": 3592, "target": "#b50", "idx": 15}, {"begin": 3593, "end": 3613, "target": "#b35", "idx": 16}, {"begin": 3615, "end": 3638, "idx": 17}, {"begin": 3639, "end": 3671, "target": "#b26", "idx": 18}, {"begin": 3857, "end": 3874, "target": "#b18", "idx": 19}, {"begin": 3875, "end": 3896, "target": "#b18", "idx": 20}, {"begin": 3897, "end": 3915, "target": "#b6", "idx": 21}, {"begin": 4287, "end": 4314, "target": "#b12", "idx": 22}, {"begin": 4315, "end": 4353, "target": "#b37", "idx": 23}, {"begin": 4354, "end": 4382, "target": "#b32", "idx": 24}, {"begin": 4383, "end": 4413, "target": "#b42", "idx": 25}, {"begin": 4725, "end": 4745, "target": "#b14", "idx": 26}, {"begin": 4746, "end": 4772, "idx": 27}, {"begin": 4831, "end": 4848, "target": "#b28", "idx": 28}, {"begin": 4849, "end": 4875, "idx": 29}, {"begin": 5727, "end": 5746, "target": "#b15", "idx": 30}, {"begin": 6742, "end": 6754, "idx": 31}, {"begin": 6927, "end": 6957, "target": "#b22", "idx": 32}, {"begin": 6958, "end": 6969, "idx": 33}, {"begin": 7753, "end": 7773, "target": "#b20", "idx": 34}, {"begin": 7774, "end": 7812, "target": "#b37", "idx": 35}, {"begin": 7813, "end": 7843, "target": "#b42", "idx": 36}, {"begin": 7875, "end": 7904, "target": "#b32", "idx": 37}, {"begin": 8189, "end": 8226, "target": "#b48", "idx": 38}, {"begin": 8231, "end": 8257, "target": "#b47", "idx": 39}, {"begin": 8263, "end": 8280, "target": "#b13", "idx": 40}, {"begin": 8290, "end": 8310, "target": "#b38", "idx": 41}, {"begin": 8511, "end": 8535, "idx": 42}, {"begin": 8902, "end": 8936, "target": "#b10", "idx": 43}, {"begin": 8937, "end": 8971, "target": "#b7", "idx": 44}, {"begin": 9054, "end": 9073, "target": "#b30", "idx": 45}, {"begin": 9350, "end": 9370, "target": "#b33", "idx": 46}, {"begin": 9371, "end": 9390, "target": "#b36", "idx": 47}, {"begin": 9391, "end": 9411, "target": "#b24", "idx": 48}, {"begin": 9858, "end": 9872, "target": "#b21", "idx": 49}, {"begin": 9873, "end": 9909, "target": "#b46", "idx": 50}, {"begin": 9910, "end": 9930, "idx": 51}, {"begin": 10514, "end": 10550, "target": "#b49", "idx": 52}, {"begin": 10560, "end": 10581, "target": "#b16", "idx": 53}, {"begin": 10954, "end": 10978, "idx": 54}, {"begin": 10979, "end": 11008, "target": "#b4", "idx": 55}, {"begin": 11101, "end": 11132, "target": "#b0", "idx": 56}, {"begin": 11133, "end": 11173, "target": "#b29", "idx": 57}, {"begin": 11207, "end": 11221, "idx": 58}, {"begin": 11222, "end": 11245, "idx": 59}, {"begin": 11403, "end": 11422, "target": "#b20", "idx": 60}, {"begin": 11937, "end": 11980, "target": "#b11", "idx": 61}, {"begin": 11981, "end": 12006, "target": "#b23", "idx": 62}, {"begin": 12007, "end": 12026, "target": "#b34", "idx": 63}, {"begin": 12155, "end": 12186, "target": "#b42", "idx": 64}, {"begin": 12187, "end": 12205, "target": "#b42", "idx": 65}, {"begin": 12206, "end": 12223, "idx": 66}, {"begin": 12539, "end": 12569, "target": "#b4", "idx": 67}, {"begin": 12770, "end": 12797, "target": "#b12", "idx": 68}, {"begin": 16670, "end": 16697, "target": "#b12", "idx": 69}, {"begin": 17212, "end": 17243, "target": "#b42", "idx": 70}, {"begin": 20616, "end": 20643, "target": "#b12", "idx": 71}, {"begin": 26860, "end": 26872, "target": "#b44", "idx": 72}, {"begin": 28072, "end": 28103, "target": "#b42", "idx": 73}, {"begin": 28104, "end": 28134, "target": "#b42", "idx": 74}], "Sentence": [{"begin": 125, "end": 245, "idx": 0}, {"begin": 246, "end": 354, "idx": 1}, {"begin": 355, "end": 475, "idx": 2}, {"begin": 476, "end": 758, "idx": 3}, {"begin": 759, "end": 856, "idx": 4}, {"begin": 857, "end": 940, "idx": 5}, {"begin": 941, "end": 1030, "idx": 6}, {"begin": 1031, "end": 1048, "idx": 7}, {"begin": 1049, "end": 1161, "idx": 8}, {"begin": 1162, "end": 1294, "idx": 9}, {"begin": 1295, "end": 1515, "idx": 10}, {"begin": 1516, "end": 1639, "idx": 11}, {"begin": 1663, "end": 1752, "idx": 12}, {"begin": 1753, "end": 1978, "idx": 13}, {"begin": 1979, "end": 2149, "idx": 14}, {"begin": 2150, "end": 2253, "idx": 15}, {"begin": 2254, "end": 2477, "idx": 16}, {"begin": 2478, "end": 2755, "idx": 17}, {"begin": 2756, "end": 3362, "idx": 18}, {"begin": 3363, "end": 3916, "idx": 19}, {"begin": 3917, "end": 4140, "idx": 20}, {"begin": 4141, "end": 4414, "idx": 21}, {"begin": 4415, "end": 4876, "idx": 22}, {"begin": 4877, "end": 5196, "idx": 23}, {"begin": 5197, "end": 5387, "idx": 24}, {"begin": 5388, "end": 5456, "idx": 25}, {"begin": 5457, "end": 5747, "idx": 26}, {"begin": 5748, "end": 5854, "idx": 27}, {"begin": 5855, "end": 6002, "idx": 28}, {"begin": 6003, "end": 6161, "idx": 29}, {"begin": 6162, "end": 6281, "idx": 30}, {"begin": 6282, "end": 6443, "idx": 31}, {"begin": 6458, "end": 6640, "idx": 32}, {"begin": 6641, "end": 6755, "idx": 33}, {"begin": 6756, "end": 6970, "idx": 34}, {"begin": 6971, "end": 7081, "idx": 35}, {"begin": 7082, "end": 7140, "idx": 36}, {"begin": 7141, "end": 7348, "idx": 37}, {"begin": 7349, "end": 7575, "idx": 38}, {"begin": 7576, "end": 7905, "idx": 39}, {"begin": 7906, "end": 8046, "idx": 40}, {"begin": 8067, "end": 8084, "idx": 41}, {"begin": 8085, "end": 8122, "idx": 42}, {"begin": 8123, "end": 8311, "idx": 43}, {"begin": 8312, "end": 8501, "idx": 44}, {"begin": 8502, "end": 8631, "idx": 45}, {"begin": 8632, "end": 8702, "idx": 46}, {"begin": 8703, "end": 8972, "idx": 47}, {"begin": 8973, "end": 9001, "idx": 48}, {"begin": 9002, "end": 9412, "idx": 49}, {"begin": 9413, "end": 9623, "idx": 50}, {"begin": 9624, "end": 9812, "idx": 51}, {"begin": 9813, "end": 9931, "idx": 52}, {"begin": 9932, "end": 10132, "idx": 53}, {"begin": 10156, "end": 10173, "idx": 54}, {"begin": 10174, "end": 10420, "idx": 55}, {"begin": 10421, "end": 10582, "idx": 56}, {"begin": 10583, "end": 10767, "idx": 57}, {"begin": 10768, "end": 11174, "idx": 58}, {"begin": 11175, "end": 11203, "idx": 59}, {"begin": 11204, "end": 11318, "idx": 60}, {"begin": 11319, "end": 11399, "idx": 61}, {"begin": 11400, "end": 11635, "idx": 62}, {"begin": 11636, "end": 11751, "idx": 63}, {"begin": 11752, "end": 12144, "idx": 64}, {"begin": 12145, "end": 12527, "idx": 65}, {"begin": 12528, "end": 12759, "idx": 66}, {"begin": 12760, "end": 13173, "idx": 67}, {"begin": 13174, "end": 13367, "idx": 68}, {"begin": 13368, "end": 13478, "idx": 69}, {"begin": 13500, "end": 13546, "idx": 70}, {"begin": 13575, "end": 13580, "idx": 71}, {"begin": 13636, "end": 14052, "idx": 72}, {"begin": 14053, "end": 14162, "idx": 73}, {"begin": 14163, "end": 14360, "idx": 74}, {"begin": 14428, "end": 14486, "idx": 75}, {"begin": 14487, "end": 14496, "idx": 76}, {"begin": 14693, "end": 14838, "idx": 77}, {"begin": 15043, "end": 15201, "idx": 78}, {"begin": 15202, "end": 15205, "idx": 79}, {"begin": 15230, "end": 15279, "idx": 80}, {"begin": 15310, "end": 15350, "idx": 81}, {"begin": 15351, "end": 15407, "idx": 82}, {"begin": 15444, "end": 15588, "idx": 83}, {"begin": 15647, "end": 15818, "idx": 84}, {"begin": 15819, "end": 15910, "idx": 85}, {"begin": 15914, "end": 15995, "idx": 86}, {"begin": 15996, "end": 16046, "idx": 87}, {"begin": 16047, "end": 16340, "idx": 88}, {"begin": 16374, "end": 16497, "idx": 89}, {"begin": 16529, "end": 16557, "idx": 90}, {"begin": 16558, "end": 16698, "idx": 91}, {"begin": 16699, "end": 16815, "idx": 92}, {"begin": 16945, "end": 17244, "idx": 93}, {"begin": 17245, "end": 17391, "idx": 94}, {"begin": 17392, "end": 17525, "idx": 95}, {"begin": 17526, "end": 17697, "idx": 96}, {"begin": 17759, "end": 17820, "idx": 97}, {"begin": 17829, "end": 17996, "idx": 98}, {"begin": 17997, "end": 18047, "idx": 99}, {"begin": 18048, "end": 18340, "idx": 100}, {"begin": 18446, "end": 18539, "idx": 101}, {"begin": 18599, "end": 18649, "idx": 102}, {"begin": 18650, "end": 18769, "idx": 103}, {"begin": 18793, "end": 18905, "idx": 104}, {"begin": 18968, "end": 19050, "idx": 105}, {"begin": 19051, "end": 19099, "idx": 106}, {"begin": 19124, "end": 19205, "idx": 107}, {"begin": 19206, "end": 19214, "idx": 108}, {"begin": 19215, "end": 19285, "idx": 109}, {"begin": 19350, "end": 19673, "idx": 110}, {"begin": 19674, "end": 19764, "idx": 111}, {"begin": 20018, "end": 20028, "idx": 112}, {"begin": 20029, "end": 20164, "idx": 113}, {"begin": 20165, "end": 20175, "idx": 114}, {"begin": 20176, "end": 20453, "idx": 115}, {"begin": 20454, "end": 20467, "idx": 116}, {"begin": 20541, "end": 20588, "idx": 117}, {"begin": 20589, "end": 20595, "idx": 118}, {"begin": 20596, "end": 20643, "idx": 119}, {"begin": 20795, "end": 20882, "idx": 120}, {"begin": 20883, "end": 20932, "idx": 121}, {"begin": 20933, "end": 21110, "idx": 122}, {"begin": 21183, "end": 21240, "idx": 123}, {"begin": 21241, "end": 21326, "idx": 124}, {"begin": 21327, "end": 21472, "idx": 125}, {"begin": 21473, "end": 21571, "idx": 126}, {"begin": 21572, "end": 21613, "idx": 127}, {"begin": 21652, "end": 21745, "idx": 128}, {"begin": 21746, "end": 21914, "idx": 129}, {"begin": 21915, "end": 22025, "idx": 130}, {"begin": 22091, "end": 22225, "idx": 131}, {"begin": 22226, "end": 22467, "idx": 132}, {"begin": 22468, "end": 22561, "idx": 133}, {"begin": 22562, "end": 22692, "idx": 134}, {"begin": 22693, "end": 22966, "idx": 135}, {"begin": 22967, "end": 22983, "idx": 136}, {"begin": 23036, "end": 23217, "idx": 137}, {"begin": 23218, "end": 23456, "idx": 138}, {"begin": 23831, "end": 23832, "idx": 139}, {"begin": 23833, "end": 23991, "idx": 140}, {"begin": 23992, "end": 23998, "idx": 141}, {"begin": 23999, "end": 24132, "idx": 142}, {"begin": 24133, "end": 24139, "idx": 143}, {"begin": 24166, "end": 24234, "idx": 144}, {"begin": 24261, "end": 24296, "idx": 145}, {"begin": 24297, "end": 24314, "idx": 146}, {"begin": 24372, "end": 24571, "idx": 147}, {"begin": 24572, "end": 24584, "idx": 148}, {"begin": 24879, "end": 25057, "idx": 149}, {"begin": 25069, "end": 25241, "idx": 150}, {"begin": 25242, "end": 25270, "idx": 151}, {"begin": 25308, "end": 25363, "idx": 152}, {"begin": 25406, "end": 25446, "idx": 153}, {"begin": 25513, "end": 25576, "idx": 154}, {"begin": 25577, "end": 25678, "idx": 155}, {"begin": 25691, "end": 25771, "idx": 156}, {"begin": 25772, "end": 25801, "idx": 157}, {"begin": 25854, "end": 26013, "idx": 158}, {"begin": 26014, "end": 26202, "idx": 159}, {"begin": 26203, "end": 26337, "idx": 160}, {"begin": 26338, "end": 26459, "idx": 161}, {"begin": 26460, "end": 26528, "idx": 162}, {"begin": 26529, "end": 26563, "idx": 163}, {"begin": 26633, "end": 26744, "idx": 164}, {"begin": 26745, "end": 26818, "idx": 165}, {"begin": 26819, "end": 26873, "idx": 166}, {"begin": 26874, "end": 27193, "idx": 167}, {"begin": 27194, "end": 27460, "idx": 168}, {"begin": 27461, "end": 27585, "idx": 169}, {"begin": 27586, "end": 27592, "idx": 170}, {"begin": 27613, "end": 27786, "idx": 171}, {"begin": 27787, "end": 27838, "idx": 172}, {"begin": 27839, "end": 28004, "idx": 173}, {"begin": 28031, "end": 28446, "idx": 174}, {"begin": 28447, "end": 28607, "idx": 175}, {"begin": 28608, "end": 28693, "idx": 176}, {"begin": 28694, "end": 28821, "idx": 177}, {"begin": 28822, "end": 28949, "idx": 178}, {"begin": 28950, "end": 29078, "idx": 179}, {"begin": 29079, "end": 29178, "idx": 180}, {"begin": 29179, "end": 29315, "idx": 181}, {"begin": 29316, "end": 29455, "idx": 182}, {"begin": 29456, "end": 29705, "idx": 183}, {"begin": 29726, "end": 29854, "idx": 184}, {"begin": 29855, "end": 29943, "idx": 185}, {"begin": 29944, "end": 30016, "idx": 186}, {"begin": 30017, "end": 30116, "idx": 187}, {"begin": 30117, "end": 30340, "idx": 188}, {"begin": 30341, "end": 30637, "idx": 189}, {"begin": 30638, "end": 30704, "idx": 190}, {"begin": 30705, "end": 30874, "idx": 191}, {"begin": 30875, "end": 31027, "idx": 192}], "ReferenceToFigure": [{"begin": 5393, "end": 5394, "target": "#fig_0", "idx": 0}, {"begin": 14483, "end": 14484, "target": "#fig_0", "idx": 1}, {"begin": 22898, "end": 22899, "target": "#fig_1", "idx": 2}, {"begin": 27844, "end": 27845, "target": "#fig_2", "idx": 3}, {"begin": 28700, "end": 28703, "target": "#fig_2", "idx": 4}, {"begin": 28709, "end": 28711, "target": "#fig_4", "idx": 5}, {"begin": 28955, "end": 28957, "target": "#fig_4", "idx": 6}, {"begin": 29084, "end": 29086, "target": "#fig_4", "idx": 7}], "Abstract": [{"begin": 115, "end": 1639, "idx": 0}], "SectionFootnote": [{"begin": 31029, "end": 31039, "idx": 0}], "ReferenceString": [{"begin": 31056, "end": 31328, "id": "b0", "idx": 0}, {"begin": 31330, "end": 31617, "id": "b1", "idx": 1}, {"begin": 31621, "end": 31681, "id": "b2", "idx": 2}, {"begin": 31685, "end": 31826, "id": "b3", "idx": 3}, {"begin": 31830, "end": 31990, "id": "b4", "idx": 4}, {"begin": 31994, "end": 32149, "id": "b5", "idx": 5}, {"begin": 32153, "end": 32391, "id": "b6", "idx": 6}, {"begin": 32395, "end": 32672, "id": "b7", "idx": 7}, {"begin": 32676, "end": 32739, "id": "b8", "idx": 8}, {"begin": 32743, "end": 32834, "id": "b9", "idx": 9}, {"begin": 32838, "end": 33091, "id": "b10", "idx": 10}, {"begin": 33095, "end": 33266, "id": "b11", "idx": 11}, {"begin": 33270, "end": 33452, "id": "b12", "idx": 12}, {"begin": 33456, "end": 33572, "id": "b13", "idx": 13}, {"begin": 33576, "end": 33702, "id": "b14", "idx": 14}, {"begin": 33706, "end": 33864, "id": "b15", "idx": 15}, {"begin": 33868, "end": 34067, "id": "b16", "idx": 16}, {"begin": 34071, "end": 34247, "id": "b17", "idx": 17}, {"begin": 34251, "end": 34943, "id": "b18", "idx": 18}, {"begin": 34947, "end": 35180, "id": "b19", "idx": 19}, {"begin": 35184, "end": 35426, "id": "b20", "idx": 20}, {"begin": 35430, "end": 35566, "id": "b21", "idx": 21}, {"begin": 35570, "end": 35845, "id": "b22", "idx": 22}, {"begin": 35849, "end": 36116, "id": "b23", "idx": 23}, {"begin": 36120, "end": 36296, "id": "b24", "idx": 24}, {"begin": 36300, "end": 36446, "id": "b25", "idx": 25}, {"begin": 36450, "end": 36854, "id": "b26", "idx": 26}, {"begin": 36858, "end": 36979, "id": "b27", "idx": 27}, {"begin": 36983, "end": 37163, "id": "b28", "idx": 28}, {"begin": 37167, "end": 37479, "id": "b29", "idx": 29}, {"begin": 37483, "end": 37674, "id": "b30", "idx": 30}, {"begin": 37678, "end": 38072, "id": "b31", "idx": 31}, {"begin": 38076, "end": 38820, "id": "b32", "idx": 32}, {"begin": 38824, "end": 38985, "id": "b33", "idx": 33}, {"begin": 38989, "end": 39127, "id": "b34", "idx": 34}, {"begin": 39131, "end": 39356, "id": "b35", "idx": 35}, {"begin": 39360, "end": 39835, "id": "b36", "idx": 36}, {"begin": 39839, "end": 40494, "id": "b37", "idx": 37}, {"begin": 40498, "end": 41145, "id": "b38", "idx": 38}, {"begin": 41149, "end": 41321, "id": "b39", "idx": 39}, {"begin": 41325, "end": 41414, "id": "b40", "idx": 40}, {"begin": 41418, "end": 41686, "id": "b41", "idx": 41}, {"begin": 41690, "end": 41861, "id": "b42", "idx": 42}, {"begin": 41865, "end": 42043, "id": "b43", "idx": 43}, {"begin": 42047, "end": 42164, "id": "b44", "idx": 44}, {"begin": 42168, "end": 42367, "id": "b45", "idx": 45}, {"begin": 42371, "end": 42547, "id": "b46", "idx": 46}, {"begin": 42551, "end": 42686, "id": "b47", "idx": 47}, {"begin": 42690, "end": 42799, "id": "b48", "idx": 48}, {"begin": 42803, "end": 43100, "id": "b49", "idx": 49}, {"begin": 43104, "end": 43279, "id": "b50", "idx": 50}, {"begin": 43283, "end": 43442, "id": "b51", "idx": 51}, {"begin": 43446, "end": 43581, "id": "b52", "idx": 52}]}}