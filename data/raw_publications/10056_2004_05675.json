{"text": "A Non-Parametric Test to Detect Data-Copying in Generative Models\n\nAbstract:\nDetecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call data-copying -where the generative model memorizes and outputs training samples or small variations thereof. We provide a three sample non-parametric test for detecting data-copying that uses the training set, a separate sample from the target distribution, and a generated sample from the model, and study the performance of our test on several canonical models and datasets.\n\nMain:\n\n\n\n1 Introduction\nOverfitting is a basic stumbling block of any learning process. While it has been studied in great detail in the context of supervised learning, it has received much less attention in the unsupervised setting, despite being just as much of a problem.\nTo start with a simple example, consider a classical kernel density estimator (KDE), which given data x 1 , . . . , x n \u2208 R d , constructs a distribution over R d by placing a Gaussian of width \u03c3 > 0 at each of these points, yielding the densityq \u03c3 (x) = 1 (2\u03c0) d/2 \u03c3 d n n i=1 exp \u2212 x \u2212 x i 2 2\u03c3 2 . (1)\nThe only parameter is the scalar \u03c3. Setting it too small makes q(x) too concentrated around the given points: a clear case of overfitting (see Appendix Figure 6). This cannot be avoided by choosing the \u03c3 that maximizes the log likelihood on the training data, since in the limit \u03c3 \u2192 0, this likelihood goes to \u221e. The classical solution is to find a parameter \u03c3 that has a low generalization gap -that is, a low gap between the training log-likelihood and the log-likelihood on a held-out validation set. This method however often does not apply to the more complex generative models that have emerged over the past decade or so, such as Variational Auto Encoders (VAEs) (Kingma and Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). These models easily involve millions of parameters, and hence overfitting is a serious concern. Yet, a major challenge in evaluating overfitting is that these models do not offer exact, tractable likelihoods. VAEs can tractably provide a log-likelihood lower bound, while GANs have no accompanying density estimate at all. Thus any method that can assess these generative models must be based only on the samples produced.\nA body of prior work has provided tests for evaluating generative models based on samples drawn from them (Salimans et al., 2016; Sajjadi et al., 2018; Wu et al., 2017; Heusel et al., 2017); however, the vast majority of these tests focus on 'mode dropping' and 'mode collapse': the tendency for a generative model to either merge or delete high-density modes of the true distribution. A generative model that simply reproduces the training set or minor variations thereof will pass most of these tests.\nIn contrast, this work formalizes and investigates a type of overfitting that we call 'data-copying': the propensity of a generative model to recreate minute variations of a subset of training examples it has seen, rather than represent the true diversity of the data distribution. An example is shown in Figure 1b; in the top region of the instance space, the generative model data-copies, or creates samples that are very close to the training samples; meanwhile, in the bottom region, it underfits.\nTo detect this, we introduce a test that relies on three independent samples: the original training sample used to produce the generative model; a separate (held-out) test sample from the underlying distribution; and a synthetic sample drawn from the generator.\nOur key insight is that an overfit generative model would produce samples that are too close to the train-  ing samples -closer on average than an independently drawn test sample from the same distribution. Thus, if a suitable distance function is available, then we can test for data-copying by testing whether the distances to the closest point in the training sample are on average smaller for the generated sample than for the test sample.\nA further complication is that modern generative models tend to behave differently in different regions of space; a configuration as in Figure 1b for example could cause a global test to fail. To address this, we use ideas from the design of non-parametric methods. We divide the instance space into cells, conduct our test separately in each cell, and then combine the results to get a sense of the average degree of data-copying.\nFinally, we explore our test experimentally on a variety of illustrative data sets and generative models. Our results demonstrate that given enough samples, our test can successfully detect data-copying in a broad range of settings.\n\n1.1 Related work\nThere has been a large body of prior work on the evaluation of generative models (Salimans et al., 2016; Lopez-Paz and Oquab, 2016; Richardson and Weiss, 2018; Sajjadi et al., 2018; Xu et al., 2018; Wu et al., 2017). Most are geared to detect some form of modecollapse or mode-dropping: the tendency to either merge or delete high-density regions of the training data. Consequently, they fail to detect even the simplest case of extreme data-copying -where a generative model memorizes and exactly reproduces a bootstrap sample from the training set. We discuss below a few such canonical tests.\nTo-date there is a wealth of techniques for evaluating whether a model mode-drops or -collapses. Tests like the popular Inception Score (IS), Frech\u00e9t Inception Distance (FID) (Heusel et al., 2017), Precision and Recall test (Sajjadi et al., 2018), and extensions thereof (Kynk\u00e4\u00e4nniemi et al., 2019; Che et al., 2016) all work by embedding samples using the features of a discriminative network such as 'InceptionV3' and checking whether the training and generated samples are similar in aggregate.\n\n2 Preliminaries\nWe begin by introducing some notation and formalizing the definitions of overfitting. Let X denote an instance space in which data points lie, and P an unknown underlying distribution on this space. A training set T is drawn from P and is used to build a generative model Q. We then wish to assess whether Q is the result of overfitting: that is, whether Q produces samples that are too close to the training data. To help ascertain this, we are able to draw two additional samples:\n\u2022 A fresh sample of n points from P ; call this P n .\n\u2022 A sample of m points from Q; call this Q m .\nAs illustrated in Figures 1a, 1b, a generative model can overfit locally in a region C \u2286 X . To characterize this, for any distribution D on X , we use D| C denote its restriction to the region C, that is,D| C (A) = D(A \u2229 C) D(C)\nfor any A \u2286 X .\n\n2.1 Definitions of Overfitting\nWe now formalize the notion of data-copying, and illustrate its distinction from other types of overfitting.\nIntuitively, data-copying refers to situations where Q is \"too close\" to the training set T ; that is, closer to T than the target distribution P happens to be. We make this quantitative by choosing a distance function d : X \u2192 R from points in X to the training set, for instance, d(x) = min t\u2208T x \u2212 t 2 , if X is a subset of Euclidean space.\nIdeally, we desire that Q's expected distance to the training set is the same as that of P 's, namelyE X\u223cP [d(X)] = E Y \u223cQ [d(Y )].\nWe may rewrite this as follows: given any distribution D over X , define L(D) to be the one-dimensional distribution of d(X) for X \u223c D. We consider data-copying to have occurred if random draws from L(P ) are systematically larger than from L(Q). The above equalized expected distance condition can be rewritten asE Y \u223cQ [d(Y )] \u2212 E X\u223cP [d(X)] = EA\u223cL(P) B\u223cL(Q) [B \u2212 A] = 0\n(2) However, we are less interested in how large the difference is, and more in how often B is larger than A. Let\u2206 T (P, Q) = Pr B > A B \u223c L(Q), A \u223c L(P )\nwhere 0 \u2264 \u2206 T (P, Q) \u2264 1 represents how 'far' Q is from training sample T as compared to true distribution P . A more interpretable yet equally meaningful condition is\u2206 T (P, Q) = EA\u223cL(P) B\u223cL(Q) [1 B>A ] \u2248 1 2\nwhich guarantees (2) if densities L(P ) and L(Q) have the same shape, but could plausibly be mean-shifted.\nIf \u2206 T (P, Q)\n1 2 , Q is data-copying training set T , since samples from Q are systematically closer to T than are samples from P . However, even if \u2206 T (P, Q) \u2265 1 2 , Q may still be data-copying. As exhibited in Figures 1b and 1c, a model Q may data-copy in one region and underfit in others. In this case, Q may be further from T than is P globally, but much closer to T locally. As such, we consider Q to be data-copying if it is overfit in a subset C \u2286 X : Definition 2.1 (Data-Copying). A generative model Q is data-copying training set T if, in some region C \u2286 X , it is systematically closer to T by distance metric d : X \u2192 R than are samples from P . Specifically, if\u2206 T (P | C , Q| C ) < 1 2\nObserve that data-copying is orthogonal to the type of overfitting addressed by many previous works (Heusel et al., 2017; Sajjadi et al., 2018), which we call 'overrepresentation'. There, Q overemphasizes some region of the instance space C \u2286 X , often a region of high density in the training set T . For the sake of completeness, we provide a formal definition below.\nDefinition 2.2 (Over-Representation). A generative model Q is over-representing P in some region C \u2286 X , if the probability of drawing Y \u223c Q is much greater than it is of drawing X \u223c P . Specifically, ifQ(C) \u2212 P (C) 0\nObserve that it is possible to over-represent without data-copying and vice versa. For example, if P is an equally weighted mixture of two Gaussians, and Q perfectly models one of them, then Q is over-representing without data-copying. On the other hand, if Q outputs a bootstrap sample of the training set T , then it is data-copying without over-representing. The focus of the rest of this work is on data-copying.\n\n3 A Test For Data-Copying\nHaving provided a formal definition, we next propose a hypothesis test to detect data-copying.\n\n3.1 A Global Test\nWe introduce our data-copying test in the global setting, when C = X . Our null hypothesis H 0 suggests that Q may equal P :H 0 : \u2206 T (P, Q) = 1 2\nThere are well-established non-parametric tests for this hypothesis, such as the Mann-Whitney U test (Mann and Whitney, 1947).Let A i \u223c L(P n ), B j \u223c L(Q m )\nbe samples of L(P ), L(Q) given by P n , Q m and their distances d(X) to training set T . The U statistic estimates the probability in Equation 3 by measuring the number of all mn pairwise comparisons in which B j > A i . An efficient and simple method to gather and interpret this test is as follows:1. Sort the n + m values L(P n ) \u222a L(Q m ) such that each instance l i \u2208 L(P n ), l j \u2208 L(Q m ) has rank\nR(l i ), R(l j ), starting from rank 1, and ending with rank n + m. L(P n ), L(Q m ) have no tied ranks with probability 1 assuming their distributions are continuous.\n2. Calculate the rank-sum for L(Q m ) denoted R Qm , and its U score denoted U Qm :R Qm = lj \u2208L(Qm) R(l j ), U Qm = R Qm \u2212 m(m + 1)\nConsequently, U Qm = ij 1 Bj >Ai .\n3. Under H 0 , U Qm is approximately normally distributed with > 20 samples in both L(Q m ) and L(P n ), allowing for the following z-scored statisticZ U L(P n ), L(Q m ); T = U Qm \u2212 \u00b5 U \u03c3 U , \u00b5 U = mn 2 , \u03c3 U = mn(m + n + 1)\nZ U provides us a data-copying statistic with normalized expectation and variance under H 0 . Z U 0 implies data-copying, Z U 0 implies underfitting. Z U < \u22125 implies that if H 0 holds, Z U is as likely as sampling a value < \u22125 from a standard normal.\nObserve that this test is completely model agnostic and uses no estimate of likelihood. It only requires a meaningful distance metric, which is becoming common practice in the evaluation of mode-collapse and -dropping (Heusel et al., 2017; Sajjadi et al., 2018) as well.\n\n3.2 Handling Heterogeneity\nAs described in Section 2.1, the above global test can be fooled by generators Q which are very close to the training data in some regions of the instance space (overfitting) but very far from the training data in others (poor modeling).\nWe handle this by introducing a local version of our test. Let \u03a0 denote any partition of the instance space X , which can be constructed in any manner. In our experiments, for instance, we run the k-means algorithm on T , so that |\u03a0| = k. As the number of training and test samples grows, we may increase k and thus the instancespace resolution of our test. Letting L \u03c0 (D) = L(D| \u03c0 ) be the distribution of distances-to-training-set within cell \u03c0 \u2208 \u03a0, we probe each cell of the partition \u03a0 individually.\nData Copying. To offer a summary statistic for data copying, we collect the z-scored Mann-Whitney U statistic, Z U , described in Section 3.1 in each cell \u03c0. Let P n (\u03c0) = |{x : x \u2208 P n , x \u2208 \u03c0}|/n denote the fraction of P n points lying in cell \u03c0, and similarly for Q m (\u03c0). The Z U test for cell \u03c0 and training set T will then be denoted as 1c for examples of these in-cell scores. For stability, we only measure data-copying for those cells significantly represented by Q, as determined by a threshold \u03c4 . Let \u03a0 \u03c4 be the set of all cells in the partition \u03a0 for which Q m (\u03c0) \u2265 \u03c4 . Then, our summary statistic for data copying averages across all cells represented by Q:Z U L \u03c0 (P n ), L \u03c0 (Q m ); T , where L \u03c0 (P n ) = {d(x) : x \u2208 P n , x \u2208 \u03c0} and similarly for L \u03c0 (Q m ). See FigureC T (P n , Q m ) := \u03c0\u2208\u03a0\u03c4 P n (\u03c0)Z U L \u03c0 (P n ), L \u03c0 (Q m ); T \u03c0\u2208\u03a0\u03c4 P n (\u03c0)\nOver-Representation. The above test will not catch a model that heavily over-or under-represents cells. For completeness, we next provide a simple representation test that is essentially used by Richardson and Weiss (2018), now with an independent test set instead of the training set.\nWith n, m \u2265 20 in cell \u03c0, we may treat Q m (\u03c0), P n (\u03c0) as Gaussian random variables. We then check the null hypothesis H 0 : 0 = P (\u03c0) \u2212 Q(\u03c0). Assuming this null hypothesis, a simple z-test is:Z \u03c0 = Q m (\u03c0) \u2212 P n (\u03c0) p 1 \u2212 p 1 n + 1 m\nwhere p = nPn(\u03c0)+mQm(\u03c0) n+m . We then report two values for a significance level s = 0.05: the number of significantly different cells ('bins') with Z \u03c0 > s (NDB over-representing), and the number with Z \u03c0 < \u2212s (NDB under-representing).\nTogether, these summary statistics -C T , NDB-over, NDB-under -detect the ways in which Q broadly represents P without directly copying the training set T .\n\n3.3 Performance Guarantees\nWe next provide some simple guarantees on the performance of the global test statistic U (Q m ). Guarantees for the average test is more complicated, and is left as a direction for future work.\nWe begin by showing that when the null hypothesis H 0 does not hold, U Qm has some desirable properties - 1 mn U Qm is a consistent estimator of the quantity of interest, \u2206 T (P, Q):\nTheorem 1. For true distribution P , model distribution Q, and distance metric d : X \u2192 R, the estimator1 mn U Qm \u2192 P \u2206(P, Q) according to the concentration inequality Pr 1 mn U Qm \u2212 \u2206(P, Q) \u2265 t \u2264 exp \u2212 2t 2 mn m + n\nFurthermore, when the model distribution Q actually matches the true distribution P , under modest assumptions we can expect 1 mn U Qm to be near 1 2 : Theorem 2. If Q = P , and the corresponding distance distributionL(Q) = L(P ) is non-atomic, then E 1 mn U Qm = 1 2 and E[Z U ] = 0\nProofs are provided in Appendices 7.1 and 7.2.\nAdditionally, we show that for a Gaussian Kernel Density Estimator, the parameter \u03c3 that satisfies the condition in Equation 2 is the \u03c3 corresponding to a maximum likelihood Gaussian KDE model. Recall that a KDE model is described byq \u03c3 (x) = 1 (2\u03c0) k/2 |T |\u03c3 k t\u2208T exp \u2212 x \u2212 t 2 2\u03c3 2 ,\nwhere the posterior probability that a random draw x \u223c q \u03c3 (x) comes from the Gaussian component centered at training point t isQ \u03c3 (t|x) = exp(\u2212 x \u2212 t 2 /(2\u03c3 2 ))\nt \u2208T exp(\u2212 x \u2212 t 2 /(2\u03c3 2 )) Lemma 3. For the kernel density estimator (4), the maximum-likehood choice of \u03c3, namely the maximizer of E X\u223cP [log q \u03c3 (X)], satisfiesE X\u223cP t\u2208T Q \u03c3 (t|X) X \u2212 t 2 = E Y \u223cQ\u03c3 t\u2208T Q \u03c3 (t|Y ) Y \u2212 t 2 See Appendix 7.3 for proof. Unless \u03c3 is large, we know that for any given x \u2208 X , t\u2208T Q \u03c3 (t|x) x \u2212 t 2 \u2248 d(x) = min t\u2208T x \u2212 t 2 . So, enforcing that E X\u223cP [d(X)] = E Y \u223cQ [d(Y )],\nand more loosely that EA\u223cL(P)B\u223cL(Q) [1 B>A ] = 1\n2 provides an excellent nonparametric approach to selecting a Gaussian KDE, and ought to be enforced for any Q attempting to emulate P ; after all, Theorem 2 points out that effectively any model with Q = P also yields this condition.\n\n4 Experiments\nHaving clarified what we mean by data-copying in theory, we turn our attention to data copying by generative models in practice 1 . We leave representation test re- sults for the appendix, since this behavior has been well studied in previous works. Specifically, we aim to answer the two following questions:\n1  wherein entire modes of P are either forgotten or averaged together. However, if a model begins to data-copy, it is definitively overfitting without mode-collapsing.\nNote that the above four baselines are all two sample tests that do not use P n as C T does. For completeness, we present experiments with an additional, three sample baseline in Appendix 7.5. Here, we repeat the 'moons' dataset experiment with the three-sample kernel MMD test originally proposed by Bounliphone et al. (2016) for generative model selection and later adapted by Esteban et al. (2017) for testing model over-fitting. We observe in Figure 11b that the threesample kMMD test does not detect data-copying, treating the MLE model similarly to overfit models with \u03c3 << \u03c3 MLE . See Appendix 7.5 for experimental details.\n\n4.2 Measuring degree of data-copying\nWe now aim to answer the second question raised at the beginning of this section: does our test statistic C T (P n , Q m ) detect and quantify data-copying?\nWe focus on three generative models: Gaussian KDEs, Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). For these experiments, we consider two baselines in addition to our method -the two-sample NN test, and the likelihood generalization gap where it can be computed or approximated.\n\n4.2.1 KDE-based tests\nFirst we consider Gaussian KDEs. While KDEs do not provide a reliable likelihood in high dimension (Theis et al., 2016), they do have several advantages as a preliminary benchmark -they allow us to directly force data-copying, and can help investigate the practical implications of the theoretical connection between the maximum likelihood KDE and C T \u2248 0 as described in Lemma 3. We explore two datasets with Gaussian KDE: the 'moons' dataset, and MNIST.\nKDEs: 'moons' dataset. Here, we repeat the experiment performed in Section 4.1, now including the C T statistic for comparison. Appendix 7.4.1 provides more experimental details, and examples of the dataset.\nResults. Figure 3a depicts how the generalization gap dwindles as KDE \u03c3 increases. While this test is capable of capturing data-copying, it is insensitive to underfitting and relies on a tractable likelihood. Figures 3b and 3c give a side-by-side depiction of C T and the two-sample NN test accuracies across a range of KDE \u03c3 values. Think of C T values as z-score standard deviations. We see that the C T statistic in Figure 3b precisely identifies the MLE model when C T \u2248 0, and responds sharply to \u03c3 values above and below \u03c3 MLE . The baseline in Figure 3c similarly identifies the MLE Q model when training accuracy \u2248 0.5, but is higher variance and less sensitive to changes in \u03c3, especially for over-fit \u03c3 \u03c3 MLE . We will see in the next experiment, that this test breaks down for more complex datasets when m |T |. KDEs: MNIST Handwritten Digits. We now extend the KDE test performed on the moons dataset to the significantly more complex MNIST handwritten digit dataset (LeCun and Cortes, 2010).\nWhile it would be convenient to directly apply the KDE \u03c3-sweeping tests discussed in the previous section, there are two primary barriers. The first is that KDE model relies on L 2 norms being perceptually meaningful, which is well understood not to be true in pixel space. The second problem is that of dimensionality: the 784-dimensional space of digits is far too high for a KDE to be even remotely efficient at interpolating the space.\nTo handle these issues, we first embed each image, x \u2208 X , to a perceptually meaningful 64-dimensional latent code, z \u2208 Z. Results. The likelihood generalization gap is depicted in Figure 3d repeating the trend seen with the 'moons' dataset.\nFigure 3e shows how C T (P n , Q m ) reacts decisively to over-and underfitting. It falsely determines the MLE \u03c3 value as slightly over-fit. However, the region of where C T transitions from over-to underfit (say \u221213 \u2264 C T \u2264 13) is relatively tight and includes the MLE \u03c3.\nMeanwhile, Figure 3f shows how -with the generated sample smaller than the training sample, m |T |the two-sample NN baseline provides no meaningful estimate of data-copying. In fact, the most data-copying models with low \u03c3 achieve the best scores closest to 0.5. Again, we are forced to use the m-subsampled T \u2282 T , and most instances of data copying are completely missed.\nThese results are promising, and demonstrate the reliability of this hypothesis testing approach to probing for data-copying across different data domains. In the next section, we explore how these tests perform on more sophisticated, non-KDE models.\n\n4.2.2 Variational Autoencoders\nGaussian KDE's may have nice theoretical properties, but are relatively ineffective in high-dimensional settings, precluding domains like images. As such, we also demonstrate our experiments on more practical neural models trained on higher dimensional image datasets (MNIST and ImageNet), with the goal of observing whether the C T statistic indicates data-copying as these models range from over-to underfit. The first neural model we consider is a Variational Autoencoder (VAE) trained on the MNIST handwritten images dataset.\nExperimetal Methodology. Unlike KDEs, VAEs do not have a single parameter that controls the degree of overfitting. Instead, similar to Bozkurt et al. (2018), we vary model complexity by increasing the width (neurons per layer) in a three-layer VAE (see Appendix 7.4.3 for details) -where higher width means a model of higher complexity. As an embedding, we pass all samples through the the convolutional autoencoder of Section 4.2.1, and collect statistics in this 64-dimensional space. Observe that likelihood is not available for VAEs; instead we compute each model's ELBO on a 10, 000 sample held out validation set, and use the ELBO approximation to the generalization gap instead.\nWe again note here, that for the NN accuracy baseline, we use a subsampled training set T as with the KDEbased MNIST tests where m = | T | = 10, 000.\nResults. Figures 4b and 4c compare the C T statistic to the NN accuracy baseline . C T behaves as it did in the previous sections: more complex models over-fit, forcing C T 0, and less complex models underfit forcing it 0. We note that the range of C T values is far less dramatic, which is to be expected since the KDEs were forced to explicitly data-copy. We observe that the ELBO spikes for models with C T near 0. Figure 4a shows the ELBO approximation of the generalization gap as the latent dimension (and number of units in each layer) is decreased. This method is entirely insensitive to over-and underfit models. This may be because the ELBO is only a lower bound, not the actual likelihood.\nThe NN baseline in Figure 4c is less interpretable, and fails to capture the overfitting trend as C T does. While all three test accuracies still follow the upward-sloping trend of Figure 3c, they do not indicate where the highest validation set ELBO is. Furthermore, the NN accuracy statistics are shifted upward when compared to the results of the previous section: all NN accuracies are above 0.5 for all latent dimensions. This is problematic. A test statistic's absolute score ought to bear significance between very different data and model domains like KDEs and VAEs.\n\n4.2.3 ImageNet GAN\nFinally, we scale our experiments up to a larger image domain.\nExperimental Methodology. We gather our test statistics on a state of the art conditional GAN, 'Big-Gan' (Brock et al., 2018), trained on the Imagenet 12 dataset (Russakovsky et al., 2015). Conditioning on an input code, this GAN will generate one of 1000 different Imagenet classes. We run our experiments separately on three classes: 'coffee', 'soap bubble', and 'schooner'. All generated, test, and training images are embedded to a 64-dimensional space by first gathering the 2048-dimensional features of an InceptionV3 network 'Pool3' layer, and then projecting them onto the 64 principal components of the training embeddings. Appendix 7.4.4 has more details.\nBeing limited to one pre-trained model, we increase model variance ('truncation threshold') instead of decreasing model complexity. As proposed by BigGan's authors, all standard normal input samples outside of this truncation threshold are resampled. The authors suggest that lower truncation thresholds, by only producing samples at the mode of the input, output higher quality samples at the cost of variety, as determined by Inception Score (IS). Similarly, the FID score finds suitable variety until truncation approaches zero.\n\nResults.\nThe results for the C T score is depicted in Figure 4d; the statistic remains well below zero until the truncation threshold is nearly maximized, indicating that Q produces samples closer to the training set than real samples tend to be. While FID finds that in aggregate the distributions are roughly similar, a closer look suggests that Q allocates too much probability mass near the training samples. A useful feature of the C T statistic is that one can examine the Z U scores it is composed of to see which of the cells \u03c0 \u2208 \u03a0 \u03c4 are or are not copying. Figure 5 shows the samples of over-and underfit clusters for two of the three classes. For both 'coffee' and 'bubble' classes, the underfit cells are more diverse than the data-copied cells. While it might seem reasonable that these generated samples are further from nearest neighbors in more diverse clusters, keep in mind that the Z U > 0 statistic indicates that they are further from training neighbors than test set samples are. For instance, the people depicted in underfit 'bubbles' cell are highly distorted.\n\nMeanwhile, the two-sample NN baseline in\n\n\n4.3 Discussion\nWe now reflect on the two questions recited at the beginning of Section 4. Firstly, it appears that many existing generative model tests do not detect data-copying. The findings of Section 4.1 demonstrate that many popular  generative model tests like FID, Precision and Recall, and Binning-Based Evaluation are wholly insensitive to explicit data-copying even in low-dimensional settings. We suggest that this is because these tests are geared to detect over-and underrepresentation more than data-copying.\nSecondly, the experiments of Section 4.2 indicate that the proposed C T (P n , Q m ) test statistic not only detects explicitly forced data-copying (as in the KDE experiments), but also detects data-copying in complex, overfit generative models like VAEs and GANs.\nIn these settings, we observe that as models overfit more, C T drops below 0 and significantly below -1.\nA limitation of the proposed C T test is the number of test samples n. Without sufficient test samples, not only is the statistic higher variance, but the instance space partition \u03a0 cannot be very fine-grain. Consequently, we are limited in our ability to manage the heterogeneity of d(X) across the instance space, and some cells may be mischaracterized. For example, in the BigGan experiment of Section 4.2.3, we are provided only 50 test samples per image class (e.g. 'soap bubble'), limiting us to an instance space partition of only three cells. Developing more data-efficient methods to handle heterogeneity may be a promising area of future work.\n\n5 Conclusion\nIn this work, we have formalized data-copying: an under-explored failure mode of generative model overfitting. We have provided preliminary tests for measuring data-copying and experiments indicating its presence in a broad class of generative models. In future work, we plan to establish more theoretical properties of datacopying, convergence guarantees of these tests, and experiments with different model parameters.\n\n7.1 Proof of Theorem 1\nA restatement of the theorem:\nFor true distribution P , model distribution Q, and distance metric d : X \u2192 R, the estimator 1 mn U Qm \u2192 P \u2206(P, Q) according to the concentration inequalityPr 1 mn U Qm \u2212 \u2206(P, Q) \u2265 t \u2264 exp \u2212 2t 2 mn m + n\nProof. We establish consistency using the following nifty lemma Lemma 4. (Bounded Differences Inequality) Suppose X 1 , . . . , X n \u2208 X are independent, and f :X n \u2192 R. Let c 1 , . . . , c n satisfy sup x1,...,xn,x i f (x 1 , . . . , x i , . . . , x n ) \u2212 f (x 1 , . . . , x i , . . . , x n ) \u2264 c i for i = 1, . . . , n. Then we have for any t > 0 Pr f \u2212 E[f ] \u2265 t \u2264 exp \u22122t 2 n i=1 c 2 i (5)\nThis directly equips us to prove the Theorem.\nIt is relatively straightforward to apply Lemma 4 to the normalized U = 1 mn U Qm . First, think of it as a function of m independent samples of X \u223c Q and n independent samples ofY \u223c P , U (X 1 , . . . , X m , Y 1 , . . . , Y n ) = 1 mn ij 1 d(Xi)>d(Yj ) U : (R d ) mn \u2192 R\nLet b i bound the change in U after substituting any X i with X i , and c j bound the change in U after substituting any Y j with Y j . Specifically sup x1,...,xm,y1,...,yn, x iU (x 1 , . . . , x i , . . . , x m , y 1 , . . . , y n ) \u2212U (x 1 , . . . , x i , . . . , x m , y 1 , . . . , y n ) \u2264 b i sup x1,...,xm,y1,...,yn, y j U (x 1 , . . . , x m , y 1 , . . . , y j , . . . y n ) \u2212U (x 1 , . . . , x m , y 1 , . . . , y j , . . . y n ) \u2264 c i\nWe then know that b i = n mn = 1 m for all i, with equality when d(x i ) < d(y j ) < d(x i ) for all j \u2208 [n]. In this case, substituting x i with x i flips n of the indicator comparisons in U from 1 to 0, and is then normalized by mn. By a similar argument, c j = m nm = 1 n for all j. Equipped with b i and c j , we may simply substitute into Equation 5 of the Bounded Differences Inequality, giving usPr U \u2212 E[U ] \u2265 t = Pr 1 mn U Qm \u2212 \u2206(\u00b5 p , \u00b5 q ) \u2265 t \u2264 exp \u22122t 2 m i=1 b 2 i + n j=1 c 2 j = exp \u22122t 2 m i=1 1 m 2 + n j=1 1 n 2 = exp \u22122t 2 1 m + 1 n = exp \u22122t 2 mn m + n 7.2 Proof of Theorem 2\nA restatement of the theorem:When Q = P , and the corresponding distance distri- bution L(Q) = L(P ) is non-atomic, E 1 mn U = 1 2\nProof. For random variables A \u223c L(P ) and B \u223c L(P ), we can partition the event space of A \u00d7 B into three disjoint events:Pr(A > B) + Pr(A < B) + Pr(A = B) = 1\nSince Q = P , the first two events have equal probability,Pr(A > B) = Pr(A < B), so 2 Pr(A > B) + Pr(A = B) = 1\nAnd since the distributions of A and B are non-atomic (i.e. Pr B = b = 0, \u2200 b \u2208 R) we have that Pr(A = B) = 0, and thus2 Pr(A > B) = 1 Pr(A > B) = \u2206(P, Q) = 1 2 7.3 Proof of Lemma 3\nLemma 3 For the kernel density estimator (1), the maximum-likehood choice of \u03c3, namely the maximizer of E X\u223cP [log q \u03c3 (X)], satisfiesE X\u223cP t\u2208T Q \u03c3 (t|X) X \u2212 t 2 = E Y \u223cQ\u03c3 t\u2208T Q \u03c3 (t|Y ) Y \u2212 t 2\nProof. We haveE X\u223cP [ln q \u03c3 (X)] = E X\u223cP \u2212 ln((2\u03c0) k/2 |T |\u03c3 k ) + ln t\u2208T exp \u2212 x \u2212 t 2 2\u03c3 2 = constant \u2212 k ln \u03c3 + E X\u223cP ln t\u2208T exp \u2212 x \u2212 t 2 2\u03c3 2\nSetting the derivative of this to zero and simplifying, we find that the maximum-likelihood \u03c3 satisfies\u03c3 2 = 1 k E X\u223cP t\u2208T Q \u03c3 (t|X) X \u2212 t 2 .\nNow, interpreting Q \u03c3 as a mixture of |T | Gaussians, and using the notation t \u2208 R T to mean that t is chosen uniformly at random from T , we haveE Y \u223cQ\u03c3 t\u2208T Q \u03c3 (t|Y ) Y \u2212 t 2 = E t\u2208 R T E Y \u223cN (t,\u03c3 2 I k ) Y \u2212 t 2 = k\u03c3 2 .\nCombining this with (6) yields the lemma.\n7.4 Procedural Details of Experiments 7.4.1 Moons Dataset, and Gaussian KDE moons dataset 'Moons' is a synthetic dataset consisting of two curved interlocking manifolds with added configurable noise. We chose to use this dataset as a proof of concept because it is low dimensional, and thus KDE friendly and easy to visualize, and we may have unlimited train, test, and validation samples.\nGaussian KDE We use a Gaussian KDE as our preliminary generative model Q because its likelihood is theoretically related to our non-parametric test. Perhaps more importantly, it is trivial to control the degree of data-copying with the bandwidth parameter \u03c3. Figures 6b, 6c, 6d provide contour plots of of a Gaussian KDE Q trained on the moons dataset with progressively larger \u03c3. With \u03c3 = 0.01, Q will effectively resample the training set. \u03c3 = 0.13 is nearly the MLE model. With \u03c3 = 0.5, the KDE struggles to capture the unique definition of T .\n\n7.4.2 Moons Experiments\nOur experiments that examined whether several baseline tests could detect data-copying (Section 4.1), and our first test of our own metric (Section 4.2.1) use the moons dataset. In both of these, we fix a training sample, T of 2000 points, a test sample P n of 1000 points, and a generated sample Q m of 1000 points. We regenerate Q m 10 times, and report the average statistic across these trials along with a single standard deviation. If the standard deviation buffer along the line is not visible, it is because the standard deviation is relatively small. We artificially set the constraint that m, n |T |, as is true for big natural datasets, and more elaborate models that are computationally burdensome to sample from.\nSection 4.1 Methods Here are the routines we used for the four baseline tests:\n\u2022 Frech\u00e9t Inception Distance (FID) (Heusel et al., 2017) : Normally, this test is run on two samples of images (T and Q m ) that are first embedded into a perceptually meaningful latent space using a discriminative neural net, like the Inception Network. By 'meaningful' we mean points that are closer together are more perceptually alike to the human eye. Unlike images in pixel space, the samples of the moons dataset require no embedding, so we run the Frech\u00e9t test directly on the samples.\nFirst, we fit two MLE Gaussians: N (\u00b5 T , \u03a3 T ) to T , and N (\u00b5 Q , \u03a3 Q ) to Q m , by collecting their respective MLE mean and covariance parameters. The statistic reported is the Frech\u00e9t distance between these two Gaussians, denoted Fr(\u2022, \u2022), which for Gaussians has a closed form:Fr N (\u00b5 T , \u03a3 T ), N (\u00b5 Q , \u03a3 Q ) = \u00b5 T \u2212 \u00b5 Q + Tr \u03a3 T \u2212 \u03a3 Q \u2212 2(\u03a3 T \u03a3 Q ) 1 /2\nNaturally, if Q is data-copying T , its MLE mean and covariance will be nearly identical, rendering this test ineffective for capturing this kind of overfitting.\n\u2022 Binning Based Evaluation (Richardson and Weiss, 2018) Specifically:Z \u03c0 = Q m (\u03c0) \u2212 T (\u03c0) p 1 \u2212 p 1 |T | + 1 m where p = |T |T (\u03c0)+mQm(\u03c0) |T |+m\n. We then perform a one-sided hypothesis test, and compute the number of positive Z \u03c0 values that are greater than the significance level of 0.05. We call this the number of statistically different bins or NDB. The NDB/k ought to equal the significance level if P = Q.\n\u2022 Two-Sample Nearest-Neighbor (Lopez-Paz and Oquab, 2016) : In this test -our primary baseline -we report the three LOO NN values discussed in Xu et al. (2018). The generated sample Q m and training sample (subsampled to have equal size, m), T \u2286 T , are joined together create sample S = T \u222a Q m of size 2m, with training samples labeled '1' and test samples labeled '0'. One then fits a 1-Nearest-Neighbor classifier to S, and reports the accuracy in predicting the training samples ('1's), the accuracy in predicting the generated samples ('0's), and the average.\nOne can expect that -when Q collapses to a few mode centers of T -the training accuracy is low, and the generated accuracy is high, thus indicating over-representation. Additionally, one could imagine that when the training and generated accuracies are near 0, we have extreme data-copying. However, as explained in Experiments section, when we are forced to subsample T , it is unlikely that a given copied training point t \u2208 T is used in the test, thus making the test result unclear.\n\u2022 Precision and Recall (Sajjadi et al., 2018) : This method offers a clever technique for scaling classical precision and recall statistics to high dimensional, complex spaces. First, all samples are embedded to Inception Network Pool3 features. Then, the author's use the following insight: for distribution's Q and P , the precision and recall curve is approximately given by the set of points:PRD(Q, P ) = {(\u03b1(\u03bb), \u03b2(\u03bb)|\u03bb \u2208 \u039b}\nwhere\u039b = {tan i r + 1 \u03c0 2 |i \u2208 [r]} \u03b1(\u03bb) = \u03c0\u2208\u03a0 min \u03bbP (\u03c0), Q(\u03c0) \u03b2(\u03bb) = \u03c0\u2208\u03a0 min P (\u03c0), Q(\u03c0) \u03bb\nand where r is the 'resolution' of the curve, the set \u03a0 is a partition of the instance space and P (\u03c0), Q(\u03c0) are the fraction of samples falling in cell \u03c0. \u03a0 is determined by running k-means on the combination of the training and generated sets. In our tests here, we set k = 5, and report the average PRD curve measured over 10 k-means clusterings (and then re-run 10 times for 10 separate trials of Q m ).\n\n7.4.3 MNIST Experiments\nThe experiments of Sections 4.2.1 and 4.2.2 use the MNIST digit dataset (LeCun and Cortes, 2010). We use a training sample, T , of size |T | = 50, 000, a test sample P n of size n = 10, 000, a validation sample V l of l = 10, 000, and create generated samples of size m = 10, 000.\nHere, for a meaningful distance metric, we create a custom embedding using a convolutional autoencoder trained using a VGG perceptual loss proposed by Zhang et al. (2018). The encoder and decoder each have four convolutional layers using batch normalization, two linear layers using dropout, and two max pool layers. The autoencoder is trained for 100 epochs with a batch size of 128 and Adam optimizer with learning rate 0.001. For each training sample t \u2208 R 784 , the encoder compresses to z \u2208 R 64 , and decoder expands back up to t \u2208 R 784 . Our loss is then where \u03b3(\u2022, \u2022) is the VGG perceptual loss, and \u03bb max{ z 2 2 \u2212 1, 0} provides a linear hinge loss outside of a unit L 2 ball. The hinge loss encourages the encoder to learn a latent representation within a bounded domain, hopefully augmenting its ability to interpolate between samples. It is worth noting that the perceptual loss is not trained on MNIST, and hopefully uses agnostic features that help keep us from overfitting. We opt to use a standard autoencoder instead of a stochastic autoencoder like a VAE, because we want to be able to exactly data-copy the training set T . Thus, we want the encoder to create a near-exact encoding and decoding of the training samples specifically. Figure 7 provides an example of linearly spaced steps between two training samples. While not perfect, we observe that half-way between the '2' and the '0' is a sample that appears perceptually to be almost almost a '2' and almost a '0'. As such, we consider the distance metric d(x) on this space used in our experiments to be meaningful.L(t, t) = \u03b3(t, t) + \u03bb max{ z 2 2 \u2212 1, 0}\n\nKDE tests:\nIn the MNIST KDE experiments, we fit each KDE Q on the 64-d latent representations of the training set T for several values of \u03c3; we gather all statistical tests in this space, and effectively only decode to visaully inspect samples. We gather the average and standard deviation of each data point across 5 trials of generating Q m . For the Two-Sample Nearest-Neighbor test, it is computationally intense to compute the nearesnt neighbor in a 64-dimensional dataset of 20,000 points T \u222a Q m 20,000 times. To limit this, we average each of the training and generated NN accuracy over 500 training and generated samples. We find this acceptable, since the test results depicted in VAE experiments: In the MNIST VAE experiments, we only use the 64-d autoencoder latent representation in computing the C T and 1-NN test scores, and not at all in training. Here, we experiment with twenty standard, fully connected, VAEs using binary cross entropy reconstruction loss. The twenty models have three hidden layers and latent dimensions ranging from d = 5 to d = 100 in steps of 5. The number of neurons in intermediate layers is approximately twice the number of the layer beneath it, so for a latent space of 50-d, the encoder architecture is 784 \u2192 400 \u2192 200 \u2192 100 \u2192 50, and the decoder architecture is the opposite.\nTo sample from a trained VAE, we sample from a standard normal with dimensionality equivalent to the VAEs latent dimension, and pass them through the VAE decoder to the 784-d image space. We then encode these generated images to the agnostic 64-d latent space of the perceptual autoencoder described at the beginning of the section, where L 2 distance is meaningful. We also encode the training sample T and test sample P n to this space, and then run the C T and two-sample NN tests. We again compute the nearest neighbor accuracies for 10,000 of the training and generated samples (the 1-NN classifier is fit on the 20,000 sample set T \u222a Q m ), which appears to be acceptable due to low test variance.\n\n7.4.4 ImageNet Experiments\nHere, we have chosen three of the one thousand Ima-geNet12 classes that 'BigGan' produces. To reiterate,   We also note that the 50 test samples provided in each class is highly limiting, only allowing us to split the instance space into about three cells and keep a reasonable number of test samples in each cell. As the number of test samples grows, so can the number of cells and the resolution of the partition. Figure 10 provides an example of where this clustering might be limited; the generated samples of the underfit cell seem hardly any different from those of the over-fit cell. A finer-grain partition is likely needed here. However, the data-copied cell to the left does appear to be very close to the training set, potentially too close according to Z U .\nIn performing these experiments, we gather the  This test provides an interesting benchmark to consider in addition to those in the main body.  et al. (2017). This checks the null hypothesis that the kMMD between T and Q m is greater than that between T and P n . A high p-value confirms this null hypothesis (as seen for all \u03c3 > \u03c3 MLE ). A p-value near 0.5 suggests that the kMMD's are approximately equal. A p-value near zero rejects the null hypothesis, suggesting that the kMMD between Q m and T is much smaller than that between P n and T . We see that the p-value remains well above 0.5 for all \u03c3 values and treats the MLE \u03c3 just as it does the overfit \u03c3 values.\nFigures 11d and 11e compare the C T and kMMD tests for twenty MNIST VAEs with decreasing complexity (latent dimension). Figures 11e again depicts the kMMD distance to training set for both the generated (orange) and test samples (blue). We observe that this test does not appear sensitive to over-parametrized VAEs (d > 50) in the same way our proposed test (Figure 11d) is. As in the 'moons' case above, it appears sensitive to underfitting (d << 50). Here, the corresponding kMMD p-values are effectively 1 for all latent dimension values, and thus are omitted.\nWe suspect that this insensitivity to data-copying is due to the fact that -for a large number of samples m, n -the kMMDs between T and P n , and between T and Q m are both likely to be near zero when Q data-copies.\nConsider the case of extreme data-copying, when Q m is simply a bootstrap sample from the training set T .\n\nFootnotes:\n1: https://github.com/casey-meehan/data-copying\n\nReferences:\n\n- Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, and Arthur Gretton. A test of relative similarity for model selection in generative models. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Pro- ceedings, 2016. URL http://arxiv.org/abs/1511. 04581.- Alican Bozkurt, Babak Esmaeili, Dana H Brooks, Jen- nifer G Dy, and Jan-Willem van de Meent. Can vaes generate novel examples? 2018.\n\n- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis, 2018.\n\n- Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Ben- gio, and Wenjie Li. Mode regularized generative adversarial networks. CoRR, abs/1612.02136, 2016. URL http://arxiv.org/abs/1612.02136.\n\n- Crist\u00f3bal Esteban, Stephanie L Hyland, and Gunnar R\u00e4tsch. Real-valued (medical) time series genera- tion with recurrent conditional gans. arXiv preprint arXiv:1706.02633, 2017.\n\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.\n\n- Ishaan Gulrajani, Colin Raffel, and Luke Metz. To- wards GAN benchmarks which require generaliza- tion. CoRR, abs/2001.03653, 2020. URL https: //arxiv.org/abs/2001.03653.\n\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. 2017.\n\n- Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\n- Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems 33 (NIPS 2019), 2019.\n\n- Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/.\n\n- David Lopez-Paz and Maxime Oquab. Revisit- ing classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.\n\n- Henry B Mann and Donald R Whitney. On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, pages 50-60, 1947.\n\n- Eitan Richardson and Yair Weiss. On gans and gmms. In Advances in Neural Information Processing Sys- tems, pages 5847-5858, 2018.\n\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan- der C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.\n\n- Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing gen- erative models via precision and recall. Advances in Neural Information Processing Systems 31 (NIPS 2017), 2018.\n\n- Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neu- ral information processing systems, pages 2234-2242, 2016.\n\n- Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alexander J. Smola, and Arthur Gretton. Gener- ative models and model criticism via optimized max- imum mean discrepancy. CoRR, abs/1611.04488, 2016. URL http://arxiv.org/abs/1611.04488.\n\n- Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative mod- els. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.01844.\n\n- Ryan Webster, Julien Rabin, Lo\u00efc Simon, and Fr\u00e9d\u00e9ric Jurie. Detecting overfitting of deep generative networks via latent recovery. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 11273-11282. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01153. URL http://openaccess.thecvf.com/content_CVPR_ 2019/html/Webster_Detecting_Overfitting_ of_Deep_Generative_Networks_via_Latent_ Recovery_CVPR_2019_paper.html.\n\n- Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis of decoder-based generative models. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL https: //openreview.net/forum?id=B1M8JF9xx.\n\n- Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberger. An em- pirical study on evaluation metrics of generative ad- versarial networks. arXiv preprint arXiv:1806.07755, 2018.\n\n- Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effective- ness of deep features as a perceptual metric. In CVPR, 2018.\n\n", "annotations": {"ReferenceToFootnote": [{"begin": 14696, "end": 14697, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 613, "end": 43281, "idx": 0}], "ReferenceToFormula": [{"begin": 15445, "end": 15446, "target": "#formula_11", "idx": 0}], "SectionReference": [{"begin": 43343, "end": 48366, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 613, "idx": 0}], "Div": [{"begin": 77, "end": 605, "idx": 0}, {"begin": 616, "end": 4753, "idx": 1}, {"begin": 4755, "end": 5865, "idx": 2}, {"begin": 5867, "end": 6712, "idx": 3}, {"begin": 6714, "end": 9880, "idx": 4}, {"begin": 9882, "end": 10002, "idx": 5}, {"begin": 10004, "end": 11817, "idx": 6}, {"begin": 11819, "end": 14367, "idx": 7}, {"begin": 14369, "end": 16460, "idx": 8}, {"begin": 16462, "end": 17585, "idx": 9}, {"begin": 17587, "end": 18088, "idx": 10}, {"begin": 18090, "end": 21360, "idx": 11}, {"begin": 21362, "end": 24034, "idx": 12}, {"begin": 24036, "end": 25315, "idx": 13}, {"begin": 25317, "end": 26400, "idx": 14}, {"begin": 26402, "end": 26443, "idx": 15}, {"begin": 26445, "end": 27991, "idx": 16}, {"begin": 27993, "end": 28426, "idx": 17}, {"begin": 28428, "end": 32713, "idx": 18}, {"begin": 32715, "end": 36959, "idx": 19}, {"begin": 36961, "end": 38898, "idx": 20}, {"begin": 38900, "end": 40926, "idx": 21}, {"begin": 40928, "end": 43281, "idx": 22}], "Head": [{"begin": 616, "end": 630, "n": "1", "idx": 0}, {"begin": 4755, "end": 4771, "n": "1.1", "idx": 1}, {"begin": 5867, "end": 5882, "n": "2", "idx": 2}, {"begin": 6714, "end": 6744, "n": "2.1", "idx": 3}, {"begin": 9882, "end": 9907, "n": "3", "idx": 4}, {"begin": 10004, "end": 10021, "n": "3.1", "idx": 5}, {"begin": 11819, "end": 11845, "n": "3.2", "idx": 6}, {"begin": 14369, "end": 14395, "n": "3.3", "idx": 7}, {"begin": 16462, "end": 16475, "n": "4", "idx": 8}, {"begin": 17587, "end": 17623, "n": "4.2", "idx": 9}, {"begin": 18090, "end": 18111, "n": "4.2.1", "idx": 10}, {"begin": 21362, "end": 21392, "n": "4.2.2", "idx": 11}, {"begin": 24036, "end": 24054, "n": "4.2.3", "idx": 12}, {"begin": 25317, "end": 25325, "idx": 13}, {"begin": 26402, "end": 26442, "idx": 14}, {"begin": 26445, "end": 26459, "n": "4.3", "idx": 15}, {"begin": 27993, "end": 28005, "n": "5", "idx": 16}, {"begin": 28428, "end": 28450, "n": "7.1", "idx": 17}, {"begin": 32715, "end": 32738, "n": "7.4.2", "idx": 18}, {"begin": 36961, "end": 36984, "n": "7.4.3", "idx": 19}, {"begin": 38900, "end": 38910, "idx": 20}, {"begin": 40928, "end": 40954, "n": "7.4.4", "idx": 21}], "Paragraph": [{"begin": 77, "end": 605, "idx": 0}, {"begin": 631, "end": 881, "idx": 1}, {"begin": 882, "end": 1127, "idx": 2}, {"begin": 1187, "end": 2376, "idx": 3}, {"begin": 2377, "end": 2880, "idx": 4}, {"begin": 2881, "end": 3382, "idx": 5}, {"begin": 3383, "end": 3644, "idx": 6}, {"begin": 3645, "end": 4088, "idx": 7}, {"begin": 4089, "end": 4520, "idx": 8}, {"begin": 4521, "end": 4753, "idx": 9}, {"begin": 4772, "end": 5367, "idx": 10}, {"begin": 5368, "end": 5865, "idx": 11}, {"begin": 5883, "end": 6365, "idx": 12}, {"begin": 6366, "end": 6419, "idx": 13}, {"begin": 6420, "end": 6466, "idx": 14}, {"begin": 6467, "end": 6672, "idx": 15}, {"begin": 6697, "end": 6712, "idx": 16}, {"begin": 6745, "end": 6853, "idx": 17}, {"begin": 6854, "end": 7196, "idx": 18}, {"begin": 7197, "end": 7298, "idx": 19}, {"begin": 7329, "end": 7643, "idx": 20}, {"begin": 7702, "end": 7815, "idx": 21}, {"begin": 7857, "end": 8024, "idx": 22}, {"begin": 8067, "end": 8173, "idx": 23}, {"begin": 8174, "end": 8187, "idx": 24}, {"begin": 8188, "end": 8850, "idx": 25}, {"begin": 8876, "end": 9245, "idx": 26}, {"begin": 9246, "end": 9449, "idx": 27}, {"begin": 9464, "end": 9880, "idx": 28}, {"begin": 9908, "end": 10002, "idx": 29}, {"begin": 10022, "end": 10146, "idx": 30}, {"begin": 10169, "end": 10295, "idx": 31}, {"begin": 10328, "end": 10629, "idx": 32}, {"begin": 10734, "end": 10901, "idx": 33}, {"begin": 10902, "end": 10985, "idx": 34}, {"begin": 11034, "end": 11068, "idx": 35}, {"begin": 11069, "end": 11219, "idx": 36}, {"begin": 11295, "end": 11546, "idx": 37}, {"begin": 11547, "end": 11817, "idx": 38}, {"begin": 11846, "end": 12083, "idx": 39}, {"begin": 12084, "end": 12588, "idx": 40}, {"begin": 12589, "end": 13261, "idx": 41}, {"begin": 13452, "end": 13737, "idx": 42}, {"begin": 13738, "end": 13932, "idx": 43}, {"begin": 13974, "end": 14210, "idx": 44}, {"begin": 14211, "end": 14367, "idx": 45}, {"begin": 14396, "end": 14589, "idx": 46}, {"begin": 14590, "end": 14772, "idx": 47}, {"begin": 14773, "end": 14876, "idx": 48}, {"begin": 14989, "end": 15206, "idx": 49}, {"begin": 15273, "end": 15319, "idx": 50}, {"begin": 15320, "end": 15553, "idx": 51}, {"begin": 15607, "end": 15735, "idx": 52}, {"begin": 15771, "end": 15935, "idx": 53}, {"begin": 16177, "end": 16206, "idx": 54}, {"begin": 16226, "end": 16460, "idx": 55}, {"begin": 16476, "end": 16785, "idx": 56}, {"begin": 16786, "end": 16954, "idx": 57}, {"begin": 16955, "end": 17585, "idx": 58}, {"begin": 17624, "end": 17780, "idx": 59}, {"begin": 17781, "end": 18088, "idx": 60}, {"begin": 18112, "end": 18567, "idx": 61}, {"begin": 18568, "end": 18775, "idx": 62}, {"begin": 18776, "end": 19780, "idx": 63}, {"begin": 19781, "end": 20220, "idx": 64}, {"begin": 20221, "end": 20462, "idx": 65}, {"begin": 20463, "end": 20735, "idx": 66}, {"begin": 20736, "end": 21109, "idx": 67}, {"begin": 21110, "end": 21360, "idx": 68}, {"begin": 21393, "end": 21922, "idx": 69}, {"begin": 21923, "end": 22608, "idx": 70}, {"begin": 22609, "end": 22758, "idx": 71}, {"begin": 22759, "end": 23459, "idx": 72}, {"begin": 23460, "end": 24034, "idx": 73}, {"begin": 24055, "end": 24117, "idx": 74}, {"begin": 24118, "end": 24783, "idx": 75}, {"begin": 24784, "end": 25315, "idx": 76}, {"begin": 25326, "end": 26400, "idx": 77}, {"begin": 26460, "end": 26967, "idx": 78}, {"begin": 26968, "end": 27232, "idx": 79}, {"begin": 27233, "end": 27337, "idx": 80}, {"begin": 27338, "end": 27991, "idx": 81}, {"begin": 28006, "end": 28426, "idx": 82}, {"begin": 28451, "end": 28480, "idx": 83}, {"begin": 28481, "end": 28637, "idx": 84}, {"begin": 28686, "end": 28846, "idx": 85}, {"begin": 29079, "end": 29124, "idx": 86}, {"begin": 29125, "end": 29304, "idx": 87}, {"begin": 29398, "end": 29575, "idx": 88}, {"begin": 29842, "end": 30245, "idx": 89}, {"begin": 30439, "end": 30468, "idx": 90}, {"begin": 30570, "end": 30692, "idx": 91}, {"begin": 30730, "end": 30788, "idx": 92}, {"begin": 30842, "end": 30961, "idx": 93}, {"begin": 31024, "end": 31158, "idx": 94}, {"begin": 31219, "end": 31233, "idx": 95}, {"begin": 31366, "end": 31469, "idx": 96}, {"begin": 31509, "end": 31655, "idx": 97}, {"begin": 31734, "end": 31775, "idx": 98}, {"begin": 31776, "end": 32165, "idx": 99}, {"begin": 32166, "end": 32713, "idx": 100}, {"begin": 32739, "end": 33464, "idx": 101}, {"begin": 33465, "end": 33543, "idx": 102}, {"begin": 33544, "end": 34037, "idx": 103}, {"begin": 34038, "end": 34320, "idx": 104}, {"begin": 34400, "end": 34561, "idx": 105}, {"begin": 34562, "end": 34631, "idx": 106}, {"begin": 34708, "end": 34976, "idx": 107}, {"begin": 34977, "end": 35542, "idx": 108}, {"begin": 35543, "end": 36029, "idx": 109}, {"begin": 36030, "end": 36426, "idx": 110}, {"begin": 36459, "end": 36464, "idx": 111}, {"begin": 36552, "end": 36959, "idx": 112}, {"begin": 36985, "end": 37265, "idx": 113}, {"begin": 37266, "end": 38858, "idx": 114}, {"begin": 38911, "end": 40222, "idx": 115}, {"begin": 40223, "end": 40926, "idx": 116}, {"begin": 40955, "end": 41725, "idx": 117}, {"begin": 41726, "end": 42394, "idx": 118}, {"begin": 42395, "end": 42958, "idx": 119}, {"begin": 42959, "end": 43174, "idx": 120}, {"begin": 43175, "end": 43281, "idx": 121}], "ReferenceToBib": [{"begin": 1857, "end": 1883, "target": "#b8", "idx": 0}, {"begin": 1927, "end": 1952, "target": "#b5", "idx": 1}, {"begin": 2483, "end": 2506, "target": "#b16", "idx": 2}, {"begin": 2507, "end": 2528, "target": "#b15", "idx": 3}, {"begin": 2529, "end": 2545, "target": "#b20", "idx": 4}, {"begin": 2546, "end": 2566, "target": "#b7", "idx": 5}, {"begin": 4853, "end": 4876, "target": "#b16", "idx": 6}, {"begin": 4877, "end": 4903, "target": "#b11", "idx": 7}, {"begin": 4904, "end": 4931, "target": "#b13", "idx": 8}, {"begin": 4932, "end": 4953, "target": "#b15", "idx": 9}, {"begin": 4954, "end": 4970, "target": "#b21", "idx": 10}, {"begin": 4971, "end": 4987, "target": "#b20", "idx": 11}, {"begin": 5543, "end": 5564, "target": "#b7", "idx": 12}, {"begin": 5592, "end": 5614, "target": "#b15", "idx": 13}, {"begin": 5639, "end": 5666, "target": "#b9", "idx": 14}, {"begin": 5667, "end": 5684, "target": "#b3", "idx": 15}, {"begin": 8976, "end": 8997, "target": "#b7", "idx": 16}, {"begin": 8998, "end": 9019, "target": "#b15", "idx": 17}, {"begin": 10270, "end": 10294, "target": "#b12", "idx": 18}, {"begin": 11765, "end": 11786, "target": "#b7", "idx": 19}, {"begin": 11787, "end": 11808, "target": "#b15", "idx": 20}, {"begin": 13647, "end": 13674, "target": "#b13", "idx": 21}, {"begin": 17256, "end": 17281, "target": "#b0", "idx": 22}, {"begin": 17334, "end": 17355, "target": "#b4", "idx": 23}, {"begin": 18211, "end": 18231, "target": "#b18", "idx": 24}, {"begin": 22058, "end": 22079, "target": "#b1", "idx": 25}, {"begin": 24223, "end": 24243, "target": "#b2", "idx": 26}, {"begin": 24280, "end": 24306, "target": "#b14", "idx": 27}, {"begin": 33579, "end": 33600, "target": "#b7", "idx": 28}, {"begin": 34589, "end": 34617, "target": "#b13", "idx": 29}, {"begin": 35007, "end": 35034, "target": "#b11", "idx": 30}, {"begin": 35120, "end": 35136, "target": "#b21", "idx": 31}, {"begin": 36053, "end": 36075, "target": "#b15", "idx": 32}, {"begin": 37057, "end": 37081, "target": "#b10", "idx": 33}, {"begin": 37417, "end": 37436, "target": "#b22", "idx": 34}, {"begin": 41870, "end": 41883, "idx": 35}], "ReferenceString": [{"begin": 43358, "end": 43714, "id": "b0", "idx": 0}, {"begin": 43716, "end": 43848, "id": "b1", "idx": 1}, {"begin": 43852, "end": 43973, "id": "b2", "idx": 2}, {"begin": 43977, "end": 44162, "id": "b3", "idx": 3}, {"begin": 44166, "end": 44342, "id": "b4", "idx": 4}, {"begin": 44346, "end": 44580, "id": "b5", "idx": 5}, {"begin": 44584, "end": 44754, "id": "b6", "idx": 6}, {"begin": 44758, "end": 44937, "id": "b7", "idx": 7}, {"begin": 44941, "end": 45046, "id": "b8", "idx": 8}, {"begin": 45050, "end": 45271, "id": "b9", "idx": 9}, {"begin": 45275, "end": 45385, "id": "b10", "idx": 10}, {"begin": 45389, "end": 45503, "id": "b11", "idx": 11}, {"begin": 45507, "end": 45689, "id": "b12", "idx": 12}, {"begin": 45693, "end": 45822, "id": "b13", "idx": 13}, {"begin": 45826, "end": 46162, "id": "b14", "idx": 14}, {"begin": 46166, "end": 46380, "id": "b15", "idx": 15}, {"begin": 46384, "end": 46590, "id": "b16", "idx": 16}, {"begin": 46594, "end": 46870, "id": "b17", "idx": 17}, {"begin": 46874, "end": 47159, "id": "b18", "idx": 18}, {"begin": 47163, "end": 47655, "id": "b19", "idx": 19}, {"begin": 47659, "end": 47982, "id": "b20", "idx": 20}, {"begin": 47986, "end": 48192, "id": "b21", "idx": 21}, {"begin": 48196, "end": 48364, "id": "b22", "idx": 22}], "Sentence": [{"begin": 77, "end": 166, "idx": 0}, {"begin": 167, "end": 337, "idx": 1}, {"begin": 338, "end": 605, "idx": 2}, {"begin": 631, "end": 694, "idx": 3}, {"begin": 695, "end": 881, "idx": 4}, {"begin": 882, "end": 995, "idx": 5}, {"begin": 996, "end": 1127, "idx": 6}, {"begin": 1187, "end": 1222, "idx": 7}, {"begin": 1223, "end": 1349, "idx": 8}, {"begin": 1350, "end": 1499, "idx": 9}, {"begin": 1500, "end": 1690, "idx": 10}, {"begin": 1691, "end": 1953, "idx": 11}, {"begin": 1954, "end": 2049, "idx": 12}, {"begin": 2050, "end": 2162, "idx": 13}, {"begin": 2163, "end": 2276, "idx": 14}, {"begin": 2277, "end": 2376, "idx": 15}, {"begin": 2377, "end": 2762, "idx": 16}, {"begin": 2763, "end": 2880, "idx": 17}, {"begin": 2881, "end": 3162, "idx": 18}, {"begin": 3163, "end": 3382, "idx": 19}, {"begin": 3383, "end": 3644, "idx": 20}, {"begin": 3645, "end": 3851, "idx": 21}, {"begin": 3852, "end": 4088, "idx": 22}, {"begin": 4089, "end": 4281, "idx": 23}, {"begin": 4282, "end": 4354, "idx": 24}, {"begin": 4355, "end": 4520, "idx": 25}, {"begin": 4521, "end": 4626, "idx": 26}, {"begin": 4627, "end": 4753, "idx": 27}, {"begin": 4772, "end": 4988, "idx": 28}, {"begin": 4989, "end": 5140, "idx": 29}, {"begin": 5141, "end": 5322, "idx": 30}, {"begin": 5323, "end": 5367, "idx": 31}, {"begin": 5368, "end": 5464, "idx": 32}, {"begin": 5465, "end": 5865, "idx": 33}, {"begin": 5883, "end": 5968, "idx": 34}, {"begin": 5969, "end": 6081, "idx": 35}, {"begin": 6082, "end": 6157, "idx": 36}, {"begin": 6158, "end": 6297, "idx": 37}, {"begin": 6298, "end": 6365, "idx": 38}, {"begin": 6366, "end": 6419, "idx": 39}, {"begin": 6420, "end": 6466, "idx": 40}, {"begin": 6467, "end": 6559, "idx": 41}, {"begin": 6560, "end": 6672, "idx": 42}, {"begin": 6697, "end": 6712, "idx": 43}, {"begin": 6745, "end": 6853, "idx": 44}, {"begin": 6854, "end": 7014, "idx": 45}, {"begin": 7015, "end": 7196, "idx": 46}, {"begin": 7197, "end": 7298, "idx": 47}, {"begin": 7329, "end": 7575, "idx": 48}, {"begin": 7576, "end": 7643, "idx": 49}, {"begin": 7702, "end": 7815, "idx": 50}, {"begin": 7857, "end": 7967, "idx": 51}, {"begin": 7968, "end": 8024, "idx": 52}, {"begin": 8067, "end": 8173, "idx": 53}, {"begin": 8174, "end": 8187, "idx": 54}, {"begin": 8188, "end": 8306, "idx": 55}, {"begin": 8307, "end": 8371, "idx": 56}, {"begin": 8372, "end": 8468, "idx": 57}, {"begin": 8469, "end": 8556, "idx": 58}, {"begin": 8557, "end": 8666, "idx": 59}, {"begin": 8667, "end": 8833, "idx": 60}, {"begin": 8834, "end": 8850, "idx": 61}, {"begin": 8876, "end": 9177, "idx": 62}, {"begin": 9178, "end": 9245, "idx": 63}, {"begin": 9246, "end": 9283, "idx": 64}, {"begin": 9284, "end": 9432, "idx": 65}, {"begin": 9433, "end": 9449, "idx": 66}, {"begin": 9464, "end": 9546, "idx": 67}, {"begin": 9547, "end": 9699, "idx": 68}, {"begin": 9700, "end": 9825, "idx": 69}, {"begin": 9826, "end": 9880, "idx": 70}, {"begin": 9908, "end": 10002, "idx": 71}, {"begin": 10022, "end": 10092, "idx": 72}, {"begin": 10093, "end": 10146, "idx": 73}, {"begin": 10169, "end": 10295, "idx": 74}, {"begin": 10328, "end": 10417, "idx": 75}, {"begin": 10418, "end": 10549, "idx": 76}, {"begin": 10550, "end": 10629, "idx": 77}, {"begin": 10734, "end": 10801, "idx": 78}, {"begin": 10802, "end": 10901, "idx": 79}, {"begin": 10902, "end": 10985, "idx": 80}, {"begin": 11034, "end": 11068, "idx": 81}, {"begin": 11069, "end": 11219, "idx": 82}, {"begin": 11295, "end": 11388, "idx": 83}, {"begin": 11389, "end": 11444, "idx": 84}, {"begin": 11445, "end": 11546, "idx": 85}, {"begin": 11547, "end": 11634, "idx": 86}, {"begin": 11635, "end": 11817, "idx": 87}, {"begin": 11846, "end": 12083, "idx": 88}, {"begin": 12084, "end": 12142, "idx": 89}, {"begin": 12143, "end": 12235, "idx": 90}, {"begin": 12236, "end": 12322, "idx": 91}, {"begin": 12323, "end": 12441, "idx": 92}, {"begin": 12442, "end": 12588, "idx": 93}, {"begin": 12589, "end": 12602, "idx": 94}, {"begin": 12603, "end": 12746, "idx": 95}, {"begin": 12747, "end": 12864, "idx": 96}, {"begin": 12865, "end": 12972, "idx": 97}, {"begin": 12973, "end": 13097, "idx": 98}, {"begin": 13098, "end": 13172, "idx": 99}, {"begin": 13173, "end": 13261, "idx": 100}, {"begin": 13452, "end": 13472, "idx": 101}, {"begin": 13473, "end": 13555, "idx": 102}, {"begin": 13556, "end": 13737, "idx": 103}, {"begin": 13738, "end": 13823, "idx": 104}, {"begin": 13824, "end": 13881, "idx": 105}, {"begin": 13882, "end": 13932, "idx": 106}, {"begin": 13974, "end": 14003, "idx": 107}, {"begin": 14004, "end": 14210, "idx": 108}, {"begin": 14211, "end": 14367, "idx": 109}, {"begin": 14396, "end": 14492, "idx": 110}, {"begin": 14493, "end": 14589, "idx": 111}, {"begin": 14590, "end": 14772, "idx": 112}, {"begin": 14773, "end": 14783, "idx": 113}, {"begin": 14784, "end": 14876, "idx": 114}, {"begin": 14989, "end": 15206, "idx": 115}, {"begin": 15273, "end": 15319, "idx": 116}, {"begin": 15320, "end": 15513, "idx": 117}, {"begin": 15514, "end": 15553, "idx": 118}, {"begin": 15607, "end": 15735, "idx": 119}, {"begin": 15771, "end": 15808, "idx": 120}, {"begin": 15809, "end": 15935, "idx": 121}, {"begin": 16177, "end": 16206, "idx": 122}, {"begin": 16226, "end": 16460, "idx": 123}, {"begin": 16476, "end": 16607, "idx": 124}, {"begin": 16608, "end": 16725, "idx": 125}, {"begin": 16726, "end": 16785, "idx": 126}, {"begin": 16786, "end": 16857, "idx": 127}, {"begin": 16858, "end": 16954, "idx": 128}, {"begin": 16955, "end": 17047, "idx": 129}, {"begin": 17048, "end": 17147, "idx": 130}, {"begin": 17148, "end": 17387, "idx": 131}, {"begin": 17388, "end": 17542, "idx": 132}, {"begin": 17543, "end": 17585, "idx": 133}, {"begin": 17624, "end": 17780, "idx": 134}, {"begin": 17781, "end": 17908, "idx": 135}, {"begin": 17909, "end": 18088, "idx": 136}, {"begin": 18112, "end": 18144, "idx": 137}, {"begin": 18145, "end": 18567, "idx": 138}, {"begin": 18568, "end": 18590, "idx": 139}, {"begin": 18591, "end": 18695, "idx": 140}, {"begin": 18696, "end": 18775, "idx": 141}, {"begin": 18776, "end": 18858, "idx": 142}, {"begin": 18859, "end": 18984, "idx": 143}, {"begin": 18985, "end": 19109, "idx": 144}, {"begin": 19110, "end": 19161, "idx": 145}, {"begin": 19162, "end": 19310, "idx": 146}, {"begin": 19311, "end": 19496, "idx": 147}, {"begin": 19497, "end": 19598, "idx": 148}, {"begin": 19599, "end": 19630, "idx": 149}, {"begin": 19631, "end": 19780, "idx": 150}, {"begin": 19781, "end": 19919, "idx": 151}, {"begin": 19920, "end": 20054, "idx": 152}, {"begin": 20055, "end": 20220, "idx": 153}, {"begin": 20221, "end": 20352, "idx": 154}, {"begin": 20353, "end": 20462, "idx": 155}, {"begin": 20463, "end": 20543, "idx": 156}, {"begin": 20544, "end": 20603, "idx": 157}, {"begin": 20604, "end": 20735, "idx": 158}, {"begin": 20736, "end": 20909, "idx": 159}, {"begin": 20910, "end": 20998, "idx": 160}, {"begin": 20999, "end": 21109, "idx": 161}, {"begin": 21110, "end": 21265, "idx": 162}, {"begin": 21266, "end": 21360, "idx": 163}, {"begin": 21393, "end": 21538, "idx": 164}, {"begin": 21539, "end": 21803, "idx": 165}, {"begin": 21804, "end": 21922, "idx": 166}, {"begin": 21923, "end": 21947, "idx": 167}, {"begin": 21948, "end": 22037, "idx": 168}, {"begin": 22038, "end": 22259, "idx": 169}, {"begin": 22260, "end": 22409, "idx": 170}, {"begin": 22410, "end": 22608, "idx": 171}, {"begin": 22609, "end": 22758, "idx": 172}, {"begin": 22759, "end": 22767, "idx": 173}, {"begin": 22768, "end": 22841, "idx": 174}, {"begin": 22842, "end": 23116, "idx": 175}, {"begin": 23117, "end": 23315, "idx": 176}, {"begin": 23316, "end": 23380, "idx": 177}, {"begin": 23381, "end": 23459, "idx": 178}, {"begin": 23460, "end": 23567, "idx": 179}, {"begin": 23568, "end": 23714, "idx": 180}, {"begin": 23715, "end": 23886, "idx": 181}, {"begin": 23887, "end": 23907, "idx": 182}, {"begin": 23908, "end": 24034, "idx": 183}, {"begin": 24055, "end": 24117, "idx": 184}, {"begin": 24118, "end": 24143, "idx": 185}, {"begin": 24144, "end": 24307, "idx": 186}, {"begin": 24308, "end": 24401, "idx": 187}, {"begin": 24402, "end": 24494, "idx": 188}, {"begin": 24495, "end": 24750, "idx": 189}, {"begin": 24751, "end": 24783, "idx": 190}, {"begin": 24784, "end": 24915, "idx": 191}, {"begin": 24916, "end": 25034, "idx": 192}, {"begin": 25035, "end": 25233, "idx": 193}, {"begin": 25234, "end": 25315, "idx": 194}, {"begin": 25326, "end": 25563, "idx": 195}, {"begin": 25564, "end": 25729, "idx": 196}, {"begin": 25730, "end": 25882, "idx": 197}, {"begin": 25883, "end": 25969, "idx": 198}, {"begin": 25970, "end": 26073, "idx": 199}, {"begin": 26074, "end": 26317, "idx": 200}, {"begin": 26318, "end": 26400, "idx": 201}, {"begin": 26460, "end": 26624, "idx": 202}, {"begin": 26625, "end": 26849, "idx": 203}, {"begin": 26850, "end": 26967, "idx": 204}, {"begin": 26968, "end": 27232, "idx": 205}, {"begin": 27233, "end": 27337, "idx": 206}, {"begin": 27338, "end": 27408, "idx": 207}, {"begin": 27409, "end": 27546, "idx": 208}, {"begin": 27547, "end": 27693, "idx": 209}, {"begin": 27694, "end": 27808, "idx": 210}, {"begin": 27809, "end": 27888, "idx": 211}, {"begin": 27889, "end": 27991, "idx": 212}, {"begin": 28006, "end": 28116, "idx": 213}, {"begin": 28117, "end": 28257, "idx": 214}, {"begin": 28258, "end": 28426, "idx": 215}, {"begin": 28451, "end": 28480, "idx": 216}, {"begin": 28481, "end": 28637, "idx": 217}, {"begin": 28686, "end": 28692, "idx": 218}, {"begin": 28693, "end": 28811, "idx": 219}, {"begin": 28812, "end": 28846, "idx": 220}, {"begin": 29079, "end": 29124, "idx": 221}, {"begin": 29125, "end": 29208, "idx": 222}, {"begin": 29209, "end": 29304, "idx": 223}, {"begin": 29398, "end": 29533, "idx": 224}, {"begin": 29534, "end": 29575, "idx": 225}, {"begin": 29842, "end": 29951, "idx": 226}, {"begin": 29952, "end": 30076, "idx": 227}, {"begin": 30077, "end": 30127, "idx": 228}, {"begin": 30128, "end": 30245, "idx": 229}, {"begin": 30439, "end": 30468, "idx": 230}, {"begin": 30570, "end": 30576, "idx": 231}, {"begin": 30577, "end": 30692, "idx": 232}, {"begin": 30730, "end": 30788, "idx": 233}, {"begin": 30842, "end": 30901, "idx": 234}, {"begin": 30902, "end": 30961, "idx": 235}, {"begin": 31024, "end": 31158, "idx": 236}, {"begin": 31219, "end": 31225, "idx": 237}, {"begin": 31226, "end": 31233, "idx": 238}, {"begin": 31366, "end": 31469, "idx": 239}, {"begin": 31509, "end": 31655, "idx": 240}, {"begin": 31734, "end": 31775, "idx": 241}, {"begin": 31776, "end": 31975, "idx": 242}, {"begin": 31976, "end": 32165, "idx": 243}, {"begin": 32166, "end": 32314, "idx": 244}, {"begin": 32315, "end": 32424, "idx": 245}, {"begin": 32425, "end": 32546, "idx": 246}, {"begin": 32547, "end": 32607, "idx": 247}, {"begin": 32608, "end": 32641, "idx": 248}, {"begin": 32642, "end": 32713, "idx": 249}, {"begin": 32739, "end": 32893, "idx": 250}, {"begin": 32894, "end": 32916, "idx": 251}, {"begin": 32917, "end": 33055, "idx": 252}, {"begin": 33056, "end": 33176, "idx": 253}, {"begin": 33177, "end": 33298, "idx": 254}, {"begin": 33299, "end": 33464, "idx": 255}, {"begin": 33465, "end": 33543, "idx": 256}, {"begin": 33544, "end": 33798, "idx": 257}, {"begin": 33799, "end": 33900, "idx": 258}, {"begin": 33901, "end": 34037, "idx": 259}, {"begin": 34038, "end": 34187, "idx": 260}, {"begin": 34188, "end": 34320, "idx": 261}, {"begin": 34400, "end": 34561, "idx": 262}, {"begin": 34562, "end": 34631, "idx": 263}, {"begin": 34708, "end": 34854, "idx": 264}, {"begin": 34855, "end": 34918, "idx": 265}, {"begin": 34919, "end": 34976, "idx": 266}, {"begin": 34977, "end": 35137, "idx": 267}, {"begin": 35138, "end": 35348, "idx": 268}, {"begin": 35349, "end": 35542, "idx": 269}, {"begin": 35543, "end": 35711, "idx": 270}, {"begin": 35712, "end": 35833, "idx": 271}, {"begin": 35834, "end": 36029, "idx": 272}, {"begin": 36030, "end": 36206, "idx": 273}, {"begin": 36207, "end": 36275, "idx": 274}, {"begin": 36276, "end": 36426, "idx": 275}, {"begin": 36459, "end": 36464, "idx": 276}, {"begin": 36552, "end": 36707, "idx": 277}, {"begin": 36708, "end": 36797, "idx": 278}, {"begin": 36798, "end": 36959, "idx": 279}, {"begin": 36985, "end": 37082, "idx": 280}, {"begin": 37083, "end": 37265, "idx": 281}, {"begin": 37266, "end": 37437, "idx": 282}, {"begin": 37438, "end": 37582, "idx": 283}, {"begin": 37583, "end": 37694, "idx": 284}, {"begin": 37695, "end": 37811, "idx": 285}, {"begin": 37812, "end": 37952, "idx": 286}, {"begin": 37953, "end": 38113, "idx": 287}, {"begin": 38114, "end": 38255, "idx": 288}, {"begin": 38256, "end": 38409, "idx": 289}, {"begin": 38410, "end": 38518, "idx": 290}, {"begin": 38519, "end": 38602, "idx": 291}, {"begin": 38603, "end": 38756, "idx": 292}, {"begin": 38757, "end": 38858, "idx": 293}, {"begin": 38911, "end": 39144, "idx": 294}, {"begin": 39145, "end": 39244, "idx": 295}, {"begin": 39245, "end": 39416, "idx": 296}, {"begin": 39417, "end": 39530, "idx": 297}, {"begin": 39531, "end": 39763, "idx": 298}, {"begin": 39764, "end": 39875, "idx": 299}, {"begin": 39876, "end": 39985, "idx": 300}, {"begin": 39986, "end": 40222, "idx": 301}, {"begin": 40223, "end": 40410, "idx": 302}, {"begin": 40411, "end": 40589, "idx": 303}, {"begin": 40590, "end": 40707, "idx": 304}, {"begin": 40708, "end": 40926, "idx": 305}, {"begin": 40955, "end": 41045, "idx": 306}, {"begin": 41046, "end": 41269, "idx": 307}, {"begin": 41270, "end": 41370, "idx": 308}, {"begin": 41371, "end": 41545, "idx": 309}, {"begin": 41546, "end": 41592, "idx": 310}, {"begin": 41593, "end": 41725, "idx": 311}, {"begin": 41726, "end": 41868, "idx": 312}, {"begin": 41869, "end": 41884, "idx": 313}, {"begin": 41885, "end": 41989, "idx": 314}, {"begin": 41990, "end": 42064, "idx": 315}, {"begin": 42065, "end": 42133, "idx": 316}, {"begin": 42134, "end": 42271, "idx": 317}, {"begin": 42272, "end": 42394, "idx": 318}, {"begin": 42395, "end": 42514, "idx": 319}, {"begin": 42515, "end": 42631, "idx": 320}, {"begin": 42632, "end": 42769, "idx": 321}, {"begin": 42770, "end": 42847, "idx": 322}, {"begin": 42848, "end": 42958, "idx": 323}, {"begin": 42959, "end": 43174, "idx": 324}, {"begin": 43175, "end": 43281, "idx": 325}], "ReferenceToFigure": [{"begin": 1346, "end": 1347, "target": "#fig_9", "idx": 0}, {"begin": 3193, "end": 3195, "target": "#fig_2", "idx": 1}, {"begin": 4232, "end": 4234, "target": "#fig_2", "idx": 2}, {"begin": 8396, "end": 8405, "target": "#fig_2", "idx": 3}, {"begin": 12932, "end": 12934, "target": "#fig_2", "idx": 4}, {"begin": 17409, "end": 17412, "target": "#fig_15", "idx": 5}, {"begin": 18792, "end": 18794, "target": "#fig_4", "idx": 6}, {"begin": 18993, "end": 19002, "target": "#fig_4", "idx": 7}, {"begin": 19202, "end": 19204, "target": "#fig_4", "idx": 8}, {"begin": 19334, "end": 19336, "target": "#fig_4", "idx": 9}, {"begin": 20409, "end": 20411, "target": "#fig_4", "idx": 10}, {"begin": 20470, "end": 20472, "target": "#fig_4", "idx": 11}, {"begin": 20754, "end": 20756, "target": "#fig_4", "idx": 12}, {"begin": 22776, "end": 22785, "target": "#fig_5", "idx": 13}, {"begin": 23184, "end": 23186, "target": "#fig_5", "idx": 14}, {"begin": 23486, "end": 23488, "target": "#fig_5", "idx": 15}, {"begin": 23648, "end": 23650, "target": "#fig_4", "idx": 16}, {"begin": 25378, "end": 25380, "target": "#fig_5", "idx": 17}, {"begin": 25890, "end": 25891, "target": "#fig_8", "idx": 18}, {"begin": 38526, "end": 38527, "idx": 19}, {"begin": 41378, "end": 41380, "target": "#fig_13", "idx": 20}, {"begin": 42761, "end": 42764, "target": "#fig_15", "idx": 21}], "Abstract": [{"begin": 67, "end": 605, "idx": 0}], "SectionFootnote": [{"begin": 43283, "end": 43341, "idx": 0}], "Footnote": [{"begin": 43294, "end": 43341, "id": "foot_0", "n": "1", "idx": 0}]}}