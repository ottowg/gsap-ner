{"text": "Quantifying the Carbon Emissions of Machine Learning\n\nAbstract:\nFrom an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions. * equal contribution Preprint. Under review.\n\n\n1 Introduction\nWhile a decade ago, only a few ML pioneers were training neural networks on GPUs (Graphical Processing Units), in recent years powerful GPUs have become increasingly accessible and used by ML practitioners worldwide. Furthermore, new models often need to beat existing challenges, which entails training on more GPUs, with larger datasets, for a longer time. This expansion brings with it ever-growing costs in terms of the energy needed to fuel it. This trend has been the subject of recent studies aiming to evaluate the climate impact of AI, which have predominantly put the focus on the environmental cost of training large-scale models connected to grids powered by fossil fuels [1, 2]. While these models are not necessarily representative of common practice, we believe that it is important to continue this conversation further and work towards defining the tools and steps that we need to assess the carbon emissions generated by the models we train, as well as to propose ways to reduce those emissions.\nIn this work, we present our Machine Learning Emissions Calculator (https://mlco2.github. io/impact/), a tool for our community to estimate the amount of carbon emissions produced by training ML models. We accompany this tool with a presentation of key concepts and an explanation of the factors impacting emissions. Finally, we end our article with some recommendations of best practices for the overall ML research community, as well as for individual researchers.\n\n2 Quantifying Carbon Emissions in Neural Network Training\nIn order to quantify carbon emissions, we use CO 2 -equivalents (CO 2 eq), which is a standardized measure used to express the global-warming potential of various greenhouse gases as a single number, i.e. as the amount of CO 2 which would have the equivalent global warming impact [3]. We will use this single metric to compare the factors and choices that impact overall amount of emissions produced by training an ML model in the sections below.\n\n2.1 Type of Energy Used\nPractically speaking, it is hard to estimate exactly the amount of CO 2 eq emitted by a cloud server in a given location because the information regarding the energy grid that it is connected to is rarely publicly available. However, if we assume that all servers are connected to local grids at their physical location, we are able to make an estimation of the amount of CO 2 eq that they emit using public data sources [4, 5]. Therefore, in order to create our emissions calculator, we gathered data regarding CO 2 eq emissions of different grid locations and cross-referenced them with known GPU server locations from the three major cloud providers: Google Cloud Platform, Microsoft Azure and Amazon Web Services 2. Our aim in doing this is to illustrate the degree of variability that exists depending on the location of a given server. For instance, in Figure 1, we show the distribution and variation in carbon emissions depending on geographical region. It can be noted that a large amount of variation can be found within a single region; for instance, servers located in North America can emit anywhere between 20g CO 2 eq/kWh in Quebec, Canada to 736.6g CO 2 eq/kWh in Iowa, USA [5].\n\n2.2 Computing Infrastructure and Training Time\nAnother, more subtle, factor in carbon emitted by a neural network is the computing infrastructure used and training time of the model. In terms of performance the number of floating point operations per second (FLOPS) of GPUs has been steadily increasing in recent years, from 100 Giga FLOPS per second in 2004 to up to 15 Tera FLOPS per second in recent hardware [6]. However, with neural network architectures becoming deeper and more complex, recent state-of-the-art models are often trained on multiples GPUs for several weeks (or months) to beat benchmark performance, requiring more and more energy.\nFinally, when it comes to defining a training procedure for ML architectures, there are several elements to consider: for starters, whether it is necessary to train a model from scratch or whether fine-tuning is adequate for the task at hand. Notably, recent research has shown that using pre-trained models with task-specific fine-tuning performs as well as training from scratch, while being more robust, for tasks in image recognition [7, 8] and NLP [9]. Furthermore, when it comes to hyperparameter search, it has been proven both empirically and theoretically that random hyperparameter search is more efficient than grid search for hyperparameter optimization [10], and there is much research being done on ways to improve the efficiency of hyperparameter optimization [11, 12], which makes it possible to continue choosing the right hyperparameters for new models without incurring superfluous computing and energy costs.\n\n3 ML Emissions Calculator and Actionable Items\nIt is difficult to provide clear-cut guidelines for ML researchers to follow in order to reduce the carbon emissions, or specific benchmarks for the training time that a given model or task warrants. Nonetheless, we think that there are certain best practices and actionable items that can be adopted by our community to reduce environmental impact of the ML domain. We present some of these, along with our ML emissions calculator, in the current section.\nQuantify Your Emissions Being informed regarding the factors that impact the quantity of carbon emissions produced by ML research is the first step to making positive changes. It is for this reason that we created our ML Emissions Calculator. This tool, currently in its alpha version, takes as input the details regarding the training of an ML model: the geographical zone of the server, the type of GPU, and the training time, and gives as output the approximate amount of CO 2 eq produced. We collected publicly available data for the 4 main variables of this computation: (i) the energy consumption of hardware (see \"Choose More Efficient Hardware\" below), (ii) the location of providers' regions of compute -which we assumed to be connected to their local grid, (iii) the region's CO 2 eq emissions per kWh and (iv) potential offsets bought by the provider.\nWe intend to adopt an open and transparent approach: the data we used is publicly available, debatable and editable through Github issues and pull requests. We are therefore open to updating data as more information becomes available. Since this paper's core goal is to raise awareness around the carbon emissions of ML, we have also included two educational sections in the website: one about learning the main notions and concepts related to this domain (e.g. RECs, carbon neutrality, etc.), the other about actionable items an individual or an organization can leverage to mitigate their carbon impact.\nChoose Cloud Providers Wisely In recent years, many cloud providers have defined ambitious sustainability goals and are offsetting their emissions through Renewable Energy Certificates (RECs) in an effort to become carbon neutral, a term used to indicate a net zero carbon footprint of an organization. Each REC bought attests that 1 MWh of renewable energy has been added to the energy grid and can be used to offset an equivalent amount of non-renewable energy. For instance, Google Cloud Platform is certified carbon neutral and funds solar and wind farms directly on local grids through RECs [13]. Microsoft Azure is also certified carbon neutral and 44% of its electricity consumption directly comes from renewable energy, according to a 2016 estimate [14]. Finally, to the best of our knowledge, while not yet 100% carbon neutral on an organizational level, Amazon Web Services is also funding renewable energy projects and some of their data centers are powered by renewable energy [15].\nAnother major energy consumption factor of server installations is the power usage effectiveness (PUE) of the centers where the GPUs are hosted, which represents the percentage of energy consumption that is used for cooling, power conversion, and other auxiliary tasks, and can vary immensely. For example, Google Cloud Services has an average PUE of 1.1, meaning that only 11% of their total energy usage is not used for the servers themselves, a ratio that they have been steadily reducing using Reinforcement Learning [16, 17]. Finally, if you rely on a local private compute infrastructure, it is also possible to engage with administrators about quantifying and offsetting the emissions produced, as well as improving the efficiency of your grid -this may help bring your organization toward carbon neutrality and have a significant impact at scale.\nSelect Data Center Location While many cloud providers are carbon neutral, some of their data centers may still be carbon intensive due to the local grid that they are connected to, whereas others will be low carbon and powered solely by renewable energy sources. Hence, selecting the data center location where an algorithm will be trained has a large impact on its direct carbon emissions. This choice can be achieved by consciously selecting the server location before dispatching your jobs. As we illustrated in previous sections, this single choice can make the direct emissions of an algorithm vary by a factor of 40, from 20g CO 2 eq/kWh in a location that uses renewable energy sources to 820g CO 2 eq/kWh in a location that solely relies on fossil fuels [5]. For a model such as VGG [18] or BERT [19], which are trained on multiple GPUs for several weeks, this can correspond to avoiding emitting several hundreds of kilograms of CO 2 eq by training on a server powered by hydroelectricity instead of fossil fuels.\nReduce Wasted Ressources Grid search is still often used in practice, in spite of its low efficiency both in terms of model performance and environmental impact. However, it has been shown that random search (and others) not only is a straightforward replacement but also has potential to significantly accelerate hyperparameter search [20, 21, 22], consequently reducing carbon emissions. Also, while failed experiments are a common part of ML research and are sometimes unavoidable, their number can often be reduced with careful design such as unit tests, integration tests, and extensive and early debugging. Uninformative experiments are also frequent (sometimes unknowingly) -they can be caused by unstable learning algorithms requiring averaging results over many random seeds. Taking the time to carry out a literature review and to understand the potential sources of noise before launching large-scale hyperparameter searches increases the chance of obtaining reproducible and statistically significant results. Hence, reducing the need to extend the experiment cycles.\nChoose More Efficient Hardware The choice of computing hardware can also have a major impact on ML emissions. To perform a comparison between different devices, their compute efficiency can be estimated in FLOPS/W. This estimation is based on their theoretical peak performance with respect to their Thermal Design Power (TDP) 3. Using this approach, it can be found that CPUs can be 10 times less efficient than GPUs while TPU 3 can be 4 to 8 times more efficient than GPUs [23] (refer to Table 4 for details). Interestingly, in contexts where low power consumption and efficiency are important, e.g., for embedded applications, GPUs such as the Jetson AGX Xavier can be 10 to 20 times more efficient than traditional GPUs.\n\n4 Discussion\nThe factors that we discussed in the current work give ML practitioners a certain amount of control over the environmental impact produced by the training of their models. We are aware that these choices are not always possible to make in practice -for instance, the choice of server location can be limited due to privacy considerations in the case of applications in the medical or financial domain, and large amounts of data may be needed to produce most robust models. However, we find that our emissions calculator is a good starting point to estimate the impact that small choices in model training can have on direct carbon emissions resulting from ML research.\nDespite our best efforts, our calculator remains simply an approximation of the true emissions produced by ML training for several reasons: to start with, there is the issue of global load balancing, i.e. if a majority of practitioners choose to run their models in a low-carbon location, the servers will get saturated and other servers will still need be used. In that perspective, the global gain will not be a 40-fold reduction of emissions, but much smaller. Furthermore, there is a lack of transparency with regards to the true quantity of emissions produced by organizations, so while we use the current best publicly-available sources, there is still a large margin of error with regards to the exact quantity of energy consumed and carbon produced -we remain open to additional data sources and numbers. Finally, while in the current version of our tool, we focus on quantifying the emissions of training ML models, there is still the issue of deploying them, since the inference process is also energy-expensive, especially if done continuously and on a large scale. This is something that should be taken into account by ML practitioners in their products that are deployed in real-world settings, for instance by using energy-efficient architectures [24] and computing infrastructure.\nThere are also more far-reaching discussions to be had regarding the environmental value of scientific knowledge in general and of ML research in particular. On one hand, there is valuable research to be done in ML especially with regards to tackling climate change [25, 26], whereas on the other hand, the emissions of the field of ML are growing quickly [1]. We do not propose the solution to this problem, but we believe that there are steps to be taken, for instance by using efficiency as an evaluation criterion (as proposed by [2]) or by taking concrete steps to reduce emissions (as proposed by the current paper). We hope that our work, along with others, will open the door for these conversations and debates to take place, to quantify the environmental impact of our field, and for positive changes that can be made to reduce it.\n\nFootnotes:\n2: The data can be found at https://github.com/mlco2/impact/tree/master/data\n3: Empirical measurement of GFLOPS/W on various ML architecture would provide more accurate numbers but we are only interested in approximate values to compare classes of devices.\n\nReferences:\n\n- Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.- Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint arXiv:1907.10597, 2019.\n\n- Simon Eggleston, Leandro Buendia, Kyoko Miwa, Todd Ngara, and Kiyoto Tanabe. 2006 IPCC guidelines for national greenhouse gas inventories, volume 5. Institute for Global Environmental Strategies Hayama, Japan, 2006.\n\n- WM To and Peter KC Lee. Ghg emissions from electricity consumption: A case study of hong kong from 2002 to 2015 and trends to 2030. Journal of cleaner production, 165:589-598, 2017.\n\n- Matthew Brander, Aman Sood, Charlotte Wylie, Amy Haughton, and Jessica Lovell. Electricity- specific emission factors for grid electricity. Ecometrica, Emissionfactors. com, 2011.\n\n- Ari Harju, Topi Siro, Filippo Federici Canova, Samuli Hakala, and Teemu Rantalaiho. Computa- tional physics on graphics processing units. In Proceedings of the 11th international conference on Applied Parallel and Scientific Computing, pages 3-26. Springer-Verlag, 2012.\n\n- Nima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd Hurst, Christopher B Kendall, Michael B Gotway, and Jianming Liang. Convolutional neural networks for medical image analysis: Full training or fine tuning? IEEE transactions on medical imaging, 35(5):1299-1312, 2016.\n\n- Keiji Yanai and Yoshiyuki Kawano. Food image recognition using deep convolutional network with pre-training and fine-tuning. In 2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 1-6. IEEE, 2015.\n\n- Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifica- tion. arXiv preprint arXiv:1801.06146, 2018.\n\n- James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281-305, 2012.\n\n- Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. arXiv preprint arXiv:1807.01774, 2018.\n\n- Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated Machine Learning, pages 3-33. Springer, 2019.\n\n- Microsoft. Beyond carbon neutral. white paper, 2018.\n\n- Amazon Web Services. Aws & sustainability, 2019.\n\n- Jim Gao. Machine learning applications for data center optimization, 2014.\n\n- Google Data Centers efficiency: How we do it. https://www.google.com/about/ datacenters/efficiency/internal/, 2019. Accessed: 2019-08-23.\n\n- Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\n- Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy- perband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560, 2016.\n\n- Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 2018.\n\n- Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. arXiv preprint arXiv:1807.01774, 2018.\n\n- Paul Teich. Tearing apart google's tpu 3.0 ai coprocessor. https://www.nextplatform. com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/, 2018.\n\n- Ermao Cai, Da-Cheng Juan, Dimitrios Stamoulis, and Diana Marculescu. Neuralpower: Predict and deploy energy-efficient convolutional neural networks. arXiv preprint arXiv:1710.05420, 2017.\n\n- David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman- Brown, et al. Tackling climate change with machine learning. arXiv preprint arXiv:1906.05433, 2019.\n\n- Victor Schmidt, Alexandra Luccioni, S. Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer Chayes, and Yoshua Bengio. Visualizing the consequences of climate change using cycle-consistent adversarial networks. CoRR, abs/1905.03709, 2019.\n\n", "annotations": {"Abstract": [{"begin": 54, "end": 865, "idx": 0}], "Head": [{"begin": 868, "end": 882, "n": "1", "idx": 0}, {"begin": 2365, "end": 2422, "n": "2", "idx": 1}, {"begin": 2872, "end": 2895, "n": "2.1", "idx": 2}, {"begin": 4092, "end": 4138, "n": "2.2", "idx": 3}, {"begin": 5676, "end": 5722, "n": "3", "idx": 4}, {"begin": 12329, "end": 12341, "n": "4", "idx": 5}], "ReferenceToBib": [{"begin": 1567, "end": 1570, "target": "#b0", "idx": 0}, {"begin": 1571, "end": 1573, "target": "#b1", "idx": 1}, {"begin": 2704, "end": 2707, "target": "#b2", "idx": 2}, {"begin": 3317, "end": 3320, "target": "#b3", "idx": 3}, {"begin": 3321, "end": 3323, "target": "#b4", "idx": 4}, {"begin": 4086, "end": 4089, "target": "#b4", "idx": 5}, {"begin": 4504, "end": 4507, "target": "#b5", "idx": 6}, {"begin": 5184, "end": 5187, "target": "#b6", "idx": 7}, {"begin": 5188, "end": 5190, "target": "#b7", "idx": 8}, {"begin": 5199, "end": 5202, "target": "#b8", "idx": 9}, {"begin": 5412, "end": 5416, "target": "#b9", "idx": 10}, {"begin": 5521, "end": 5525, "target": "#b10", "idx": 11}, {"begin": 5526, "end": 5529, "target": "#b11", "idx": 12}, {"begin": 8245, "end": 8249, "idx": 13}, {"begin": 8406, "end": 8410, "target": "#b12", "idx": 14}, {"begin": 8638, "end": 8642, "target": "#b13", "idx": 15}, {"begin": 9165, "end": 9169, "target": "#b14", "idx": 16}, {"begin": 9170, "end": 9173, "target": "#b15", "idx": 17}, {"begin": 10262, "end": 10265, "target": "#b4", "idx": 18}, {"begin": 10291, "end": 10295, "target": "#b16", "idx": 19}, {"begin": 10304, "end": 10308, "target": "#b17", "idx": 20}, {"begin": 10859, "end": 10863, "target": "#b18", "idx": 21}, {"begin": 10864, "end": 10867, "target": "#b19", "idx": 22}, {"begin": 10868, "end": 10871, "target": "#b20", "idx": 23}, {"begin": 12078, "end": 12082, "target": "#b21", "idx": 24}, {"begin": 14273, "end": 14277, "target": "#b22", "idx": 25}, {"begin": 14574, "end": 14578, "target": "#b23", "idx": 26}, {"begin": 14579, "end": 14582, "target": "#b24", "idx": 27}, {"begin": 14664, "end": 14667, "target": "#b0", "idx": 28}, {"begin": 14842, "end": 14845, "target": "#b1", "idx": 29}], "ReferenceToFootnote": [{"begin": 3613, "end": 3614, "target": "#foot_0", "idx": 0}, {"begin": 11930, "end": 11931, "target": "#foot_1", "idx": 1}], "SectionFootnote": [{"begin": 15151, "end": 15418, "idx": 0}], "ReferenceString": [{"begin": 15435, "end": 15583, "id": "b0", "idx": 0}, {"begin": 15585, "end": 15692, "id": "b1", "idx": 1}, {"begin": 15696, "end": 15911, "id": "b2", "idx": 2}, {"begin": 15915, "end": 16096, "id": "b3", "idx": 3}, {"begin": 16100, "end": 16279, "id": "b4", "idx": 4}, {"begin": 16283, "end": 16553, "id": "b5", "idx": 5}, {"begin": 16557, "end": 16829, "id": "b6", "idx": 6}, {"begin": 16833, "end": 17058, "id": "b7", "idx": 7}, {"begin": 17062, "end": 17199, "id": "b8", "idx": 8}, {"begin": 17203, "end": 17345, "id": "b9", "idx": 9}, {"begin": 17349, "end": 17499, "id": "b10", "idx": 10}, {"begin": 17503, "end": 17624, "id": "b11", "idx": 11}, {"begin": 17628, "end": 17680, "id": "b12", "idx": 12}, {"begin": 17684, "end": 17732, "id": "b13", "idx": 13}, {"begin": 17736, "end": 17810, "id": "b14", "idx": 14}, {"begin": 17814, "end": 17951, "id": "b15", "idx": 15}, {"begin": 17955, "end": 18097, "id": "b16", "idx": 16}, {"begin": 18101, "end": 18287, "id": "b17", "idx": 17}, {"begin": 18291, "end": 18488, "id": "b18", "idx": 18}, {"begin": 18492, "end": 18687, "id": "b19", "idx": 19}, {"begin": 18691, "end": 18841, "id": "b20", "idx": 20}, {"begin": 18845, "end": 18997, "id": "b21", "idx": 21}, {"begin": 19001, "end": 19188, "id": "b22", "idx": 22}, {"begin": 19192, "end": 19461, "id": "b23", "idx": 23}, {"begin": 19465, "end": 19715, "id": "b24", "idx": 24}], "ReferenceToTable": [{"begin": 12099, "end": 12100, "idx": 0}], "Footnote": [{"begin": 15162, "end": 15238, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 15239, "end": 15418, "id": "foot_1", "n": "3", "idx": 1}], "Paragraph": [{"begin": 64, "end": 865, "idx": 0}, {"begin": 883, "end": 1896, "idx": 1}, {"begin": 1897, "end": 2363, "idx": 2}, {"begin": 2423, "end": 2870, "idx": 3}, {"begin": 2896, "end": 4090, "idx": 4}, {"begin": 4139, "end": 4745, "idx": 5}, {"begin": 4746, "end": 5674, "idx": 6}, {"begin": 5723, "end": 6179, "idx": 7}, {"begin": 6180, "end": 7042, "idx": 8}, {"begin": 7043, "end": 7648, "idx": 9}, {"begin": 7649, "end": 8643, "idx": 10}, {"begin": 8644, "end": 9498, "idx": 11}, {"begin": 9499, "end": 10522, "idx": 12}, {"begin": 10523, "end": 11602, "idx": 13}, {"begin": 11603, "end": 12327, "idx": 14}, {"begin": 12342, "end": 13010, "idx": 15}, {"begin": 13011, "end": 14307, "idx": 16}, {"begin": 14308, "end": 15149, "idx": 17}], "SectionHeader": [{"begin": 0, "end": 865, "idx": 0}], "SectionReference": [{"begin": 15420, "end": 19717, "idx": 0}], "Sentence": [{"begin": 64, "end": 224, "idx": 0}, {"begin": 225, "end": 439, "idx": 1}, {"begin": 440, "end": 628, "idx": 2}, {"begin": 629, "end": 820, "idx": 3}, {"begin": 821, "end": 851, "idx": 4}, {"begin": 852, "end": 865, "idx": 5}, {"begin": 883, "end": 1099, "idx": 6}, {"begin": 1100, "end": 1241, "idx": 7}, {"begin": 1242, "end": 1332, "idx": 8}, {"begin": 1333, "end": 1574, "idx": 9}, {"begin": 1575, "end": 1896, "idx": 10}, {"begin": 1897, "end": 1986, "idx": 11}, {"begin": 1987, "end": 2099, "idx": 12}, {"begin": 2100, "end": 2213, "idx": 13}, {"begin": 2214, "end": 2363, "idx": 14}, {"begin": 2423, "end": 2708, "idx": 15}, {"begin": 2709, "end": 2870, "idx": 16}, {"begin": 2896, "end": 3120, "idx": 17}, {"begin": 3121, "end": 3324, "idx": 18}, {"begin": 3325, "end": 3615, "idx": 19}, {"begin": 3616, "end": 3737, "idx": 20}, {"begin": 3738, "end": 3857, "idx": 21}, {"begin": 3858, "end": 4090, "idx": 22}, {"begin": 4139, "end": 4274, "idx": 23}, {"begin": 4275, "end": 4508, "idx": 24}, {"begin": 4509, "end": 4745, "idx": 25}, {"begin": 4746, "end": 4988, "idx": 26}, {"begin": 4989, "end": 5203, "idx": 27}, {"begin": 5204, "end": 5674, "idx": 28}, {"begin": 5723, "end": 5922, "idx": 29}, {"begin": 5923, "end": 6089, "idx": 30}, {"begin": 6090, "end": 6179, "idx": 31}, {"begin": 6180, "end": 6355, "idx": 32}, {"begin": 6356, "end": 6422, "idx": 33}, {"begin": 6423, "end": 6672, "idx": 34}, {"begin": 6673, "end": 7042, "idx": 35}, {"begin": 7043, "end": 7199, "idx": 36}, {"begin": 7200, "end": 7277, "idx": 37}, {"begin": 7278, "end": 7504, "idx": 38}, {"begin": 7505, "end": 7648, "idx": 39}, {"begin": 7649, "end": 7951, "idx": 40}, {"begin": 7952, "end": 8112, "idx": 41}, {"begin": 8113, "end": 8250, "idx": 42}, {"begin": 8251, "end": 8411, "idx": 43}, {"begin": 8412, "end": 8643, "idx": 44}, {"begin": 8644, "end": 8937, "idx": 45}, {"begin": 8938, "end": 9174, "idx": 46}, {"begin": 9175, "end": 9498, "idx": 47}, {"begin": 9499, "end": 9762, "idx": 48}, {"begin": 9763, "end": 9890, "idx": 49}, {"begin": 9891, "end": 9993, "idx": 50}, {"begin": 9994, "end": 10266, "idx": 51}, {"begin": 10267, "end": 10522, "idx": 52}, {"begin": 10523, "end": 10684, "idx": 53}, {"begin": 10685, "end": 10912, "idx": 54}, {"begin": 10913, "end": 11135, "idx": 55}, {"begin": 11136, "end": 11307, "idx": 56}, {"begin": 11308, "end": 11544, "idx": 57}, {"begin": 11545, "end": 11602, "idx": 58}, {"begin": 11603, "end": 11712, "idx": 59}, {"begin": 11713, "end": 11817, "idx": 60}, {"begin": 11818, "end": 11932, "idx": 61}, {"begin": 11933, "end": 12114, "idx": 62}, {"begin": 12115, "end": 12327, "idx": 63}, {"begin": 12342, "end": 12513, "idx": 64}, {"begin": 12514, "end": 12814, "idx": 65}, {"begin": 12815, "end": 13010, "idx": 66}, {"begin": 13011, "end": 13373, "idx": 67}, {"begin": 13374, "end": 13474, "idx": 68}, {"begin": 13475, "end": 13823, "idx": 69}, {"begin": 13824, "end": 14087, "idx": 70}, {"begin": 14088, "end": 14307, "idx": 71}, {"begin": 14308, "end": 14465, "idx": 72}, {"begin": 14466, "end": 14668, "idx": 73}, {"begin": 14669, "end": 14930, "idx": 74}, {"begin": 14931, "end": 15149, "idx": 75}], "ReferenceToFigure": [{"begin": 3762, "end": 3763, "target": "#fig_0", "idx": 0}], "Div": [{"begin": 64, "end": 865, "idx": 0}, {"begin": 868, "end": 2363, "idx": 1}, {"begin": 2365, "end": 2870, "idx": 2}, {"begin": 2872, "end": 4090, "idx": 3}, {"begin": 4092, "end": 5674, "idx": 4}, {"begin": 5676, "end": 12327, "idx": 5}, {"begin": 12329, "end": 15149, "idx": 6}], "SectionMain": [{"begin": 865, "end": 15149, "idx": 0}]}}