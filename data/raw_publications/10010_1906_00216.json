{"text": "Robust Learning under Label Noise with Iterative Noise-Filtering\n\nAbstract:\nWe consider the problem of training a model under the presence of label noise. Current approaches identify samples with potentially incorrect labels and reduce their influence on the learning process by either assigning lower weights to them or completely removing them from the training set. In the first case the model however still learns from noisy labels; in the latter approach, good training data can be lost. In this paper, we propose an iterative semi-supervised mechanism for robust learning which excludes noisy labels but is still able to learn from the corresponding samples. To this end, we add an unsupervised loss term that also serves as a regularizer against the remaining label noise. We evaluate our approach on common classification tasks with different noise ratios. Our robust models outperform the stateof-the-art methods by a large margin. Especially for very large noise ratios, we achieve up to 20% absolute improvement compared to the previous best model.\n\nMain:\n\n\n\n1. Introduction\nIn many supervised learning applications, a clean labeled dataset is the key to success. However, in real-world scenarios, label noise inevitably originates from different sources such as inconsistent labelers or the difficulty of the labeling task itself. In many classification tasks for example samples that cannot be squeezed into a strict categorical scheme will lead to inconsistent labels.\nWith traditional supervised learning, the present label noise decreases the performance of classification models since they tend to over-fit to the samples with noisy labels. This results in lower accuracy and inferior generalization properties. To avoid the negative influence of noisy labels, a common approach is to use sample-dependent loss weights as learning regularizers (Jiang et al., 2017; Ren et al., 2018). However, the performance of these mechanisms strongly depends on the respective hyperparameters that are difficult to set.\nTypically the loss weights w are restricted to w \u2208 [0, 1] by design to resemble a probability of a noisy label given a sample. In a supervised learning framework, however, even with tiny (e.g., 0.01) loss weights, the model could still receive a strong learning signal from noisy samples (as, e.g., in (Ren et al., 2018)). A perfect case is assigning weights w = 0 to samples with noisy labels which, however, implies ignoring those samples and results in a smaller training dataset.\nIn this paper, instead of training in a supervised framework, we learn from the samples with noisy labels in an unsupervised way. Since the input data are not noisy but only the labels, semi-supervised learning can still exploit the raw data samples. By keeping those samples rather than remov- ing them from training our proposed method can be more strict when it comes to removing potentially noisy labels.\nIn more detail, we propose a learning scheme consisting of (1) iterative filtering of noisy labels and (2) semi-supervised learning to regularize the problem in the vicinity of noisy samples. Fig. 2 shows a simplified overview of the concept. We refer to the proposed training procedure as Iterative Filtering with Semi-Supervised Learning (IF-SSL). To the best of our knowledge, we propose the first approach that only removes the noisy labels instead of the complete data samples using filtering. Our approach requires no new dedicated mechanism for robust learning and utilizes only existing standard components for learning.\nThe proposed algorithm was evaluated on classification tasks for CIFAR-10 & CIFAR-100 with a varying label noise ratio from 0% to 80%. We show results both for a clean validation set and a noisy one. In both cases, we show that using the filtered data as unlabeled samples significantly outperforms complete removal of the data. As a consequence, the proposed model consistently outperforms state of the art at all levels of label noise; see Fig. 1. Despite the simplicity of the training pipeline, our approach shows robust performance even in case of high noise ratios. The source code will be made available together with the published paper.\n\n2. Robust Learning with Iterative\nnoise-filtering 2.1. Overview Fig. 2 shows an overview of our proposed approach. In the beginning, we assume that the labels of the training set are noisy (up to a certain noise ratio). We use a small validation set to measure the improvement in model performance. In each iteration, we first apply semi-supervised training until we find the best model w.r.t. the performance on the validation set (e.g., by early-stopping). In the next step, we use the moving-average-prediction results of the best model to filter out potentially noisy labels based on the strategy defined in Section 2.2. In the next iteration, we again use all data and the new filtered label set as input for the model training.\nThe iterative training procedure stops when no better model can be found. Our filtering pipeline only requires a standard component of training deep learning models.\nTo provide a powerful regularizer against label noise, the semi-supervised model treats all data points as additional unlabeled samples. Concretely, in the first iteration, the model learns from supervised and unsupervised learning objectives on the complete dataset. Subsequently, the unsupervised learning objective continuously derives learning signals from all data points while the supervised learning objective is computed only on a filtered set of labeled samples. Over these iterations, the label noise in the training set is expected to reduce.\nIn the following, we give more details about the combination of this training and filtering procedure with existing techniques from semi-supervised learning. Let train and valid(D i , D val ) denote a training procedure which will be explained in detail in Section 2.3. Using these notations, the label filtering algorithm is given in Algorithm 1.\n\n2.2. Iterative Filtering\n\n\nLet\nThe label filtering is performed on the original label set from iteration 0. In this way, clean labels erroneously removed in an earlier iteration (e.g., labels of hard to classify samples) can be used for the model training again. This is a major difference to typical iterative filtering approaches where the filtering at iteration i is restricted to training samples from the respective iteration only.\nWe apply a variant of easy sample mining and filter out training samples based on the model's agreement with the provided label. That means the labels are only used for supervised training if in the current epoch the model predicts the respective label to be the correct class with the highest Algorithm 1 Iterative noisy labels filtering Input: D 0 , D val , n max iterations Output: M best Initialize M best := train and valid(D 0 , D val ) i := 1 while true do likelihood. This is reflected in Algorithm 1 line 12 to line 14.M i := train and valid(D i , D val ) if acc(M best , D val ) \u2265 acc(M i , D val ) or i > n max iterations then return M best end M best := M i D f ilter := D 0 for (x, l j ) in D f ilter do {l 1 : c 1 , . . . , l m : c m }: = M i (x) if c j = max(c i , i = 1, m) then Replace l j by l \u2205 in D f ilter end end i := i + 1 D i := D f ilter end\nThe model's predictions required for filtering can be stored during training directly. However, the predictions for noisy samples tend to fluctuate. For example, take a cat wrongly labeled as a tiger. Other cat samples would encourage the model to predict the given cat image as a cat. Contrary, the wrong label tiger regularly pulls the model back to predict the cat as a tiger. Hence, using the model's predictions gathered in one single training epoch for filtering is suboptimal.\nInstead, we propose to collect the sample predictions over multiple training epochs. This scheme is displayed in Fig. 3. For each sample, we store the moving averaged predictions, accumulated over the last iterations. Besides having a more stable basis for the filtering step, our proposed procedure also leads to negligible memory and computation overhead.\nDue to continuous training of the best model from the previous model, computation time can be significantly reduced, compared to re-training the model from scratch. On the new filtered dataset, the model must only slowly adapt to the new noise ratio contained in the training set. Depending on the computation budget, a maximal number of iterations for filtering can be set to save time.\nMoreover, the new training procedure does not require specific mechanisms or algorithms which need to be implemented or fine-tuned. Implementation-wise, it can be realized by looping the standard training procedure and filter potentially noisy samples at the end of each training run.\n\n2.3. Unsupervised learning to counter label noise\nAlthough the proposed learning procedure is not restricted to classification tasks, in this work, we explain the procedure for classification as a use-case.\nModel training is performed using two types of learning objectives: (1) supervised and (2) unsupervised losses. Supervised learning from noisy-labeled samples is straightforward and can be done with typical n-way-classification losses. The unsupervised learning objective, however, requires a design choice of which data to be used (defined in Section 2.2) and how to learn from them.\n\n2.3.1. LEARNING FROM UNLABELED DATA\nWe learn from all data points in a semi-supervised fashion. Concretely, in addition to supervised learning with filtered labels, unsupervised learning is applied to the entire dataset. Our learning strategy can take advantage of unsupervised learning from a large dataset, and therefore it has a potentially large regularization effect against label noise. Unsupervised learning objectives impose additional constraints on all samples, which are hard to follow for wrongly labeled samples. These constraints could be a preference of extreme predictions (Entropy-loss) or non-fluctuating model predictions over many past iterations (Mean-teacher-loss). Both constraints are explained in the following.\n\nEntropy minimization\nThe typical entropy loss for semisupervised learning is shown in Fig. 8. It encourages the model to provide extreme predictions (such as 0 or 1) for each sample. Over a large number of samples, the model should balance its predictions over all classes.\nThe entropy loss can easily be applied to all samples to express the uncertainty about the provided labels. Alternatively, the loss can be combined with a strict filtering strategy, as in our work, which removes the labels of potentially wrongly labeled samples.\nFor a large noise ratio, predictions of wrongly labeled samples fluctuate strongly over previous training iterations. Am-  plifying these network decisions could lead to even noisier models model. Combined with iterative filtering, the framework will have to rely on a single noisy model snapshot.\nIn the case of an unsuitable snapshot, the filtering step will make many wrong decisions.\nMean Teacher model A better way to perform semisupervised learning and counteract label noise is to employ the Mean Teacher model (Tarvainen & Valpola, 2017). The Mean Teacher model follows the student-teacher learning procedure from (Hinton et al., 2015). The main idea is to create a virtuous learning cycle, in which the student continually learns to surpass the (better) teacher. Concretely, the Mean Teacher is an exponential moving average of the student models over training iterations.\nIn contrast to learning from the entropy-loss, the Mean-Teacher solves precisely the problem of noisy models snapshots. The teacher-model is a moving-average from the past training iterations and hence much more stable than a single snapshot.\nThe training of such a model is shown in Fig. 5 Mean Teacher model for iterative filtering Given the setting in Section 2.2, we apply the Mean Teacher algorithm in each iteration i in the train and valid(D i , D val ) procedure as follows.\n\u2022 -Update the weights of M s i using the selected optimizer -Update the weights of M t i as an exponential moving average of the student weights -Evaluate performance of M s i and M t i over D val to verify the early stopping criteria.\n\u2022 Return the best M t i The consistency loss between students and teachers output distribution can be realized with Mean-Square-Error or Kullback-Leibler-divergence.\nOverlapping data split between labeled and unlabeled samples While traditionally the dataset is strictly divided into non-overlapping labeled and unlabeled sets, we treat all samples also as unsupervised samples, even if they are in the set of filtered, labeled samples. This is important since despite the filtering the provided labels can be wrong. By considering them additionally as unsupervised samples, the consistency of the model prediction for a potentially noisy sample is evaluated among many other samples, resulting in more consistent model predictions. Therefore, learning from all samples in an unsupervised fashion provides a stronger regularization effect against label noise.\n\n3. Related Works\nDifferent approaches to counter label noise have been proposed in (Azadi et al., 2015; Reed et al., 2014; Ren et al., 2018; Jiang et al., 2017; Jenni & Favaro, 2018). Some of these works (Azadi et al., 2015; Ren et al., 2018) require additional clean training data. Often, the loss for potentially noisy labels is re-weighted softly to push the model away from the wrong label (Jiang et al., 2017; Ren et al., 2018).\nCompared to these works, we perform an extreme filtering by setting the sample weight of the potentially wrongly labeled samples to 0. These labels are no longer used for the supervised objective of the task. Moreover, we perform the filtering step very seldom, in contrast to epoch-wise-samples re-weighting of previous approaches. Furthermore, contrary to all previous robust learning approaches, we utilize iterative training combined with semi-supervised learning to combat label noise for the first time.\nDespite recent advances in semi-supervised learning (Rasmus et al., 2015; Makhzani et al., 2015; Kingma et al., 2014; Kumar et al., 2017; Springenberg, 2015; Miyato et al., 2018; Dai et al., 2017), it has not been considered as a regularization technique against label noise. Semi-supervised learning often uses generative modeling (Kingma & Welling, 2013; Kingma et al., 2016; Rezende et al., 2014; Goodfellow et al., 2014) as an auxiliary task. In contrast to using generative models, the Mean Teacher model proposed in (Tarvainen & Valpola, 2017) has a more stable training procedure. The Mean Teacher does not require any additional generative model. More details are explained in Section 2.3.\nTypically, unsupervised learning is only applied to unlabeled data. Contrary, in our approach, unsupervised learning is applied to all samples to expresses the uncertainty of the provided labels.\nAlthough previous robust learning approaches such as (Wang et al., 2018) also use iterative training and filtering, their approach does not employ learning from removed samples in an unsupervised fashion. Furthermore, they always filter strictly, i.e., each sample removal decision is final.\nIn IF-SSL we only filter potentially noisy labels from the original label set, but still, use the corresponding instances for unsupervised learning. This gives the model a chance to revert a wrong filtering decision in earlier iterations.\nFurther, our framework is intentionally kept more simple and generic than previous techniques. The focus of our framework is the iterative filtering of noisy labels while learning from all samples in an unsupervised fashion as a form of regularization. This paradigm is hence easily transferable to other tasks than classification.  . We analyze the typical situation with uniform noise, in which a label is randomly flipped to another class. Further experiments on ImageNet-ILSVRC is in the Appendix.\n\n4. Evaluation\n\n\n4.1.2. COMPARISONS TO RELATED WORKS\nWe compare our framework IF-SSL (Iterative Filtering + Semi-supervised Learning) to previous robust learning approaches such as MentorNet (Jiang et al., 2017), Learned and random sample weights from (Ren et al., 2018), S-Model (Goldberger & Ben-Reuven, 2016), bi-level learning (Jenni & Favaro, 2018), Reed-Hard (Reed et al., 2014) and Iterative learning in open-set problems (Wang et al., 2018).\nHyperparameters and early-stopping are determined on the noisy validation set. This is possible because the noise of the validation and training sets is not correlated. Hence, higher validation performance often results in superior test performance.\nAdditionally, (Ren et al., 2018) considered the setting of having a small clean validation set of 1000 images. For comparison purposes, we also experiment with a small clean set for early stopping.\nWhenever possible, we adopt the performances of their methods from the corresponding publications. Sometimes, not all numbers are reported in these publications.\n\n4.1.3. NETWORK CONFIGURATION AND TRAINING\nFor the basic training of semi-supervised models, we use a Mean Teacher model (Tarvainen & Valpola, 2017) available on GitHub 1. The students and teacher networks are residual networks (He et al., 2016) with 26 layers. They are trained with Shake-Shake-regularization (Gastaldi, 2017). We use the PyTorch (Paszke et al., 2017) implementation of the network and keep the training settings close to (Tarvainen & Valpola, 2017). The network is trained with Stochastic Gradient Descent. In each filtering iteration, the model is trained for a maximum of 300 epochs, with a patience of 50 epochs. For more training details, see the appendix.\nTo filter the noise iteratively, we use the early stopping strategy based on the validation set. After the best model is found, we use it to filter out potentially noisy samples from the noisy training label set at iteration 0. In the next iteration, the previously best model is fine-tuned on the new dataset. All data is used for unsupervised learning, while supervised learning only considers the filtered labels set at the current iteration. We stop the iterative filtering if no better model is found.\n\n4.1.4. STRUCTURE OF ANALYSIS\nWe start with the analysis of our model's performance under different noise ratios. We compare our performance to other previously reported approaches in learning under different noise ratios using the accuracy metric on CIFAR-10 and CIFAR-100. The subsequent ablation study highlights the importance of each component in our framework.\nFurther, we analyze the consequence of applying our iterative filtering scheme to different network architectures. Afterwards, we show the performance of simple unsupervised learning objectives, with and without our iterative filtering scheme. For more experiments, we refer to the supplemental material.\n\n4.2. Robust Learning Performance Evaluation\n\n\n4.2.1. MODEL ACCURACY UNDER LABEL NOISE\nResults for typical scenarios with noise ratio of 40% or 80% on CIFAR-10 and CIFAR-100 are shown in Tab. 2. More results are visualized in Fig. 1 (CIFAR-10) and Fig. 6a (CIFAR-100). The baseline model is the typical ResNet-26 with a n-way-classification loss (Negative-Log-likelihoodobjective).\nCompared to the model baseline and other previously reported approaches, IF-SSL outperforms them by a large margin. Even in areas of high noise ratio up to 80%, the classification performance of our model remains highly robust. Despite the noisy validation set, our model still identifies the noisy labels and filters them out. On CIFAR-10 and CIFAR-100, our model IF-SSL achieves 20% and 7% absolute improvement over previously reported results.\nA small clean validation set gives the model an even better estimate of the generalization error on unseen data (IF-SSL*). Due to the iterative filtering scheme, our model always attempts to improve the performance on the validation set as much as possible, without doing gradient steps    Naive training or leaving out any of the proposed mechanism leads to rapid performance decrease. Our framework combines the strength of both techniques to form an extremely effective regularizer against learning from label noise.\n\n4.2.3. ITERATIVE FILTERING WITH DIFFERENT\n\n\nARCHITECTURES\nTab. 4 shows the effect of iterative filtering on various architectures. For traditional network training, Resnet26 performs best and slightly better than its shallower counterpart Resnet18. Extremely deep architectures like Resnet101 suffer more from the high-noise ratios.\nWith the proposed iterative filtering, the performance gaps between different models are massively reduced. After iterative filtering, Resnet26 and Resnet18 perform similarly Tab. 5 shows different semi-supervised learning strategies with and without iterative filtering. The push-away-loss corresponds to assigning negative weights to potentially noisy labels. The entropy loss minimizes the network's uncertainty on a set of samples. Since our labels are all potentially noisy, it is meaningful to apply this loss to all training samples instead of removed samples only. Hence we compare both variants. The Mean-teacher loss is always applied to all samples (details in the appendix).\nWithout filtering: Learning from the entropy-loss performs second-best, when the uncertainty is minimized on all samples. Without the previous filtering step, there is no set of unlabeled samples to perform a traditional semi-supervisedlearning. The Mean-teacher performs best since the teacher represents a stable model state, aggregated over multiple iterations.\nWith filtering: Applying entropy-loss to all samples or only unsupervised samples leads to very similar performance. Both are better than the standard push-away-loss. Our Mean Teacher achieves by far the best performance, due to the temporal ensemble of models and sample predictions for filtering.\n\n5. Conclusion\nIn this work, we propose a training pipeline for robust learning. Our method relies on two key components: (1) iterative filtering of potentially noisy labels, and (2) regularization by learning from all raw data samples in an unsupervised fashion.\nWe have shown that neither iterative noise filtering (IF) nor semi-supervised learning (SSL) alone is sufficient to achieve competitive performance. Contrary, we combine IF and SSL and extend them with crucial novel components for more robust learning.\nUnlike previous filtering approaches, we always filter the initial label set provided at the beginning. Furthermore, we utilize a temporal ensemble of model predictions as the basis for the filtering step.\nThe proposed algorithm is evaluated on classification tasks for CIFAR-10 and CIFAR-100 with a varying label noise ratio from 0% to 80%. We show results both for a clean validation set and a noisy one. In both cases, we show that using the filtered data as unlabeled samples significantly outperforms complete removal of the data. As a consequence, the proposed model consistently outperforms state of the art at all levels of label noise. Despite the simplicity of the training pipeline, our approach shows robust performance even in case of high noise ratios.\n\nA. Large-scale classification on ImageNet-ILSVRC-2015\nTab. 7 shows the precision@1 and @5 of various models, given 40% label noise in the training set. Our networks are based on ResNext18 and Resnext50. Note that MentorNet (Jiang et al., 2017) uses Resnet101 (P@1:78.25)  (Goyal et al., 2017), which has similar performance compared to Resnext50 (P@1: 77.8) (Xie et al., 2017)\n\nB. Complete removal of samples\nTab. 7 shows the results of deleting samples from the training set. It leads to large performances gaps compared to our strategy (IF-SSL), which considers the removed samples as unlabeled data. In case of a considerable label noise of 80%, the gap is close to 9%.\nContinuously using the filtered samples lead to significantly better results. The unsupervised-loss provides meaningful  Contrary, with our effective filtering strategy, both models slowly increase their performance while the training accuracy approaches 100%. Hence, by using iterative filtering, our model could erase the inconsistency in the provided labels set.\n\nC. Training process\n\n\nD. Training details D.1. CIFAR-10 and CIFAR-100\nNetwork training For the training our model IF-SSL, we use the standard configuration provided by (Tarvainen & Valpola, 2017) 2 . Concretely, we use the SGD-optimizer with Nesterov (Sutskever et al., 2013) momentum, a learning rate of 0.05 with cosine learning rate annealing (Loshchilov & Hutter, 2016), a weight decay of 2e-4, max iteration per filtering step of 300, patience of 50 epochs, total epochs count of 600.\nFor basic training of baselines models without semisupervised learning, we had to set the learning rate to 0.01.\nIn the case of higher learning rates, the loss typically explodes. Every other option is kept the same.\nSemi-supervised learning For the mean teacher training, additional hyperparameters are required. In both cases of CIFAR-10 and CIFAR-100, we again take the standard configuration with the consistency loss to mean-squared-error and a consistency weight: 100.0, logit distance cost: 0.01, consistency-ramp-up:5. The total batch-size is 512, with 124 samples being reserved for labeled samples, 388 for unlabeled data. Each epoch is defined as a complete processing of all unlabeled data. When training without semisupervised-learning, the entire batch is used for labeled data.\n\nData augmentation\nThe data are normalized to zeromean and standard-variance of one. Further, we use realtime data augmentation with random translation and reflection, subsequently random horizontal flip. The standard PyTorch-library provides these transformations.\n\nD.2. ImageNet-ILSVRC-2015\nNetwork Training The network used for evaluation were ResNet (He et al., 2016) and Resnext (Xie et al., 2017) for training. All ResNext variants use a cardinality of 32 and base width of 4 (32x4d). ResNext models follow the same structure as their Resnet counterparts, except for the cardinality and base width.\nAll other configurations are kept as close as possible to (Tarvainen & Valpola, 2017). The initial learning rate to handle large batches (Goyal et al., 2017) is set to 0.1; the base learning rate is 0.025 with a single cycle of cosine annealing.\nSemi-supervised learning Due to the large images, the batch size is set to 40 in total with 20/20 for labeled and unlabeled samples respectively. We found the Kullback-2 https://github.com/CuriousAI/mean-teacher divergence leads to no meaningful network training. Hence, we set the consistency loss to mean-squared-error, with a weight of 1000. We use consistency ramp up of 5 epochs to give the mean teacher more time in the beginning. Weight decay is set to 5e-5; patience is four epochs to stop training in the current filtering iteration.\nFiltering We filter noisy samples with the topk=5 strategy, instead of topk=1 as on CIFAR-10 and CIFAR-100. That means the samples are kept for supervised training if their provided label lies within the top 5 predictions of the model. The main reason is that each image of ImageNet might contain multiple objects. Filtering with topk=1 is too strict and would lead to a small recall of the correct samples detection.\nData Augmentation For all data, we normalize the RGBimages by the mean: (0.\n\nE. Losses\nFor the learning of wrongly labeled samples, Fig. 8 shows the relationship between the typical reweighting scheme and our baseline push-away-loss. Typically, reweighting is applied directly to the losses with samples weights w (i) for each sample i as shown in Eq. 4 min w (i) j N LL(y(i) label |x (i) , D)\nD is the dataset,x (i) and y (i) label are the samples i and its noisy label. w (i) j is the samples weight for the sample i at step j. Negative samples weights w (i) j are often assigned to push the network away from the wrong labels. Let w\nIn other words, we perform gradient ascent for wrongly labeled samples. However, the Negative-log-likelihood is not designed for gradient ascent. Hence the gradients of wrongly labeled samples vanish if the prediction is too close to the noisy label. This effect is similar to the training of\n\nFootnotes:\n1: https://github.com/CuriousAI/mean-teacher\n\nReferences:\n\n- Azadi, S., Feng, J., Jegelka, S., and Darrell, T. Auxiliary im- age regularization for deep cnns with noisy labels. arXiv preprint arXiv:1511.07069, 2015.- Dai, Z., Yang, Z., Yang, F., Cohen, W. W., and Salakhutdi- nov, R. R. Good semi-supervised learning that requires a bad gan. In Advances in Neural Information Processing Systems, pp. 6510-6520, 2017.\n\n- Gastaldi, X. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.\n\n- Goldberger, J. and Ben-Reuven, E. Training deep neural- networks using a noise adaptation layer. 2016.\n\n- Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative Adversarial Nets. pp. 9.\n\n- Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672-2680, 2014.\n\n- Goyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n- Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\n- Jenni, S. and Favaro, P. Deep bilevel learning. In ECCV, 2018.\n\n- Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. Men- torNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels. arXiv:1712.05055 [cs], December 2017. URL http://arxiv.org/ abs/1712.05055. arXiv: 1712.05055.\n\n- Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\n- Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative mod- els. In Advances in neural information processing sys- tems, pp. 3581-3589, 2014.\n\n- Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. Improved variational in- ference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pp. 4743-4751, 2016.\n\n- Kumar, A., Sattigeri, P., and Fletcher, T. Semi-supervised learning with gans: manifold invariance with improved inference. In Advances in Neural Information Processing Systems, pp. 5534-5544, 2017.\n\n- Loshchilov, I. and Hutter, F. Sgdr: Stochastic gra- dient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\n- Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.\n\n- Miyato, T., Maeda, S.-i., Ishii, S., and Koyama, M. Virtual adversarial training: a regularization method for super- vised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018.\n\n- Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in pytorch. 2017.\n\n- Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and Raiko, T. Semi-supervised learning with ladder networks. In Advances in Neural Information Processing Systems, pp. 3546-3554, 2015.\n\n- Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A. Training deep neural networks on noisy la- bels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.\n\n- Ren, M., Zeng, W., Yang, B., and Urtasun, R. Learn- ing to Reweight Examples for Robust Deep Learn- ing. arXiv:1803.09050 [cs, stat], March 2018. URL http://arxiv.org/abs/1803.09050. arXiv: 1803.09050.\n\n- Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep gen- erative models. arXiv preprint arXiv:1401.4082, 2014.\n\n- Springenberg, J. T. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.\n\n- Sutskever, I., Martens, J., Dahl, G. E., and Hinton, G. E. On the importance of initialization and momentum in deep learning. ICML (3), 28(1139-1147):5, 2013.\n\n- Tarvainen, A. and Valpola, H. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195-1204, 2017.\n\n- Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.-T. Iterative learning with open-set noisy labels. arXiv preprint arXiv:1804.00092, 2018.\n\n- Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., and He, K. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1492-1500, 2017.\n\n", "annotations": {"ReferenceToFootnote": [{"begin": 17246, "end": 17247, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 1067, "end": 28318, "idx": 0}], "SectionReference": [{"begin": 28377, "end": 33233, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1067, "idx": 0}], "Div": [{"begin": 76, "end": 1059, "idx": 0}, {"begin": 1070, "end": 4191, "idx": 1}, {"begin": 4193, "end": 5994, "idx": 2}, {"begin": 5996, "end": 6021, "idx": 3}, {"begin": 6023, "end": 8814, "idx": 4}, {"begin": 8816, "end": 9407, "idx": 5}, {"begin": 9409, "end": 10145, "idx": 6}, {"begin": 10147, "end": 13144, "idx": 7}, {"begin": 13146, "end": 16016, "idx": 8}, {"begin": 16018, "end": 16032, "idx": 9}, {"begin": 16034, "end": 17076, "idx": 10}, {"begin": 17078, "end": 18263, "idx": 11}, {"begin": 18265, "end": 18935, "idx": 12}, {"begin": 18937, "end": 18981, "idx": 13}, {"begin": 18983, "end": 20284, "idx": 14}, {"begin": 20286, "end": 20328, "idx": 15}, {"begin": 20330, "end": 21969, "idx": 16}, {"begin": 21971, "end": 23253, "idx": 17}, {"begin": 23255, "end": 23631, "idx": 18}, {"begin": 23633, "end": 24293, "idx": 19}, {"begin": 24295, "end": 24315, "idx": 20}, {"begin": 24317, "end": 25577, "idx": 21}, {"begin": 25579, "end": 25843, "idx": 22}, {"begin": 25845, "end": 27465, "idx": 23}, {"begin": 27467, "end": 28318, "idx": 24}], "Head": [{"begin": 1070, "end": 1085, "n": "1.", "idx": 0}, {"begin": 4193, "end": 4226, "n": "2.", "idx": 1}, {"begin": 5996, "end": 6020, "n": "2.2.", "idx": 2}, {"begin": 6023, "end": 6026, "idx": 3}, {"begin": 8816, "end": 8865, "n": "2.3.", "idx": 4}, {"begin": 9409, "end": 9444, "n": "2.3.1.", "idx": 5}, {"begin": 10147, "end": 10167, "idx": 6}, {"begin": 13146, "end": 13162, "n": "3.", "idx": 7}, {"begin": 16018, "end": 16031, "n": "4.", "idx": 8}, {"begin": 16034, "end": 16069, "n": "4.1.2.", "idx": 9}, {"begin": 17078, "end": 17119, "n": "4.1.3.", "idx": 10}, {"begin": 18265, "end": 18293, "n": "4.1.4.", "idx": 11}, {"begin": 18937, "end": 18980, "n": "4.2.", "idx": 12}, {"begin": 18983, "end": 19022, "n": "4.2.1.", "idx": 13}, {"begin": 20286, "end": 20327, "n": "4.2.3.", "idx": 14}, {"begin": 20330, "end": 20343, "idx": 15}, {"begin": 21971, "end": 21984, "n": "5.", "idx": 16}, {"begin": 23255, "end": 23308, "idx": 17}, {"begin": 23633, "end": 23663, "idx": 18}, {"begin": 24295, "end": 24314, "idx": 19}, {"begin": 24317, "end": 24364, "idx": 20}, {"begin": 25579, "end": 25596, "idx": 21}, {"begin": 25845, "end": 25870, "idx": 22}, {"begin": 27467, "end": 27476, "idx": 23}], "Paragraph": [{"begin": 76, "end": 1059, "idx": 0}, {"begin": 1086, "end": 1482, "idx": 1}, {"begin": 1483, "end": 2023, "idx": 2}, {"begin": 2024, "end": 2507, "idx": 3}, {"begin": 2508, "end": 2916, "idx": 4}, {"begin": 2917, "end": 3545, "idx": 5}, {"begin": 3546, "end": 4191, "idx": 6}, {"begin": 4227, "end": 4926, "idx": 7}, {"begin": 4927, "end": 5092, "idx": 8}, {"begin": 5093, "end": 5646, "idx": 9}, {"begin": 5647, "end": 5994, "idx": 10}, {"begin": 6027, "end": 6432, "idx": 11}, {"begin": 6433, "end": 6961, "idx": 12}, {"begin": 7300, "end": 7783, "idx": 13}, {"begin": 7784, "end": 8141, "idx": 14}, {"begin": 8142, "end": 8529, "idx": 15}, {"begin": 8530, "end": 8814, "idx": 16}, {"begin": 8866, "end": 9022, "idx": 17}, {"begin": 9023, "end": 9407, "idx": 18}, {"begin": 9445, "end": 10145, "idx": 19}, {"begin": 10168, "end": 10420, "idx": 20}, {"begin": 10421, "end": 10683, "idx": 21}, {"begin": 10684, "end": 10981, "idx": 22}, {"begin": 10982, "end": 11071, "idx": 23}, {"begin": 11072, "end": 11565, "idx": 24}, {"begin": 11566, "end": 11808, "idx": 25}, {"begin": 11809, "end": 12048, "idx": 26}, {"begin": 12049, "end": 12284, "idx": 27}, {"begin": 12285, "end": 12450, "idx": 28}, {"begin": 12451, "end": 13144, "idx": 29}, {"begin": 13163, "end": 13579, "idx": 30}, {"begin": 13580, "end": 14089, "idx": 31}, {"begin": 14090, "end": 14787, "idx": 32}, {"begin": 14788, "end": 14983, "idx": 33}, {"begin": 14984, "end": 15275, "idx": 34}, {"begin": 15276, "end": 15514, "idx": 35}, {"begin": 15515, "end": 16016, "idx": 36}, {"begin": 16070, "end": 16466, "idx": 37}, {"begin": 16467, "end": 16716, "idx": 38}, {"begin": 16717, "end": 16914, "idx": 39}, {"begin": 16915, "end": 17076, "idx": 40}, {"begin": 17120, "end": 17756, "idx": 41}, {"begin": 17757, "end": 18263, "idx": 42}, {"begin": 18294, "end": 18630, "idx": 43}, {"begin": 18631, "end": 18935, "idx": 44}, {"begin": 19023, "end": 19317, "idx": 45}, {"begin": 19318, "end": 19764, "idx": 46}, {"begin": 19765, "end": 20284, "idx": 47}, {"begin": 20344, "end": 20618, "idx": 48}, {"begin": 20619, "end": 21305, "idx": 49}, {"begin": 21306, "end": 21670, "idx": 50}, {"begin": 21671, "end": 21969, "idx": 51}, {"begin": 21985, "end": 22233, "idx": 52}, {"begin": 22234, "end": 22486, "idx": 53}, {"begin": 22487, "end": 22692, "idx": 54}, {"begin": 22693, "end": 23253, "idx": 55}, {"begin": 23309, "end": 23631, "idx": 56}, {"begin": 23664, "end": 23927, "idx": 57}, {"begin": 23928, "end": 24293, "idx": 58}, {"begin": 24365, "end": 24784, "idx": 59}, {"begin": 24785, "end": 24897, "idx": 60}, {"begin": 24898, "end": 25001, "idx": 61}, {"begin": 25002, "end": 25577, "idx": 62}, {"begin": 25597, "end": 25843, "idx": 63}, {"begin": 25871, "end": 26182, "idx": 64}, {"begin": 26183, "end": 26428, "idx": 65}, {"begin": 26429, "end": 26971, "idx": 66}, {"begin": 26972, "end": 27389, "idx": 67}, {"begin": 27390, "end": 27465, "idx": 68}, {"begin": 27477, "end": 27762, "idx": 69}, {"begin": 27784, "end": 28025, "idx": 70}, {"begin": 28026, "end": 28318, "idx": 71}], "ReferenceToBib": [{"begin": 1861, "end": 1881, "target": "#b10", "idx": 0}, {"begin": 1882, "end": 1899, "target": "#b21", "idx": 1}, {"begin": 2326, "end": 2344, "target": "#b21", "idx": 2}, {"begin": 11202, "end": 11229, "target": "#b25", "idx": 3}, {"begin": 11306, "end": 11327, "target": "#b8", "idx": 4}, {"begin": 13229, "end": 13249, "target": "#b0", "idx": 5}, {"begin": 13250, "end": 13268, "target": "#b20", "idx": 6}, {"begin": 13269, "end": 13286, "target": "#b21", "idx": 7}, {"begin": 13287, "end": 13306, "target": "#b10", "idx": 8}, {"begin": 13307, "end": 13328, "target": "#b9", "idx": 9}, {"begin": 13350, "end": 13370, "target": "#b0", "idx": 10}, {"begin": 13371, "end": 13388, "target": "#b21", "idx": 11}, {"begin": 13540, "end": 13560, "target": "#b10", "idx": 12}, {"begin": 13561, "end": 13578, "target": "#b21", "idx": 13}, {"begin": 14142, "end": 14163, "target": "#b19", "idx": 14}, {"begin": 14164, "end": 14186, "target": "#b16", "idx": 15}, {"begin": 14187, "end": 14207, "target": "#b12", "idx": 16}, {"begin": 14208, "end": 14227, "target": "#b14", "idx": 17}, {"begin": 14228, "end": 14247, "target": "#b23", "idx": 18}, {"begin": 14248, "end": 14268, "target": "#b17", "idx": 19}, {"begin": 14269, "end": 14286, "target": "#b1", "idx": 20}, {"begin": 14422, "end": 14446, "target": "#b11", "idx": 21}, {"begin": 14447, "end": 14467, "target": "#b13", "idx": 22}, {"begin": 14468, "end": 14489, "target": "#b22", "idx": 23}, {"begin": 14490, "end": 14514, "target": "#b5", "idx": 24}, {"begin": 14612, "end": 14639, "target": "#b25", "idx": 25}, {"begin": 15037, "end": 15055, "target": "#b26", "idx": 26}, {"begin": 16208, "end": 16228, "target": "#b10", "idx": 27}, {"begin": 16269, "end": 16287, "target": "#b21", "idx": 28}, {"begin": 16297, "end": 16328, "target": "#b3", "idx": 29}, {"begin": 16348, "end": 16370, "target": "#b9", "idx": 30}, {"begin": 16372, "end": 16401, "idx": 31}, {"begin": 16446, "end": 16465, "target": "#b26", "idx": 32}, {"begin": 16731, "end": 16749, "target": "#b21", "idx": 33}, {"begin": 17198, "end": 17225, "target": "#b25", "idx": 34}, {"begin": 17305, "end": 17322, "target": "#b7", "idx": 35}, {"begin": 17388, "end": 17404, "target": "#b2", "idx": 36}, {"begin": 17425, "end": 17446, "target": "#b18", "idx": 37}, {"begin": 17517, "end": 17544, "target": "#b25", "idx": 38}, {"begin": 23478, "end": 23498, "target": "#b10", "idx": 39}, {"begin": 23527, "end": 23547, "target": "#b6", "idx": 40}, {"begin": 23613, "end": 23631, "target": "#b27", "idx": 41}, {"begin": 24463, "end": 24490, "target": "#b25", "idx": 42}, {"begin": 24546, "end": 24570, "target": "#b24", "idx": 43}, {"begin": 24641, "end": 24668, "target": "#b15", "idx": 44}, {"begin": 25932, "end": 25949, "target": "#b7", "idx": 45}, {"begin": 25962, "end": 25980, "target": "#b27", "idx": 46}, {"begin": 26241, "end": 26268, "target": "#b25", "idx": 47}, {"begin": 26320, "end": 26340, "target": "#b6", "idx": 48}], "ReferenceString": [{"begin": 28392, "end": 28546, "id": "b0", "idx": 0}, {"begin": 28548, "end": 28747, "id": "b1", "idx": 1}, {"begin": 28751, "end": 28830, "id": "b2", "idx": 2}, {"begin": 28834, "end": 28936, "id": "b3", "idx": 3}, {"begin": 28940, "end": 29088, "id": "b4", "idx": 4}, {"begin": 29092, "end": 29308, "id": "b5", "idx": 5}, {"begin": 29312, "end": 29523, "id": "b6", "idx": 6}, {"begin": 29527, "end": 29715, "id": "b7", "idx": 7}, {"begin": 29719, "end": 29841, "id": "b8", "idx": 8}, {"begin": 29845, "end": 29907, "id": "b9", "idx": 9}, {"begin": 29911, "end": 30160, "id": "b10", "idx": 10}, {"begin": 30164, "end": 30264, "id": "b11", "idx": 11}, {"begin": 30268, "end": 30460, "id": "b12", "idx": 12}, {"begin": 30464, "end": 30691, "id": "b13", "idx": 13}, {"begin": 30695, "end": 30893, "id": "b14", "idx": 14}, {"begin": 30897, "end": 31021, "id": "b15", "idx": 15}, {"begin": 31025, "end": 31156, "id": "b16", "idx": 16}, {"begin": 31160, "end": 31382, "id": "b17", "idx": 17}, {"begin": 31386, "end": 31550, "id": "b18", "idx": 18}, {"begin": 31554, "end": 31741, "id": "b19", "idx": 19}, {"begin": 31745, "end": 31926, "id": "b20", "idx": 20}, {"begin": 31930, "end": 32131, "id": "b21", "idx": 21}, {"begin": 32135, "end": 32300, "id": "b22", "idx": 22}, {"begin": 32304, "end": 32454, "id": "b23", "idx": 23}, {"begin": 32458, "end": 32616, "id": "b24", "idx": 24}, {"begin": 32620, "end": 32845, "id": "b25", "idx": 25}, {"begin": 32849, "end": 33007, "id": "b26", "idx": 26}, {"begin": 33011, "end": 33231, "id": "b27", "idx": 27}], "Sentence": [{"begin": 76, "end": 154, "idx": 0}, {"begin": 155, "end": 368, "idx": 1}, {"begin": 369, "end": 492, "idx": 2}, {"begin": 493, "end": 664, "idx": 3}, {"begin": 665, "end": 779, "idx": 4}, {"begin": 780, "end": 864, "idx": 5}, {"begin": 865, "end": 940, "idx": 6}, {"begin": 941, "end": 1059, "idx": 7}, {"begin": 1086, "end": 1174, "idx": 8}, {"begin": 1175, "end": 1342, "idx": 9}, {"begin": 1343, "end": 1482, "idx": 10}, {"begin": 1483, "end": 1657, "idx": 11}, {"begin": 1658, "end": 1728, "idx": 12}, {"begin": 1729, "end": 1900, "idx": 13}, {"begin": 1901, "end": 2023, "idx": 14}, {"begin": 2024, "end": 2150, "idx": 15}, {"begin": 2151, "end": 2346, "idx": 16}, {"begin": 2347, "end": 2507, "idx": 17}, {"begin": 2508, "end": 2637, "idx": 18}, {"begin": 2638, "end": 2758, "idx": 19}, {"begin": 2759, "end": 2916, "idx": 20}, {"begin": 2917, "end": 3108, "idx": 21}, {"begin": 3109, "end": 3159, "idx": 22}, {"begin": 3160, "end": 3266, "idx": 23}, {"begin": 3267, "end": 3415, "idx": 24}, {"begin": 3416, "end": 3545, "idx": 25}, {"begin": 3546, "end": 3680, "idx": 26}, {"begin": 3681, "end": 3745, "idx": 27}, {"begin": 3746, "end": 3874, "idx": 28}, {"begin": 3875, "end": 3995, "idx": 29}, {"begin": 3996, "end": 4117, "idx": 30}, {"begin": 4118, "end": 4191, "idx": 31}, {"begin": 4227, "end": 4247, "idx": 32}, {"begin": 4248, "end": 4307, "idx": 33}, {"begin": 4308, "end": 4412, "idx": 34}, {"begin": 4413, "end": 4491, "idx": 35}, {"begin": 4492, "end": 4651, "idx": 36}, {"begin": 4652, "end": 4817, "idx": 37}, {"begin": 4818, "end": 4926, "idx": 38}, {"begin": 4927, "end": 5000, "idx": 39}, {"begin": 5001, "end": 5092, "idx": 40}, {"begin": 5093, "end": 5229, "idx": 41}, {"begin": 5230, "end": 5360, "idx": 42}, {"begin": 5361, "end": 5564, "idx": 43}, {"begin": 5565, "end": 5646, "idx": 44}, {"begin": 5647, "end": 5804, "idx": 45}, {"begin": 5805, "end": 5916, "idx": 46}, {"begin": 5917, "end": 5994, "idx": 47}, {"begin": 6027, "end": 6258, "idx": 48}, {"begin": 6259, "end": 6432, "idx": 49}, {"begin": 6433, "end": 6561, "idx": 50}, {"begin": 6562, "end": 6908, "idx": 51}, {"begin": 6909, "end": 6961, "idx": 52}, {"begin": 7300, "end": 7386, "idx": 53}, {"begin": 7387, "end": 7448, "idx": 54}, {"begin": 7449, "end": 7500, "idx": 55}, {"begin": 7501, "end": 7585, "idx": 56}, {"begin": 7586, "end": 7679, "idx": 57}, {"begin": 7680, "end": 7783, "idx": 58}, {"begin": 7784, "end": 7868, "idx": 59}, {"begin": 7869, "end": 7904, "idx": 60}, {"begin": 7905, "end": 8001, "idx": 61}, {"begin": 8002, "end": 8141, "idx": 62}, {"begin": 8142, "end": 8306, "idx": 63}, {"begin": 8307, "end": 8422, "idx": 64}, {"begin": 8423, "end": 8529, "idx": 65}, {"begin": 8530, "end": 8661, "idx": 66}, {"begin": 8662, "end": 8814, "idx": 67}, {"begin": 8866, "end": 9022, "idx": 68}, {"begin": 9023, "end": 9134, "idx": 69}, {"begin": 9135, "end": 9258, "idx": 70}, {"begin": 9259, "end": 9407, "idx": 71}, {"begin": 9445, "end": 9504, "idx": 72}, {"begin": 9505, "end": 9629, "idx": 73}, {"begin": 9630, "end": 9801, "idx": 74}, {"begin": 9802, "end": 9934, "idx": 75}, {"begin": 9935, "end": 10096, "idx": 76}, {"begin": 10097, "end": 10145, "idx": 77}, {"begin": 10168, "end": 10240, "idx": 78}, {"begin": 10241, "end": 10329, "idx": 79}, {"begin": 10330, "end": 10420, "idx": 80}, {"begin": 10421, "end": 10528, "idx": 81}, {"begin": 10529, "end": 10683, "idx": 82}, {"begin": 10684, "end": 10801, "idx": 83}, {"begin": 10802, "end": 10880, "idx": 84}, {"begin": 10881, "end": 10981, "idx": 85}, {"begin": 10982, "end": 11071, "idx": 86}, {"begin": 11072, "end": 11230, "idx": 87}, {"begin": 11231, "end": 11328, "idx": 88}, {"begin": 11329, "end": 11455, "idx": 89}, {"begin": 11456, "end": 11565, "idx": 90}, {"begin": 11566, "end": 11685, "idx": 91}, {"begin": 11686, "end": 11808, "idx": 92}, {"begin": 11809, "end": 12048, "idx": 93}, {"begin": 12049, "end": 12284, "idx": 94}, {"begin": 12285, "end": 12450, "idx": 95}, {"begin": 12451, "end": 12721, "idx": 96}, {"begin": 12722, "end": 12801, "idx": 97}, {"begin": 12802, "end": 13017, "idx": 98}, {"begin": 13018, "end": 13144, "idx": 99}, {"begin": 13163, "end": 13329, "idx": 100}, {"begin": 13330, "end": 13428, "idx": 101}, {"begin": 13429, "end": 13579, "idx": 102}, {"begin": 13580, "end": 13788, "idx": 103}, {"begin": 13789, "end": 13912, "idx": 104}, {"begin": 13913, "end": 14089, "idx": 105}, {"begin": 14090, "end": 14365, "idx": 106}, {"begin": 14366, "end": 14536, "idx": 107}, {"begin": 14537, "end": 14677, "idx": 108}, {"begin": 14678, "end": 14744, "idx": 109}, {"begin": 14745, "end": 14787, "idx": 110}, {"begin": 14788, "end": 14855, "idx": 111}, {"begin": 14856, "end": 14983, "idx": 112}, {"begin": 14984, "end": 15188, "idx": 113}, {"begin": 15189, "end": 15275, "idx": 114}, {"begin": 15276, "end": 15424, "idx": 115}, {"begin": 15425, "end": 15514, "idx": 116}, {"begin": 15515, "end": 15609, "idx": 117}, {"begin": 15610, "end": 15767, "idx": 118}, {"begin": 15768, "end": 15849, "idx": 119}, {"begin": 15850, "end": 15957, "idx": 120}, {"begin": 15958, "end": 16016, "idx": 121}, {"begin": 16070, "end": 16466, "idx": 122}, {"begin": 16467, "end": 16545, "idx": 123}, {"begin": 16546, "end": 16635, "idx": 124}, {"begin": 16636, "end": 16716, "idx": 125}, {"begin": 16717, "end": 16827, "idx": 126}, {"begin": 16828, "end": 16914, "idx": 127}, {"begin": 16915, "end": 17013, "idx": 128}, {"begin": 17014, "end": 17076, "idx": 129}, {"begin": 17120, "end": 17248, "idx": 130}, {"begin": 17249, "end": 17338, "idx": 131}, {"begin": 17339, "end": 17405, "idx": 132}, {"begin": 17406, "end": 17545, "idx": 133}, {"begin": 17546, "end": 17602, "idx": 134}, {"begin": 17603, "end": 17711, "idx": 135}, {"begin": 17712, "end": 17756, "idx": 136}, {"begin": 17757, "end": 17853, "idx": 137}, {"begin": 17854, "end": 18067, "idx": 138}, {"begin": 18068, "end": 18202, "idx": 139}, {"begin": 18203, "end": 18263, "idx": 140}, {"begin": 18294, "end": 18377, "idx": 141}, {"begin": 18378, "end": 18538, "idx": 142}, {"begin": 18539, "end": 18630, "idx": 143}, {"begin": 18631, "end": 18745, "idx": 144}, {"begin": 18746, "end": 18874, "idx": 145}, {"begin": 18875, "end": 18935, "idx": 146}, {"begin": 19023, "end": 19130, "idx": 147}, {"begin": 19131, "end": 19204, "idx": 148}, {"begin": 19205, "end": 19317, "idx": 149}, {"begin": 19318, "end": 19433, "idx": 150}, {"begin": 19434, "end": 19545, "idx": 151}, {"begin": 19546, "end": 19645, "idx": 152}, {"begin": 19646, "end": 19764, "idx": 153}, {"begin": 19765, "end": 19887, "idx": 154}, {"begin": 19888, "end": 20151, "idx": 155}, {"begin": 20152, "end": 20284, "idx": 156}, {"begin": 20344, "end": 20416, "idx": 157}, {"begin": 20417, "end": 20534, "idx": 158}, {"begin": 20535, "end": 20618, "idx": 159}, {"begin": 20619, "end": 20726, "idx": 160}, {"begin": 20727, "end": 20890, "idx": 161}, {"begin": 20891, "end": 20980, "idx": 162}, {"begin": 20981, "end": 21054, "idx": 163}, {"begin": 21055, "end": 21191, "idx": 164}, {"begin": 21192, "end": 21223, "idx": 165}, {"begin": 21224, "end": 21305, "idx": 166}, {"begin": 21306, "end": 21427, "idx": 167}, {"begin": 21428, "end": 21551, "idx": 168}, {"begin": 21552, "end": 21670, "idx": 169}, {"begin": 21671, "end": 21787, "idx": 170}, {"begin": 21788, "end": 21837, "idx": 171}, {"begin": 21838, "end": 21969, "idx": 172}, {"begin": 21985, "end": 22050, "idx": 173}, {"begin": 22051, "end": 22233, "idx": 174}, {"begin": 22234, "end": 22382, "idx": 175}, {"begin": 22383, "end": 22486, "idx": 176}, {"begin": 22487, "end": 22590, "idx": 177}, {"begin": 22591, "end": 22692, "idx": 178}, {"begin": 22693, "end": 22828, "idx": 179}, {"begin": 22829, "end": 22893, "idx": 180}, {"begin": 22894, "end": 23022, "idx": 181}, {"begin": 23023, "end": 23131, "idx": 182}, {"begin": 23132, "end": 23253, "idx": 183}, {"begin": 23309, "end": 23406, "idx": 184}, {"begin": 23407, "end": 23457, "idx": 185}, {"begin": 23458, "end": 23525, "idx": 186}, {"begin": 23526, "end": 23631, "idx": 187}, {"begin": 23664, "end": 23731, "idx": 188}, {"begin": 23732, "end": 23857, "idx": 189}, {"begin": 23858, "end": 23927, "idx": 190}, {"begin": 23928, "end": 24005, "idx": 191}, {"begin": 24006, "end": 24188, "idx": 192}, {"begin": 24189, "end": 24293, "idx": 193}, {"begin": 24365, "end": 24494, "idx": 194}, {"begin": 24495, "end": 24784, "idx": 195}, {"begin": 24785, "end": 24897, "idx": 196}, {"begin": 24898, "end": 24964, "idx": 197}, {"begin": 24965, "end": 25001, "idx": 198}, {"begin": 25002, "end": 25098, "idx": 199}, {"begin": 25099, "end": 25311, "idx": 200}, {"begin": 25312, "end": 25417, "idx": 201}, {"begin": 25418, "end": 25487, "idx": 202}, {"begin": 25488, "end": 25577, "idx": 203}, {"begin": 25597, "end": 25662, "idx": 204}, {"begin": 25663, "end": 25782, "idx": 205}, {"begin": 25783, "end": 25843, "idx": 206}, {"begin": 25871, "end": 25994, "idx": 207}, {"begin": 25995, "end": 26068, "idx": 208}, {"begin": 26069, "end": 26182, "idx": 209}, {"begin": 26183, "end": 26269, "idx": 210}, {"begin": 26270, "end": 26428, "idx": 211}, {"begin": 26429, "end": 26574, "idx": 212}, {"begin": 26575, "end": 26640, "idx": 213}, {"begin": 26641, "end": 26692, "idx": 214}, {"begin": 26693, "end": 26773, "idx": 215}, {"begin": 26774, "end": 26865, "idx": 216}, {"begin": 26866, "end": 26971, "idx": 217}, {"begin": 26972, "end": 27079, "idx": 218}, {"begin": 27080, "end": 27207, "idx": 219}, {"begin": 27208, "end": 27286, "idx": 220}, {"begin": 27287, "end": 27389, "idx": 221}, {"begin": 27390, "end": 27465, "idx": 222}, {"begin": 27477, "end": 27623, "idx": 223}, {"begin": 27624, "end": 27762, "idx": 224}, {"begin": 27784, "end": 27861, "idx": 225}, {"begin": 27862, "end": 27919, "idx": 226}, {"begin": 27920, "end": 28019, "idx": 227}, {"begin": 28020, "end": 28025, "idx": 228}, {"begin": 28026, "end": 28097, "idx": 229}, {"begin": 28098, "end": 28171, "idx": 230}, {"begin": 28172, "end": 28276, "idx": 231}, {"begin": 28277, "end": 28318, "idx": 232}], "ReferenceToFigure": [{"begin": 3114, "end": 3115, "target": "#fig_1", "idx": 0}, {"begin": 3993, "end": 3994, "target": "#fig_0", "idx": 1}, {"begin": 4262, "end": 4263, "target": "#fig_1", "idx": 2}, {"begin": 7902, "end": 7903, "target": "#fig_3", "idx": 3}, {"begin": 10238, "end": 10239, "idx": 4}, {"begin": 11855, "end": 11856, "target": "#fig_5", "idx": 5}, {"begin": 19167, "end": 19168, "target": "#fig_0", "idx": 6}, {"begin": 19189, "end": 19191, "idx": 7}, {"begin": 27527, "end": 27528, "idx": 8}], "Abstract": [{"begin": 66, "end": 1059, "idx": 0}], "SectionFootnote": [{"begin": 28320, "end": 28375, "idx": 0}], "Footnote": [{"begin": 28331, "end": 28375, "id": "foot_0", "n": "1", "idx": 0}]}}