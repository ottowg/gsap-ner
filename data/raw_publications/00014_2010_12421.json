{"text": "TWEETEVAL: Unified Benchmark and Comparative Evaluation for Tweet Classification\n\nAbstract:\nThe experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domainspecific data. In this paper, we propose a new evaluation framework (TWEETEVAL) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora.\n\n\n1 Introduction\nModern NLP systems are typically ill-equipped when applied to noisy user-generated text. The high-paced, conversational and idiosyncratic nature of social media, paired with platform-specific restrictions (e.g., Twitter's character limit), requires tackling additional challenges, for example, POS tagging (Derczynski et al., 2013), lexical normalization (Han and Baldwin, 2011; Baldwin et al., 2015), or named entity recognition (Ritter et al., 2011; Baldwin et al., 2013). In other more generic contexts, these challenges can be considered solved or are simply non-existent. Moreover, other apparently simple tasks such as sentiment analysis have proven to be hard on Twitter data (Poria et al., 2020), among others, due to limited amount of contextual cues available in short texts (Kim et al., 2014). In addition to these and other inherent difficulties, advances in NLP for user-generated data are hindered by its highly fragmented landscape and the lack of a unified evaluation framework. In the current era of pretraining and Language Models (LMs), this is particularly relevant, as these models exhibit a versatility that currently cannot be gauged comparably across Twitter datasets and tasks. This is not the case, however, in more ordinary textual genres and domains. For instance, well known benchmarks like SentEval (Conneau and Kiela, 2018), GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) include standard NLP tasks such as language inference, paraphrase detection or sentiment analysis, among others. It is undisputable that these benchmarks have contributed to the fast development of language understanding techniques, and LMs in particular, as they have enabled comprehensive evaluations across several tasks in fair and reproducible conditions.\nWe thus take inspiration from the above to develop TWEETEVAL, a benchmark for tweet classification in English. TWEETEVAL is a standardized test bed for seven tweet classification tasks. These are: sentiment analysis, emotion recognition, offensive language detection, hate speech detection, stance prediction, emoji prediction, and irony detection. We develop a unified framework, unified criteria for train/validation/test splits, and evaluate strong baselines inspired by current SotA in these tasks. We also evaluate transformer-based models, trained entirely and partially on Twitter data, with which we aim to establish a competitive high bar for subsequent contributions. The contributions of this paper are therefore as follows: (1) we compile, curate and release a suite of tasks under the umbrella of a new benchmark: TWEETEVAL 1 , a unified framework comprising several tweet classification tasks; and (2) we evaluate state-of-theart LMs in this new framework, and shed light on the effect of training with different corpora.\n\nDataset Tweet Label\nEmoji Thx for showing this newbie passholder around @ Disneyland Emotion I love swimming for the same reason I love meditating...the feeling of weightlessness. joy Hate Another illegal alien that shouldn't be in America killed an innocent American couple! #BuildThatWall hateful Irony\nLeaving whilst its dark is fun. #not ironic Offensive Are we all ready to sit and watch Indakurate Passcott play football? non-offensive Sentiment Hmmmmm where are the #BlackLivesMatter when matters like this a rise... kids are a disgrace!! negative Stance(fem) Rather be an \"ugly\" feminist then be these sad people that throws hat on people that believes in equality! in favour Table 1 : Tweet samples for each of the tasks we consider in TweetEval, alongside their label in their original datasets. We use (fem) to refer to the feminism subset of the stance detection dataset.\n\n2 TweetEval: The Benchmark\nIn this section, we describe the compilation, curation and unification procedure behind the construction of TWEETEVAL and its corresponding tasks, as well as relevant statistics and evaluation metrics. We also show, in Table 1, a sample tweet and its corresponding label from the original task.\n\n2.1 Tasks\nEmotion Recognition. This task consists of recognizing the emotion evoked by a tweet. We use the dataset of the most participated task of Se-mEval2018, \"Affects in Tweets\" (Mohammad et al., 2018). The original competition was framed as a multi-label classification problem, including 11 emotions. The integration into TWEETEVAL consists of re-purposing this multi-label dataset into multi-class classification, keeping only the tweets labeled with a single emotion. Since the amount of tweets with single labels was scarce, we selected the most common four emotions (Anger, Joy, Sadness, Optimism) 2.\nEmoji Prediction. This task consists in, given a tweet, predicting its most likely emoji, and is based on the Emoji Prediction challenge at Semeval2018 (Barbieri et al., 2018). It only considers tweets with one emoji (irrespective of its position), which is used as classification label. The test set is the same as in the original publication, but we limit the training and validation splits to 50,000 tweets, in order to comply with Twitter distribution policies. The label set comprises 20 different emoji, and due to their skewed distribution, this task proved to be highly difficult, with low overall numbers. Specifically, more than 42% of the tweets are labeled with the 3 most frequent emoji ( , , and ).\n\nTask\nLab Irony Detection. This task consists of recognizing whether a tweet includes ironic intents or not. We use the Subtask A dataset of the SemEval2018 Irony Detection challenge (Van Hee et al., 2018). Note that this dataset was artificially balanced to make the task more accessible.\nHate Speech Detection. This task consists in predicting whether a tweet is hateful or not against any of two target communities: immigrants and women. Our dataset of choice stems from the Se-mEval2019 Hateval challenge (Basile et al., 2019).\nOffensive Language Identification. This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset (Zampieri et al., 2019).\nSentiment Analysis. The goal for the sentiment analysis task is to recognize if a tweet is positive, negative or neutral. We use the Semeval2017 dataset for Subtask A (Rosenthal et al., 2019), which includes data from previous runs (2013, 2014, 2015, and 2016) of the same SemEval task.\nStance Detection. Stance detection is the task to determine, given a piece of text, whether the author has a favourable, neutral, or negative position towards a proposition or target. We use the SemEval2016 shared task on Detecting Stance in Tweets (Mohammad et al., 2016). In the original task, five target domains are given: abortion, atheism, climate change, feminism and Hillary Clinton. Unlike the other tasks, training is provided separately for each target domain, which we use to extract individual validation sets.\n\n2.2 Statistics and evaluation metrics\nTable 2 includes the TWEETEVAL datasets statistics after unification. 3 Data sizes range from a few hundred instances for training to over 40,000. Note that the preprocessing pipeline is equal for all tasks: user mentions are anonymized line breaks and website links are removed.\nEvaluation metrics. We use the same evaluation metric from the original tasks, which is macroaveraged F1 over all classes, in most cases. There are three exceptions: stance (macro-averaged of F1 of favor and against classes), irony (F1 of ironic class), and sentiment analysis (macro-averaged recall). Similar to GLUE (Wang et al., 2019b), we also introduce a global metric (TE) based on the average of all dataset-specific metrics.\n\n3 Language Models for Tweet Classification\nTransformer-based LMs such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) or XL-NET (Yang et al., 2019) have taken the NLP field by storm, outperforming previous linear models and neural network methods based on LSTMs or CNNs in many tasks, including sentence and text classification (Wang et al., 2019b). The functioning of these language models for tweet classification is conceptually simple. First, they are trained on a large unlabeled corpus. Then, they are fine-tuned to the task for where an appropriate training set exists. For social media text, however, one may question whether existing pretrained models trained on standard corpora are optimal. We thus compare three different strategies which differ in the training data: (1) Using an existing large pre-trained LM; (2) using an existing architecture, but training from scratch using only Twitter data; and (3) starting with an original pretrained LM and continue to train with Twitter data, keeping the original tokenizer and the same masked LM loss.\nWe consider these three techniques as we are interested in exploring whether a Twitter-specific LM should be trained on Twitter only or if it should be initialized with weights learned during pre-training on standard corpora, and then be trained on Twitter. The latter option has indeed three theoretical advantages: (1) these models are generally trained on large amounts of text corpora, and reproducing the same experiment would be extremely expensive even if we had same amount of Twitter data; (2) learning on different types of text corpora make the models more robust and knowledgeable about the world; and (3) some models such as RoBERTa (Liu et al., 2019) or GPT-2 (Radford et al., 2019) are not unfamiliar with internet language and slang, as part of their underlying training corpora contains Reddit data (38GB).\n\n4 Evaluation\n\n\n4.1 Experimental setting\nNeural language model. Among all the available language models we selected RoBERTa (Liu et al., 2019) as it is one of the top performing systems in GLUE. Moreover, it does not employ the Next Sentence Prediction (NSP) loss (Devlin et al., 2018), making the model more suitable for Twitter where most tweets are composed of a single sentence. Language model pre-training. We use three different RoBERTa variants: pre-trained RoBERTabase 4 (RoB-Bs), the same model but re-trained on Twitter (RoB-RT) and trained on Twitter from scratch (RoB-Tw). RoB-RT and RoB-Tw are trained with early stopping on the validation split and learning rate 1.0e \u2212 5. Both models converged after about 8/9 days on 8 NVIDIA V100 GPUs. 5  Twitter corpus. We train RoB-RT and RoB-Tw on 60M tweets 6 obtained by extracting a large corpus of English tweets 7 (using the automatic labeling provided by Twitter). We only considered tweetsMetric M-F1 M-F1 M-F1 F (i) M-F1 M-Rec AVG (F (a) ,F (f ) ) TE\nTable 3 : TweetEval validation and test results. For neural models we report both the average result from three runs and its standard deviation, and the best result according to the validation set (parentheses). SotA results correspond to the best systems in the original shared tasks -they are included for completeness as they not directly comparable. Splits might differ, and * indicates that a larger training set is used.\nwith at least three tokens and without URLs, as to avoid bot tweets and spam advertising.\nClassification fine-tuning. We use the same classification fine-tuning method used in Liu et al. ( 2019): we add one dense layer to reduce the dimensions of the RoBERTa's last layer to the number of labels in the classification task, and fine-tune the model on each classification task, training all the parameters simultaneously. We run a minimum parameter search on the starting learning rate (1.0e \u22123 , 1.0e \u22124 , 1.0e \u22125 , and 1.0e \u22126 ), use early stopping (5 epochs) on the validation set and run each experiment three times with different seeds (1,2,3). Then, we select the highest performing learning rate on the validation set, and use the corresponding model to evaluate on the test set.\nBaselines. FastText (Joulin et al., 2017) provides an efficient baseline based on standard features and subword units. We also include an SVM-based baseline with both word and character n-gram features, a model and feature set that has seen great success in recent Twitter-based shared tasks such as emoji prediction (C \u00b8\u00f6ltekin and Rama, 2018) and stance prediction (Mohammad et al., 2018). We finally report the results of a bi-directional LSTM. 8 Both FastText and the LSTM use 100-dimensional FastText word embeddings (Bojanowski et al., 2017) trained on the 60M Twitter corpus for the 8 The LSTM has 128 cells, an embedding layer of 100 dimensions, dropout (0.5) and, similarly to the language models, the four learning rate values are tuned in the validation set. lookup table initialization.\n\n4.2 Results\nTable 3 shows the results of all comparison systems on TWEETEVAL. Perhaps surprisingly, RoBERTa-Base (RoB-Bs) performs well on all tasks, even outperforming the model trained on Twitter data only (RoB-Tw) in most tasks. This can also be attributed to the fact that Twitter is not only noisy text, and formal text can be also found regularly (Hu et al., 2013; Xu, 2017). Using more Twitter data for training might further improve the results of RoB-Tw, but this would also translate into an even more expensive training. However, RoBERTa-Base coupled with additional training on the same Twitter corpus (i.e. RoB-RT) proves more effective.\nThe only task where a model trained from scratch on Twitter performs better is Irony detection, where RoB-Tw shows to better generalize (RoB-RT F1 drops 13 points from validation to test set, while Rob-Tw F1 5 points). This can be due to two factors: (1) irony used on social media might differ from irony on standard text, (2) tweets in our training data are generally short (79.3 characters on average compared to over 100 characters for most other tasks), and therefore tokenizing the text in less word pieces, and potentially less OOVs, becomes more important to generalize. We note that the low results in the task of emoji prediction (when compared to those obtained in the official SemEval task) are due to the downscaling of the training data. Because of Twitter's data distribution policy, at TWEETEVAL we release at most 50k tweets per task, whereas in the original competition, by id sharing, the training data was one order of magnitude bigger. As for the results in the hate speech task, the difference in performance between validation and test set is mainly due to these splits being collected at different timespans, as pointed out by the organizers of the task (Basile et al., 2019). This causes a disparity in topic distribution and thus low performance of the systems optimized towards the validation set.\n\n4.3 Tokenizer analysis\nTable 4 includes number of tokens 9 per tweet for each of the tasks and the difference between word pieces of the pre-trained RoBERTa-base and RoBERTa trained on Twitter from scratch. This comparison is useful to understand if a model recognizes more or less tokens: if the difference between the two RoBERTa tokenizers is high, it means that one model had to split more times a word. We can note that the biggest difference in wordpieces between RoB-Bs and Rob-Tw is 6.8% in the hate detection task. This is expected as these tweets include less standard words, such as insults. On the other hand, except for perhaps emotion recognition and offensive language identification, the difference is not significant, considering that the original RoBERTa tokenizer was not trained on Twitter text. Moreover, even if the tokenizer of Rob-RT was not retrained from scratch, this does not mean that Rob-RT could not learn new tokens as they could be learned as sequence of characters during the language modeling re-training phase. This is also the case of emoji, which were not learned in the original RoBERTa model, but BTE includes all their Unicode bytes.\n\n5 Conclusion\nWe have presented TWEETEVAL, a unified benchmark for tweet classification consisting of seven heterogeneous tasks that are core to social media NLP research. Along with the benchmark, we have included strong baselines as reference, and ran an analysis of LMs with different training strategies. Our results suggest that using a pre-trained LM may be sufficient, but can improve if topped with extra-training on in-domain data. Offensive 28.4 \u00b120.9 41.9 \u00b120.9 39.4 \u00b119.7 5.7 \u00b18.5\nStance 20.6 \u00b17.1 30.7 \u00b17.1 30.5 \u00b16.9 0.5 \u00b14.8\nTable 4 : Tokenization statistics for all TWEETEVAL tasks. \"Tokens\" is the average number of tokens in each tweet using Twikenizer. RoB-RT and Rob-Tw refers to the average number of word pieces after tokenization with the original Roberta-base and with the model trained from scratch. \"Diff\" is the relative difference (%) of tokens in each tweet between these two tokenizers (if the difference is positive, the original RoBERTa includes more tokens). For stance detection, we computed the average statistics among the five targets.\nFor this initial benchmark and in the interest of reproducibility and accessibility, we focused on a fixed setting (i.e. classification). However, we acknowledge that other important tasks may need to be evaluated differently. Thus, for future work we would like to include more tasks in the context of social media NLP research. Potential improvements include, for example, accounting for the original multi-label nature of emotion classification, or covering more than only 20 emoji in emoji prediction. There are also other scenarios to be addressed as well, like sequence tagging (Baldwin et al., 2015; Gimpel et al., 2018), multimodality (Schifanella et al., 2016; Lu et al., 2018), and codeswitching tasks (Barman et al., 2014; Vilares et al., 2016). This is similar to the evolution of GLUE (Wang et al., 2019b) into SuperGLUE (Wang et al., 2019a), with both benchmarks contributing to the development of the field in different ways. It is also important to highlight that these datasets do not represent their underlying tasks as a whole but only a subsample, and therefore contain biases -automatic models trained on them might not be able to generalize to other specific settings (Augenstein et al., 2017; Wiegand et al., 2019).\nFinally, this benchmark could foster research in multitask learning. The fact that several similar tasks co-exist (e.g. sentiment analysis and emotion recognition, or hate speech detection and offensive language identification) can lead to interesting analyses where the similarity of these tasks is exploited.\n\nFootnotes:\n2: We selected those emotions with a minimum frequency of\n300: examples in the training set.\n3: The validation sets are randomly sampled from the training set for those tasks where no validation split is provided in the original dataset.\n4: RoBERTa-base was trained on 160G of uncompresed text.\n5: We used the Huggingface transformers library . The estimated cost for each language model is USD 4,000 on Google Cloud.\n6: 584 million tokens (3.6G of uncompressed text).\n7: Crawled with the stream API from May'18 to August'19.\n9: Tokenized with the Twitter-specific \"Twikenizer\": github.com/Guilherme-Routar/Twikenizer\n\nReferences:\n\n- Isabelle Augenstein, Leon Derczynski, and Kalina Bontcheva. 2017. Generalisation in named entity recognition: A quantitative analysis. Computer Speech & Language, 44:61-83.- Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources? In Proceedings of the Sixth International Joint Confer- ence on Natural Language Processing, pages 356- 364, Nagoya, Japan. Asian Federation of Natural Language Processing.\n\n- Timothy Baldwin, Marie-Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. 2015. Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition. In Proceedings of the Workshop on Noisy User-generated Text, pages 126- 135.\n\n- Francesco Barbieri, Jose Camacho-Collados,\n\n- Francesco Ronzano, Luis Espinosa Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion. 2018. Semeval 2018 task 2: Multilingual emoji prediction. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 24-33.\n\n- Utsab Barman, Amitava Das, Joachim Wagner, and Jen- nifer Foster. 2014. Code mixing: A challenge for language identification in the language of social me- dia. In Proceedings of the first workshop on compu- tational approaches to code switching, pages 13-23.\n\n- Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela San- guinetti. 2019. SemEval-2019 task 5: Multilin- gual detection of hate speech against immigrants and women in twitter. In Proceedings of the 13th Inter- national Workshop on Semantic Evaluation, pages 54-63, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\n- Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.\n\n- C \u00b8agr\u0131 C \u00b8\u00f6ltekin and Taraka Rama. 2018. T\u00fcbingen- oslo at semeval-2018 task 2: Svms perform better than rnns in emoji prediction. In Proceedings of The 12th International Workshop on Semantic Eval- uation, pages 34-38.\n\n- Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence repre- sentations. In Proceedings of the Eleventh Interna- tional Conference on Language Resources and Eval- uation (LREC-2018), Miyazaki, Japan. European Languages Resources Association (ELRA).\n\n- Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013, pages 198-206, Hissar, Bulgaria. INCOMA Ltd. Shoumen, BULGARIA.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\n- Kevin Gimpel, Nathan Schneider, Brendan O'Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2018. Part-of-speech tagging for twitter: Annotation, features, and experiments.\n\n- Bo Han and Timothy Baldwin. 2011. Lexical normal- isation of short text messages: Makn sens a# twit- ter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies-Volume 1, pages 368- 378. Association for Computational Linguistics.\n\n- Yuheng Hu, Kartik Talamadupula, and Subbarao Kambhampati. 2013. Dude, srsly?: The surprisingly formal nature of twitter's language. In Seventh In- ternational AAAI Conference on Weblogs and Social Media.\n\n- Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427-431, Valencia, Spain. Association for Computational Linguistics.\n\n- Suin Kim, Ingmar Weber, Li Wei, and Alice Oh. 2014. Sociolinguistic analysis of twitter in multilingual so- cieties. In Proceedings of the 25th ACM conference on Hypertext and social media, pages 243-248.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\n- Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang, and Heng Ji. 2018. Visual attention model for name tagging in multimodal social media. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1990-1999.\n\n- Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. Semeval- 2018 task 1: Affect in tweets. In Proceedings of the 12th international workshop on semantic evaluation, pages 1-17.\n\n- Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob- hani, Xiaodan Zhu, and Colin Cherry. 2016. Semeval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31-41.\n\n- Soujanya Poria, Devamanyu Hazarika, Navonil Ma- jumder, and Rada Mihalcea. 2020. Beneath the tip of the iceberg: Current challenges and new direc- tions in sentiment analysis research. arXiv preprint arXiv:2005.00357.\n\n- Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. 2018. Improving language un- derstanding with unsupervised learning. Technical report, OpenAI.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.\n\n- Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the conference on empiri- cal methods in natural language processing, pages 1524-1534. Association for Computational Linguis- tics.\n\n- Sara Rosenthal, Noura Farra, and Preslav Nakov. 2019. Semeval-2017 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.00741.\n\n- Rossano Schifanella, Paloma de Juan, Joel Tetreault, and Liangliang Cao. 2016. Detecting Sarcasm in Multimodal Social Platforms. arXiv e-prints, page arXiv:1608.02289.\n\n- Cynthia Van Hee, Els Lefever, and V\u00e9ronique Hoste. 2018. Semeval-2018 task 3: Irony detection in en- glish tweets. In Proceedings of The 12th Interna- tional Workshop on Semantic Evaluation, pages 39- 50.\n\n- David Vilares, Miguel A Alonso, and Carlos G\u00f3mez- Rodr\u00edguez. 2016. En-es-cs: An english-spanish code-switching twitter corpus for multilingual sen- timent analysis. In Proceedings of the Tenth Interna- tional Conference on Language Resources and Eval- uation (LREC'16), pages 4149-4153.\n\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Proceedings of NeurIPS.\n\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of ICLR.\n\n- Michael Wiegand, Josef Ruppenhofer, and Thomas Kleinbauer. 2019. Detection of Abusive Language: the Problem of Biased Datasets. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 602-608, Minneapolis, Minnesota. Association for Computational Linguis- tics.\n\n- Wei Xu. 2017. From shakespeare to twitter: What are language styles all about? In Proceedings of the Workshop on Stylistic Variation, pages 1-9.\n\n- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural in- formation processing systems, pages 5754-5764.\n\n- Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. SemEval-2019 task 6: Identifying and catego- rizing offensive language in social media (OffensE- val). In Proceedings of the 13th International Work- shop on Semantic Evaluation, pages 75-86, Min- neapolis, Minnesota, USA. Association for Compu- tational Linguistics.\n\n", "annotations": {"Abstract": [{"begin": 82, "end": 933, "idx": 0}], "Head": [{"begin": 936, "end": 950, "n": "1", "idx": 0}, {"begin": 3766, "end": 3785, "idx": 1}, {"begin": 4651, "end": 4677, "n": "2", "idx": 2}, {"begin": 4974, "end": 4983, "n": "2.1", "idx": 3}, {"begin": 6299, "end": 6303, "idx": 4}, {"begin": 7863, "end": 7900, "n": "2.2", "idx": 5}, {"begin": 8615, "end": 8657, "n": "3", "idx": 6}, {"begin": 10510, "end": 10522, "n": "4", "idx": 7}, {"begin": 10525, "end": 10549, "n": "4.1", "idx": 8}, {"begin": 13535, "end": 13546, "n": "4.2", "idx": 9}, {"begin": 15512, "end": 15534, "n": "4.3", "idx": 10}, {"begin": 16688, "end": 16700, "n": "5", "idx": 11}], "ReferenceToBib": [{"begin": 1257, "end": 1282, "target": "#b10", "idx": 0}, {"begin": 1306, "end": 1329, "target": "#b14", "idx": 1}, {"begin": 1330, "end": 1351, "target": "#b2", "idx": 2}, {"begin": 1381, "end": 1402, "target": "#b25", "idx": 3}, {"begin": 1403, "end": 1424, "target": "#b1", "idx": 4}, {"begin": 1634, "end": 1654, "target": "#b22", "idx": 5}, {"begin": 1736, "end": 1754, "target": "#b17", "idx": 6}, {"begin": 2280, "end": 2305, "target": "#b9", "idx": 7}, {"begin": 2312, "end": 2332, "target": "#b31", "idx": 8}, {"begin": 2347, "end": 2367, "target": "#b30", "idx": 9}, {"begin": 5156, "end": 5179, "target": "#b20", "idx": 10}, {"begin": 5737, "end": 5760, "idx": 11}, {"begin": 6481, "end": 6503, "target": "#b28", "idx": 12}, {"begin": 6807, "end": 6828, "target": "#b6", "idx": 13}, {"begin": 7026, "end": 7049, "target": "#b35", "idx": 14}, {"begin": 7218, "end": 7242, "target": "#b26", "idx": 15}, {"begin": 7283, "end": 7311, "idx": 16}, {"begin": 7587, "end": 7610, "target": "#b21", "idx": 17}, {"begin": 8499, "end": 8519, "target": "#b31", "idx": 18}, {"begin": 8692, "end": 8714, "target": "#b23", "idx": 19}, {"begin": 8721, "end": 8742, "target": "#b12", "idx": 20}, {"begin": 8753, "end": 8772, "target": "#b34", "idx": 21}, {"begin": 8953, "end": 8973, "target": "#b31", "idx": 22}, {"begin": 10331, "end": 10349, "target": "#b18", "idx": 23}, {"begin": 10359, "end": 10381, "target": "#b24", "idx": 24}, {"begin": 10633, "end": 10651, "target": "#b18", "idx": 25}, {"begin": 10773, "end": 10794, "target": "#b11", "idx": 26}, {"begin": 12755, "end": 12776, "target": "#b16", "idx": 27}, {"begin": 13102, "end": 13125, "target": "#b20", "idx": 28}, {"begin": 13257, "end": 13282, "target": "#b7", "idx": 29}, {"begin": 13888, "end": 13905, "target": "#b15", "idx": 30}, {"begin": 13906, "end": 13915, "target": "#b33", "idx": 31}, {"begin": 15364, "end": 15385, "target": "#b6", "idx": 32}, {"begin": 18343, "end": 18365, "target": "#b2", "idx": 33}, {"begin": 18366, "end": 18386, "target": "#b13", "idx": 34}, {"begin": 18402, "end": 18428, "target": "#b27", "idx": 35}, {"begin": 18429, "end": 18445, "target": "#b19", "idx": 36}, {"begin": 18471, "end": 18492, "target": "#b5", "idx": 37}, {"begin": 18493, "end": 18514, "target": "#b29", "idx": 38}, {"begin": 18557, "end": 18577, "target": "#b31", "idx": 39}, {"begin": 18593, "end": 18613, "target": "#b30", "idx": 40}, {"begin": 18949, "end": 18974, "target": "#b0", "idx": 41}, {"begin": 18975, "end": 18996, "target": "#b32", "idx": 42}], "ReferenceToFootnote": [{"begin": 5582, "end": 5583, "target": "#foot_0", "idx": 0}, {"begin": 7971, "end": 7972, "target": "#foot_2", "idx": 1}, {"begin": 10986, "end": 10987, "target": "#foot_3", "idx": 2}, {"begin": 11193, "end": 11194, "target": "#foot_4", "idx": 3}, {"begin": 11322, "end": 11323, "target": "#foot_5", "idx": 4}, {"begin": 11380, "end": 11381, "target": "#foot_6", "idx": 5}, {"begin": 15569, "end": 15570, "target": "#foot_7", "idx": 6}], "SectionFootnote": [{"begin": 19310, "end": 19938, "idx": 0}], "ReferenceString": [{"begin": 19955, "end": 20127, "id": "b0", "idx": 0}, {"begin": 20129, "end": 20440, "id": "b1", "idx": 1}, {"begin": 20444, "end": 20744, "id": "b2", "idx": 2}, {"begin": 20748, "end": 20790, "id": "b3", "idx": 3}, {"begin": 20794, "end": 21049, "id": "b4", "idx": 4}, {"begin": 21053, "end": 21311, "id": "b5", "idx": 5}, {"begin": 21315, "end": 21735, "id": "b6", "idx": 6}, {"begin": 21739, "end": 21936, "id": "b7", "idx": 7}, {"begin": 21940, "end": 22160, "id": "b8", "idx": 8}, {"begin": 22164, "end": 22451, "id": "b9", "idx": 9}, {"begin": 22455, "end": 22767, "id": "b10", "idx": 10}, {"begin": 22771, "end": 22959, "id": "b11", "idx": 11}, {"begin": 22963, "end": 23385, "id": "b12", "idx": 12}, {"begin": 23389, "end": 23634, "id": "b13", "idx": 13}, {"begin": 23638, "end": 23934, "id": "b14", "idx": 14}, {"begin": 23938, "end": 24141, "id": "b15", "idx": 15}, {"begin": 24145, "end": 24481, "id": "b16", "idx": 16}, {"begin": 24485, "end": 24689, "id": "b17", "idx": 17}, {"begin": 24693, "end": 24928, "id": "b18", "idx": 18}, {"begin": 24932, "end": 25202, "id": "b19", "idx": 19}, {"begin": 25206, "end": 25418, "id": "b20", "idx": 20}, {"begin": 25422, "end": 25665, "id": "b21", "idx": 21}, {"begin": 25669, "end": 25886, "id": "b22", "idx": 22}, {"begin": 25890, "end": 26053, "id": "b23", "idx": 23}, {"begin": 26057, "end": 26221, "id": "b24", "idx": 24}, {"begin": 26225, "end": 26484, "id": "b25", "idx": 25}, {"begin": 26488, "end": 26626, "id": "b26", "idx": 26}, {"begin": 26630, "end": 26797, "id": "b27", "idx": 27}, {"begin": 26801, "end": 27005, "id": "b28", "idx": 28}, {"begin": 27009, "end": 27295, "id": "b29", "idx": 29}, {"begin": 27299, "end": 27540, "id": "b30", "idx": 30}, {"begin": 27544, "end": 27754, "id": "b31", "idx": 31}, {"begin": 27758, "end": 28152, "id": "b32", "idx": 32}, {"begin": 28156, "end": 28300, "id": "b33", "idx": 33}, {"begin": 28304, "end": 28550, "id": "b34", "idx": 34}, {"begin": 28554, "end": 28923, "id": "b35", "idx": 35}], "ReferenceToTable": [{"begin": 4456, "end": 4457, "idx": 0}, {"begin": 4903, "end": 4904, "idx": 1}, {"begin": 7907, "end": 7908, "target": "#tab_0", "idx": 2}, {"begin": 11528, "end": 11529, "idx": 3}, {"begin": 13553, "end": 13554, "idx": 4}, {"begin": 15541, "end": 15542, "idx": 5}, {"begin": 17232, "end": 17233, "idx": 6}], "Footnote": [{"begin": 19321, "end": 19378, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 19379, "end": 19413, "id": "foot_1", "n": "300", "idx": 1}, {"begin": 19414, "end": 19558, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 19559, "end": 19615, "id": "foot_3", "n": "4", "idx": 3}, {"begin": 19616, "end": 19738, "id": "foot_4", "n": "5", "idx": 4}, {"begin": 19739, "end": 19789, "id": "foot_5", "n": "6", "idx": 5}, {"begin": 19790, "end": 19846, "id": "foot_6", "n": "7", "idx": 6}, {"begin": 19847, "end": 19938, "id": "foot_7", "n": "9", "idx": 7}], "ReferenceToFormula": [{"begin": 12138, "end": 12142, "idx": 0}], "Paragraph": [{"begin": 92, "end": 933, "idx": 0}, {"begin": 951, "end": 2728, "idx": 1}, {"begin": 2729, "end": 3764, "idx": 2}, {"begin": 3786, "end": 4070, "idx": 3}, {"begin": 4071, "end": 4649, "idx": 4}, {"begin": 4678, "end": 4972, "idx": 5}, {"begin": 4984, "end": 5584, "idx": 6}, {"begin": 5585, "end": 6297, "idx": 7}, {"begin": 6304, "end": 6587, "idx": 8}, {"begin": 6588, "end": 6829, "idx": 9}, {"begin": 6830, "end": 7050, "idx": 10}, {"begin": 7051, "end": 7337, "idx": 11}, {"begin": 7338, "end": 7861, "idx": 12}, {"begin": 7901, "end": 8180, "idx": 13}, {"begin": 8181, "end": 8613, "idx": 14}, {"begin": 8658, "end": 9684, "idx": 15}, {"begin": 9685, "end": 10508, "idx": 16}, {"begin": 10550, "end": 11459, "idx": 17}, {"begin": 11522, "end": 11948, "idx": 18}, {"begin": 11949, "end": 12038, "idx": 19}, {"begin": 12039, "end": 12734, "idx": 20}, {"begin": 12735, "end": 13533, "idx": 21}, {"begin": 13547, "end": 14185, "idx": 22}, {"begin": 14186, "end": 15510, "idx": 23}, {"begin": 15535, "end": 16686, "idx": 24}, {"begin": 16701, "end": 17179, "idx": 25}, {"begin": 17180, "end": 17225, "idx": 26}, {"begin": 17226, "end": 17758, "idx": 27}, {"begin": 17759, "end": 18997, "idx": 28}, {"begin": 18998, "end": 19308, "idx": 29}], "SectionHeader": [{"begin": 0, "end": 933, "idx": 0}], "SectionReference": [{"begin": 19940, "end": 28925, "idx": 0}], "Sentence": [{"begin": 92, "end": 185, "idx": 0}, {"begin": 186, "end": 326, "idx": 1}, {"begin": 327, "end": 509, "idx": 2}, {"begin": 510, "end": 647, "idx": 3}, {"begin": 648, "end": 773, "idx": 4}, {"begin": 774, "end": 933, "idx": 5}, {"begin": 951, "end": 1039, "idx": 6}, {"begin": 1040, "end": 1425, "idx": 7}, {"begin": 1426, "end": 1527, "idx": 8}, {"begin": 1528, "end": 1755, "idx": 9}, {"begin": 1756, "end": 1945, "idx": 10}, {"begin": 1946, "end": 2153, "idx": 11}, {"begin": 2154, "end": 2229, "idx": 12}, {"begin": 2230, "end": 2480, "idx": 13}, {"begin": 2481, "end": 2728, "idx": 14}, {"begin": 2729, "end": 2839, "idx": 15}, {"begin": 2840, "end": 2914, "idx": 16}, {"begin": 2915, "end": 3077, "idx": 17}, {"begin": 3078, "end": 3231, "idx": 18}, {"begin": 3232, "end": 3406, "idx": 19}, {"begin": 3407, "end": 3764, "idx": 20}, {"begin": 3786, "end": 3945, "idx": 21}, {"begin": 3946, "end": 4041, "idx": 22}, {"begin": 4042, "end": 4070, "idx": 23}, {"begin": 4071, "end": 4102, "idx": 24}, {"begin": 4103, "end": 4193, "idx": 25}, {"begin": 4194, "end": 4571, "idx": 26}, {"begin": 4572, "end": 4649, "idx": 27}, {"begin": 4678, "end": 4879, "idx": 28}, {"begin": 4880, "end": 4972, "idx": 29}, {"begin": 4984, "end": 5004, "idx": 30}, {"begin": 5005, "end": 5069, "idx": 31}, {"begin": 5070, "end": 5180, "idx": 32}, {"begin": 5181, "end": 5280, "idx": 33}, {"begin": 5281, "end": 5449, "idx": 34}, {"begin": 5450, "end": 5584, "idx": 35}, {"begin": 5585, "end": 5602, "idx": 36}, {"begin": 5603, "end": 5761, "idx": 37}, {"begin": 5762, "end": 5872, "idx": 38}, {"begin": 5873, "end": 6050, "idx": 39}, {"begin": 6051, "end": 6199, "idx": 40}, {"begin": 6200, "end": 6297, "idx": 41}, {"begin": 6304, "end": 6324, "idx": 42}, {"begin": 6325, "end": 6406, "idx": 43}, {"begin": 6407, "end": 6504, "idx": 44}, {"begin": 6505, "end": 6587, "idx": 45}, {"begin": 6588, "end": 6610, "idx": 46}, {"begin": 6611, "end": 6738, "idx": 47}, {"begin": 6739, "end": 6829, "idx": 48}, {"begin": 6830, "end": 6864, "idx": 49}, {"begin": 6865, "end": 6961, "idx": 50}, {"begin": 6962, "end": 7050, "idx": 51}, {"begin": 7051, "end": 7070, "idx": 52}, {"begin": 7071, "end": 7172, "idx": 53}, {"begin": 7173, "end": 7337, "idx": 54}, {"begin": 7338, "end": 7355, "idx": 55}, {"begin": 7356, "end": 7521, "idx": 56}, {"begin": 7522, "end": 7611, "idx": 57}, {"begin": 7612, "end": 7729, "idx": 58}, {"begin": 7730, "end": 7861, "idx": 59}, {"begin": 7901, "end": 7972, "idx": 60}, {"begin": 7973, "end": 8047, "idx": 61}, {"begin": 8048, "end": 8180, "idx": 62}, {"begin": 8181, "end": 8200, "idx": 63}, {"begin": 8201, "end": 8318, "idx": 64}, {"begin": 8319, "end": 8482, "idx": 65}, {"begin": 8483, "end": 8613, "idx": 66}, {"begin": 8658, "end": 8974, "idx": 67}, {"begin": 8975, "end": 9064, "idx": 68}, {"begin": 9065, "end": 9117, "idx": 69}, {"begin": 9118, "end": 9201, "idx": 70}, {"begin": 9202, "end": 9326, "idx": 71}, {"begin": 9327, "end": 9684, "idx": 72}, {"begin": 9685, "end": 9942, "idx": 73}, {"begin": 9943, "end": 10508, "idx": 74}, {"begin": 10550, "end": 10572, "idx": 75}, {"begin": 10573, "end": 10703, "idx": 76}, {"begin": 10704, "end": 10891, "idx": 77}, {"begin": 10892, "end": 10920, "idx": 78}, {"begin": 10921, "end": 11093, "idx": 79}, {"begin": 11094, "end": 11195, "idx": 80}, {"begin": 11196, "end": 11280, "idx": 81}, {"begin": 11281, "end": 11433, "idx": 82}, {"begin": 11434, "end": 11459, "idx": 83}, {"begin": 11522, "end": 11570, "idx": 84}, {"begin": 11571, "end": 11733, "idx": 85}, {"begin": 11734, "end": 11875, "idx": 86}, {"begin": 11876, "end": 11948, "idx": 87}, {"begin": 11949, "end": 12038, "idx": 88}, {"begin": 12039, "end": 12066, "idx": 89}, {"begin": 12067, "end": 12369, "idx": 90}, {"begin": 12370, "end": 12597, "idx": 91}, {"begin": 12598, "end": 12734, "idx": 92}, {"begin": 12735, "end": 12745, "idx": 93}, {"begin": 12746, "end": 12853, "idx": 94}, {"begin": 12854, "end": 13126, "idx": 95}, {"begin": 13127, "end": 13184, "idx": 96}, {"begin": 13185, "end": 13504, "idx": 97}, {"begin": 13505, "end": 13533, "idx": 98}, {"begin": 13547, "end": 13612, "idx": 99}, {"begin": 13613, "end": 13766, "idx": 100}, {"begin": 13767, "end": 13916, "idx": 101}, {"begin": 13917, "end": 14066, "idx": 102}, {"begin": 14067, "end": 14154, "idx": 103}, {"begin": 14155, "end": 14185, "idx": 104}, {"begin": 14186, "end": 14404, "idx": 105}, {"begin": 14405, "end": 14764, "idx": 106}, {"begin": 14765, "end": 14937, "idx": 107}, {"begin": 14938, "end": 15142, "idx": 108}, {"begin": 15143, "end": 15386, "idx": 109}, {"begin": 15387, "end": 15510, "idx": 110}, {"begin": 15535, "end": 15718, "idx": 111}, {"begin": 15719, "end": 15919, "idx": 112}, {"begin": 15920, "end": 16035, "idx": 113}, {"begin": 16036, "end": 16114, "idx": 114}, {"begin": 16115, "end": 16327, "idx": 115}, {"begin": 16328, "end": 16558, "idx": 116}, {"begin": 16559, "end": 16686, "idx": 117}, {"begin": 16701, "end": 16858, "idx": 118}, {"begin": 16859, "end": 16995, "idx": 119}, {"begin": 16996, "end": 17127, "idx": 120}, {"begin": 17128, "end": 17179, "idx": 121}, {"begin": 17180, "end": 17225, "idx": 122}, {"begin": 17226, "end": 17284, "idx": 123}, {"begin": 17285, "end": 17357, "idx": 124}, {"begin": 17358, "end": 17510, "idx": 125}, {"begin": 17511, "end": 17677, "idx": 126}, {"begin": 17678, "end": 17758, "idx": 127}, {"begin": 17759, "end": 17879, "idx": 128}, {"begin": 17880, "end": 17896, "idx": 129}, {"begin": 17897, "end": 17985, "idx": 130}, {"begin": 17986, "end": 18088, "idx": 131}, {"begin": 18089, "end": 18264, "idx": 132}, {"begin": 18265, "end": 18515, "idx": 133}, {"begin": 18516, "end": 18699, "idx": 134}, {"begin": 18700, "end": 18997, "idx": 135}, {"begin": 18998, "end": 19066, "idx": 136}, {"begin": 19067, "end": 19117, "idx": 137}, {"begin": 19118, "end": 19308, "idx": 138}], "Div": [{"begin": 92, "end": 933, "idx": 0}, {"begin": 936, "end": 3764, "idx": 1}, {"begin": 3766, "end": 4649, "idx": 2}, {"begin": 4651, "end": 4972, "idx": 3}, {"begin": 4974, "end": 6297, "idx": 4}, {"begin": 6299, "end": 7861, "idx": 5}, {"begin": 7863, "end": 8613, "idx": 6}, {"begin": 8615, "end": 10508, "idx": 7}, {"begin": 10510, "end": 10523, "idx": 8}, {"begin": 10525, "end": 13533, "idx": 9}, {"begin": 13535, "end": 15510, "idx": 10}, {"begin": 15512, "end": 16686, "idx": 11}, {"begin": 16688, "end": 19308, "idx": 12}], "SectionMain": [{"begin": 933, "end": 19308, "idx": 0}]}}