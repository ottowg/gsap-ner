{"text": "Robustness Verification of Quantum Classifiers\n\nAbstract:\nSeveral important models of machine learning algorithms have been successfully generalized to the quantum world, with potential speedup to training classical classifiers and applications to data analytics in quantum physics that can be implemented on the near future quantum computers. However, quantum noise is a major obstacle to the practical implementation of quantum machine learning. In this work, we define a formal framework for the robustness verification and analysis of quantum machine learning algorithms against noises. A robust bound is derived and an algorithm is developed to check whether or not a quantum machine learning algorithm is robust with respect to quantum training data. In particular, this algorithm can find adversarial examples during checking. Our approach is implemented on Google's TensorFlow Quantum and can verify the robustness of quantum machine learning algorithms with respect to a small disturbance of noises, derived from the surrounding environment. The effectiveness of our robust bound and algorithm is confirmed by the experimental results, including quantum bits classification as the \"Hello World\" example, quantum phase recognition and cluster excitation detection from real world intractable physical problems, and the classification of MNIST from the classical world.\n\nMain:\n\n\n\n1 Introduction\nIn the last few years, the successful interplay between machine learning and quantum physics shed new light on both fields. On the one hand, machine learning has been dramatically developed to satisfy the need of the industry over the past two decades. At the same time, many challenging quantum physical problems have been solved by automated learning. Notably, inaccessible quantum many-body problems have been solved by neural networks, one instance of machine learning [1]. On the other hand, as the new model of computation under quantum mechanics, quantum computing has been proved that it can arXiv:2008.07230v2 [quant-ph] 31 May 2021 (exponentially) speed up classical algorithms for some important problems [2]. This motivates the development of quantum machine learning and provides the possibility of improving the existing computational power of machine learning to a new level (see the review papers [3, 4] for the details). After that, quantum machine learning was integrated into solving real world problems in quantum physics. One essential example is that quantum convolutional neural networks inspired by machine learning were proposed to implement quantum phase recognition [5]. Quantum phase recognition asks whether a given input quantum state belongs to a particular quantum phase of matter. At the same time, more provable advantages of quantum machine learning than the classical counterpart have been reported. For instance, the training complexity of quantum models has an exponential improvement on certain tasks [6]. Stepping into industries, Google recently built up a framework TensorFlow Quantum for the design and training of quantum machine learning within its famous classical machine learning platform-TensorFlow [7].\nEven though quantum machine learning outperforms the classical counterpart in some way, the difficulties in the classical world are expected to be encountered in the quantum case. Classical machine learning has been found to be vulnerable to intentionally-crafted adversarial examples (e.g.  [8, 9]). Adversarial examples are inputs to a machine learning algorithm that an attacker has crafted to cause the algorithm to make a mistake. One essential mission of machine learning is to prove the absence of or detect adversarial examples used in the defense strategy-adversarial training [10] -appending adversarial examples to the training dataset and retraining the machine learning algorithm to be robust to these examples. However, this goal is not easily achieved [11]. The machine learning community has developed several interesting ideas on designing specific attack algorithms (e.g.  [12, 10]) to generate adversarial examples, which is far from measuring the robustness against any adversary. Recently, the formal method community has taken initial steps in this direction [13, 14, 15, 16], by verifying the robustness of classical machine learning algorithms in a provable way: either a formal guarantee that the algorithms are robust for a given input or a counter-example (adversarial example) is provided if an input is not robust. Some tools have been developed, such as VerifAI [17] and NNV [18]. This phenomenon of the vulnerability is more common in the quantum world since quantum noise is inevitable in quantum computation, at least in the current NISQ (Noisy Intermediate-Scale Quantum) era, and thus led to a series of recent works on quantum machine learning robustness against specific noises. For example, Lu et al. [19] studied the robustness to various classical adversarial attacks; Du et al. [20] proved that appending depolarization noise in quantum circuits for classifications, a robust bound against adversaries can be derived; Liu and Wittek [21] gave a robust bound for the quantum noise coming from a special unitary group. Very recently, Weber et al. [22] formalized a link between binary quantum hypothesis testing [23] and robust quantum machine learning algorithms for classification tasks.\nUp to our best knowledge, the existing studies of quantum machine learning robustness only consider the situation of a known noise source. However, a fundamental difference between quantum and classical machine learning is that the quantum attacker is usually the surroundings instead of humans in the classical case, and the information of the environment is unknown. To protect against an unknown adversary, we need to derive a robust guarantee against a worst-case scenario, from which the commonly-assumed known noise sources (e.g. depolarization noise [20]) are usually far. Yet in the case of unknown noise, several basic issues are still unsolved:\n-In theory, it is unclear how to compute a tight and even the optimal bound of the robustness for any given quantum machine learning algorithm. -In practice, an efficient way to find an adversarial example, which can be used to retraining the algorithm to defense the noise, is lacking. Indeed, we do not even know which metric is a better choice measuring the robustness against noise, the same as the classical case against human attackers [24].\nIn this work, we define a formal framework for the robustness verification and analysis of quantum machine learning algorithms against noises in which the above problems can be studied in a principled way. More specifically, we choose to use fidelity as the metric measuring the robustness as it is one of the most widely used quantity to quantify the uncertainty of noise in the process of quantum computation, and commonly used in quantum engineering and experimental communities (e.g.  [25, 26]). Based on this, an analytical robust bound for any quantum machine learning classification algorithm is obtained and can be applied to approximately checking the robustness of quantum machine learning algorithms. Furthermore, we show that computing the optimal robust bound can be reduced to solving a Semidefinite Programming (SDP) problem. These results lead to an algorithm to exactly and efficiently check whether or not a quantum machine learning algorithm is robust with respect to the training data. A special strength of this algorithm is that it can identify useful new training data (adversarial examples) during checking, and these data can be used to implement adversarial training as the same as classical robustness verification. The effectiveness of our robust bound and algorithms is confirmed by the case studies of quantum bits classification as the \"Hello World\" example of quantum machine learning algorithms, quantum phase recognition and cluster excitation detection from real world intractable physical problems, and the classification of MNIST from the classical world.\nIn summary, the main technical contributions of the paper are as follows.\n-Computing the optimal robust bound of quantum machine classification algorithms is reduced to an SDP (Semidefinite Programming) problem; -An efficient algorithm to check the robustness of quantum machine learning algorithms and detect adversarial examples is developed; -The implementation of the robustness verification algorithm on Google's TensorFlow Quantum;\n-Case studies -Checking the robustness of several popular quantum machine learning algorithms for quantum bits classification, cluster excitation detection and the classification of MNIST (which are all implemented in Google's TensorFlow Quantum), and quantum phase recognition.\n\n2 Quantum Data and Computation Models\nFor convenience of the reader, in this section, we recall some basic concepts of quantum data (states) and the quantum computation model. The basic data of classical computers are bits, represented by two digits 0 and 1. In quantum computing, quantum bits (qubit) play the same role. A qubit is expressed by a normalized complex vector |\u03c6 = a b = a|0 + b|1 with complex numbers a and b satisfying the normalization condition |a| 2 + |b| 2 = 1.\nHere, |0 = 1 0 , |1 = 0 1 correspond to bits 0, 1 respectively, and {|0 , |1 } is an orthonormal basis of a 2-dimensional Hilbert (linear) space. In general, for a quantum computer consisting of n qubits, a quantum datum is a normalized complex vector |\u03c8 in a 2 n -dimensional Hilbert space H. Such a |\u03c8 is usually called a pure state in the literature of quantum computation. As a model for computation, a quantum circuit consists of a sequence of, say m quantum logic gates. Each quantum gate can be mathematically represented by a unitary matrixU i on H, i.e., U \u2020 i U i = U i U \u2020 i = I,\nwhere U \u2020 i is the conjugate transpose of U i and I is the identity matrix on H. Then the circuit is represented by the unitary matrix U = U m \u2022 \u2022 \u2022 U 1 . If the quantum datum |\u03c8 is input to the circuit, then the output is a quantum datum:|\u03c8 = U |\u03c8 .\nIn practice, a quantum datum may not be completely known and can be thought of as a mixed state or ensemble {(p k , |\u03c8 k )} k , meaning that it is at |\u03c8 k with probability p k . Mathematically, it can be described by a density operator \u03c1 (Hermitian positive semidefinite matrix with unit trace 5) on H:\u03c1 = k p k |\u03c8 k \u03c8 k |,\nwhere\u03c8 k | is the conjugate transpose of |\u03c8 k , i.e., |\u03c8 k = \u03c8 k | \u2020 .\nIn this case, the model of quantum computation is tuned to be a super-operator E, i.e. a mapping from matrices to matrices. It can be written as\u03c1 = E(\u03c1)\nHere, \u03c1 and \u03c1 are the input and output data (mixed states) of quantum computation E, respectively. Not every super-operator E is meaningful in physics. It is required to satisfy the following conditions:\n-E is trace-preserving: tr(E(\u03c1)) = tr(\u03c1) for all mixed state \u03c1 on H; -E is completely positive: for any Hilbert space H , the trivially extended operator id H \u2297E maps density operators to density operators on H \u2297 H, where \u2297 denotes the tensor product and id H is the identity map on H .\nSuch a super-operator E admits a Kraus matrix form [2] : there exists a set of matrices {E k } k on H such thatE(\u03c1) = k E k \u03c1E \u2020 k .\nHere {E k } k is called Kraus matrices of E.\nThe behind dynamics of quantum computers is governed by quantum mechanics, which is applied at the microscopic scale (near or less than 10 \u22129 meters). At this level, we cannot directly readout the quantum data as the same to the classical counterpart. The only way to extract information from it is through a quantum measurement, which is mathematically modeled by a set {M k } m k=1 of matrices on its state (Hilbert) space H with k M \u2020 k M k = I. This observing process is probabilistic: if the system is currently in state \u03c1, then a measurement outcome k is obtained with probabilityp k = tr(M \u2020 k M k \u03c1).\nAfter the measurement, the system's state will be collapsed (changed), depending on the measurement outcome k, which is vitally different to the classical computation. If the outcome is k, the post-measurement state becomes\u03c1 k = M k \u03c1M \u2020 k tr(M \u2020 k M k \u03c1) .\nThis special property makes it hard to accurately estimate the distribution {p k } k unless enough many copies of \u03c1 are provided. In summary, quantum data have two different forms -pure state |\u03c8 and mixed state \u03c1 corresponding to the computation model as a unitary matrix U or a super-operator E, respectively. Not surprisingly, the latter is a generalization of the former by putting:\u03c1 = |\u03c8 \u03c8|, E(\u03c1) = U \u03c1U \u2020 .\nBecause of this, the results obtained for mixed states \u03c1 can also be applied to pure states |\u03c8 . Thus, in this paper, we mainly consider mixed states as the quantum data and super-operators as the model of quantum computation.\n\n3 Quantum Classification Algorithms\nIn this section, we briefly recall quantum classification algorithms. They are designed for classification of quantum data. Essentially, they share the same basic ideas with their classical counterparts but deal with quantum data in the quantum computation model.\n\n3.1 Basic Definitions\nIn this paper, we focus on a specific learning model called quantum supervised classification. Given a Hilbert space H, we write D(H) for the set of all (mixed) quantum states on H (see its definition in Eq. ( 2)).\nDefinition 1. A quantum classification algorithm A is a mapping D(H) \u2192 C, where C is the set of classes we are interested in.\nFollowing the training strategy of classical machine learning, the classification A is learned through a dataset T instead of being pre-defined. This training dataset T = {(\u03c1 i , c i )} N i=1 consists of N < \u221e pairs (\u03c1 i , c i ), meaning that quantum state \u03c1 i belongs to class c i . To learn A, we initialize a quantum learning model -a parameterized quantum circuit (including measurement control) E \u03b8 and a measurement {M k } k\u2208C . Mathematically, the circuit can be modelled as a quantum super-operator E \u03b8 (see its definition in Eq.( 3)), and \u03b8 is a set of free parameters that can be tuned. Then for each k \u2208 C, we can compute the probability of the measurement outcome being k:f k (\u03b8, \u03c1) = tr(M \u2020 k M k E \u03b8 (\u03c1)).\nIt is worth noting that, as we mentioned before, measuring quantum state \u03c1 is probabilistic and \u03c1 will be changed after measuring. So, in practice, accurately estimating f k (\u03b8, \u03c1) for all k \u2208 C requires enough many copies of \u03c1, which is not the same to the classical case, where a single copy of classical data often meets the training process. The quantum classification algorithm A outputs the class label c for a quantum state \u03c1 using the following condition:A(\u03b8, \u03c1) = arg max k tr(M \u2020 k M k E \u03b8 (\u03c1)). ()\nThe learning is carried out as \u03b8 is optimized to minimize the empirical riskmin \u03b8 1 N N i=1 L(f (\u03b8, \u03c1 i ), c i ),\nwhere L refers to a predefined loss function, f (\u03b8, \u03c1) is a probability vector with each f k (\u03b8, \u03c1), k \u2208 C as its element, and c i is also seen as a probability vector with the entry corresponding to c i being 1 and others being 0. The goal is to find the optimized parameters \u03b8 * minimizing the risk in Eq.( 8) for the given dataset T . Mean-squared error (MSE) is the most popular instance of the empirical risk, i.e., the loss function L is squared error:L(f (\u03b8, \u03c1 i ), c i ) = 1 C f (\u03b8, \u03c1 i ) \u2212 c i\nAs one can see in the above learning process, the main differences between classical and quantum machine learning algorithms are the learning models and data.\nIn this paper, we focus on the well-trained quantum classification algorithm A, usually called a quantum classifier. Here, A is said to be well-trained if training and validation accuracy are both high (\u2265 95%). The training (validation) accuracy is the frequency that A successfully classifies the data in a training (validation) dataset. A validation dataset is mathematically equivalent to a training dataset but only for testing A rather than learning A. In this context, \u03b8 * is naturally omitted, i.e., A(\u03c1) = A(\u03b8 * , \u03c1) andE(\u03c1) = E \u03b8 * (\u03c1). Briefly, A only consists of a super-operator E and a measurement {M k } k , denoted by A = (E, {M k } k ).\n\n3.2 An Illustrative Example\nLet us further illustrate the above definitions by a concrete example-Quantum Convolutional Neural Networks (QCNNs) [5], one of the most popular and successful quantum learning models. QCNN extends the main features and structures of the Convolutional Neural Networks (CNNs) to quantum computing. TheU 1 U 1 U 1 U 1 U 1 U 1 U 1 U 1 U 1 V 1 V 1 V 1 V 1 V 1 MCUG V 1 U 2 U 2 U 2 U 2 V 2 V 2 F Measurement U Unitary matrix (a) (b)\nFig. 1 : Simple example of CNN and QCNN. QCNN, like CNN, consists of a convolution layer that finds a new state and a pooling layer that reduces the size of the model. Here, MCUG stands for measurement control unitary gate, i.e., unitary matrix V 1 is applied on the circuit if and only if the measurement outcome is 1.\nmodel of QCNN applies the convolution layer and the pooling layer from CNNs to quantum systems, as shown in Figure 1 (b). The layout proceeds as follows:\n1 The convolution layer (circuit) applies multiple qubit gates U i between adjacent qubits to find a new state;\n2 The pooling layer reduces the size of the quantum system by measuring a fraction of qubits, and the outcomes determine unitary V i applied to nearby qubits; 3 Repeat the convolution layer and pooling layer defined in 1-2; 4 When the size of the system is sufficiently small, the fully connected layer is applied as a unitary matrix F on the remaining qubits.\nThe input of QCNNs is an unknown quantum state \u03c1 in and the output is obtained by measuring a fixed number of output qubits. As in the classical case, the learning model (defined as the number of convolution and pooling layers) is fixed, but the involved quantum gates (i.e. unitary matrices) U i , V j , F themselves are learned by the above learning process.\nRemark 1. Quantum machine learning can also be used to do classical machine learning tasks. Image classification, for example, is one of the most successful applications of Neural Networks (NNs). To explore the possible advantage of quantum computing, Quantum Neural Networks (QNNs) have been used to classify images in [27, 28]). It is shown that by encoding images to a quantum state \u03c1 in , QNNs can achieve high accuracy in image classification. We will present a quantum classifier for the classification of MNIST as an example in the evaluation section.\n\n4 Robustness\nAn important issue in classical machine learning is: how robust is a classification algorithm to adversarial perturbations. A similar issue exists for quantum classifiers against quantum noise. Intuitively, the robustness of quantum classifier A is the ability to make correct classification with a small perturbation to the input states. Then a quantum state \u03c3 is considered as an adversarial example if it is similar to a benign state \u03c1, but \u03c1 is correctly classified and \u03c3 is classified into a class different from that of \u03c1. Formally, Definition 2 (Adversarial Example). Suppose we are given a quantum classifier A(\u2022), an input example (\u03c1, c), a distance metric D(\u2022, \u2022) and a small enough threshold value \u03b5 > 0. Then \u03c3 is said to be an \u03b5-adversarial example of \u03c1 if the following is true(A(\u03c1) = c) \u2227 (A(\u03c3) = c) \u2227 (D(\u03c1, \u03c3) \u2264 \u03b5).\nThe leftmost condition A(\u03c1) = c asserts that \u03c1 is correctly classified, the middle condition A(\u03c3) = c means that \u03c3 is incorrectly classified, and the rightmost condition D(\u03c1, \u03c3) \u2264 \u03b5 indicates that \u03c1 and \u03c3 are similar (i.e., their distance is small). Sometimes, without any ambiguity, \u03c3 is called an adversarial example of \u03c1 if \u03b5 is preset. Notably, by the above definition, if A incorrectly classifies \u03c1, then we do not need to consider the corresponding adversarial examples. This is the correctness issue of quantum classifier A rather than the robustness issue. Hence, in the following discussions, we only consider the set of all correctly recognized states.\n\nThe absence of adversarial examples leads to robustness.\nDefinition 3 (Adversarial Robustness). Let A be a quantum classifier. Then \u03c1 is \u03b5-robust for A if there is no adversarial example of \u03c1.\nThe major problem concerning us in this paper is the following: Problem 1 (Robustness Verification Problem). Given a quantum classifier A(\u2022) and an input example (\u03c1, c). Check whether or notA(\u03c3) = c for all \u03c3 \u2208 N \u03b5 (\u03c1), where N \u03b5 (\u03c1) is the \u03b5-neighbourhood of \u03c1 as N \u03b5 (\u03c1) = {\u03c3 \u2208 D(H) : D(\u03c1, \u03c3) \u2264 \u03b5)}.\nIf not, then an adversarial example (counter-example) \u03c3 \u2208 N \u03b5 (\u03c1) is provided.\nObviously, if \u03b4 is a robust bound for an input example (\u03c1, c) such that A(\u03c3) = c for any state \u03c3 \u2208 N \u03b4 (\u03c1), then for any \u03b5 \u2264 \u03b4 (i.e. N \u03b5 (\u03c1) \u2286 N \u03b4 (\u03c1)), there is no \u03b5-adversarial example of \u03c1. It is a challenging problem to compute the optimal robust bound \u03b4 * = max \u03b4 so that there is no \u03b5-adversarial example if and only if \u03b5 \u2264 \u03b4 * .\nThe above adversarial robustness of quantum states can be generalized to a notion of robustness for quantum classifiers: Definition 4 (Robust Accuracy). Let A be a quantum classifier. The \u03b5robust accuracy of A is the proportion of \u03b5-robust states in the training dataset. Remark 2. Here, the robust accuracy is defined with respect to the training dataset. In some applications, the dataset can be chosen as another set of quantum states with correct classifications, such as a validation dataset or a combination of it with the training dataset.\nThe reader should notice that the above definitions of robustness for quantum classifiers are similar to those for classical classifiers. But an intrinsic distinctness between them comes from the choice of distance D(\u2022, \u2022). In the classical case, humans play the role of the adversary, and then such a distance should promise that a small perturbation is imperceptible to humans, and vice versa. Otherwise, we cannot take the advantage of machine learning over human's distinguishability. For instance, in image recognition, the distance should reflect the perceptual similarity in the sense that humans would consider adversarial examples generated by it perceptually similar to benign image [24]. In the quantum case, it is essential to choose a distance D that is meaningful in quantum physics. In this paper, we choose to use the distance:D(\u03c1, \u03c3) = 1 \u2212 F (\u03c1, \u03c3)\ndefined by fidelityF (\u03c1, \u03c3) = [tr( \u221a \u03c1\u03c3 \u221a \u03c1)] 2 .\nHere\u221a \u03c1 = k \u221a \u03bb k |\u03c8 k \u03c8 k | if \u03c1 admits the spectral decomposition k \u03bb k |\u03c8 k \u03c8 k |.\nFidelity is one of the most widely used quantities to quantify such uncertainty of noise by the experimental quantum physics and quantum engineering communities (see e.g.  [29, 30]).\nRemark 3. The trace distance has been used in recent literature (e.g.  [20]) for some issues related to quantum robustness verification:T (\u03c1, \u03c3) = 1 2 \u03c1 \u2212 \u03c3 tr = 1 2 tr[ (\u03c1 \u2212 \u03c3) \u2020 (\u03c1 \u2212 \u03c3)].\nIt is a generalization of the total variation distance, which is a distance measure for probability distributions. So far, to the best of our knowledge, there is no discussion about which distance is better in the literature. Here, we argue that fidelity is better than trace distance in the context of quantum machine learning against quantum noise. As we know, state distinguishability is the basis of measuring the effect of noise on quantum computation. The main difference between trace distance T (\u03c1, \u03c3) and fidelity F (\u03c1, \u03c3) is the number of copies of states \u03c1 and \u03c3 as the resource required in the experiments for distinguishing them. More precisely, trace distance quantifies the maximum probability of correctly guessing through a measurement whether \u03c1 or \u03c3 was prepared, while fidelity asserts the same quantity whence infinitely many samples of \u03c1 and \u03c3 can be supplied (See Appendix A of the extended version of this paper [31] for more details). In quantum machine learning, a large enough number of copies of the states are the precondition of statistics in Eq.( 6) for learning and classification. Thus, fidelity is more suitable than trace distance for our purpose.\n\n5 Robust Bound\nIn this section, we develop a theoretic basis for robustness verification of quantum classifiers. After setting the distance D to be the one defined by fidelity, a robust bound can be derived.\nLemma 1 (Robust Bound). Given a quantum classifier A = (E, {M k } k\u2208C ) and a quantum state \u03c1. Let p 1 and p 2 be the first and second largest elements of{tr(M \u2020 k M k E(\u03c1))} k , respectively. If \u221a p 1 \u2212 \u221a p 2 > \u221a 2\u03b5, then \u03c1 is \u03b5-robust.\nProof. See Appendix B of the extended version of this paper [31].\nThe above robust bound gives us a quick robustness verification by the measurement outcomes of \u03c1 without searching any possible adversarial examples. Furthermore, it also can be used to compute an under-approximation of the robust accuracy of A by one-by-one checking the robustness of quantum states in the training dataset. We will see that the robust bound and the induced robust accuracy scales well in the later experiments. However,\u221a p 1 \u2212 \u221a p 2 > \u221a 2\u03b5 is not a necessary condition of \u03b5-robustness. Fortunately, when \u221a p 1 \u2212 \u221a p 2 \u2264 \u221a 2\u03b5\n, we can compute the optimal robust bound by Semidefinite Programming (SDP). Recall that SDP is a convex optimization concerned with the optimization of a linear objective function over the intersection of the cone of positive semidefinite matrices with an affine space. It has the form min tr(CX)subject to tr(A k X) \u2264 b k , for k = 1, . . . , m X \u2265 0\nwhere C, A 1 , . . . , A m are all Hermitian n\u00d7n matrices (i.e. A \u2020 = A), and X is the optimization variable n \u00d7 n matrix with X \u2265 0, i.e., X is positive semidefinite. Many efficient solvers have been developed for solving SDPs-not only compute the minimal value, but also output a corresponding optimal solution X. The following two theorems show that that checking \u03b5-robustness and computing optimal robust bound of quantum states can both be reduced to an SDP.\nTheorem 1 (\u03b5-robustness Verification). Let A = (E, {M k } k\u2208C ) be a quantum classifier and \u03c1 be a state with A(\u03c1) = l. Then \u03c1 is \u03b5-robust if and only if for all k \u2208 C and k = l, the following problem has no solution (feasibility problem):min \u03c3\u2208D(H) 0 subject to \u03c3 \u2265 0 tr(\u03c3) = 1 tr[(M \u2020 l M l \u2212 M \u2020 k M k )E(\u03c3)] \u2264 0 1 \u2212 F (\u03c1, \u03c3) \u2264 \u03b5\nProof. See Appendix C of the extended version of this paper [31].\nActually, the objective function 0 in the above theorem can be chosen as any constant number.\nTheorem 2 (Optimal Robust Bound). Let A and \u03c1 be as in Theorem 1 with A(\u03c1) = l, and let \u03b4 k be the solution of the following problem:\u03b4 k = min \u03c3\u2208D(H) 1 \u2212 F (\u03c1, \u03c3) subject to \u03c3 \u2265 0 tr(\u03c3) = 1 tr[(M \u2020 l M l \u2212 M \u2020 k M k )E(\u03c3)] \u2264 0\nwhere if the problem is unsolved, then \u03b4 k = +\u221e. Then \u03b4 = min k =l \u03b4 k is the optimal robust bound of \u03c1.\nProof. The proof is similar to Theorem 1.\nRemark 4. One may wonder why checking \u03b5-robustness and computing the optimal robust bound can always be reduced to an SDP. This is indeed implied by the basic quantum mechanics postulate of linearity; more specifically, all of the super-operators and measurements used in quantum machine learning algorithms are linear. In contrast, the functions represented by the neural networks in classical machine learning may be nonlinear as the pooling layer is not linear. As a result, the reduced optimization problem for the robustness verification is not convex (e.g.  [32]). For overcoming this difficulty, many different methods have been developed to encode the nonlinear activation functions as linear constraints.\nExamples include NSVerify [33], MIPVerify [34], ILP [35] and ImageStar [13].\nAlgorithm 1 StateRobustnessVerifier(A, \u03b5, \u03c1, l) return false and \u03c3 k * 9: end ifRequire: A = (E, {M k } k\u2208C )\n\n6 Robustness Verification Algorithms\nIn this section, we develop several algorithms for verifying the robustness of quantum classifiers based on the theoretic results presented in the last section.\nFirst, let us consider the robustness of a given quantum state \u03c1. In many applications (as shown in our experiments in Section 7), we are required to check whether \u03c1 is \u03b5-robust for an arbitrarily given threshold \u03b5. Note that once we computed the optimal robust bound \u03b4, checking \u03b5-robustness of \u03c1 is equivalent to compare \u03b5 and \u03b4; that is, \u03b5 \u2264 \u03b4 if and only if \u03c1 is \u03b5-robust. Combining this simple observation with Theorem 1, we obtain Algorithm 1 for checking the \u03b5robustness of \u03c1 and finding the minimum adversarial perturbation \u03b4 caused by quantum noise. The main cost of Algorithm 1 incurs in solving SDPs in Line 2, which scales as O(n 6.5 ) by interior point methods [36], where n is the number of rows of the semidefinite matrix \u03c1 in SDP, i.e., the dimension of Hilbert space of the quantum states in our case. As we need to apply an SDP solver for |C| \u2212 1 times in Line 1, the total complexity is as follows.\nTheorem 3. The worst case complexity of Algorithm 1 is O(|C| \u2022 n 6.5 ), where n is the dimension of input state \u03c1 and |C| is the number of the set C of classes we are interested in. Now we turn to consider the robustness of a quantum classifier A. Algorithm 2 is designed for checking robustness of A by combining Algorithm 1 with Lemma 1 (see the discussion in the paragraph after Lemma 1). A major benefit of formal robustness verification for classical classifiers is perhaps that it can be used to detect a counter-example (adversarial example) for a given input (see e.g.  [13, 14, 15, 16]). This benefit is kept in Algorithm 2 for the robustness verification of quantum classifiers. In particular, we are able to extend the technique of adversarial training in classical machine learning [10] into the quantum case: an adversarial example \u03c3 is automatically generated once \u03b5-robustness of \u03c1 fails, and then by appending (\u03c3, l) into the training dataset, we can retrain A to improve the robustness of the classifier.\n\nAlgorithm 2 RobustnessVerifier(A, \u03b5, T )\nRequire: A = (E, {M k } k\u2208C ) is a well-trained quantum classifier, \u03b5 < 1 is a real number, T = {(\u03c1i, li)} is the training dataset of A Ensure: The robust accuracy RA and a set R = {< \u03c3j, ij >}, where for each j, \u03c1j is an \u03b5-adversarial example of \u03c1i j ; R can be an empty set if all states in T are \u03b5-robust. 1: R = \u2205 be an empty set.\n/return RA = 1 \u2212 |R| |T | , R // |R| = 0 if R is a empty set\nTo analyze the complexity of Algorithm 2, we first see by Theorem 2 that for evaluating the robustness of A-computing its robust accuracy and finding its adversarial examples, one need to call Algorithm 1 for each quantum state in the training dataset, which costs O(|C| \u2022 n 6.5 ). Thus, the total complexity of robustness verification is O(|T | \u2022 |C| \u2022 n 6.5 ), where |T | is the number of elements in the training dataset T . However, the robust bound given in Lemma 1 can help to speed up the process by quickly finding all potential non-robust states, as the complexity of finding the bound is only O(|C| \u2022 n 3 ), which is the cost of |C| times of the multiplication of two n \u00d7 n matrices. In practice, this bound scales well, as confirmed by our experiments presented in Section 7. Therefore, a good strategy for implementing the robustness verification is that we first use robust bound to pick up all potential non-robust states from the given training dataset T and store them in a set T . Then we check all left candidates in the training dataset T one-by-one using Algorithm 1 and use a set R to record the found adversarial examples and the corresponding indexes of states. This strategy can significantly reduce the complexity to O(|T | \u2022 |C| \u2022 n 6.5 ). Indeed, our experiments show that the robust bound given in Lemma 1 scales very well in the sense of |T | |T |.\nRemark 5. Thanks to the linearity of the quantum learning model determined by the basic postulate of quantum mechanics, the robustness verification of quantum classifiers can be done in an efficient way (with polynomial time complexity in the size of input state). It is usually not the case in verifying the robustness of classical machine learning algorithms. For example, DNNs are often non-linear and non-convex and verifying even some simple properties of them can be an NP-complete problem [37].\nSurprisingly, the robustness verification problem for quantum classifiers becomes much harder if we are required to find adversarial examples in pure states. Roughly speaking, the reason is that the set of all pure states is not convex, and thus computing the optimal robust bound for pure states is not an SDP, as in Theorem 2. We can prove that it is a Quadratically Constrained Quadratic Program (QCQP), an optimization problem where both the objective function and the constraints are quadratic functions (see Appendix D of the extended version of this paper [31] for the proof), which is NP-hard. Algorithm 1 can be adapted to this pure state robustness verification by calling a QCQP solver instead of an SDP solver in Line 2. Subsequently, Algorithm 2 can use this new version of Algorithm 1 as a subroutine to compute the corresponding robust accuracy and find adversarial examples of pure states. We will evaluate the QCQP-based robustness verification in the case study of MNIST classification in which handwritten digits are encoded in pure states.\n\n7 Evaluation\nAlgorithm 2 is implemented on TensorFlow Quantum -a platform of Google for designing and training quantum machine learning algorithms, by calling an SDP solver -CVXPY: Python Software for Disciplined Convex Programming [38]. This section aims to evaluate our approach with experiments on some concrete examples. This section is arranged as follows. In Subsections 7.1-7.4, we present several well-trained quantum classifiers. Then the evaluation is carried out in Subsection 7.5 by applying Algorithm 2 to check the robustness verification of these classifiers and find their adversarial examples if existing.\nTo demonstrate our method as sufficiently as possible, we check the robustness of four quantum classifiers. We begin with a \"Hello World\" examplequbits classification, and then we step in two quantum classifiers applied to real world tasks -quantum phase recognition and cluster excitation detection, which are both fundamental and hard problems in quantum physics. At last, to compare with classical robustness verification, we consider the classification of MNIST by encoding handwritten digital images into quantum data. These experiments cover all illustrated examples of TensorFlow Quantum.\n\n7.1 Quantum Bits Classification\nA \"Hello World\" example of quantum machine learning is quantum bits classification [7]. The aim is to implement a binary classification for regions on a single qubit, i.e., a perceptron for qubits. Specifically, two random normalized vectors |a and |b (pure states) in the X-Z plane of the Bloch sphere are chosen. Around these two vectors, we randomly sample two sets of quantum data points; the objective is to learn a quantum gate to distinguish the two sets. A concrete instance of this type is shown in Fig. 8), we use Adam optimizer [39] to update \u03b8. After training, we achieve both 100% training and validation accuracy, and the final parameter \u03b8 is 0.4835.\n\n7.2 Quantum Phase Recognition\nQuantum phase recognition (QPR) of one dimensional many-body systems has been attacked by quantum convolutional neural networks (QCNNs) proposed by Cong et al. [5]. Consider a Z 2 \u00d7 Z 2 symmetry-protected topological (SPT) phase P and the ground states of a family of Hamiltonians on spin-1/2 chain with open boundary conditions:H = \u2212J N \u22122 i=1 Z i X i+1 Z i+2 \u2212 h 1 N i=1 X i \u2212 h 2 N \u22121 i=1 X i X i+1\nwhere X i , Z i are Pauli matrices [2] for the spin at site i, and the Z 2 \u00d7Z 2 symmetry is generated by X even(odd) = i\u2208even(odd) X i . The goal is to identify whether the ground state |\u03c8 of H belongs to phase P when H is regarded as a function of (h 1 /J, h 2 /J). For small N , a numerical simulation can be used to exactly solve this problem [5]; See Fig. 4a in Appendix E of the extended version of this paper [31] for the exact phase boundary points (blue and red diamonds) between SPT phase and non-SPT (paramagnetic or antiferromagnetic) phase for N = 6. Thus the 6-qubit instance is an excellent testbed for different new methods and techniques of QPR. Here, we train a QCNN model to implement 6-qubit QPR in this setting.\nTo generate the dataset for training, we sample a serials of Hamiltonian H with h 2 /J = 0, uniformly varying h 1 /J from 0 to 1.2 and compute their corresponding ground states; see the gray line of Fig. 4a in Appendix E of the extended version of this paper [31]. For the testing, we uniformly sample a set of validation data from two random regions of the 2-dimensional space 1 /J, h 2 /J); see the two dashed rectangles of Fig. 4a. Finally, we obtain 1000 training data and 400 validation data. Our parameterized QCNN circuit is shown in Fig. 4b in Appendix E of the extended version of this paper [31], and the unitaries U i , V j , F are parameterized with generalized Gell-Mann matrix basis [40] : U = exp(\u2212i j \u03b8 j \u039b j ), where \u039b j is a matrix and \u03b8 j is a real number; the total number of parameters \u03b8 j , \u039b j is 114. For the outcome measurement of one qubit, we use measurement M = {M 0 = |+ +|, M 1 = |\u2212 \u2212|} to predict that input states belongs to P with output 0, where |\u00b1 = 1 \u221a 2 (|0 \u00b1 |1 ). Targeting to minimizing the MSE form of Eq. ( 8), we use Adam optimizer to update the 114 parameters. After training, 97.7% training accuracy and 95.25% validation accuracy are obtained. At the same time, our classifier conducts a phase diagram (the colorful figure in Fig. 4a), where the learned phase boundary almost perfectly matches the exact one gotten by the numerical simulation. All these results indicate that our classifier is well-trained.\n\n7.3 Cluster Excitation Detection\nThe task of cluster excitation detection is to train a quantum classifier to detect if a prepared cluster state is \"excited\" or not [7]. Excitations are represented with a X rotation on one qubit. A large enough rotation is deemed to be an excited state and is labeled by 0, while a rotation that isn't large enough is labeled by 1 and is not deemed to be an excited state. Here, we demonstrate this classification task with 6 qubits. We use the circuit shown in Fig. 5a of Appendix E in the extended version of this paper [31] to generate training (840) and validation (360) samples. The circuit generates cluster state by performing a X rotation (we omit angle \u03b8) on one quibit. The rotation angle \u03b8 is ranging from \u2212\u03c0 to \u03c0 and if \u2212\u03c0/2 \u2264 \u03b8 \u2264 \u03c0/2, the label of the output state is 1; otherwise, the label is 0. The classification circuit model (a quantum convolutional neural network) uses the same structure in TensorFlow Quantum [7], shown in Fig. 5b of Appendix E in the extended version of this paper [31]. The explicit parameterization of C i , P j can be found in [7]. The final measurementM = {M 0 = |0 0|, M 1 = |1 1|}.\nTargeting to minimizing the MSE form of Eq. ( 8), we use Adam optimizer to update all C i , P j . We achieve 99.76% training accuracy and 99.44% validation accuracy.\n\n7.4 The Classification of MNIST\nHandwritten digit recognition is one of the most popular tasks in the classical machine learning zoo. The archetypical training and validation data come from the MNIST dataset which consists of 55,000 training samples handwritten digits [41]. These digits have been labeled by humans as representing one of the ten digits from number 0 to 9, and are in the form of gray-scale images that contains 28 \u00d7 28 pixels. Each pixel has a grayscale value ranging from 0 to 255. Quantum machine learning has been used to distinguish a too simplified version of MNIST by downscaling the image sizes to 8 \u00d7 8 pixels. Subsequently, the numbers represented by this version of MNIST can not be perceptually recognized [7]. Here, we build up a quantum classifier to recognize a MNIST version of 16 \u00d7 16 pixels (see second column images of Fig. 3). As demonstrated in [7], we select out 700 images of number 3 and 700 images of number 6 to form our training (1000) and validation (400) datasets. Then we downscale those 28 \u00d7 28 images to 2 4 \u00d7 2 4 images (fitting the size of quantum data), and encode them into the pure states of 8 qubits via amplitude encoding. Amplitude encoding uses the amplitude of computational basis to represent vectors with normalization:(x 0 , x 2 , . . . , x n\u22121 ) \u2192 n\u22121 i=0 x i n\u22121 j=0 |x j | 2 |i .\nwhere {|i } is a set of orthogonal basis of the 8 qubits state space. The normalization doesn't change the pattern of those images. For learning a quantum classifier, we use the QCNN model in Fig. 6 of Appendix E in the extended version of this paper [31] and use measurementM = {M 0 = |+ +|, M 1 = |\u2212 \u2212|}.\nThe output of measurement M indicates the numbers: output 1 for number 3 and output 0 for number 6. The explicit parameterization of those C i , P j can be found in [7]. Again we use Adam optimizer to update the model parameters minimizing the MSE form of Eq.( 8). We finally achieve 98.4% training accuracy and 97.5% validation accuracy.\n\n7.5 Robustness Verification\nNow, we start to check the \u03b5-robustness for the above four well-trained classifiers presented in the previous four subsections.\nIn practical applications, the value of robustness \u03b5 in Definition 3 represents the ability of state preparation by quantum controls. For example, the state-ofthe-art is that a single qubit can be prepared with fidelity 99.99% (e.g.  [29, 30]). Here, we choose four different values of \u03b5 in each experiment. To show the scalability of our robust bound given in Lemma 1, we use it to develop an algorithm (Algorithm 3 in Appendix F of the extended version of this paper [31]) to under-approximate the robust accuracy, which is computed by Algorithm 2. Algorithm 3 is a subroutine of Algorithm 2 without calling an SDP solver (whenever a potential non-robust state can be detected by the robust bound in Lemma 1). We compare the verification times by Algorithms 2 and 3. The experiments are done on a computer with the following configurations: Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz \u00d7 8 Processor, 15.8 GiB Memory, Ubuntu 18.04.5 LTS, with CVXPY: Python Software for Disciplined Convex Programming [38] for solving SDP, and a SciPy solver for finding the minimum of constrained nonlinear multivariable function for solving QCQP. The experi-Robust Accuracy (in Percent) mental results are given in Tables 1-4. As an example, we illustrate the details of the result for the case of \u03b5 = 0.001 in Table 1. First, we only apply our robust bound in Lemma 1 to pick up all potential non-robust states from the 800 points in the training dataset. Then 95 points are left. Thus, the under-approximation of the robust accuracy computed by Algorithm 3 (in Appendix F of the extended version of this paper [31]) is 88.13%. Next, we check the 0.001-robustness by Algorithm 2. Indeed, only 80 of the points detected by the above robust bound are non-robust and the exact robust accuracy is 90.00%. We also compare the verification time of the two approaches to the robust accuracy. See the second column in Table 1 for the detail, and other experiment results of \u03b5-robustness are also summarized in the same table. Tables 1-4 for the verification results show that in all of these experiments, the robust bound obtained in Lemma 1 scales very well, and the robustness verification by Algorithm 3 costs significantly less time (< 2s) than the way of computing the optimal robust bound by Algorithm 2. For example, for quantum phase recognition, for \u03b5 = 0.0001, 0.0002\u03b5 = 0.001 \u03b5 = 0.002 \u03b5 = 0.003 \u03b5 = 0.\u03b5 = 0.0001 \u03b5 = 0.0002 \u03b5 = 0.0003 \u03b5 = 0.\n\n8 Conclusion\nIn this work, we initiate the research of the formal robustness verification of quantum machine learning algorithms against unknown quantum noise. We found an analytical robustness bound which can be efficiently computed to under-approximate the robust accuracy in practical applications. Furthermore, we developed a robustness verification algorithm that can exactly verify the \u03b5- For topics for future research, it should be useful in practical applications to find an efficient method that over-approximates the robust accuracy of quantum classifiers. Combined the under-approximation approach developed in this work, it can help us to more accurately and fast estimate the robust accuracy. In classical machine learning, there exist some works in the literature to achieve this task. For instance, ImageStars, a new set representation, is introduced in [13] to perform efficient set-based analysis by combining operations on concrete images with linear programming, which leads to efficient over-approximative analysis of classical convolutional neural networks.\nTensor networks are one of the best-known data structures for implementing large-scale quantum classifiers (e.g. QCNNs with 45 qubits in [5]). For practical applications, we are going to incorporate tensor networks into our robustness verification algorithm so that it can scale up to achieve the demand of NISQ devices (of \u2265 50 qubits).\nMore generally, further investigations are required to better understand the role of the robustness in quantum machine learning, especially through more experiments on real world applications like learning phases of quantum manybody systems.\nwhere \u03b7 = \u2212 ln 4\u03b5 const . In terms of trace distance, we have:P correct \u2248 1 \u2212 \u03b5 \u21d2 \u03b7 \u03c1 \u2212 \u03c3 2 tr \u2265 N \u2265 2\u03b7 2 \u03c1 \u2212 \u03c3 tr \u2212 \u03c1 \u2212 \u03c3 2 tr\nwhich is derived by the Fuchs-van de Graaf inequalities1 \u2212 F (\u03c1, \u03c3) \u2264 T (\u03c1, \u03c3) \u2264 1 \u2212 F (\u03c1, \u03c3) \u21d2 2 \u03c1 \u2212 \u03c3 tr \u2212 \u03c1 \u2212 \u03c3 2 tr 2 \u2264 C(\u03c1, \u03c3) \u2264 \u03c1 \u2212 \u03c3 2 tr .\nBy the above inequalities about N , we can see that infidelity gives a better (linear) estimation of how many samples we need to accurately discriminate \u03c1 from \u03c3 than trace distance which gives a polynomial estimation of N . For instance, when \u03b5 = 1 4 const \u2022 e \u2212100 (then \u03b7 = 100), for the same value of infidelity and trace distance, saying 0.01, the estimations of N is [10 4 , 2 \u00d7 10 4 ] and [10051, 10 6 ], respectively. Thus fidelity is more suitable as the metric D(\u03c1, \u03c3) = 1 \u2212 F (\u03c1, \u03c3) in Eq.( 9) in the scenario of quantum machine learning that many copies of quantum states are provided.\n\nB Proof of Lemma 1\nProof. For any \u03c3 with F (\u03c1, \u03c3) \u2265 1 \u2212 \u03b5, by the monotonicity of the fidelity [2, Theorem 9.6], we haveF (E(\u03c1), E(\u03c3)) \u2265 F (\u03c1, \u03c3) \u2265 1 \u2212 \u03b5. Let p = ( \u221a p 1 , \u221a p 2 , . . . , \u221a p n ) \u2208 R n , where p k = tr(M \u2020 k M k E(\u03c1)) q = ( \u221a q 1 , \u221a q 2 , . . . , \u221a q n ) \u2208 R n , where q k = tr(M \u2020 k M k E(\u03c3))\nWithout loss of generality, we assume p 1 \u2265 p 2 \u2265 . . . \u2265 p n . We use x \u2022 y and x = \u221a x \u2022 x to denote the inner product of x, y and the 2-norm, respectively. Then p and q both have unit norms:p = k p k = 1 and q = k q k = 1.\nBy the definition of fidelity, we havep \u2022 q = k \u221a p k q k \u2265 F (E(\u03c1), E(\u03c3)) \u2265 \u221a 1 \u2212 \u03b5.\nWe see that any probability distribution R = (r 1 , r 2 , . . . , r n ), with r k \u2265 0 and k r k = 1 can be viewed as a unit vector ( \u221a r 1 , \u221a r 2 , . . . , \u221a r n ) and the fidelity of two distributions is the inner product of their corresponding unit vectors.\nNext, we prove that the unit vector form p of \u03c1 can be used to obtain a robust bound for it. First, we find a vector that has maximum inner product with p and is within another class rather than the belonging class of \u03c1. This can be done by solving the following optimization problem:max . x \u2022 p s.t. x = 1 n j=2 (x 1 \u2212 x j ) = 0 x = (x 1 , . . . , x n ) \u2208 R n x \u2265 0\nWith constraint x = 1, we have x \u2022 p = (a x) \u2022 p/ a x for any a > 0. Thus, let y = a x, the above optimization problem is rewritten as:max . y \u2022 p/ y s.t. y > 0 n j=2 (y 1 \u2212 y j ) = 0 y = (y 1 , . . . , y n ) \u2208 R n y \u2265 0\nThe objective function is not changed by multiplying the numerator and denominator with a positive number. Thus, we can assume y \u2022 p = 1 and obtain the following problem min . y* 1 = y * j , j = 2 Then let x(\u03bb) = \u03bb p + (1 \u2212 \u03bb) y * , 0 \u2264 \u03bb \u2264 1 For the case of y * 1 \u2264 y * 2 , with p 1 \u2265 p 2 , we have \u221a p 1 \u2212 \u221a p 2 \u2265 0 and y * 1 \u2212 y * 2 \u2264 0.\nthen there exists 0 \u2264 \u03bb 0 \u2264 1 such that\u03bb 0 ( \u221a p 1 \u2212 \u221a p 2 ) + (1 \u2212 \u03bb 0 )(y * 1 \u2212 y * 2 ) = 0\nwhich is equivalent to\u03bb 0 \u221a p 1 + (1 \u2212 \u03bb 0 )y * 1 = \u03bb 0 \u221a p 2 + (1 \u2212 \u03bb 0 )y * 2 .\nSo the first and second elements of x(\u03bb 0 ) are equal.\nWe have\nx(\u03bb 0 ) \u2212 p = (1 \u2212 \u03bb 0 )( y * \u2212 p) and ( y * \u2212 p) \u2022 p = 0, which means that y * \u2212 p and p are orthogonal. Thenx(\u03bb 0 ) 2 = x(\u03bb 0 ) \u2212 p + p 2 = x(\u03bb 0 ) \u2212 p 2 + p 2 = (1 \u2212 \u03bb 0 ) 2 y * \u2212 p 2 + p 2 \u2264 y * \u2212 p 2 + p 2 = y * \u2212 p + p 2 = y * 2 .\nWe find a vector x(\u03bb 0 ) satisfies y 1 \u2212 y 2 = 0 and x(\u03bb 0 ) 2 \u2264 y * 2 . Now we consider the situation of y * 1 > y * 2 . We have y * j = y * 1 > y * 2 and p 2 \u2265 p j , and following the same analysis in the above, we can find 0 < \u03bb 0 \u2264 1 such that the second and j-th elements of x(\u03bb 0 ) are equal and which is contradict to y * 2 is the optimal value. Thus, the optimal value is achieved at a vector y satisfying y 1 \u2212 y 2 = 0, then the problem can be reformulated as min . y Using the Lagrange multiplier method, the optimal value of problem ( 13) isx(\u03bb 0 ) 2 = x(\u03bb 0 ) \u2212 p + p 2 = x(\u03bb 0 ) \u2212 p 2 + p 22 2 \u2212 ( \u221a p 1 \u2212 \u221a p 2 ) 2 .\nThen the optimal value of problem ( 10) is1 \u2212 ( \u221a p 1 \u2212 \u221a p 2 ) 2 2 .\nTherefore, if1 \u2212 ( \u221a p 1 \u2212 \u221a p 2 ) 2 2 < \u221a 1 \u2212 \u03b5 \u21d4 \u221a p 1 \u2212 \u221a p 2 > \u221a 2\u03b5,\nthen for any vector q with \u221a 1 \u2212 \u03b5 \u2264 p \u2022 q, we have the corresponding quantum state \u03c3 with F (\u03c1, \u03c3) \u2265 1 \u2212 \u03b5 and \u03c3 is classified into the class of \u03c1. In other words, \u03c1 is \u03b5-robust.\n\nC Proof of Theorem 1\nProof. The sufficient direction directly follows from the definition of adversarial robustness in Definition 3. For the necessary direction, if there is k \u2208 C such that there is a solution \u03c3 in the above problem, then A(\u03c3) = k, i.e., \u03c3 is classified in the class k. That is \u03c3 is an \u03b5-adversarial example of \u03c1 and \u03c1 is not \u03b5-robust. Now we prove that the above problem can be reduced to a SDP. First, it is easy to verify D(H), the set of quantum states on H, is a convex set of positive semidefinite matrices. Computing F (\u03c1, \u03c3) can be reduced to solving a SDP [44]. Thus replacing F (\u03c1, \u03c3) by the SDP, the above problem is a SDP problem by noting that tr(\u03c3) = 1 is equivalent to tr(\u03c3) \u2264 1 and \u2212tr(\u03c3) \u2264 \u22121.\n\nD Pure State Robustness Verification\nIn this section, we discuss the robustness verification for pure states, i.e. pure state |\u03c8 against adversarial examples of pure states. That is that all quantum states in the training dataset and their adversarial examples are pure states. Then, by Theorem 2 and noting that the set of all pure states is not convex, computing the optimal robust bound for pure states is not an SDP. But we can prove it is a Quadratically Constrained Quadratic Program (QCQP), which is hard to be solved.\nIn mathematical optimization, a QCQP is an optimization problem in which both the objective function and the constraints are quadratic functions. It has the form min .\n1 2\nx T P 0 x + q T 0 x subject to 1 2\nx T P i x + q T i x + r i \u2264 0, for i = 1, . . . , m Ax = b where P 0 , P 1 , . . . , P m are n \u00d7 n matrices and x \u2208 R n is the optimization variable. The problem is convex, if P 0 , P 1 , . . . , P m are all positive semidefinite, but non-convex, if these matrices are neither positive nor negative semidefinite. In general, solving QCQP is an NP-hard problem. As we can see, the above QCQP is non-convex, so we cannot efficiently compute the pure state optimal robust bound like the mixed state case by SDP. However, there are some numerical tools designed to solve QCQP-not only compute the minimal value but also output a corresponding optimal solution |\u03d5 , as SDP solvers. Some methods have been developed to approximately solving non-convex QCQPs in a reasonable time. For example, non-convex QCQPs with non-positive off-diagonal elements can be exactly solved by SDP relaxations [45]. Therefore, there may have a polynomial-time algorithm to solve this specific form of QCQP, and we left this problem as a future research.\n\nE Training Models\nH H H H H R x R x R x R x R x R x (a) C 1 C 1 C 1 C 1 C 1 C 1 C 1 P 1 P 1 P 1 P 1 P 1 P 1 C 2 C 2 C 21 C 1 C 1 C 1 C 1 C 1 C 1 C 1 C 1 P 1 P 1 P 1 P 1 P 1 P 1 P 1 P 1 C 2 C 2 C 2\n\nFootnotes:\n5: \u03c1 has unit trace if tr(\u03c1) = 1, where trace tr(\u03c1) of \u03c1 is defined as the summation of diagonal elements of \u03c1.\n2: 2 ,where C is the number of classes in C, and \u2022 2 is the l 2 -norm.\n\nReferences:\n\n- Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602-606, 2017.- Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum in- formation. Cambridge university press, 2010.\n\n- Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195-202, 2017.\n\n- Vedran Dunjko and Hans J Briegel. Machine learning & artificial intelligence in the quantum domain: a review of recent progress. Reports on Progress in Physics, 81(7):074001, 2018.\n\n- Iris Cong, Soonwon Choi, and Mikhail D Lukin. Quantum convolutional neural networks. Nature Physics, 15(12):1273-1278, 2019.\n\n- Hsin-Yuan Huang, Richard Kueng, and John Preskill. Information-theoretic bounds on quantum advantage in machine learning. arXiv preprint arXiv:2101.02464, 2021.\n\n- Michael Broughton, Guillaume Verdon, Trevor McCourt, Antonio J Martinez, Jae Hyeon Yoo, Sergei V Isakov, Philip Massey, Murphy Yuezhen Niu, Ramin Halavati, Evan Peters, et al. Tensorflow quantum: A software framework for quantum machine learning. arXiv preprint arXiv:2003.02989, 2020. See https://www.tensorflow.org/quantum for the platform.\n\n- Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43-58, 2011.\n\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and har- nessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\n- Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancou- ver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. Open- Review.net, 2018.\n\n- Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 3-14, 2017.\n\n- Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\n\n- Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T Johnson. Verifi- cation of deep convolutional neural networks using imagestars. In International Conference on Computer Aided Verification, pages 507-526. Springer, 2020.\n\n- Yizhak Yisrael Elboher, Justin Gottschlich, and Guy Katz. An abstraction-based framework for neural network verification. In International Conference on Com- puter Aided Verification, pages 43-65. Springer, 2020.\n\n- Daniel J Fremont, Johnathan Chiu, Dragos D Margineantu, Denis Osipychev, and Sanjit A Seshia. Formal analysis and redesign of a neural network-based aircraft taxiing system with verifai. In International Conference on Computer Aided Ver- ification, pages 122-134. Springer, 2020.\n\n- Marta Z. Kwiatkowska. Safety verification for deep neural networks with provable guarantees (invited paper). In Wan J. Fokkink and Rob van Glabbeek, editors, 30th International Conference on Concurrency Theory, CONCUR 2019, August 27-30, 2019, Amsterdam, the Netherlands, volume 140 of LIPIcs, pages 1:1-1:5. Schloss Dagstuhl -Leibniz-Zentrum f\u00fcr Informatik, 2019.\n\n- Tommaso Dreossi, Daniel J Fremont, Shromona Ghosh, Edward Kim, Hadi Ravan- bakhsh, Marcell Vazquez-Chanlatte, and Sanjit A Seshia. Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems. In International Conference on Computer Aided Verification, pages 432-442. Springer, 2019.\n\n- Hoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen, Weiming Xiang, Stanley Bak, and Taylor T Johnson. Nnv: The neural network verification tool for deep neural networks and learning-enabled cyber-physical systems. In International Conference on Computer Aided Verifica- tion, pages 3-17. Springer, 2020.\n\n- Sirui Lu, Lu-Ming Duan, and Dong-Ling Deng. Quantum adversarial machine learning. Phys. Rev. Research, 2:033212, Aug 2020.\n\n- Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, and Nana Liu. Quantum noise protects quantum classifiers against adversaries. arXiv preprint arXiv:2003.09416, 2020.\n\n- Nana Liu and Peter Wittek. Vulnerability of quantum classification to adversarial perturbations. Physical Review A, 101(6):062331, 2020.\n\n- Maurice Weber, Nana Liu, Bo Li, Ce Zhang, and Zhikuan Zhao. Optimal prov- able robustness of quantum classification via quantum hypothesis testing. arXiv preprint arXiv:2009.10064, 2020.\n\n- Carl W Helstrom. Detection theory and quantum mechanics. Information and Control, 10(3):254-291, 1967.\n\n- Mahmood Sharif, Lujo Bauer, and Michael K Reiter. On the suitability of lp- norms for creating and preventing adversarial examples. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1605-1613, 2018.\n\n- T Figueiredo Roque, Aashish A Clerk, and Hugo Ribeiro. Engineering fast high- fidelity quantum operations with constrained interactions. npj Quantum Informa- tion, 7(1):1-17, 2021.\n\n- Boyan T Torosov and Nikolay V Vitanov. Smooth composite pulses for high-fidelity quantum information processing. Physical Review A, 83(5):053420, 2011.\n\n- Edward Farhi, Hartmut Neven, et al. Classification with quantum neural networks on near term processors. Quantum Review Letters, 1(2 (2020)):10-37686, 2020.\n\n- Seunghyeok Oh, Jaeho Choi, and Joongheon Kim. A tutorial on quantum convo- lutional neural networks (qcnn). In 2020 International Conference on Information and Communication Technology Convergence (ICTC), pages 236-239. IEEE, 2020.\n\n- AH Myerson, DJ Szwer, SC Webster, DTC Allcock, MJ Curtis, G Imreh, JA Sher- man, DN Stacey, AM Steane, and DM Lucas. High-fidelity readout of trapped-ion qubits. Physical Review Letters, 100(20):200502, 2008.\n\n- AH Burrell, DJ Szwer, SC Webster, and DM Lucas. Scalable simultaneous multi- qubit readout with 99. 99% single-shot fidelity. Physical Review A, 81(4):040302, 2010.\n\n- Ji Guan, Wang Fang, and Mingsheng Ying. Robustness verification of quantum classifiers. arXiv preprint arXiv:2008.07230, 2020.\n\n- Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, and Marta Kwiatkowska. Global robustness evaluation of deep neural networks with prov- able guarantees for the hamming distance. In Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 5944-5952. ijcai.org, 2019.\n\n- Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu neural networks. arXiv preprint arXiv:1706.07351, 2017.\n\n- Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\n- Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Criminisi. Measuring neural net robustness with constraints. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2613-2621, 2016.\n\n- Richard Y Zhang and Javad Lavaei. Sparse semidefinite programs with near-linear time complexity. In 2018 IEEE Conference on Decision and Control (CDC), pages 1624-1631. IEEE, 2018.\n\n- Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In Interna- tional Conference on Computer Aided Verification, pages 97-117. Springer, 2017.\n\n- Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling lan- guage for convex optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.\n\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimiza- tion. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\n- Reinhold A Bertlmann and Philipp Krammer. Bloch vectors for qudits. Journal of Physics A: Mathematical and Theoretical, 41(23):235303, may 2008.\n\n- Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 2010.\n\n- Koenraad MR Audenaert, John Calsamiglia, Ram\u00f3n Munoz-Tapia, Emilio Bagan, Ll Masanes, Antonio Acin, and Frank Verstraete. Discriminating states: The quan- tum chernoff bound. Physical review letters, 98(16):160501, 2007.\n\n- Robin J Blume-Kohout. How distinguishable are two quantum processes? or what is the error rate of a quantum gate? Technical report, Sandia National Lab.(SNL- NM), Albuquerque, NM (United States), 2017.\n\n- John Watrous. Semidefinite programs for completely bounded norms. Theory Comput., 5(1):217-238, 2009.\n\n- Sunyoung Kim and Masakazu Kojima. Exact solutions of some nonconvex quadratic optimization problems via sdp and socp relaxations. Computational optimization and applications, 26(2):143-154, 2003.\n\n", "annotations": {"ReferenceToTable": [{"begin": 42394, "end": 42397, "target": "#tab_5", "idx": 0}, {"begin": 42489, "end": 42490, "target": "#tab_1", "idx": 1}, {"begin": 43089, "end": 43090, "target": "#tab_1", "idx": 2}, {"begin": 43198, "end": 43201, "target": "#tab_5", "idx": 3}], "ReferenceToFootnote": [{"begin": 10416, "end": 10417, "target": "#foot_0", "idx": 0}], "SectionMain": [{"begin": 1384, "end": 52087, "idx": 0}], "ReferenceToFormula": [{"begin": 13379, "end": 13380, "target": "#formula_2", "idx": 0}, {"begin": 14049, "end": 14050, "target": "#formula_4", "idx": 1}, {"begin": 15162, "end": 15163, "target": "#formula_12", "idx": 2}, {"begin": 23911, "end": 23912, "target": "#formula_9", "idx": 3}, {"begin": 34941, "end": 34942, "target": "#formula_12", "idx": 4}, {"begin": 37307, "end": 37308, "target": "#formula_12", "idx": 5}, {"begin": 38921, "end": 38922, "target": "#formula_12", "idx": 6}, {"begin": 40955, "end": 40956, "target": "#formula_12", "idx": 7}, {"begin": 46056, "end": 46057, "idx": 8}, {"begin": 48990, "end": 48992, "idx": 9}, {"begin": 49111, "end": 49113, "target": "#formula_40", "idx": 10}], "SectionReference": [{"begin": 52284, "end": 61822, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1384, "idx": 0}], "Div": [{"begin": 58, "end": 1376, "idx": 0}, {"begin": 1387, "end": 8796, "idx": 1}, {"begin": 8798, "end": 12844, "idx": 2}, {"begin": 12846, "end": 13145, "idx": 3}, {"begin": 13147, "end": 16167, "idx": 4}, {"begin": 16169, "end": 18491, "idx": 5}, {"begin": 18493, "end": 20000, "idx": 6}, {"begin": 20002, "end": 24015, "idx": 7}, {"begin": 24017, "end": 27657, "idx": 8}, {"begin": 27659, "end": 29796, "idx": 9}, {"begin": 29798, "end": 33174, "idx": 10}, {"begin": 33176, "end": 34394, "idx": 11}, {"begin": 34396, "end": 35092, "idx": 12}, {"begin": 35094, "end": 37711, "idx": 13}, {"begin": 37713, "end": 39040, "idx": 14}, {"begin": 39042, "end": 41032, "idx": 15}, {"begin": 41034, "end": 43617, "idx": 16}, {"begin": 43619, "end": 46151, "idx": 17}, {"begin": 46153, "end": 49397, "idx": 18}, {"begin": 49399, "end": 50126, "idx": 19}, {"begin": 50128, "end": 51889, "idx": 20}, {"begin": 51891, "end": 52087, "idx": 21}], "Head": [{"begin": 1387, "end": 1401, "n": "1", "idx": 0}, {"begin": 8798, "end": 8835, "n": "2", "idx": 1}, {"begin": 12846, "end": 12881, "n": "3", "idx": 2}, {"begin": 13147, "end": 13168, "n": "3.1", "idx": 3}, {"begin": 16169, "end": 16196, "n": "3.2", "idx": 4}, {"begin": 18493, "end": 18505, "n": "4", "idx": 5}, {"begin": 20002, "end": 20058, "idx": 6}, {"begin": 24017, "end": 24031, "n": "5", "idx": 7}, {"begin": 27659, "end": 27695, "n": "6", "idx": 8}, {"begin": 29798, "end": 29838, "idx": 9}, {"begin": 33176, "end": 33188, "n": "7", "idx": 10}, {"begin": 34396, "end": 34427, "n": "7.1", "idx": 11}, {"begin": 35094, "end": 35123, "n": "7.2", "idx": 12}, {"begin": 37713, "end": 37745, "n": "7.3", "idx": 13}, {"begin": 39042, "end": 39073, "n": "7.4", "idx": 14}, {"begin": 41034, "end": 41061, "n": "7.5", "idx": 15}, {"begin": 43619, "end": 43631, "n": "8", "idx": 16}, {"begin": 46153, "end": 46171, "idx": 17}, {"begin": 49399, "end": 49419, "idx": 18}, {"begin": 50128, "end": 50164, "idx": 19}, {"begin": 51891, "end": 51908, "idx": 20}], "Paragraph": [{"begin": 58, "end": 1376, "idx": 0}, {"begin": 1402, "end": 3154, "idx": 1}, {"begin": 3155, "end": 5383, "idx": 2}, {"begin": 5384, "end": 6038, "idx": 3}, {"begin": 6039, "end": 6486, "idx": 4}, {"begin": 6487, "end": 8079, "idx": 5}, {"begin": 8080, "end": 8153, "idx": 6}, {"begin": 8154, "end": 8517, "idx": 7}, {"begin": 8518, "end": 8796, "idx": 8}, {"begin": 8836, "end": 9279, "idx": 9}, {"begin": 9280, "end": 9828, "idx": 10}, {"begin": 9871, "end": 10110, "idx": 11}, {"begin": 10122, "end": 10424, "idx": 12}, {"begin": 10446, "end": 10451, "idx": 13}, {"begin": 10517, "end": 10661, "idx": 14}, {"begin": 10670, "end": 10873, "idx": 15}, {"begin": 10874, "end": 11160, "idx": 16}, {"begin": 11161, "end": 11272, "idx": 17}, {"begin": 11294, "end": 11338, "idx": 18}, {"begin": 11339, "end": 11925, "idx": 19}, {"begin": 11948, "end": 12171, "idx": 20}, {"begin": 12206, "end": 12591, "idx": 21}, {"begin": 12618, "end": 12844, "idx": 22}, {"begin": 12882, "end": 13145, "idx": 23}, {"begin": 13169, "end": 13383, "idx": 24}, {"begin": 13384, "end": 13509, "idx": 25}, {"begin": 13510, "end": 14194, "idx": 26}, {"begin": 14230, "end": 14693, "idx": 27}, {"begin": 14739, "end": 14815, "idx": 28}, {"begin": 14853, "end": 15311, "idx": 29}, {"begin": 15356, "end": 15514, "idx": 30}, {"begin": 15515, "end": 16043, "idx": 31}, {"begin": 16197, "end": 16497, "idx": 32}, {"begin": 16625, "end": 16944, "idx": 33}, {"begin": 16945, "end": 17098, "idx": 34}, {"begin": 17099, "end": 17210, "idx": 35}, {"begin": 17211, "end": 17571, "idx": 36}, {"begin": 17572, "end": 17932, "idx": 37}, {"begin": 17933, "end": 18491, "idx": 38}, {"begin": 18506, "end": 19297, "idx": 39}, {"begin": 19338, "end": 20000, "idx": 40}, {"begin": 20059, "end": 20194, "idx": 41}, {"begin": 20195, "end": 20385, "idx": 42}, {"begin": 20497, "end": 20575, "idx": 43}, {"begin": 20576, "end": 20911, "idx": 44}, {"begin": 20912, "end": 21458, "idx": 45}, {"begin": 21459, "end": 22302, "idx": 46}, {"begin": 22325, "end": 22344, "idx": 47}, {"begin": 22375, "end": 22379, "idx": 48}, {"begin": 22461, "end": 22643, "idx": 49}, {"begin": 22644, "end": 22780, "idx": 50}, {"begin": 22834, "end": 24015, "idx": 51}, {"begin": 24032, "end": 24224, "idx": 52}, {"begin": 24225, "end": 24379, "idx": 53}, {"begin": 24463, "end": 24528, "idx": 54}, {"begin": 24529, "end": 24967, "idx": 55}, {"begin": 25073, "end": 25370, "idx": 56}, {"begin": 25426, "end": 25889, "idx": 57}, {"begin": 25890, "end": 26129, "idx": 58}, {"begin": 26223, "end": 26288, "idx": 59}, {"begin": 26289, "end": 26382, "idx": 60}, {"begin": 26383, "end": 26516, "idx": 61}, {"begin": 26610, "end": 26714, "idx": 62}, {"begin": 26715, "end": 26756, "idx": 63}, {"begin": 26757, "end": 27470, "idx": 64}, {"begin": 27471, "end": 27547, "idx": 65}, {"begin": 27548, "end": 27628, "idx": 66}, {"begin": 27696, "end": 27856, "idx": 67}, {"begin": 27857, "end": 28774, "idx": 68}, {"begin": 28775, "end": 29796, "idx": 69}, {"begin": 29839, "end": 30173, "idx": 70}, {"begin": 30174, "end": 30175, "idx": 71}, {"begin": 30235, "end": 31612, "idx": 72}, {"begin": 31613, "end": 32114, "idx": 73}, {"begin": 32115, "end": 33174, "idx": 74}, {"begin": 33189, "end": 33798, "idx": 75}, {"begin": 33799, "end": 34394, "idx": 76}, {"begin": 34428, "end": 35092, "idx": 77}, {"begin": 35124, "end": 35453, "idx": 78}, {"begin": 35526, "end": 36257, "idx": 79}, {"begin": 36258, "end": 37711, "idx": 80}, {"begin": 37746, "end": 38843, "idx": 81}, {"begin": 38875, "end": 39040, "idx": 82}, {"begin": 39074, "end": 40322, "idx": 83}, {"begin": 40387, "end": 40662, "idx": 84}, {"begin": 40694, "end": 41032, "idx": 85}, {"begin": 41062, "end": 41189, "idx": 86}, {"begin": 41190, "end": 43542, "idx": 87}, {"begin": 43632, "end": 44698, "idx": 88}, {"begin": 44699, "end": 45036, "idx": 89}, {"begin": 45037, "end": 45278, "idx": 90}, {"begin": 45279, "end": 45341, "idx": 91}, {"begin": 45407, "end": 45462, "idx": 92}, {"begin": 45554, "end": 46151, "idx": 93}, {"begin": 46172, "end": 46273, "idx": 94}, {"begin": 46466, "end": 46659, "idx": 95}, {"begin": 46692, "end": 46730, "idx": 96}, {"begin": 46778, "end": 47038, "idx": 97}, {"begin": 47039, "end": 47323, "idx": 98}, {"begin": 47406, "end": 47541, "idx": 99}, {"begin": 47627, "end": 47804, "idx": 100}, {"begin": 47968, "end": 48007, "idx": 101}, {"begin": 48062, "end": 48084, "idx": 102}, {"begin": 48144, "end": 48198, "idx": 103}, {"begin": 48199, "end": 48206, "idx": 104}, {"begin": 48207, "end": 48317, "idx": 105}, {"begin": 48444, "end": 48996, "idx": 106}, {"begin": 49075, "end": 49117, "idx": 107}, {"begin": 49145, "end": 49158, "idx": 108}, {"begin": 49218, "end": 49397, "idx": 109}, {"begin": 49420, "end": 50126, "idx": 110}, {"begin": 50165, "end": 50653, "idx": 111}, {"begin": 50654, "end": 50821, "idx": 112}, {"begin": 50822, "end": 50825, "idx": 113}, {"begin": 50826, "end": 50860, "idx": 114}, {"begin": 50861, "end": 51889, "idx": 115}], "ReferenceToBib": [{"begin": 1875, "end": 1878, "target": "#b0", "idx": 0}, {"begin": 2118, "end": 2121, "target": "#b1", "idx": 1}, {"begin": 2315, "end": 2318, "target": "#b2", "idx": 2}, {"begin": 2319, "end": 2321, "target": "#b3", "idx": 3}, {"begin": 2595, "end": 2598, "target": "#b4", "idx": 4}, {"begin": 2942, "end": 2945, "target": "#b5", "idx": 5}, {"begin": 3150, "end": 3153, "target": "#b6", "idx": 6}, {"begin": 3447, "end": 3450, "target": "#b7", "idx": 7}, {"begin": 3451, "end": 3453, "target": "#b8", "idx": 8}, {"begin": 3741, "end": 3745, "target": "#b9", "idx": 9}, {"begin": 3922, "end": 3926, "target": "#b10", "idx": 10}, {"begin": 4046, "end": 4050, "target": "#b11", "idx": 11}, {"begin": 4051, "end": 4054, "target": "#b9", "idx": 12}, {"begin": 4236, "end": 4240, "target": "#b12", "idx": 13}, {"begin": 4241, "end": 4244, "target": "#b13", "idx": 14}, {"begin": 4245, "end": 4248, "target": "#b14", "idx": 15}, {"begin": 4249, "end": 4252, "target": "#b15", "idx": 16}, {"begin": 4547, "end": 4551, "target": "#b16", "idx": 17}, {"begin": 4560, "end": 4564, "target": "#b17", "idx": 18}, {"begin": 4894, "end": 4898, "target": "#b18", "idx": 19}, {"begin": 4974, "end": 4978, "target": "#b19", "idx": 20}, {"begin": 5129, "end": 5133, "target": "#b20", "idx": 21}, {"begin": 5241, "end": 5245, "target": "#b21", "idx": 22}, {"begin": 5306, "end": 5310, "target": "#b22", "idx": 23}, {"begin": 5941, "end": 5945, "target": "#b19", "idx": 24}, {"begin": 6481, "end": 6485, "target": "#b23", "idx": 25}, {"begin": 6976, "end": 6980, "target": "#b24", "idx": 26}, {"begin": 6981, "end": 6984, "target": "#b25", "idx": 27}, {"begin": 11212, "end": 11215, "target": "#b1", "idx": 28}, {"begin": 16313, "end": 16316, "target": "#b4", "idx": 29}, {"begin": 18253, "end": 18257, "target": "#b26", "idx": 30}, {"begin": 18258, "end": 18261, "target": "#b27", "idx": 31}, {"begin": 22152, "end": 22156, "target": "#b23", "idx": 32}, {"begin": 22633, "end": 22637, "target": "#b28", "idx": 33}, {"begin": 22638, "end": 22641, "target": "#b29", "idx": 34}, {"begin": 22715, "end": 22719, "target": "#b19", "idx": 35}, {"begin": 23769, "end": 23773, "target": "#b30", "idx": 36}, {"begin": 24523, "end": 24527, "target": "#b30", "idx": 37}, {"begin": 26283, "end": 26287, "target": "#b30", "idx": 38}, {"begin": 27321, "end": 27325, "target": "#b31", "idx": 39}, {"begin": 27497, "end": 27501, "target": "#b32", "idx": 40}, {"begin": 27513, "end": 27517, "target": "#b33", "idx": 41}, {"begin": 27523, "end": 27527, "target": "#b34", "idx": 42}, {"begin": 27542, "end": 27546, "target": "#b12", "idx": 43}, {"begin": 28531, "end": 28535, "target": "#b35", "idx": 44}, {"begin": 29353, "end": 29357, "target": "#b12", "idx": 45}, {"begin": 29358, "end": 29361, "target": "#b13", "idx": 46}, {"begin": 29362, "end": 29365, "target": "#b14", "idx": 47}, {"begin": 29366, "end": 29369, "target": "#b15", "idx": 48}, {"begin": 29569, "end": 29573, "target": "#b9", "idx": 49}, {"begin": 32109, "end": 32113, "target": "#b36", "idx": 50}, {"begin": 32678, "end": 32682, "target": "#b30", "idx": 51}, {"begin": 33408, "end": 33412, "target": "#b37", "idx": 52}, {"begin": 34511, "end": 34514, "target": "#b6", "idx": 53}, {"begin": 34967, "end": 34971, "target": "#b38", "idx": 54}, {"begin": 35284, "end": 35287, "target": "#b4", "idx": 55}, {"begin": 35561, "end": 35564, "target": "#b1", "idx": 56}, {"begin": 35872, "end": 35875, "target": "#b4", "idx": 57}, {"begin": 35941, "end": 35945, "target": "#b30", "idx": 58}, {"begin": 36517, "end": 36521, "target": "#b30", "idx": 59}, {"begin": 36859, "end": 36863, "target": "#b30", "idx": 60}, {"begin": 36955, "end": 36959, "target": "#b39", "idx": 61}, {"begin": 37878, "end": 37881, "target": "#b6", "idx": 62}, {"begin": 38269, "end": 38273, "target": "#b30", "idx": 63}, {"begin": 38678, "end": 38681, "target": "#b6", "idx": 64}, {"begin": 38752, "end": 38756, "target": "#b30", "idx": 65}, {"begin": 38817, "end": 38820, "target": "#b6", "idx": 66}, {"begin": 39311, "end": 39315, "target": "#b40", "idx": 67}, {"begin": 39777, "end": 39780, "target": "#b6", "idx": 68}, {"begin": 39925, "end": 39928, "target": "#b6", "idx": 69}, {"begin": 40638, "end": 40642, "target": "#b30", "idx": 70}, {"begin": 40859, "end": 40862, "target": "#b6", "idx": 71}, {"begin": 41424, "end": 41428, "target": "#b28", "idx": 72}, {"begin": 41429, "end": 41432, "target": "#b29", "idx": 73}, {"begin": 41659, "end": 41663, "target": "#b30", "idx": 74}, {"begin": 42188, "end": 42192, "target": "#b37", "idx": 75}, {"begin": 42784, "end": 42788, "target": "#b30", "idx": 76}, {"begin": 44489, "end": 44493, "target": "#b12", "idx": 77}, {"begin": 44836, "end": 44839, "target": "#b4", "idx": 78}, {"begin": 49981, "end": 49985, "target": "#b43", "idx": 79}, {"begin": 51746, "end": 51750, "target": "#b44", "idx": 80}], "ReferenceString": [{"begin": 52299, "end": 52440, "id": "b0", "idx": 0}, {"begin": 52442, "end": 52560, "id": "b1", "idx": 1}, {"begin": 52564, "end": 52719, "id": "b2", "idx": 2}, {"begin": 52723, "end": 52903, "id": "b3", "idx": 3}, {"begin": 52907, "end": 53031, "id": "b4", "idx": 4}, {"begin": 53035, "end": 53195, "id": "b5", "idx": 5}, {"begin": 53199, "end": 53541, "id": "b6", "idx": 6}, {"begin": 53545, "end": 53760, "id": "b7", "idx": 7}, {"begin": 53764, "end": 54054, "id": "b8", "idx": 8}, {"begin": 54058, "end": 54384, "id": "b9", "idx": 9}, {"begin": 54388, "end": 54600, "id": "b10", "idx": 10}, {"begin": 54604, "end": 54734, "id": "b11", "idx": 11}, {"begin": 54738, "end": 54966, "id": "b12", "idx": 12}, {"begin": 54970, "end": 55182, "id": "b13", "idx": 13}, {"begin": 55186, "end": 55465, "id": "b14", "idx": 14}, {"begin": 55469, "end": 55833, "id": "b15", "idx": 15}, {"begin": 55837, "end": 56154, "id": "b16", "idx": 16}, {"begin": 56158, "end": 56496, "id": "b17", "idx": 17}, {"begin": 56500, "end": 56622, "id": "b18", "idx": 18}, {"begin": 56626, "end": 56797, "id": "b19", "idx": 19}, {"begin": 56801, "end": 56937, "id": "b20", "idx": 20}, {"begin": 56941, "end": 57127, "id": "b21", "idx": 21}, {"begin": 57131, "end": 57233, "id": "b22", "idx": 22}, {"begin": 57237, "end": 57483, "id": "b23", "idx": 23}, {"begin": 57487, "end": 57667, "id": "b24", "idx": 24}, {"begin": 57671, "end": 57822, "id": "b25", "idx": 25}, {"begin": 57826, "end": 57982, "id": "b26", "idx": 26}, {"begin": 57986, "end": 58217, "id": "b27", "idx": 27}, {"begin": 58221, "end": 58429, "id": "b28", "idx": 28}, {"begin": 58433, "end": 58597, "id": "b29", "idx": 29}, {"begin": 58601, "end": 58727, "id": "b30", "idx": 30}, {"begin": 58731, "end": 59122, "id": "b31", "idx": 31}, {"begin": 59126, "end": 59276, "id": "b32", "idx": 32}, {"begin": 59280, "end": 59529, "id": "b33", "idx": 33}, {"begin": 59533, "end": 59977, "id": "b34", "idx": 34}, {"begin": 59981, "end": 60161, "id": "b35", "idx": 35}, {"begin": 60165, "end": 60404, "id": "b36", "idx": 36}, {"begin": 60408, "end": 60566, "id": "b37", "idx": 37}, {"begin": 60570, "end": 60830, "id": "b38", "idx": 38}, {"begin": 60834, "end": 60978, "id": "b39", "idx": 39}, {"begin": 60982, "end": 61087, "id": "b40", "idx": 40}, {"begin": 61091, "end": 61311, "id": "b41", "idx": 41}, {"begin": 61315, "end": 61516, "id": "b42", "idx": 42}, {"begin": 61520, "end": 61621, "id": "b43", "idx": 43}, {"begin": 61625, "end": 61820, "id": "b44", "idx": 44}], "Sentence": [{"begin": 58, "end": 343, "idx": 0}, {"begin": 344, "end": 447, "idx": 1}, {"begin": 448, "end": 590, "idx": 2}, {"begin": 591, "end": 756, "idx": 3}, {"begin": 757, "end": 833, "idx": 4}, {"begin": 834, "end": 1050, "idx": 5}, {"begin": 1051, "end": 1376, "idx": 6}, {"begin": 1402, "end": 1525, "idx": 7}, {"begin": 1526, "end": 1654, "idx": 8}, {"begin": 1655, "end": 1755, "idx": 9}, {"begin": 1756, "end": 1879, "idx": 10}, {"begin": 1880, "end": 2020, "idx": 11}, {"begin": 2021, "end": 2122, "idx": 12}, {"begin": 2123, "end": 2339, "idx": 13}, {"begin": 2340, "end": 2444, "idx": 14}, {"begin": 2445, "end": 2599, "idx": 15}, {"begin": 2600, "end": 2715, "idx": 16}, {"begin": 2716, "end": 2837, "idx": 17}, {"begin": 2838, "end": 2946, "idx": 18}, {"begin": 2947, "end": 3154, "idx": 19}, {"begin": 3155, "end": 3334, "idx": 20}, {"begin": 3335, "end": 3445, "idx": 21}, {"begin": 3446, "end": 3455, "idx": 22}, {"begin": 3456, "end": 3590, "idx": 23}, {"begin": 3591, "end": 3879, "idx": 24}, {"begin": 3880, "end": 3927, "idx": 25}, {"begin": 3928, "end": 4044, "idx": 26}, {"begin": 4045, "end": 4155, "idx": 27}, {"begin": 4156, "end": 4498, "idx": 28}, {"begin": 4499, "end": 4565, "idx": 29}, {"begin": 4566, "end": 4870, "idx": 30}, {"begin": 4871, "end": 5212, "idx": 31}, {"begin": 5213, "end": 5383, "idx": 32}, {"begin": 5384, "end": 5522, "idx": 33}, {"begin": 5523, "end": 5752, "idx": 34}, {"begin": 5753, "end": 5919, "idx": 35}, {"begin": 5920, "end": 5963, "idx": 36}, {"begin": 5964, "end": 6038, "idx": 37}, {"begin": 6039, "end": 6182, "idx": 38}, {"begin": 6183, "end": 6325, "idx": 39}, {"begin": 6326, "end": 6486, "idx": 40}, {"begin": 6487, "end": 6692, "idx": 41}, {"begin": 6693, "end": 6974, "idx": 42}, {"begin": 6975, "end": 6986, "idx": 43}, {"begin": 6987, "end": 7198, "idx": 44}, {"begin": 7199, "end": 7327, "idx": 45}, {"begin": 7328, "end": 7492, "idx": 46}, {"begin": 7493, "end": 7729, "idx": 47}, {"begin": 7730, "end": 8079, "idx": 48}, {"begin": 8080, "end": 8153, "idx": 49}, {"begin": 8154, "end": 8517, "idx": 50}, {"begin": 8518, "end": 8796, "idx": 51}, {"begin": 8836, "end": 8973, "idx": 52}, {"begin": 8974, "end": 9056, "idx": 53}, {"begin": 9057, "end": 9119, "idx": 54}, {"begin": 9120, "end": 9279, "idx": 55}, {"begin": 9280, "end": 9425, "idx": 56}, {"begin": 9426, "end": 9573, "idx": 57}, {"begin": 9574, "end": 9656, "idx": 58}, {"begin": 9657, "end": 9756, "idx": 59}, {"begin": 9757, "end": 9828, "idx": 60}, {"begin": 9871, "end": 10025, "idx": 61}, {"begin": 10026, "end": 10110, "idx": 62}, {"begin": 10122, "end": 10299, "idx": 63}, {"begin": 10300, "end": 10424, "idx": 64}, {"begin": 10446, "end": 10451, "idx": 65}, {"begin": 10517, "end": 10640, "idx": 66}, {"begin": 10641, "end": 10661, "idx": 67}, {"begin": 10670, "end": 10768, "idx": 68}, {"begin": 10769, "end": 10821, "idx": 69}, {"begin": 10822, "end": 10873, "idx": 70}, {"begin": 10874, "end": 11160, "idx": 71}, {"begin": 11161, "end": 11272, "idx": 72}, {"begin": 11294, "end": 11338, "idx": 73}, {"begin": 11339, "end": 11489, "idx": 74}, {"begin": 11490, "end": 11590, "idx": 75}, {"begin": 11591, "end": 11787, "idx": 76}, {"begin": 11788, "end": 11925, "idx": 77}, {"begin": 11948, "end": 12115, "idx": 78}, {"begin": 12116, "end": 12171, "idx": 79}, {"begin": 12206, "end": 12335, "idx": 80}, {"begin": 12336, "end": 12516, "idx": 81}, {"begin": 12517, "end": 12591, "idx": 82}, {"begin": 12618, "end": 12714, "idx": 83}, {"begin": 12715, "end": 12844, "idx": 84}, {"begin": 12882, "end": 12951, "idx": 85}, {"begin": 12952, "end": 13005, "idx": 86}, {"begin": 13006, "end": 13145, "idx": 87}, {"begin": 13169, "end": 13263, "idx": 88}, {"begin": 13264, "end": 13383, "idx": 89}, {"begin": 13384, "end": 13397, "idx": 90}, {"begin": 13398, "end": 13509, "idx": 91}, {"begin": 13510, "end": 13654, "idx": 92}, {"begin": 13655, "end": 13793, "idx": 93}, {"begin": 13794, "end": 13944, "idx": 94}, {"begin": 13945, "end": 14106, "idx": 95}, {"begin": 14107, "end": 14194, "idx": 96}, {"begin": 14230, "end": 14360, "idx": 97}, {"begin": 14361, "end": 14575, "idx": 98}, {"begin": 14576, "end": 14693, "idx": 99}, {"begin": 14739, "end": 14815, "idx": 100}, {"begin": 14853, "end": 15190, "idx": 101}, {"begin": 15191, "end": 15311, "idx": 102}, {"begin": 15356, "end": 15514, "idx": 103}, {"begin": 15515, "end": 15631, "idx": 104}, {"begin": 15632, "end": 15725, "idx": 105}, {"begin": 15726, "end": 15853, "idx": 106}, {"begin": 15854, "end": 16043, "idx": 107}, {"begin": 16197, "end": 16381, "idx": 108}, {"begin": 16382, "end": 16493, "idx": 109}, {"begin": 16494, "end": 16497, "idx": 110}, {"begin": 16625, "end": 16665, "idx": 111}, {"begin": 16666, "end": 16792, "idx": 112}, {"begin": 16793, "end": 16944, "idx": 113}, {"begin": 16945, "end": 17066, "idx": 114}, {"begin": 17067, "end": 17098, "idx": 115}, {"begin": 17099, "end": 17210, "idx": 116}, {"begin": 17211, "end": 17571, "idx": 117}, {"begin": 17572, "end": 17696, "idx": 118}, {"begin": 17697, "end": 17846, "idx": 119}, {"begin": 17847, "end": 17932, "idx": 120}, {"begin": 17933, "end": 17942, "idx": 121}, {"begin": 17943, "end": 18024, "idx": 122}, {"begin": 18025, "end": 18128, "idx": 123}, {"begin": 18129, "end": 18263, "idx": 124}, {"begin": 18264, "end": 18381, "idx": 125}, {"begin": 18382, "end": 18491, "idx": 126}, {"begin": 18506, "end": 18629, "idx": 127}, {"begin": 18630, "end": 18699, "idx": 128}, {"begin": 18700, "end": 18844, "idx": 129}, {"begin": 18845, "end": 19034, "idx": 130}, {"begin": 19035, "end": 19080, "idx": 131}, {"begin": 19081, "end": 19297, "idx": 132}, {"begin": 19338, "end": 19587, "idx": 133}, {"begin": 19588, "end": 19677, "idx": 134}, {"begin": 19678, "end": 19814, "idx": 135}, {"begin": 19815, "end": 19902, "idx": 136}, {"begin": 19903, "end": 20000, "idx": 137}, {"begin": 20059, "end": 20097, "idx": 138}, {"begin": 20098, "end": 20128, "idx": 139}, {"begin": 20129, "end": 20194, "idx": 140}, {"begin": 20195, "end": 20303, "idx": 141}, {"begin": 20304, "end": 20364, "idx": 142}, {"begin": 20365, "end": 20385, "idx": 143}, {"begin": 20497, "end": 20575, "idx": 144}, {"begin": 20576, "end": 20708, "idx": 145}, {"begin": 20709, "end": 20768, "idx": 146}, {"begin": 20769, "end": 20911, "idx": 147}, {"begin": 20912, "end": 21064, "idx": 148}, {"begin": 21065, "end": 21095, "idx": 149}, {"begin": 21096, "end": 21183, "idx": 150}, {"begin": 21184, "end": 21268, "idx": 151}, {"begin": 21269, "end": 21458, "idx": 152}, {"begin": 21459, "end": 21596, "idx": 153}, {"begin": 21597, "end": 21682, "idx": 154}, {"begin": 21683, "end": 21854, "idx": 155}, {"begin": 21855, "end": 21947, "idx": 156}, {"begin": 21948, "end": 22157, "idx": 157}, {"begin": 22158, "end": 22256, "idx": 158}, {"begin": 22257, "end": 22302, "idx": 159}, {"begin": 22325, "end": 22344, "idx": 160}, {"begin": 22375, "end": 22379, "idx": 161}, {"begin": 22461, "end": 22631, "idx": 162}, {"begin": 22632, "end": 22643, "idx": 163}, {"begin": 22644, "end": 22713, "idx": 164}, {"begin": 22714, "end": 22780, "idx": 165}, {"begin": 22834, "end": 22948, "idx": 166}, {"begin": 22949, "end": 23059, "idx": 167}, {"begin": 23060, "end": 23184, "idx": 168}, {"begin": 23185, "end": 23291, "idx": 169}, {"begin": 23292, "end": 23476, "idx": 170}, {"begin": 23477, "end": 23792, "idx": 171}, {"begin": 23793, "end": 23946, "idx": 172}, {"begin": 23947, "end": 24015, "idx": 173}, {"begin": 24032, "end": 24129, "idx": 174}, {"begin": 24130, "end": 24224, "idx": 175}, {"begin": 24225, "end": 24248, "idx": 176}, {"begin": 24249, "end": 24319, "idx": 177}, {"begin": 24320, "end": 24379, "idx": 178}, {"begin": 24463, "end": 24469, "idx": 179}, {"begin": 24470, "end": 24528, "idx": 180}, {"begin": 24529, "end": 24678, "idx": 181}, {"begin": 24679, "end": 24854, "idx": 182}, {"begin": 24855, "end": 24958, "idx": 183}, {"begin": 24959, "end": 24967, "idx": 184}, {"begin": 25073, "end": 25149, "idx": 185}, {"begin": 25150, "end": 25343, "idx": 186}, {"begin": 25344, "end": 25370, "idx": 187}, {"begin": 25426, "end": 25446, "idx": 188}, {"begin": 25447, "end": 25489, "idx": 189}, {"begin": 25490, "end": 25593, "idx": 190}, {"begin": 25594, "end": 25741, "idx": 191}, {"begin": 25742, "end": 25889, "idx": 192}, {"begin": 25890, "end": 25928, "idx": 193}, {"begin": 25929, "end": 26009, "idx": 194}, {"begin": 26010, "end": 26129, "idx": 195}, {"begin": 26223, "end": 26229, "idx": 196}, {"begin": 26230, "end": 26288, "idx": 197}, {"begin": 26289, "end": 26382, "idx": 198}, {"begin": 26383, "end": 26416, "idx": 199}, {"begin": 26417, "end": 26516, "idx": 200}, {"begin": 26610, "end": 26658, "idx": 201}, {"begin": 26659, "end": 26714, "idx": 202}, {"begin": 26715, "end": 26721, "idx": 203}, {"begin": 26722, "end": 26756, "idx": 204}, {"begin": 26757, "end": 26879, "idx": 205}, {"begin": 26880, "end": 27076, "idx": 206}, {"begin": 27077, "end": 27221, "idx": 207}, {"begin": 27222, "end": 27319, "idx": 208}, {"begin": 27320, "end": 27327, "idx": 209}, {"begin": 27328, "end": 27470, "idx": 210}, {"begin": 27471, "end": 27547, "idx": 211}, {"begin": 27548, "end": 27628, "idx": 212}, {"begin": 27696, "end": 27856, "idx": 213}, {"begin": 27857, "end": 27922, "idx": 214}, {"begin": 27923, "end": 28072, "idx": 215}, {"begin": 28073, "end": 28233, "idx": 216}, {"begin": 28234, "end": 28415, "idx": 217}, {"begin": 28416, "end": 28675, "idx": 218}, {"begin": 28676, "end": 28774, "idx": 219}, {"begin": 28775, "end": 28956, "idx": 220}, {"begin": 28957, "end": 29166, "idx": 221}, {"begin": 29167, "end": 29351, "idx": 222}, {"begin": 29352, "end": 29371, "idx": 223}, {"begin": 29372, "end": 29463, "idx": 224}, {"begin": 29464, "end": 29796, "idx": 225}, {"begin": 29839, "end": 30147, "idx": 226}, {"begin": 30148, "end": 30173, "idx": 227}, {"begin": 30174, "end": 30175, "idx": 228}, {"begin": 30235, "end": 30516, "idx": 229}, {"begin": 30517, "end": 30662, "idx": 230}, {"begin": 30663, "end": 30928, "idx": 231}, {"begin": 30929, "end": 31232, "idx": 232}, {"begin": 31233, "end": 31419, "idx": 233}, {"begin": 31420, "end": 31500, "idx": 234}, {"begin": 31501, "end": 31612, "idx": 235}, {"begin": 31613, "end": 31877, "idx": 236}, {"begin": 31878, "end": 31974, "idx": 237}, {"begin": 31975, "end": 32114, "idx": 238}, {"begin": 32115, "end": 32272, "idx": 239}, {"begin": 32273, "end": 32716, "idx": 240}, {"begin": 32717, "end": 33020, "idx": 241}, {"begin": 33021, "end": 33174, "idx": 242}, {"begin": 33189, "end": 33413, "idx": 243}, {"begin": 33414, "end": 33500, "idx": 244}, {"begin": 33501, "end": 33537, "idx": 245}, {"begin": 33538, "end": 33561, "idx": 246}, {"begin": 33562, "end": 33614, "idx": 247}, {"begin": 33615, "end": 33798, "idx": 248}, {"begin": 33799, "end": 33906, "idx": 249}, {"begin": 33907, "end": 34164, "idx": 250}, {"begin": 34165, "end": 34322, "idx": 251}, {"begin": 34323, "end": 34394, "idx": 252}, {"begin": 34428, "end": 34515, "idx": 253}, {"begin": 34516, "end": 34625, "idx": 254}, {"begin": 34626, "end": 34742, "idx": 255}, {"begin": 34743, "end": 34890, "idx": 256}, {"begin": 34891, "end": 34984, "idx": 257}, {"begin": 34985, "end": 35092, "idx": 258}, {"begin": 35124, "end": 35288, "idx": 259}, {"begin": 35289, "end": 35453, "idx": 260}, {"begin": 35526, "end": 35662, "idx": 261}, {"begin": 35663, "end": 35792, "idx": 262}, {"begin": 35793, "end": 36088, "idx": 263}, {"begin": 36089, "end": 36187, "idx": 264}, {"begin": 36188, "end": 36257, "idx": 265}, {"begin": 36258, "end": 36522, "idx": 266}, {"begin": 36523, "end": 36692, "idx": 267}, {"begin": 36693, "end": 36755, "idx": 268}, {"begin": 36756, "end": 37082, "idx": 269}, {"begin": 37083, "end": 37260, "idx": 270}, {"begin": 37261, "end": 37362, "idx": 271}, {"begin": 37363, "end": 37447, "idx": 272}, {"begin": 37448, "end": 37647, "idx": 273}, {"begin": 37648, "end": 37711, "idx": 274}, {"begin": 37746, "end": 37882, "idx": 275}, {"begin": 37883, "end": 37942, "idx": 276}, {"begin": 37943, "end": 38119, "idx": 277}, {"begin": 38120, "end": 38180, "idx": 278}, {"begin": 38181, "end": 38330, "idx": 279}, {"begin": 38331, "end": 38426, "idx": 280}, {"begin": 38427, "end": 38757, "idx": 281}, {"begin": 38758, "end": 38821, "idx": 282}, {"begin": 38822, "end": 38843, "idx": 283}, {"begin": 38875, "end": 38972, "idx": 284}, {"begin": 38973, "end": 39040, "idx": 285}, {"begin": 39074, "end": 39175, "idx": 286}, {"begin": 39176, "end": 39316, "idx": 287}, {"begin": 39317, "end": 39486, "idx": 288}, {"begin": 39487, "end": 39542, "idx": 289}, {"begin": 39543, "end": 39678, "idx": 290}, {"begin": 39679, "end": 39781, "idx": 291}, {"begin": 39782, "end": 39905, "idx": 292}, {"begin": 39906, "end": 40052, "idx": 293}, {"begin": 40053, "end": 40220, "idx": 294}, {"begin": 40221, "end": 40322, "idx": 295}, {"begin": 40387, "end": 40456, "idx": 296}, {"begin": 40457, "end": 40518, "idx": 297}, {"begin": 40519, "end": 40662, "idx": 298}, {"begin": 40694, "end": 40793, "idx": 299}, {"begin": 40794, "end": 40863, "idx": 300}, {"begin": 40864, "end": 40958, "idx": 301}, {"begin": 40959, "end": 41032, "idx": 302}, {"begin": 41062, "end": 41189, "idx": 303}, {"begin": 41190, "end": 41323, "idx": 304}, {"begin": 41324, "end": 41422, "idx": 305}, {"begin": 41423, "end": 41434, "idx": 306}, {"begin": 41435, "end": 41497, "idx": 307}, {"begin": 41498, "end": 41901, "idx": 308}, {"begin": 41902, "end": 41958, "idx": 309}, {"begin": 41959, "end": 42318, "idx": 310}, {"begin": 42319, "end": 42398, "idx": 311}, {"begin": 42399, "end": 42491, "idx": 312}, {"begin": 42492, "end": 42628, "idx": 313}, {"begin": 42629, "end": 42653, "idx": 314}, {"begin": 42654, "end": 42800, "idx": 315}, {"begin": 42801, "end": 42973, "idx": 316}, {"begin": 42974, "end": 43057, "idx": 317}, {"begin": 43058, "end": 43190, "idx": 318}, {"begin": 43191, "end": 43542, "idx": 319}, {"begin": 43632, "end": 43778, "idx": 320}, {"begin": 43779, "end": 43920, "idx": 321}, {"begin": 43921, "end": 44186, "idx": 322}, {"begin": 44187, "end": 44325, "idx": 323}, {"begin": 44326, "end": 44419, "idx": 324}, {"begin": 44420, "end": 44698, "idx": 325}, {"begin": 44699, "end": 44811, "idx": 326}, {"begin": 44812, "end": 44841, "idx": 327}, {"begin": 44842, "end": 45036, "idx": 328}, {"begin": 45037, "end": 45278, "idx": 329}, {"begin": 45279, "end": 45304, "idx": 330}, {"begin": 45305, "end": 45341, "idx": 331}, {"begin": 45407, "end": 45462, "idx": 332}, {"begin": 45554, "end": 45778, "idx": 333}, {"begin": 45779, "end": 45979, "idx": 334}, {"begin": 45980, "end": 46151, "idx": 335}, {"begin": 46172, "end": 46178, "idx": 336}, {"begin": 46179, "end": 46273, "idx": 337}, {"begin": 46466, "end": 46521, "idx": 338}, {"begin": 46522, "end": 46529, "idx": 339}, {"begin": 46530, "end": 46624, "idx": 340}, {"begin": 46625, "end": 46659, "idx": 341}, {"begin": 46692, "end": 46730, "idx": 342}, {"begin": 46778, "end": 46841, "idx": 343}, {"begin": 46842, "end": 46932, "idx": 344}, {"begin": 46933, "end": 47038, "idx": 345}, {"begin": 47039, "end": 47131, "idx": 346}, {"begin": 47132, "end": 47259, "idx": 347}, {"begin": 47260, "end": 47323, "idx": 348}, {"begin": 47406, "end": 47541, "idx": 349}, {"begin": 47627, "end": 47733, "idx": 350}, {"begin": 47734, "end": 47802, "idx": 351}, {"begin": 47803, "end": 47804, "idx": 352}, {"begin": 47968, "end": 48007, "idx": 353}, {"begin": 48062, "end": 48084, "idx": 354}, {"begin": 48144, "end": 48198, "idx": 355}, {"begin": 48199, "end": 48206, "idx": 356}, {"begin": 48207, "end": 48312, "idx": 357}, {"begin": 48313, "end": 48317, "idx": 358}, {"begin": 48444, "end": 48516, "idx": 359}, {"begin": 48517, "end": 48565, "idx": 360}, {"begin": 48566, "end": 48796, "idx": 361}, {"begin": 48797, "end": 48918, "idx": 362}, {"begin": 48919, "end": 48996, "idx": 363}, {"begin": 49075, "end": 49117, "idx": 364}, {"begin": 49145, "end": 49158, "idx": 365}, {"begin": 49218, "end": 49366, "idx": 366}, {"begin": 49367, "end": 49397, "idx": 367}, {"begin": 49420, "end": 49426, "idx": 368}, {"begin": 49427, "end": 49531, "idx": 369}, {"begin": 49532, "end": 49685, "idx": 370}, {"begin": 49686, "end": 49751, "idx": 371}, {"begin": 49752, "end": 49812, "idx": 372}, {"begin": 49813, "end": 49929, "idx": 373}, {"begin": 49930, "end": 49986, "idx": 374}, {"begin": 49987, "end": 50126, "idx": 375}, {"begin": 50165, "end": 50301, "idx": 376}, {"begin": 50302, "end": 50405, "idx": 377}, {"begin": 50406, "end": 50548, "idx": 378}, {"begin": 50549, "end": 50653, "idx": 379}, {"begin": 50654, "end": 50799, "idx": 380}, {"begin": 50800, "end": 50821, "idx": 381}, {"begin": 50822, "end": 50825, "idx": 382}, {"begin": 50826, "end": 50860, "idx": 383}, {"begin": 50861, "end": 50908, "idx": 384}, {"begin": 50909, "end": 50943, "idx": 385}, {"begin": 50944, "end": 51010, "idx": 386}, {"begin": 51011, "end": 51054, "idx": 387}, {"begin": 51055, "end": 51173, "idx": 388}, {"begin": 51174, "end": 51221, "idx": 389}, {"begin": 51222, "end": 51369, "idx": 390}, {"begin": 51370, "end": 51537, "idx": 391}, {"begin": 51538, "end": 51634, "idx": 392}, {"begin": 51635, "end": 51751, "idx": 393}, {"begin": 51752, "end": 51889, "idx": 394}], "ReferenceToFigure": [{"begin": 16630, "end": 16631, "target": "#fig_7", "idx": 0}, {"begin": 17060, "end": 17061, "target": "#fig_7", "idx": 1}, {"begin": 35886, "end": 35888, "target": "#fig_9", "idx": 2}, {"begin": 36462, "end": 36464, "target": "#fig_9", "idx": 3}, {"begin": 36689, "end": 36691, "target": "#fig_9", "idx": 4}, {"begin": 36804, "end": 36806, "target": "#fig_9", "idx": 5}, {"begin": 37535, "end": 37537, "target": "#fig_9", "idx": 6}, {"begin": 38214, "end": 38216, "target": "#fig_11", "idx": 7}, {"begin": 38697, "end": 38699, "target": "#fig_11", "idx": 8}, {"begin": 39902, "end": 39903, "target": "#fig_3", "idx": 9}, {"begin": 40584, "end": 40585, "idx": 10}], "Abstract": [{"begin": 48, "end": 1376, "idx": 0}], "SectionFootnote": [{"begin": 52089, "end": 52282, "idx": 0}], "Footnote": [{"begin": 52100, "end": 52211, "id": "foot_0", "n": "5", "idx": 0}, {"begin": 52212, "end": 52282, "id": "foot_1", "n": "2", "idx": 1}]}}