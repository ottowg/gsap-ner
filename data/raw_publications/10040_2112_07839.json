{"text": "LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization\n\nAbstract:\nFederated optimization (FedOpt), which targets at collaboratively training a learning model across a large number of distributed clients, is vital for federated learning. The primary concerns in FedOpt can be attributed to the model divergence and communication efficiency, which significantly affect the performance. In this paper, we propose a new method, i.e., LoSAC, to learn from heterogeneous distributed data more efficiently. Its key algorithmic insight is to locally update the estimate for the global full gradient after each regular local model update. Thus, LoSAC can keep clients' information refreshed in a more compact way. In particular, we have studied the convergence result for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information leakage from the recent technique Deep Leakage Gradients (DLG). Finally, experiments have verified the superiority of LoSAC comparing with state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves communication efficiency by more than 100% on average, mitigates the model divergence problem and equips with the defense ability against DLG. CCS Concepts: \u2022 Computing methodologies \u2192 Parallel algorithms; \u2022 Theory of computation \u2192 Mathematical optimization.\n\nMain:\n\n\n\n1 INTRODUCTION\nFederated optimization (FedOpt) is essentially a distributed optimization in machine learning under the specific setting that data is unevenly distributed over a large number of clients [11, 16, 17, 25, 40].\nIt has provided a feasible solution to collaboratively train a high-quality model without data exchanging. Thus, interests in applying FedOpt to the major areas have been greatly increased, e.g., healthcare [6, 9, 26] and smart city applications [37, 44]. Technically, the utmost goal of FedOpt is to reach a high communication efficiency under the federated settings [17, 20, 21, 25].\nFedOpt differs from the traditional distributed optimization [19] in data heterogeneity and client sampling respectively, i.e., the data is generally not independent and identical distribution (non-IID) due to each client's fashion, and only partial clients perform multiple local updates before communication with the server in FedOpt. However, the recent studies have shown that data heterogeneity (e.g., non-IID data) will seriously degrade the performance in FedOpt [11, 13, 14, 20, 21, 43]. This can be attributed to the \"model divergence\" in FedOpt [13, 21]. To be specific, multiple local update steps make each participating client approach to the individual optimum targeting at optimizing the local loss function instead of the global one. Under the client sampling, this problem is worsened. On one hand, several recent methods attempt to mitigate the model divergence problem [13, 20, 23], however, there is still much room for the improvement in communication efficiency. On the other hand, in FedOpt applications, many mobile devices may frequently go offline, and this necessitates the demand for the fast convergent algorithm to reach a complete learning [21].\nTo this end, we summarize the two major challenges for the FedOpt design [14, 20, 21, 43] : (1). The model divergence problem. It is mainly resulted from data heterogeneity and client sampling, and will lead to a slow and unstable convergence. Since each client can only access to its own dataset, this makes the problem difficult to remedy; (2). The limited communication capability. With a large number of clients, it requires the algorithm equipping with a fast speed. The recent representative methods for handling these challenges are MimeSVRG [12] and SCAFFOLD [13]. They utilize the global variance reduction, which nevertheless may suffer from the outdated information.\nLoSAC incorporates the delayed gradients that estimate the global full gradient on clients. Moreover, they are updated with the newest information at each local iteration, and then aggregated with the information from the participated clients. The double estimates have improves the estimation accuracy, while maintaining the low computation complexity. For further improving the communication efficiency, we adopt the common strategy in FedOpt to perform multiple local iterations. While LoSAC has demonstrated its success to remedy the two key challenges in FedOpt, the performance improvements are prominent comparing to state-of-the-art FedOpt algorithms. The contributions of this work can be summarized as the following three aspects:\n\u2022 A novel FedOpt algorithm is proposed, i.e., LoSAC. We innovatively estimate the global full gradient on clients as the search direction, which targets at effectively handling the model divergence problem and substantially improving the communication efficiency. We further extend LoSAC to its proximal version and verifies the extraordinary performance. \u2022 We have studied the theoretical results of LoSAC, including the convergence result of O ( 1 / ),\nwhere  and  are communication round and local iteration respectively; the global variance reduction which guarantees the fast speed of LoSAC and the defense ability against DLG. \u2022 Extensive experiments are conducted in the settings of IID data and non-IID data. It has shown LoSAC equips with the strong capability to overcome the model divergence problem. Moreover, it has exhibited quite high communication efficiency (i.e., more than 50% fewer communication rounds than the state-of-the-art method to reach a specific accuracy), defense ability against DLG. In particular, its proximal version equips with the strong capability in solving the nonsmooth problem while the competitive algorithm fails to solve it.\nThe rest of the paper is organized as follows: Section II is the related work discussing the existing popular FedOpt methods. Section III illustrates our proposed method by the motivation of the naive extension of SAGA, which only considers local gradient estimate. Then, we incorporate the global gradient estimate and formulate LoSAC. We further extend our proposed method to the proximal version for solving a wide class of nonsmooth problems. In Section IV, we study the theoretical properties of LoSAC, which has the global variance reduction on the search direction, is able to defense against DLG, and ensures the convergence and mitigates the model divergence problem. We conduct extensive experiments to verify the effectiveness of LoSAC in Section V. Mathematical notations: [] means the integer set {1 : }. \u0394 :=  + \u2212  is presented as 's increment when it has the updated value  + . The gradient operator for a smooth function  is denoted as \u2207 and the statistical expectation is provided by E. We use  2 -norm and for simplicity it is denoted as \u2225\u2022\u2225. \u27e8, \u27e9 is the inner product. Moreover,  is called -smoothness if \u2225\u2207 () \u2212 \u2207 () \u2225 \u2264  \u2225 \u2212  \u2225, where  > 0 is the Lipschitz constant, and that  is strongly convex with  > 0 satisfies  () \u2265  () + \u27e8\u2207 (),  \u2212 \u27e9 +  2 \u2225 \u2212  \u2225 2 . prox   () is the proximal operator defined in the following: prox   () = armin   () + 1 /2 \u2225 \u2212  \u2225 2  2 .\n2 RELATED WORK\n\n2.1 Stochastic Optimization\nConsidering there are  data samples ( is large), the stochastic optimization aims to solvemin \ud835\udc65 \ud835\udc53 (\ud835\udc65) = 1 \ud835\udc5b \u2211\ufe01 \ud835\udc5b \ud835\udc56=1 \ud835\udc53 \ud835\udc56 (\ud835\udc65),\nwhere  \u2208 R  is the model, and   : R  \u2192 R is the loss function with respect to the th sample. One of the most popular method stochastic gradient descend (SGD) [31] utilizes a small mini-batch of samples I to calculate () = 1 /|I |  \u2208I   () and update the model  via\ud835\udc65 \ud835\udc61 +1 = \ud835\udc65 \ud835\udc61 \u2212 \ud835\udf02 \u2022 \ud835\udc54(\ud835\udc65 \ud835\udc61 ),\nwhere  is the step-size. Although () is an unbiased estimator of the full gradient \u2207 (), it may have large variance leading to a slow convergence [4]. Thus, how to control and reduce stochastic variance during mini-batch optimization is a central issue.\nThe variance reduction techniques [8] have been developed to solve the above issue and greatly accelerate the convergence of SGD. Exemplar algorithms are stochastic variance reduction gradient method (SVRG) [10], stochastic average gradient method (SAG) [33] and its extension SAGA [7]. In particular, SAGA utilizes full gradient without direct calculation and is instead updated with the newest partial information at each iteration:\ud835\udc65 \ud835\udc61 +1 = \ud835\udc65 \ud835\udc61 \u2212 \ud835\udf02 \u2022 \u2207\ud835\udc53 \ud835\udc57 (\ud835\udc65 \ud835\udc61 ) \u2212 \u2207\ud835\udc53 \ud835\udc57 (\ud835\udc67 \ud835\udc61 \ud835\udc57 ) + 1 \ud835\udc5b \u2211\ufe01 \ud835\udc5b \ud835\udc56=1 \u2207\ud835\udc53 \ud835\udc56 (\ud835\udc67 \ud835\udc61 \ud835\udc56 ) ,\nwhere   is the delayed model and is updated via:\ud835\udc67 \ud835\udc61 +1 \ud835\udc57 = \ud835\udc65 \ud835\udc61 \ud835\udc56 , if \ud835\udc57th data is sampled, \ud835\udc67 \ud835\udc61 \ud835\udc57 , otherwise.\nLet \u03c6 :=  =1 \u2207  (   ) and    = \u2207  (  \u22121 ). Mimicking the efficient implementation in SAG [33], and SAGA can be equivalently carried out in real applications as\ud835\udc65 \ud835\udc61 +1 = \ud835\udc65 \ud835\udc61 \u2212 \ud835\udf02 \u2022 \u2207\ud835\udc53 \ud835\udc57 (\ud835\udc65 \ud835\udc61 ) \u2212 \ud835\udc66 \ud835\udc61 \ud835\udc57 + 1 \ud835\udc5b \ud835\udf19 \ud835\udc61 , ()\nwhere \u03c6 is updated via \u03c6+1 = \u03c6 \u2212    + \u2207  (  ).\n(5)O (\ud835\udc3e\ud835\udc51) O ( 1 /\ud835\udc45) \u2713 High \u2713 \u2713\nMimeSVRG [12] Multiple SVRG updatesO (\ud835\udc3e\ud835\udc51 + \ud835\udc5b \ud835\udc56 \ud835\udc51) O ( 1 /\ud835\udc45) \u2713 High \u00d7 \u2713 FedProx [20]\nSingle update with PGDO (\ud835\udc51) O ( 1 /\ud835\udc45) Limited Medium \u2713 \u00d7 MFL [24] Momentum gradient descent O (\ud835\udc3e\ud835\udc51) O ( 1 /\ud835\udc45) \u00d7 Low \u00d7 \u00d7 LoSAC (ours) Multiple global SAGA based on variance reduced GD update O (\ud835\udc3e\ud835\udc51) O ( 1 /\ud835\udc45) \u2713 Very high \u2713 \u2713\nSAGA has been shown the fast convergence speed. Moreover, it has the low computation level as SGD since only a single gradient is calculated during each update step.\nUnfortunately, directly adapting these methods to FedOpt may not be effective since the fast convergence may cause the client quickly moving towards the individual optimum instead of the global one [21].\n\n2.2 Federated Optimization (FedOpt)\nIn [30], a quantized version of FedAvg, known as FedPaq, is proposed for reducing the message overload. Mimicking the adaptation of SGD to FedOpt, the momentum gradient descent method which is a variant of SGD has been modified to fit for federated learning (MFL) [24]. Similarly, FedAdam [29] accommodated Adam [15] to FedOpt. However, these methods still suffer from the model divergence problem [21, 38, 41, 43]. A possible solution is to incorporate a quadratic restriction for the model divergence, which was known as FedProx [20]. Another solution may be the control variate for variance reduction, and based on which VRL-SGD has shown faster speed even with non-IID data [23]. However, VRL-SGD does not support the client sampling which is more practical in FL. Furthermore, with the control variate, SCAFFOLD has shown the significant performance improvement for the data heterogeneity problem in FedOpt [13]. Its core notion is to estimate the full gradient for the local search direction. Most recently, a framework called Mime was proposed [12], which adapts popular centralized algorithms (e.g., SGD, Adam etc.) to FedOpt.\nAs is mentioned in Section 2.1, the naive adaptation of the variance reduction strategy in the federated settings may worsen the convergence. This is due to the reason that variance reduction is applied for the local gradient estimation, which is biased from the global gradient. Both the SCAFFOLD and Mime framework are motivated by the idea of global variance reduction. However, the global full gradient estimates are kept over the whole local iterations, which may use outdated information and thus these methods' capability to correct the model divergence is limited. For LoSAC, both the local and global information are compactly utilized to keep the accurate estimate of the global gradient. Thus LoSAC can reach the high quality of global variance reduction. For the details, we compare the recent FedOpt methods in Table 1.\n\n3 LOSAC ALGORITHM\nIn this section, we describe LoSAC to handle the major challenges in FedOpt. We first formulate the federated optimization problem, which aims to be solved by collaboration via many clients. Suppose there are  clients, and each client  \u2208 [ ] has the local loss function   () with its own dataset D  containing   samples, i.e.,   () =   =1  , (), where  , () is a single loss function calculated by using the th data in D  . Moreover, the total dataset over all clients are denoted by D, i.e., D = \u222a  \u2208 [ ] D  . As in the literature [11-13, 16, 25], the FedOpt aims to collaboratively solve the following empirical risk minimization problem over  clients:min \ud835\udc65 \ud835\udc53 (\ud835\udc65) = 1 \ud835\udc41 \u2211\ufe01 \ud835\udc41 \ud835\udc56=1 \ud835\udc53 \ud835\udc56 (\ud835\udc65), ()\nwhere  is the averaged loss function. The model as the optimization variable satisfies  \u2208 R  . Moreover, the above functions satisfy  : R  \u2192 R,   : R  \u2192 R and  , : R  \u2192 R.(\ud835\udc65 \ud835\udc56 , \u03c6\ud835\udc56 , \ud835\udc66 \ud835\udc56,\ud835\udc57 ) \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \ud835\udc65 \ud835\udc56 \u2190 \ud835\udc65 \ud835\udc56 \u2212 \ud835\udf02{\ud835\udc5b \ud835\udc56 \u2207\ud835\udc53 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \ud835\udc56 ) \u2212 \ud835\udc5b \ud835\udc56 \ud835\udc66 \ud835\udc56,\ud835\udc57 + \u03c6\ud835\udc56 }, \u03c6\ud835\udc56 \u2190 \u03c6\ud835\udc56 \u2212 \ud835\udc66 \ud835\udc56,\ud835\udc57 + \u2207\ud835\udc53 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \u2212 \ud835\udc56 ), \ud835\udc66 \ud835\udc56,\ud835\udc57 \u2190 \u2207\ud835\udc53 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \u2212 \ud835\udc56 ),\nwhere  Sample clients S \u2286 [ ] and transmit  to client  \u2208 S.\n\n8:\nClients implement steps 9-14 in parallel for  \u2208 S:9:\nAfter receiving, set   \u2190 . Update (  ,   ,  , ) in order via (9). Calculate: \u0394  \u2190   \u2212 .\n\n14:\nClient  transmits \u0394  to the server. 15: end for 3.1.1 Limitation of FedSaga. As shown in (7), the variance reduction is realized by the local gradient \u2207  ({  }) on client , which is biased from the global gradient \u2207 (). Moreover, the multiple local update steps will make the local model fast approach to the local individual optimum (based on the local loss function) instead of the global one [21] (See the empirical results that FedSaga even performs worse than FedAvg). Since only a small portion of the clients participate in the update on each communication round, the aggregated model will be further biased from the optimal global one. Intuitively, one can think of the global aggregation step in the aspect of the SGD update step, with the gradient only containing the partial information from the participated clients.\n\n3.2 Local Stochastic Average Control\nAs is discussed, FedSaga uses partial local information for the local update which results in bias from the global information. Hence, we propose LoSAC, which uses and updates the global information estimates to make up for the bias. To be specific, LoSAC updates the local model on each client  \u2208 S via\ud835\udc65 \ud835\udc56 \u2190\ud835\udc65 \ud835\udc56 \u2212\ud835\udf02{ 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b ({\ud835\udc67 \ud835\udc5b }) \u2212\ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc67 \ud835\udc56,\ud835\udc57 ) +\ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \ud835\udc56 )},\nwhere  , is the delayed local model and updated via (8). It can be seen the key difference between LoSAC and FedSaga is that LoSAC has used the estimate for the global information, i.e.,  =1 \u2207  ({  }), while FedSaga only uses the local one.\nFor the local step, we denote   :=  =1 \u2207  ({  }), which is the delayed global gradient. At the beginning of the local step, the server transmits  to each client  \u2208 S as the initialized   . Then, the following equations in order, which are equivalent to (10), are carried out multiple iterations:  Fig. 1. Illustration of local updates in LoSAC. First, the local dataset block is randomly chosen for the gradient calculation \u2207 , (  ). Second, note  , stores the delayed gradient \u2207 , ( , ), therefore, the delayed gradient  , can be replaced by the calculated gradient \u2207 , (  ). Third, with \u2207 , (  ) and  , , the global gradient estimate  +  is formulated. Fourth, the local update is performed to obtain  +  . Fifth,  , is updated with the gradient \u2207 , (  ).(\ud835\udc65 \ud835\udc56 , \ud835\udf19 \ud835\udc56 , \ud835\udc66 \ud835\udc56,\ud835\udc57 ) \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \ud835\udc65 \ud835\udc56 \u2190 \ud835\udc65 \ud835\udc56 \u2212 \ud835\udf02 1 /\ud835\udc41\ud835\udf19 \ud835\udc56 \u2212 \ud835\udc66 \ud835\udc56,\ud835\udc57 + \ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \ud835\udc56 ) , \ud835\udf19 \ud835\udc56 \u2190 \ud835\udf19 \ud835\udc56 \u2212 \ud835\udc66 \ud835\udc56,\ud835\udc57 + \ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \u2212 \ud835\udc56 ), \ud835\udc66 \ud835\udc56,\ud835\udc57 \u2190 \ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \u2212 \ud835\udc56 ).\nupdate its own \u2207  while others are remained unchanged, owing to the FedOpt setting that client  can only have access to its own dataset D  , i.e.,\u0394\ud835\udf19 \ud835\udc56 = \ud835\udf19 + \ud835\udc56 \u2212 \ud835\udf19 \ud835\udc56 = \u2207\ud835\udc53 \ud835\udc56 ({\ud835\udc67 + \ud835\udc56 }) \u2212 \u2207\ud835\udc53 \ud835\udc56 ({\ud835\udc67 \ud835\udc56 }).\nThe local update procedure can be illustrated in Fig. 1.\nFor the global step, the server receives all the update quantities (\u0394  , \u0394  ) for  \u2208 S and performs the aggregation:\ud835\udc65 \u2190 \ud835\udc65 + 1 /\ud835\udc41 \u2211\ufe01 \ud835\udc56 \u2208S \u0394\ud835\udc65 \ud835\udc56 , and \ud835\udf19 \u2190 \ud835\udf19 + \ud835\udc41 /\ud835\udc46 \u2211\ufe01 \ud835\udc56 \u2208S \u0394\ud835\udf19 \ud835\udc56 .\nWhile   is compactly updated with the local information, it is also aggregated with the global information. Therefore, the twice estimates make   reach an accurate estimate for the global gradient. The detail of LoSAC is summarized in Algorithm 2. Remark 1. In real applications, a block of data points can be bundled for evaluating a single loss function  , and in this way, the memory cost on client  will be  ( \u2308    /\u2309), where  is the number of data points in each block.\n\n3.3 Extension with Proximal Operator\nProximal operator has been shown as an effective tool for solving nonsmooth, constrained, largescale, or distributed problems [28]. In this subsection, we extend our proposed method with proximal operator for solving a wider class of problems, such as  1 regularization or low-rank matrix estimation. Specifically, the problem under federated settings can be formulated in the following:min \ud835\udc65 \u2208R \ud835\udc51 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc56=1 \ud835\udc53 \ud835\udc56 (\ud835\udc65) + \u03a8(\ud835\udc65),\nwhere  \u2208 R  is the model,   () : R  \u2192 R is the local loss function, and\u03a8 : R \ud835\udc51 \u2192 R is the nonsmooth convex regularizer. When \ud835\udc53 \ud835\udc56 (\ud835\udc65) = 1 /\ud835\udc5b \ud835\udc56 (\ud835\udc51 \ud835\udc56 ,\ud835\udc66 \ud835\udc56 ) \u2208D \ud835\udc56 \ud835\udc65 \ud835\udc47 \ud835\udc51 \ud835\udc56 \u2212 \ud835\udc66 \ud835\udc56 2\n, where D  is the local Server implements steps 5-6:\n5:\nUpdate (, ) via ( 13).\n6:\nSample clients S \u2286 [ ] and transmit (, ) to client  \u2208 S.\n\n7:\nClients implement steps 8-14 in parallel for  \u2208 S:8:\nAfter receiving, set   \u2190  and   \u2190 .\n\n9:\nfor  = 1, . . . , do 10:\nSample index  from [  ].\n11:\nUpdate (  ,   ,  , ) in order via (11).\n12:\nend for 13:\nCalculate: \u0394  \u2190   \u2212  and \u0394  \u2190   \u2212 .\n\n14:\nClient  transmits (\u0394  , \u0394  ) to the server. 15: end for dataset on client , and \u03a8 =  \u2225\u2022\u2225 1 , the problem is known as LASSO [32]. As for the local gradient descent based proximal step, it can be derived:\ud835\udc65 + \ud835\udc56 \u2190 argmin \ud835\udc67 1 2\ud835\udf02 \u2225\ud835\udc67 \u2212 (\ud835\udc65 \ud835\udc56 \u2212 \ud835\udf02\u2207\ud835\udc53 \ud835\udc56 (\ud835\udc65 \ud835\udc56 )) \u2225 + \u03a8(\ud835\udc65 \ud835\udc56 ),\nwhich can be solved via  +  \u2190 prox \u03a8 (  \u2212 \u2207  (  )). Mimicking the PGD step, we incorporate the global gradient estimate in LoSAC to replace the local gradient in (15), which subsequently lead to\ud835\udc65 \ud835\udc56 \u2190 prox \ud835\udf02\u03a8 (\ud835\udc65 \ud835\udc56 \u2212 \ud835\udf02 1 /\ud835\udc41\ud835\udf19 \ud835\udc56 \u2212 \ud835\udc66 \ud835\udc56,\ud835\udc57 + \ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65 \ud835\udc56 ) ).\nWhile adopting (16), we maintain the local estimate for   and the global aggregation for (, ), the resulting algorithm is called LoSAC-Prox. In Section 5, we will apply LoSAC-Prox for solving the low-rank matrix estimation problem for further showing the superiority over the state-of-the-art algorithm.\n\n4 THEORETICAL ANALYSIS\nIn this section, the theoretical analysis of LoSAC is presented. Specifically, we first provide the global variance reduction of LoSAC to show that the variance of the search direction is vanishing. Moreover, we also analyze the enhanced defense ability of LoSAC in gradient leakage. Then we study the convergence analysis, of which one challenge is resulted from the multiple local iterations. Moreover, it can be seen in ( 10) that the delayed local models {  } of client  make the convergence analysis difficult for evaluation, since after a few iterations, the delayed local gradient \u2207  ({  }) has the mixed arguments. Finally, we show LoSAC equips with the ability in handling model divergence.\n\n4.1 Global Variance Reduction\nThe variance of the search direction in LoSAC is progressively reduced to zero comparing to the SGD update in FedAvg. Moreover, it maintains the robustness in LoSAC for the convergence.\nComparing with the recent method MimeSVRG [12] and SCAFFOLD [13], LoSAC equips with the benefit of compactly refreshed global variance reduction. Hence its convergence performance can be improved comparing to MimeSVRG and SCAFFOLD, which is demonstrated in the numerical experiments. In the following, we provide the global variance reduction in Lemma 1:\nLemma 1. Suppose the sequence {  } generated by Algorithm 1 is expected to converge, i.e., E \u2225  \u2212  * \u2225 \u2192 0. Moreover, for the th client, denote the search direction in the local update asg\ud835\udc61 \ud835\udc56 = 1 /\ud835\udc41\ud835\udf19 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 ) + \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 \ud835\udc61 \ud835\udc56 ).\nThen, the variance of the search direction g  is progressively vanished, i.e., E\u2225 g  \u2212 E( g  ) \u2225 2 \u2192 0.\n\n4.2 Defense Ability to the Gradient Leakage\nAn important benefit of LoSAC is the enhancement of the defense ability to the recent technique Deep Leakage from Gradients (DLG) [45] which aims to obtain the information leakage from the gradient. As the illustration, we denote   (; D  ) =   () to explicitly emphasize the dependency on the input sample D  . Then, the DLG is defined as follows:\nDefinition 1 (DLG [45]). For an algorithm A, let the associated gradient be \u2207  (; D  ) and the model parameter be , andD \u2032 \ud835\udc56 = arg min D\ud835\udc56 \u2207\ud835\udc53 \ud835\udc56 (\ud835\udc65; D\ud835\udc56 ) \u2212 \u2207\ud835\udc53 \ud835\udc56 (\ud835\udc65; D \ud835\udc56 ) 2 .\nIf a Malicious Attacker (MA) is able to obtain D  by finding D \u2032  above, then the algorithm A suffers from Deep Leakage from Gradients (DLG).\nHence, with the technique of DLG, MA is able to progressively match the gradient \u2207  (, D  ) by minimizing the difference between the \"dummy gradient\" \u2207  (, D ) and \u2207  (, D  ). Moreover, when the optimum is reached, MA steals the data D  from the th client, i.e., D \u2032  = D  . According to the above definition, MA is not able to apply DLG algorithm to obtain the local dataset in LoSAC. We illustrate this from two reasons. First, the th client's gradient is evaluated at many delayed local models {  }, i.e., \u2207  ({   }, D  ). Moreover, {  } are stored locally that MA is difficult to obtain; second, LoSAC transmits the increments \u0394  and \u0394  instead of the gradients. Howover, the distributed SGD [45] and Mime framework [12] communicates the local gradient with the the local model, i.e.,  and \u2207  (; D  ), hence MA is able to steal the local data D  by DLG technique.\n\n4.3 Convergence Result\nIn this subsection, we study the convergence property of our proposed method. We first show the progress of each communication round in Lemma 2. Particularly, we need the following regular assumptions. where () and  , () are the unbiased estimates of \u2207 () and \u2207  () respectively. A2. The second-order moments of all the unbiased gradient estimates are bounded, i.e., E \u2225() \u2225 2  \u2264   andE \ud835\udc57 \ud835\udc54 \ud835\udc56,\ud835\udc57 (\ud835\udc65) 2 \u2264 \ud835\udeff \ud835\udc53 .\nHere, Assumptions A1 and A2 have been regularly made for convergent analysis in optimization literatures [14, 21, 35, 36, 42] and [1, 21, 36, 42], respectively. Moreover, Note that Assumptions A1 and A2 imply that the second-order moments of the gradients \u2207 () and \u2207  () are also bounded, i.e., E \u2225\u2207 () \u2225 2 \u2264   \u2212   and E \u2225\u2207  () \u2225 2 \u2264   \u2212   .\nFrom (10), we intuitively have the approximate gradient descent (GD) step in each local iteration\ud835\udc65 \ud835\udc61 +1 \ud835\udc56 \u2243 \ud835\udc65 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udf02 \ud835\udc41 \u2211\ufe01 \ud835\udc41 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b ({\ud835\udc67 \ud835\udc61 \ud835\udc5b }).\nHence, the local iteration progress can be bounded above with reference to the GD theory [22] and subsequently, the one round progress can be obtained in the following: Lemma 2. Suppose functions  , {  } and { , } are strongly convex and -smooth that satisfy Assumption 1, and denote  * as the optimal point, if  is sufficiently large 1, there exist positive variables \u210e 2 ,  \u2032 2 and  \u2032 2 such thatE\u2225\ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 * \u2225 2 \u2264 \u22122\ud835\udf02\ud835\udc46\ud835\udc47 \ud835\udc41 E{\ud835\udc53 (\ud835\udc65 \ud835\udc5f ) \u2212 \ud835\udc53 (\ud835\udc65 * )} + (1 \u2212 \ud835\udf02\u210e 2 )E\u2225\ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u2225 2 + \ud835\udf06 \u2032 2 \ud835\udf02 2 + \ud835\udf08 \u2032 2 \ud835\udf02 3 . ()\nIt should be noted that the assumption that  , is strongly convex is strong in real applications, e.g.,  , is the loss function in a neural network, but this assumption can be simply realized by appending a  2 regularization term to  , to form the strongly convex function.\nBased on Lemma 2, the convergence speed can be obtained in the following.\nTheorem 3. Suppose functions  , {  } and { , } satisfy A1 and A2. Given a positive sequence {  } defined by   = (1 \u2212 \u210e 2 ) \u2212 , LoSAC has a weighted averaging convergence speed:1 \ud835\udc4a \ud835\udc45 \u2211\ufe01 \ud835\udc45 \ud835\udc5f =0 \ud835\udc64 \ud835\udc5f E{\ud835\udc53 (\ud835\udc65 \ud835\udc5f ) \u2212 \ud835\udc53 (\ud835\udc65 * )} \u2264 O ( \ud835\udc41 \ud835\udf06 \u2032 2\ud835\udc46\ud835\udc47\u210e\ud835\udc45 ). ()\nwhere   :=   =0   , \u210e and  \u2032 are positive.\nIt can be seen the convergence speed will be faster if there are more local iterations  while the computation complexity is also increased. Furthermore, if more clients (larger |S|) are participated in model training, it will be faster for convergence. Moreover, our analysis does not assume data heterogeneity while [13] does. This is due to the reason that   estimates the global information. We intuitively and empirically illustrate this in Section 4.4 and numerical experiments respectively.\n\n4.4 Handling Model Divergence\nLoSAC is expected to equip with the capability to overcome the model divergence problem that resulted from data heterogeneity and client sampling in FedOpt. As an intuitive illustration of this, the local update step on client  in expectation can be approximated as E  ( [25], logistic regression and low rank matrix estimation [27] as the training models. Specifically, 2NN is a fully connected neural network with 2 hidden layers with 200 ReLU activation functions in the each hidden layer and a softmax output.\n5.1.2 Datasets. Three real datasets are chosen for overall performance, ablation study and DLG study, namely MNIST [18], Human Activity Recognition Using Smartphones dataset (HAR) [3] and Epileptic Seizure Recognition dataset (ESR) [2]. We choose MNIST dataset since it has been widely applied for the study. Moroever, HAR and ESR are chosen due to the increasing interests and the large potential for the FedOpt applications in mobile devices and healthcare, respectively. Specifically, we use 60, 000 for training and 10, 000 for testing in MNIST, 7, 352 for training and 2, 947 for testing in HAR, and 9, 200 for training and 2, 300 for testing in ESR. Moreover, we choose synthetic dataset for the low rank matrix estimation.\nTable 2. Ablation study: measured by the communication rounds for LoSAC and SCAFFOLD to reach a specific accuracy (85% for all datasets), loss and test accuracy. We consider the different local iteration  and the local dataset division  ( also corresponds to the local memory size for storing  , ) for calculating the gradient for comparisons to show the efficient computation and communication of LoSAC. Other parameters are set as  = 10 and  = 10 \u22124 . Moreover, the accuracy is measured at round 500, which is sufficient for reaching a satisfactory accuracy. 5.1.3 Compared algorithms. We compare LoSAC with five representative baseline algorithms FedAvg [25], FedCM [39], SCAFFOLD [13], FedADMM [34] and MimeSVRG [12]. Moreover, to show the ineffectiveness of the naive extension of SAGA to FedOpt, we have also implemented FedSaga.\n\nDatasets Methods\n\n\nRounds\n(\ud835\udc47 = 2) Loss (\ud835\udc47 = 2) ACC (\ud835\udc47 = 2) Rounds (\ud835\udc47 = 4) Loss (\ud835\udc47 = 4) ACC (\ud835\udc47 = 4) Rounds (\ud835\udc47 = 6) Loss (\ud835\udc47 = 6) ACC (\ud835\udc47 = 6) MNIST SCAFFOLD (\ud835\udc40 = 2)As is mentioned, FedAvg improves the communication efficiency comparing to FedSGD [25].\nFedCM [39] adopts the momentum strategy in FedOpt. SCAFFOLD [13] equips with the capability in handling non-IID data. Particularly, MimeSVRG is developed by adapting SVRG [10] to FedOpt using the framework Mime [12]. Especially, FedADMM adapts the alternating direction method of multipliers (ADMM) to FedOpt [34], which are known to conveniently and efficiently solve the nonsmooth optimization problems [5].\nFor the default parameters for all algorithms, ( , ) = (1, 000, 50) for MNIST cases and ( , ) = (100, 10) for HAR and ESR cases. We set the local data division  = 5 and the local iteration  = 5. Particularly, the step size is set to yield as the best performance as possible for each algorithm, i.e.,  = 4 \u00d7 10 \u22124 for MNIST cases and  = 10 \u22124 for HAR and ESR cases. As for FedADMM [34], the details of FedADMM solving low rank matrix estimation are in Appendix E. Different from [25] that the local update in FedAvg traverses all the dataset for multiple epochs, we follow [13] that all algorithms are implemented by sampling a mini-batch of data samples for search direction in the local update.\n\n5.1.4 Evaluation tasks.\nFor the overall performance and ablation study, the cross entropy and the classification accuracy are evaluated. Specifically, the overall performance is to show the general performance with different data and parameter settings, with the comparison to the baseline algorithms; the ablation study aims to show the communication and the computation efficiency of the algorithms for reaching a specific accuracy. Moreover, for the IID setting, the datasets are shuffled, and for the non-IID setting, the datasets are sorted by the labels. Then the datasets are divided evenly into  clients. Note since MimeSVRG's performance is comparative to SCAFFOLD and its computational complexity is twice of SCAFFOLD and LoSAC, hence for the parameter sensitivity study, it is only implemented with MNIST. For DLG study, we mainly consider the similarity measured by the Frobenius norm between the estimated data samples by DLG and the real data samples. For the low rank matrix estimation, FedADMM [34] and the proximal versions of LoSAC (LoSAC-Prox) and SCAFFOLD (SCAFFOLD-Prox) are implemented with the evaluation of the recovery matrix error and the recovery matrix rank.\n\n5.2 Overall Performance\n\n\n5.2.1 Data Heterogeneity.\nIn this subsection, we study the effect of the data heterogeneity on LoSAC and show the impacts of the local and global variance reduction schemes. Specifically, FedSaga utilizes local variance reduction, while SCAFFOLD, MimeSVRG and LoSAC uses global one. Moreover, FedAvg is implemented as the benchmark. For the experimental settings, the MNIST dataset is utilized for training and testing. We choose (1 \u2212 %) of the uniformly shuffled training dataset and distribute it to  = 1, 000 clients. Then, the left % of the training dataset is set to be non-IID, namely is sorted by the labels and distributed to all clients. Therefore, the larger the parameter % is, the data is more heterogeneous. For other parameters settings,  is set to  = 50, the step size is chosen as  = 4 \u00d7 10 \u22124 for all algorithms since it has shown the best performances. Moreover, the iteration number is (, ) = (100, 5). For LoSAC and FedSaga, Since the local dataset is divided into  blocks, the parameter is set as  = 5. As Fig. 2 shows, when the data is more heterogenous (namely % is larger), both FedAvg and FedSaga suffers from the data heterogeneity problem more seriously. For SCAFFOLD, MimeSVRG and LoSAC, since they all utilize the global information to correct the bias from the global model in the local update, the data heterogeneity problem has the little impact on the performance. Hence, the global variance reduction is much more robust to the data heterogeneity problem than the local one. Moreover, it can be seen that FedCM also has the impact of data heterogeneity, but the impact is not as serious as FedAvg and FedSaga. This can be attributed to the global aggregation of the local momentum term in FedCM, which has mitigated the model divergence problem.\n\n5.2.2 Parameter Sensitivity.\nWe conduct extensive experiments to study the effects of { ,  }, which play significant roles on the convergence results in Theorem 3. The experiments use IID and non-IID settings for each evaluation respectively. The results are shown in Figs. 3\u223c4, Fig. 5 and Fig. 6 with MNIST, HAR and ESR datasets respectively. In particular, HAR and ESR datasets correspond to the mobile and the medical applications respectively. While we uses the default settings for MNIST, we fix  = 100,  = 10 \u22124 and  = 5 for HAR and ESR. In general, larger  and  lead to faster speed and better classification performance for LoSAC. This matches well with the convergence results in Theorem 3.\nIn Figs. 3\u223c4, while our proposed method exhibits the prominent performance improvements, the non-IID data has significantly degraded the performances of FedAvg and MimeSVRG. With regard to the comparison of FedSaga and FedAvg, it can be seen that even with the acceleration scheme (here it is variance reduction), FedSaga performs worse than FedAvg. Although the acceleration scheme has shown the effectiveness in centralized optimization methods, it is not effective in the naive extension of SAGA to the FedOpt settings. This may due to the reason that the acceleration scheme using only local information may adversely lead FedSaga to fast approach the local optimum instead of the global one, resulting in large bias from the global optimum. For FedCM, more clients participated for local updates in FedCM can generally lead to the higher performances, this can be attributed to that more clients will contribute more information. Moreover, the non-IID case has affected the performance of FedCM, and more local steps in FedCM will adversely lead performance degradation. This is due to the reason that FedCM uses momentum acceleration in local model update, but with non-IID setting, it will accelerate the speed of model divergence and lead to performance degradation. In particular for MimeSVRG, it shows the large fluctuations in the non-IID setting, and more local iterations will lead to larger fluctuations. While SCAFFOLD and our method exhibit the strong capability in handling data heterogeneity problem, our method outperforms SCAFFOLD. Thus, it demonstrates the strong capability of LoSAC for mitigating the model divergence problem.\nFor HAR, Fig. 5 has shown the performances of all methods with different  and  . In general, the result matches the Theorem 3 that larger  and  leads to better performances. In particular, our proposed method with  = 2 even exhibits better performances than SCAFFOLD with  = 8. This means that with only 25% of the computational complexity in SCAFFOLD, our proposed method still yields quite high performances. However, it shows the large fluctuations of LoSAC in the initial few updates. The reason may due to the randomness in the delayed full gradient that has brought the large variance in the initial updates, when  , seriously differs from   . When the algorithm progresses, it is expected to satisfy  , \u2192   , and the variance begins to reduce.\nFig. 5 has also exhibited the cases with ESR dataset using the same settings. Here, both SCAFFOLD and FedAvg have been significantly affected by the model divergence problem, while our proposed method has demonstrated the remarkable performances.\n\n5.3 Ablation Study\nWe have shown the overall performance with different parameter and data heterogeneity settings. In this subsection, we continue to conduct the ablation study with different local memory (corresponding to the local data division ) and local iterations. We implement SCAFFOLD as the benchmark since it performs the second best in overall performance. Moroever, we adopt the non-IID setting. The step size for all cases is set to  = 10 \u22124 for the algorithms to yield as the best performances as possible. We set ( , ) = (100, 10) for all cases. { ,  } are tuned to for the comparisons. In general, it shows the significantly higher communication and computation efficiency over SCAFFOLD, which also demonstrates the effectiveness of the estimate for the global full gradient. The results are shown in Table 2. 2 that LoSAC requires much fewer communication rounds than SCAFFOLD to reach a given accuracy. In particular, the communication efficiency is improved by more than 300% for ESR case and 100% in average for all cases. Thus, LoSAC is communication quite efficient. This is because LoSAC estimates the global gradient more accurately than SCAFFOLD, which has accelerated the convergence speed and mitigated the model divergence problem. 2 further demonstrates the high computation efficiency of LoSAC. To be specific, when LoSAC with  = 2 and  = 3, it requires comparable communication rounds to reach the specific accuracy with SCAFFOLD with  = 6. This means with only around 33% computation complexity of SCAFFOLD, LoSAC can still yield higher communication efficiency.\n\n5.3.1 Communication efficiency. It can be seen from Table\n\n\n5.3.2 Computation efficiency. Table\n\n\n5.3.3 Local memory.\nFor LoSAC, each client  needs to spend the sufficient memory to store  , ,  \u2208 [], depending on the partitioning of the local dataset D  . Hence, we choose different memory sizes for the evaluations, i.e.,  = {2, 3, 5}. It also corresponds to  divisions of the local datasets. Table 2 indicates that larger memory size leads to a better performance for LoSAC. This may due to the reason that the local estimation of the global full gradient is improved with a larger memory size, and thus is less affected by the non-IID data. However, it will also cost local resources. In FL applications, clients may have limited memory, e.g., the case for mobile phones, thus we suggest a better trade-off between the performance and the resource. Note when  = 2, the storage cost of LoSAC is O (2) and the same with SCAFFOLD (SCAFFOLD requires to store the control variates and the model parameter for each local iteration), LoSAC yields much better performances than SCAFFOLD, i.e., more than 40% averaged performance improvements. Fig. 6. The performance evaluations on the defense against DLG. Binary logistic regression on the federated datasets is collaboratively performed over all clients. Moreover, GD is applied for solving the DLG problem to steal the dataset on the 1st client.\n\n5.4 Defense Against DLG\nWe study the defense ability of each algorithm against DLG in Figure 6. Specifically, except for LoSAC, SCAFFOLD, FedAvg and MimeSVRG, we also implement DSGD since it has been the major attack objective by DLG [19]). The real datasets MNIST, HAR and ESR are utilized for the evaluations. For simplicity, we perform each FedOpt on logistic regression for binary classification on all datasets. Specifically, for MNIST, the label is set to 0 when it is smaller or equal to 5 and 1 otherwise; for HAR, the label is set to 0 when it is smaller or equal to 3 and 1 otherwise; For ESR, it has two classes that match well with binary logistic regression. We set ( , ) = (100, 100), i.e., all the clients participate in the local update in each round. All local datasets are utilized for the search direction in each local step, i.e.,  = 1. Furthermore, we apply gradient descent (GD) to perform the DLG attack in Definition 1. We tune the step size in the (a) MNIST and HAR cases:  = 10 \u22124 for all algorithms and   = 10 \u22123 for GD in DLG, (b) ESR case:  = 10 \u22123 for all algorithms and   = 10 \u22123 for GD in DLG. For each case, the GD in DLG has the iteration number 100. For simplicity without loss of generalization, we perform the DLG attack aiming to obtain the 1st client's data samples D 1 in the 5th round for each algorithm in all cases. We denote D1 as the obtained data samples by DLG, and use the metric \u2225 D1 \u2212 D 1 \u2225  for performance evaluations. Therefore, smaller value of the metric \u2225 D1 \u2212 D 1 \u2225  means more successful for DLG to obtain the local dataset D 1 .\nAs shown in Fig. 6, both DSGD and MimeSVRG are vulnerable to the DLG since the estimated data samples are more and more approaching to the true data samples. This is because they transmits the gradients and the corresponding models, which satisfies exactly the DLG attack in Definition 1. As for FedAvg, SCAFFOLD and LoSAC, the DLG aims to obtain the data samples based on the term 1 / \u2022 ( + 1 \u2212 ), which is essentially the averaged gradients over the  local steps. Under DLG attacks, the estimated data samples in FedAvg, SCAFFOLD and LoSAC are more and more divergent from the true data samples, which demonstrates the capabilities of these algorithms to defense against DLG.\n\n5.5 Low Rank Matrix Estimation\nWe further verify LoSAC for solving the problem of low rank matrix estimation via the comparisons with SCAFFOLD. First, the problem can be formulated in the following:min \ud835\udc4b 1 /\ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc56=1 \ud835\udc53 \ud835\udc56 (\ud835\udc4b ) + \ud835\udf06 \u2225\ud835\udc4b \u2225 * ,\nwhere   ( ) =   =1 (\u27e8,   \u27e9 \u2212   ) 2 ,   \u2208 R \u00d7 is the data sample,   is the noisy observation and \u2225\u2022\u2225 * denotes the nuclear norm of a matrix to obtain a low rank matrix solution  * . The problem (20) targets at recovering the low rank matrix  \u2208 R \u00d7 from the noisy observations   . The proximal version of the state-of-the-art algorithm SCAFFOLD, namely SCAFFOLD-Prox, and FedADMM are studied as the comparisons. In particular, FedADMM is known to be convenient and efficient to solve the nonsmooth optimization problem [5, 34]. For LoSAC-Prox and SCAFFOLD-Prox, solving the above problem via the proximal operation, we can derive\ud835\udc4b * \u2190 argmin \ud835\udc4d \ud835\udf06 \u2225\ud835\udc4d \u2225 * + 1 2\ud835\udf02 \u2225\ud835\udc4d \u2212 (\ud835\udc4b \u2212 \ud835\udf02 \u2207\ud835\udc53 \ud835\udc56 (\ud835\udc4b )) \u2225 2\n, where we denote \u2207  ( ) as the global gradient estimate. Then, the above problem can be solved via  * =  \u2022 diag prox  \u2225 \u2022 \u2225 1 () \u2022   , where  ,  and  can be conveniently obtained via the singular value decomposition of the matrix ( \u2212  \u2207  ( )). For FedADMM [34] solving low rank matrix estimation, the details are in Appendix E.\nWe evaluate the algorithms on the synthetic dataset for simplicity with the known ground truth   \u2208 R \u00d7 , which is obtained as follows:\ud835\udc4b \ud835\udc3a = \ud835\udc3c \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 0 \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58\u00d7(\ud835\udc51\u2212\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58) 0 (\ud835\udc51\u2212\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58)\u00d7\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 0 (\ud835\udc51\u2212\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58)\u00d7(\ud835\udc51\u2212\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58)\nAs for the synthetic dataset generation, each element of a data sample   is randomly generated by N (0.1, 1). Moreover,   \u2208 R is generated by   = \u27e8  ,   \u27e9 + N (0, 0.1). Specifically, we set  = 64 for   and five scenarios of the matrix rank are studied, i.e.,  (  ) = [32, 16, 8, 4, 2]. We set ( , ) = (100, 10) and the local dataset division to be  = 5 for each algorithm. We tune the step size  = 2 \u00d7 10 \u22123 for each algorithm to yield as the best performance as possible. We adopt two metrics for performance evaluation, namely the recovery matrix error \u2225  \u2212   \u2225  and the rank of the recovery matrix  (  ) (we count the number of singular values that are greater than 10 \u22123 ).\nThe results are shown in Fig. 7. It can be seen although ADMM has shown to conveniently and efficiently solve the nonsmooth optimization problem, with FedOpt settings (e.g., partial client participation), its performance is substantially degraded. In particular for SCAFFOLD, although it performs satisfactory results in the overall performance and ablation study, SCAFFOLD works poor in the low rank matrix estimation and cannot precisely recover the ground truth matrix, i.e., while the recovery matrix error is large, the rank of the estimated matrix significantly differs from the rank of the ground truth matrix. For LoSAC-Prox, it has shown the superiority in solving the problem. The recovery matrix   matches very well with the ground truth matrix   , i.e., it has the smallest recovery error and the EXACT rank with   .\n\n6 CONCLUSION\nDue to the data heterogeneity and client sampling, the performance of federated optimization suffers from degradation. Although there are several works attempting to mitigate these problems, none of them could well address them. We proposed a new FedOpt algorithm LoSAC for handling the challenges by compactly estimating the global gradient. Moreover, we extend LoSAC to its proximal version for solving a wider class of problems. We demonstrate the effectiveness of LoSAC via theoretical guarantees and the empirical studies. It shows that LoSAC equips with the strong ability in handling the model divergence problem, and the high communication and computation efficiency over the state-of-the-art methods. Especially in the low-rank matrix estimation problem, LoSAC has demonstrated its superior performances over SCAFFOLD and FedADMM, i.e., it can recover very well the true matrix with the exact rank. It is worth mentioning that LoSAC has the defense ability against the information leakage from the gradient.\n\nA USEFUL LEMMAS\nFor proving Lemma 2, we provide the following useful lemmas. First, we present the Lemma 4, of which the relaxed triangle inequalities have provided quite important tools for evaluating the update progress in each round. Based on Lemma 4, the inequalities in Lemma 5 can be derived. Lemma 4. For vectors { 1 , . . . ,   } in R  , the following inequalities hold:\ud835\udc4e \ud835\udc56 + \ud835\udc4e \ud835\udc57 2 \u2264 (1 + \ud835\udefc) \u2225\ud835\udc4e \ud835\udc56 \u2225 2 + (1 + 1 \ud835\udefc ) \ud835\udc4e \ud835\udc57 2 , and\u2211\ufe01 \ud835\udc5a \ud835\udc56=1 \ud835\udc4e \ud835\udc56 2 \u2264 \ud835\udc5a \u2211\ufe01 \ud835\udc5a \ud835\udc56=1 \u2225\ud835\udc4e \ud835\udc56 \u2225 2 , ()\nwhere  > 0.\nLemma 5. Suppose  is a strongly convex function with  > 0 and has -smoothness Lispschitz gradient, then the following inequalities holds:\u27e8\u2207\ud835\udc53 (\ud835\udc65), \ud835\udc67 \u2212 \ud835\udc66\u27e9 \u2264\ud835\udc53 (\ud835\udc67) \u2212 \ud835\udc53 (\ud835\udc66) + (\ud835\udc3f + \ud835\udefd\ud835\udc3f \u2212 \ud835\udf07 /4) \u2225\ud835\udc66 \u2212 \ud835\udc67 \u2225 2 + (1 + \ud835\udefd \u22121 )\ud835\udc3f \u2225\ud835\udc67 \u2212 \ud835\udc65 \u2225 2 , ()\nand moreover\u27e8\u2207\ud835\udc53 (\ud835\udc65), \ud835\udc67 \u2212 \ud835\udc66\u27e9 \u2265\ud835\udc53 (\ud835\udc67) \u2212 \ud835\udc53 (\ud835\udc66) + \ud835\udf07 /[2(1+\ud835\udefc) ] \u2225\ud835\udc66 \u2212 \ud835\udc67 \u2225 2 \u2212 ( \ud835\udc3f /2 + \ud835\udf07 /2\ud835\udefc) \u2225\ud835\udc67 \u2212 \ud835\udc65 \u2225 2 . ()\nwhere  > 0 and  > 0.\nProof : Lemma 5 can be derived with Lemma 4 and the definitions of -smoothness and convexity. Specifically, since  is strongly convex and has -smooth gradient, it implies that  \u2265 , hold by the definitions of -smoothness and strongly convexity respectively. Combining the two inequalities yields:\u27e8\u2207\ud835\udc53 (\ud835\udc65), \ud835\udc67 \u2212 \ud835\udc66\u27e9 \u2264 \ud835\udc53 (\ud835\udc67) \u2212 \ud835\udc53 (\ud835\udc66) + \ud835\udc3f \u2225\ud835\udc65 \u2212 \ud835\udc66 \u2225 2 \u2212 \ud835\udf07 4 \u2225\ud835\udc67 \u2212 \ud835\udc66 \u2225 2 ,\nwhere we have used the triangle inequality. We further use the triangle inequality as follows:\u2225\ud835\udc65 \u2212 \ud835\udc66 \u2225 2 \u2264 (1 + \ud835\udefd) \u2225\ud835\udc66 \u2212 \ud835\udc67 \u2225 2 + (1 + \ud835\udefd \u22121 ) \u2225\ud835\udc67 \u2212 \ud835\udc65 \u2225 2 ,\nwhich we substitute into (27) that can lead to the desired result in (24). For the second inequality in (25), it follows from the first inequality in (27) that\u27e8\u2207\ud835\udc53 (\ud835\udc65), \ud835\udc67 \u2212 \ud835\udc66\u27e9 \u2265 \ud835\udc53 (\ud835\udc67) \u2212 \ud835\udc53 (\ud835\udc66) \u2212 \ud835\udc3f 2 \u2225\ud835\udc65 \u2212 \ud835\udc67 \u2225 2 + \ud835\udf07 2 \u2225\ud835\udc66 \u2212 \ud835\udc65 \u2225 2 . ()\nFurthermore, the triangle inequality with  > 0 is utilized and it yields:\u2225\ud835\udc66 \u2212 \ud835\udc65 \u2225 2 \u2265 1 /(1+\ud835\udefc) \u2225\ud835\udc66 \u2212 \ud835\udc67 \u2225 2 \u2212 1 /\ud835\udefc \u2225\ud835\udc67 \u2212 \ud835\udc65 \u2225 2 . ()\nBy combining ( 29) and (30), we can obtain the desired result in (25) and this completes the proof of Lemma 5.\n\nB PROOF OF LEMMA 2\nProof. The proof of Lemma 2 is based on the results of Lemmas 4 and 5, which are in the Appendix A. We first recall the following inequalities hold respectively:\ud835\udc53 (\ud835\udc66) \u2264 \ud835\udc53 (\ud835\udc65) + \u27e8\u2207\ud835\udc53 (\ud835\udc65), \ud835\udc66 \u2212 \ud835\udc65\u27e9 + \ud835\udc3f 2 \u2225\ud835\udc65 \u2212 \ud835\udc66 \u2225 2 , and\u2225\u2207\ud835\udc53 (\ud835\udc66) \u2212 \u2207\ud835\udc53 (\ud835\udc65) \u2225 \u2265 \ud835\udf07 \u2225\ud835\udc66 \u2212 \ud835\udc65 \u2225 .\nThen we start with evaluating \u0394   within  iterations given as\u0394\ud835\udc65 \ud835\udc5f \ud835\udc56 = \u2212\ud835\udf02 \u2211\ufe01 \ud835\udc47 \u22121 \ud835\udc61 =0 { 1 \ud835\udc41 \u2211\ufe01 \ud835\udc41 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b ({\ud835\udc67 \ud835\udc61 \ud835\udc5b }) \u2212 \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 ) + \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 \ud835\udc61 \ud835\udc56 )}.\nNext, we aim to evaluate the progress in one round. To be specific:E \ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 * 2 = E \ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 \ud835\udc5f 2 + E \u2225\ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u2225 2 + 2E\u27e8\ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 \ud835\udc5f , \ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u27e9. ()\nWe first expand the term E   +1 \u2212   2 as follows, then the upper bound can be further derived (with  0 :=  2  2  / 3 ):E \ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 \ud835\udc5f 2 \u2264 \ud835\udc46 2 \ud835\udc41 3 \ud835\udc41 \u2211\ufe01 \ud835\udc56=1 E \u0394\ud835\udc65 \ud835\udc5f \ud835\udc56 2 \u2264 \ud835\udc50 0 \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u2225 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b ({\ud835\udc67 \ud835\udc61 \ud835\udc5b }) \u2212 \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 ) + \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 \ud835\udc61 \ud835\udc56 ) \u2225 2 \ud835\udc50 0 \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u2225 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b ({\ud835\udc67 \ud835\udc61 \ud835\udc5b }) \u2212 1 \ud835\udc41 \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \u2207\ud835\udc53 \ud835\udc5b (\ud835\udc65 * ) \u2212 \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 ) + \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 * ) + \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 \ud835\udc61 \ud835\udc56 ) \u2212 \ud835\udc54 \ud835\udc56,\ud835\udc57 \ud835\udc61 (\ud835\udc65 * ) \u2225 2 ,\nthus by defining  1 := 3)\nwhere we have adopted the relaxed triangle inequality in Lemma 4. By using the condition of Lipschitz continuity, it yields:E \ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 \ud835\udc5f 2 \u2264 \ud835\udc50 1 \ud835\udc3f \ud835\udc41 \u2211\ufe01 \ud835\udc56,\ud835\udc61,\ud835\udc5b,\ud835\udc57 E\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc65 * \u2225 2 + \ud835\udc50 1 \ud835\udc5b \ud835\udc5a \ud835\udc3f \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u2225\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \ud835\udc61 \u2212 \ud835\udc65 * \u2225 2 + \ud835\udc50 1 \ud835\udc5b \ud835\udc5a \ud835\udc3f \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u2225\ud835\udc65 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udc65 * \u2225 2 .\n(37) The above equality has used the fact that  * is the optimal point which satisfies the first-order condition, i.e., 1 /  =1 \u2207  ( * ) = 0. Moreover, the above inequalities include the terms E\u2225 , , \u2212 * \u2225 2 and E\u2225 , , \u2212  * \u2225 2 , directly evaluating them is extremely difficult since they are randomly selected and updated. Note that both the gradient variance and second-order moment are bounded, it implies from (32) that:\u2225\ud835\udc66 \u2212 \ud835\udc65 \u2225 \u2264 (\ud835\udeff \ud835\udc53 \u2212 \ud835\udf0e \ud835\udc53 ) \u2022 2 /\ud835\udf07 := M \ud835\udc63\nfor any  and  in the domain of  . Therefore, we use (38) for bounding E\u2225 , , \u2212 * \u2225 2 and E\u2225 , , \u2212 * \u2225 2 . Moreover, to bound the last term E\u2225   \u2212  * \u2225 2 , the one iteration progress for the client  can be expanded as follows:E\u2225\ud835\udc65 \ud835\udc61 +1 \ud835\udc56 \u2212 \ud835\udc65 * \u2225 2 \u2264 E\u2225\ud835\udc65 \ud835\udc61 +1 \ud835\udc56 \u2212 \ud835\udc65 \ud835\udc61 \ud835\udc56 \u2225 2 + E\u2225\ud835\udc65 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udc65 * \u2225 2 + 2E\u27e8\ud835\udc65 \ud835\udc61 +1 \ud835\udc56 \u2212 \ud835\udc65 \ud835\udc61 \ud835\udc56 , \ud835\udc65 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udc65 * \u27e9,\nwhich has the same form with (34). We omit the duplicated procedure and use the inequality E\u2225   \u2212  * \u2225 2 \u2264 M  . Hence, by combining (35) and (38), the following result can be derived:E \ud835\udc65 \ud835\udc5f +1 \u2212 \ud835\udc65 \ud835\udc5f 2 \u2264 3\ud835\udf02 2 \ud835\udc46 2 \ud835\udc47 2 \ud835\udc5b \ud835\udc5a \ud835\udc3fM \ud835\udc63 (|D | + 2\ud835\udc5b \ud835\udc5a \ud835\udc41 ) \ud835\udc41 3 . ()\nWe next evaluate the term 2E\u27e8\nThen, we evaluate (41) term by term for convenience. For the first and third terms, they can bounded by applying (25), while the second term can be bounded by utilizing (24). Therefore, it yields the inequalities term by term in the following:\nThe evaluation of the first term ( 3 :=  / ):\u22122\ud835\udf02\ud835\udc46 \ud835\udc41 3 \u2211\ufe01 \ud835\udc56,\ud835\udc61,\ud835\udc5b,\ud835\udc57 E\u27e8\u2207\ud835\udc53 \ud835\udc5b,\ud835\udc57 (\ud835\udc67 \ud835\udc56,\ud835\udc61 \ud835\udc5b,\ud835\udc57 ), \ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u27e9 \u2264 \u22122\ud835\udc50 3 E{\ud835\udc53 (\ud835\udc65 \ud835\udc5f ) \u2212 \ud835\udc53 (\ud835\udc65 * )} \u2212 \ud835\udc50 3 |D |\ud835\udf07 \ud835\udc41 (1 + \ud835\udefc) E\u2225\ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u2225 2 + \ud835\udc3f + \ud835\udf07 \ud835\udefc \ud835\udc50 3 \ud835\udc41 2 \ud835\udc47 \u2211\ufe01 \ud835\udc56,\ud835\udc61,\ud835\udc5b,\ud835\udc57 E\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc65 \ud835\udc5f \u2225 2 ;\nThe evaluation of the second term:2\ud835\udf02\ud835\udc46 \ud835\udc41 2 \u2211\ufe01 \ud835\udc56,\ud835\udc61,\ud835\udc57 E\u27e8\u2207\ud835\udc53 \ud835\udc56,\ud835\udc57 (\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 ), \ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u27e9 \u22642\ud835\udc50 2 E{\ud835\udc53 (\ud835\udc65 \ud835\udc5f ) \u2212 \ud835\udc53 (\ud835\udc65 * )} + 2\ud835\udc50 3 |D | \ud835\udc41 (\ud835\udc3f + \ud835\udefd\ud835\udc3f \u2212 \ud835\udf07)E\u2225\ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u2225 2 + 2\ud835\udc50 3 (1 + \ud835\udefd \u22121 ) \ud835\udc41 \u2211\ufe01 \ud835\udc56,\ud835\udc61,\ud835\udc57 E\u2225\ud835\udc67 \ud835\udc61 \ud835\udc56,\ud835\udc57 \u2212 \ud835\udc65 \ud835\udc5f \u2225 2 . ()\nThe evaluation of the third term:\u22122\ud835\udf02\ud835\udc46 \ud835\udc41 2 \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u27e8\u2207\ud835\udc53 \ud835\udc56 (\ud835\udc65 \ud835\udc61 \ud835\udc56 ), \ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u27e9 \u2264 \u22122\ud835\udc50 3 E{\ud835\udc53 (\ud835\udc65 \ud835\udc5f ) \u2212 \ud835\udc53 (\ud835\udc65 * )} \u2212 \ud835\udc50 3 \ud835\udf07 1 + \ud835\udefc E\u2225\ud835\udc65 \ud835\udc5f \u2212 \ud835\udc65 * \u2225 2 + \ud835\udc3f + \ud835\udf07 \ud835\udefc \ud835\udc50 3 \ud835\udc41\ud835\udc47 \u2022 \u2211\ufe01 \ud835\udc56,\ud835\udc61 E\u2225\ud835\udc65 \ud835\udc61 \ud835\udc56 \u2212 \ud835\udc65 \ud835\udc5f \u2225 2 .\nConsequently, if  is sufficiently large, we assume all the locally stored variables { , } in client  are expected to be participated for the update at least once, and we assume the update times is  for all locally stored parameters { , }. Moreover, we denote  \u2032 as the latest update iteration for evaluating E\u2225 , , \u2212  * \u2225 2 ,  \u2032\u2032 as the second latest update, and so on. First of all, the inequalities (42)-( 44) can be rewritten with the relaxed triangle inequality and listed as follows:\nThe evaluation of the first term ( 4 :=  / ):\u22122\ud835\udf02\ud835\udc46 \ud835\udc41 3 \u2211\ufe01 \ud835\udc56,\nwhere the first inequality has used the relaxed triangle inequality. Moreover, recall \u2225 \u2212  \u2225 2 \u2264 M  for all  and  in the domain of  , . Hence, we haveE\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \u2032 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc67 \ud835\udc56,\ud835\udc61 \u2032\u2032 \ud835\udc5b,\ud835\udc57 \u2225 2 \u2264 9\ud835\udf02 2 \ud835\udc5b 2 \ud835\udc5a \ud835\udc3fM \ud835\udc63 .\nNext, we evaluate the second term 2E\u27e8 (55)\nSubstituting the above results into (48), we can obtain the following:E\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \u2032 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc65 * \u2225 2 \u2264(1 \u2212 \ud835\udf02\ud835\udf08)E\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \u2032\u2032 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc65 * \u2225 2 + \ud835\udf02\ud835\udf06 + 9\ud835\udf02 2 \ud835\udc5b 2 \ud835\udc5a \ud835\udc3fM \ud835\udc63 ,\nwhere  and  are provided respectively as\ud835\udf08 = \ud835\udf07 (|D | + \ud835\udc41 \ud835\udc5b \ud835\udc5a ) \ud835\udc41 (1 + \ud835\udefc) \u2212 2\ud835\udc5b \ud835\udc5a\ud835\udc56\ud835\udc5b \ud835\udc3f + \ud835\udefd\ud835\udc3f \u2212 \ud835\udf07\nand (57)\ud835\udf06 = \ud835\udc5b \ud835\udc5a + |D | \ud835\udc41 \ud835\udc3f + \ud835\udf07 \ud835\udefc M \ud835\udc63 + 2\ud835\udc5b \ud835\udc5a (1 + \ud835\udefd \u22121 )\ud835\udc3fM \ud835\udc63 .\nRecall that we have assumed the update time for  is , hence we further have the following inequalityE\u2225\ud835\udc67 \ud835\udc56,\ud835\udc61 \u2032 \ud835\udc5b,\ud835\udc57 \u2212 \ud835\udc65 * \u2225 2 \u2264 (1 \u2212 \ud835\udf02\ud835\udf08) \ud835\udf0f M \ud835\udc63 + \ud835\udf06 + 9\ud835\udf02\ud835\udc5b 2 \ud835\udc5a \ud835\udc3fM \ud835\udc63 \ud835\udc63 \u2212 \ud835\udf06 + 9\ud835\udf02\ud835\udc5b 2 \ud835\udc5a \ud835\udc3fM \ud835\udc63 \ud835\udc63 \u2264 \ud835\udf02 (\ud835\udf0f\ud835\udf08M \ud835\udc63 + \ud835\udf0f\ud835\udf06) + 9\ud835\udf02 2 \ud835\udc5b 2 \ud835\udc5a \ud835\udc3fM \ud835\udc63\nholds when 46), (47), (48), (59), (60) substituting into (34), the following one round progress can be derived:\nIn fact, if  > 1+ 1\u2212 , (67) may be satisfied. For the convergence study, we assume all these conditions are satisfied for simplicity.\n\nC PROOF OF THEOREM 3\nProof. according the inequality in Lemma 2, we have the following,\ud835\udc5d \ud835\udc5f +1 \u2264 \u22122\ud835\udf02\ud835\udc46\ud835\udc47 \ud835\udc41 \u03a6 \ud835\udc5f + (1 \u2212 \ud835\udf02\u210e)\ud835\udc5d \ud835\udc5f + \ud835\udf06 \u2032 \ud835\udf02 2 + \ud835\udf08 \u2032 \ud835\udf02 3 ,\nwhere we define \u03a6  := E{ (  ) \u2212  ( * )}. Hence, we construct a positive sequence {  } defined by   = (1 \u2212 \u210e 2 ) \u2212 , and it follows:1 \ud835\udc4a \ud835\udc45 \u2211\ufe01 \ud835\udc45 \ud835\udc5f =0 \ud835\udc64 \ud835\udc5f \u03a6 \ud835\udc5f \u2264 \ud835\udc41 \ud835\udc5d 0 2\ud835\udc46\ud835\udc47 \ud835\udf02\ud835\udc4a \ud835\udc45 + \ud835\udc41 \ud835\udf06 \u2032 \ud835\udf02 2\ud835\udc46\ud835\udc47 ,\nwhere we have defined   :=   =0   . Note when  \u2265 1 /3\u210e, (1 \u2212 \u210e) +1 \u2264 exp{\u2212\u210e} \u2264 exp{\u2212 1 /3} \u2264 3 /4, it leads to   = 1 /4\u210e(1 \u2212 \u210e) \u2212 . Substituting the results into (69), the weighted one round progress can be further simplifies as follows:1 \ud835\udc4a \ud835\udc45 \u2211\ufe01 \ud835\udc45 \ud835\udc5f =0 \ud835\udc64 \ud835\udc5f \u03a6 \ud835\udc5f \u2264 2\u210e\ud835\udc41 \ud835\udc5d 0 \ud835\udc46\ud835\udc47 exp{\u2212\ud835\udf02\u210e\ud835\udc45} + \ud835\udc41 \ud835\udf06 \u2032 \ud835\udf02 2\ud835\udc46\ud835\udc47 .\nTherefore, we discuss the following two cases for the choice of the step size :\n\u2022 if 1 /3\u210e \u2264 \u03b7 \u2264 log{max(1,4\u210e 2  0 /) } /\u210e, then we select the step size to be  = \u03b7 and it follows1 \ud835\udc4a \ud835\udc45 \u2211\ufe01 \ud835\udc45 \ud835\udc5f =0 \ud835\udc64 \ud835\udc5f \u03a6 \ud835\udc5f \u2264 2\u210e\ud835\udc41 \ud835\udc5d 0 \ud835\udc46\ud835\udc47 exp{\u2212 \u03b7\u210e\ud835\udc45} + O \ud835\udc41 \ud835\udf06 \u2032 2\ud835\udc46\ud835\udc47\u210e\ud835\udc45 ,\n\u2022 if \u03b7 > log{max(1,4\u210e 2  0 /) } /\u210e, then we select the step size to be  = log{max(1,4\u210e 2  0 /) } /\u210e, and 1 /    =0   \u03a6  \u2264 O (   \u2032 2 \u210e ). Thus, the desired results are obtained and this completes the proof of Theorem 3. \u25a1\n\nD PROOF OF LEMMA 1\nProof. On the communication round  , recall we have used\nwhere  \u2208 R \u00d7 is the consensus variable,   \u2208 R \u00d7 can be viewed as the local model. Obviously, (75) and (74) are equivalent. Then, the augmented Lagrangian function can be further derived [5] :L (\ud835\udc4b\nwhere   \u2208 R \u00d7 is the Lagrangian multiplier, \u27e8, \u27e9 denotes the matrix inner product,  > 0 is the regularization parameter and \u03a0 = {  ,  = 1, . . . ,  }. Subsequently, classical ADMM [5] solves the problem (75) via the following iterative procedure at the th iteration:\ud835\udc4b\n\nFootnotes:\nACM Trans. Knowl. Discov. Data., Vol. 1, No. 1, Article . Publication date: February 2023.\n1: We assume \ud835\udc47 is sufficiently large such that the locally stored {\ud835\udc67 \ud835\udc56,\ud835\udc57 } is updated at least once.\n2: To be more exact, \ud835\udc57 should be \ud835\udc57 \ud835\udc61 \u2032 , for notational simplicity, we use \ud835\udc57 since it is nontrivial in our derivation. ACM Trans. Knowl. Discov. Data., Vol. 1, No. 1, Article . Publication date: February 2023.\n\nReferences:\n\n- Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric Renggli. 2018. The Convergence of Sparsified Gradient Methods. In Advances in Neural Information Processing Systems 31. 5973-5983.- RG Andrzejak, K Lehnertz, C Rieke, F Mormann, P David, and CE Elger. 2001. Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state. Phys. Rev. E. (2001).\n\n- Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge L. Reyes-Ortiz. 2013. A Public Domain Dataset for Human Activity Recognition Using Smartphones.. In 21th ESANN,Bruges, Belgium.\n\n- L\u00e9on Bottou. 2010. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010. Springer, 177-186.\n\n- Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning 3 (01 2011), 1-122.\n\n- Theodora S. Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch. Paschalidis, and Wei Shi. 2018. Federated learning of predictive models from federated Electronic Health Records. International Journal of Medical Informatics 112 (2018), 59-67.\n\n- A. Defazio, F. Bach, and S. Julien. 2014. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. Advances in Neural Information Processing Systems 27 (2014), 1646-1654.\n\n- Robert M Gower, Mark Schmidt, Francis Bach, and Peter Richtarik. 2020. Variance-reduced methods for machine learning. Technical Report. arXiv preprint arXiv:2010.00892.\n\n- Li Huang, Andrew L. Shea, Huining Qian, Aditya Masurkar, Hao Deng, and Dianbo Liu. 2019. Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records. Journal of Biomedical Informatics (2019), 103-291.\n\n- Rie Johnson and Tong Zhang. 2013. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. NIPS (2013), 315-323.\n\n- Peter Kairouz, H. Brendan McMahan, and Brendan Avent etc. 2019. Advances and Open Problems in Federated Learning. CoRR abs/1712.07557 (2019). arXiv:1712.07557 https://arxiv.org/abs/1912.04977\n\n- Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. 2020. Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning. arXiv:2008.03606 [cs.LG]\n\n- Sai Praneeth Reddy Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Jakkam Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In ICML.\n\n- A. Khaled, Mishchenko A., K. Mishchenko, and P. and Richt\u00e1rik. 2020. Tighter theory for local SGD on indentical and heterogeneous data. In In AISTATS.\n\n- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations.\n\n- Jakub Konecn\u00fd, H. Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. 2016. Federated Optimization: Distributed Machine Learning for On-Device Intelligence. CoRR abs/1610.02527 (2016). arXiv:1610.02527 http://arxiv.org/abs/ 1610.02527\n\n- Jakub Konecn\u00fd, H. Brendan McMahan, Felix X. Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. 2016. Federated Learning: Strategies for Improving Communication Efficiency. CoRR abs/1610.05492 (2016). arXiv:1610.05492 http://arxiv.org/abs/1610.05492\n\n- Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, Vol. 86. 2278-2324.\n\n- Mu Li, David G. Anderson, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. 2014. Scaling Distributed Machine Learning with the Parameter Server. In OSDI.\n\n- Tian Li, Anit Kumar Sahu, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith. 2020. Federated Optimization in Heterogeneous Networks. Proceedings of the 3rd MLSys Conference, Austin, TX, USA.\n\n- Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2020. On the Convergence of FedAvg on Non-IID Data. In ICLR.\n\n- Zhize Li and Peter Richtarik. 2021. A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization. arXiv:2102.01375 [cs.LG]\n\n- X. Liang, S. Shen, J. Liu, Z. Pan, E. Chen, and Y. Cheng. 2019. Variance Reduced Local SGD with Lower Communication Complexity. (2019). https://arxiv.org/abs/1912.12844\n\n- Wei Liu, Li Chen, Yunfei Chen, and Wenyi Zhang. 2020. Accelerating Federated Learning via Momentum Gradient Descent. IEEE Transactions on Parallel and Distributed Systems 31, 8 (2020), 1754-1766.\n\n- Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication- Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th AISTATS.\n\n- Sheller MJ, Reina GA, Edwards B, Martin J, and Bakas S. 2019. Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation. Brainlesion (2019), 92-104.\n\n- Sahand Negahban and Martin J. Wainwright. 2010. Estimation of (near) low-rank matrices with noise and high- dimensional scaling. In ICML.\n\n- Neal Parikh and Stephen P. Boyd. 2014. Proximal Algorithms. Found. Trends Optim. 1, 3 (2014), 127-239.\n\n- Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u00fd, Sanjiv Kumar, and H. Brendan McMahan. 2020. Adaptive Federated Optimization. arXiv:2003.00295 [cs.LG]\n\n- Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. 2020. Fed- PAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization (Proceedings of Machine Learning Research, Vol. 108). PMLR, Online, 2021-2031.\n\n- Herbert Robbins and Sutton Monro. 1951. A Stochastic Approximation Method. Ann. Math. Statist. 22, 3 (1951), 400-407.\n\n- Tibshirani Robert. 1996. Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society 58, 1 (1996), 267-288.\n\n- M. Schmidt, N. Roux, J. Liu, and F. Bach. 2016. Minimizing finite sums with the stochastic average gradient. Mathematical programming 162, 1-2 (2016), 83-112.\n\n- Zhou Shenglong and Li Geoffrey Ye. 2022. Federated Learning via Inexact ADMM. arXiv:2204.10607 [math.OC]\n\n- Sebastian U. Stich. 2019. Local SGD Converges Fast and Communicates Little. In ICLR.\n\n- Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. Sparsified SGD with Memory. In NIPS.\n\n- Leye Wang, Han Yu, and Xiao Han. 2020. Federated Crowdsensing: Framework and Challenges. arXiv:2011.03208 [cs.CR]\n\n- Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin Chan. 2019. Adaptive Federated Learning in Resource Constrained Edge Computing Systems. IEEE journal on selected areas in communications 37, 6 (2019), 1205-1221.\n\n- Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. 2021. FedCM: Federated Learning with Client-level Momentum. arXiv:2106.10874 [cs.LG]\n\n- Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine Learning: Concept and Applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1-19.\n\n- H. Yu, S. Shen, S. Yang, and S. Zhu. 2019. Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning. In AAAI.\n\n- Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. 2013. Communication-Efficient Algorithms for Statistical Optimization. Journal of Machine Learning Research 14, 68 (2013), 3321-3363.\n\n- Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. Federated Learning with Non-IID Data. CoRR abs/1806.00582 (2018). http://arxiv.org/abs/1806.00582\n\n- Zhaohua Zheng, Yize Zhou, Yilong Sun, Zhang Wang, Boyi Liu, and Keqiu Li. 2021. Federated Learning in Smart Cities: A Comprehensive Survey. arXiv:2102.01375 [cs.LG]\n\n- Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep Leakage from Gradients. In Advances in Neural Information Processing Systems 32. 14774-14784.\n\n", "annotations": {"ReferenceToTable": [{"begin": 11422, "end": 11423, "target": "#tab_0", "idx": 0}, {"begin": 24871, "end": 24872, "idx": 1}, {"begin": 34226, "end": 34227, "idx": 2}, {"begin": 34229, "end": 34230, "idx": 3}, {"begin": 34663, "end": 34664, "idx": 4}, {"begin": 35399, "end": 35400, "idx": 5}], "ReferenceToFootnote": [{"begin": 22286, "end": 22287, "target": "#foot_1", "idx": 0}, {"begin": 48340, "end": 48341, "target": "#foot_2", "idx": 1}], "SectionMain": [{"begin": 1355, "end": 50902, "idx": 0}], "ReferenceToFormula": [{"begin": 16676, "end": 16678, "target": "#formula_16", "idx": 0}, {"begin": 18256, "end": 18258, "target": "#formula_13", "idx": 1}, {"begin": 44380, "end": 44382, "target": "#formula_43", "idx": 2}, {"begin": 48106, "end": 48108, "target": "#formula_64", "idx": 3}, {"begin": 49029, "end": 49031, "idx": 4}], "SectionReference": [{"begin": 51318, "end": 59674, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1355, "idx": 0}], "Div": [{"begin": 98, "end": 1347, "idx": 0}, {"begin": 1358, "end": 7129, "idx": 1}, {"begin": 7131, "end": 9419, "idx": 2}, {"begin": 9421, "end": 11424, "idx": 3}, {"begin": 11426, "end": 12517, "idx": 4}, {"begin": 12519, "end": 12662, "idx": 5}, {"begin": 12664, "end": 13496, "idx": 6}, {"begin": 13498, "end": 15961, "idx": 7}, {"begin": 15963, "end": 16740, "idx": 8}, {"begin": 16742, "end": 16833, "idx": 9}, {"begin": 16835, "end": 16983, "idx": 10}, {"begin": 16985, "end": 17806, "idx": 11}, {"begin": 17808, "end": 18530, "idx": 12}, {"begin": 18532, "end": 19453, "idx": 13}, {"begin": 19455, "end": 21029, "idx": 14}, {"begin": 21031, "end": 23589, "idx": 15}, {"begin": 23591, "end": 25700, "idx": 16}, {"begin": 25702, "end": 25719, "idx": 17}, {"begin": 25721, "end": 27057, "idx": 18}, {"begin": 27059, "end": 28245, "idx": 19}, {"begin": 28247, "end": 28271, "idx": 20}, {"begin": 28273, "end": 30052, "idx": 21}, {"begin": 30054, "end": 33401, "idx": 22}, {"begin": 33403, "end": 34997, "idx": 23}, {"begin": 34999, "end": 35057, "idx": 24}, {"begin": 35059, "end": 35095, "idx": 25}, {"begin": 35097, "end": 36392, "idx": 26}, {"begin": 36394, "end": 38659, "idx": 27}, {"begin": 38661, "end": 41620, "idx": 28}, {"begin": 41622, "end": 42651, "idx": 29}, {"begin": 42653, "end": 44475, "idx": 30}, {"begin": 44477, "end": 49263, "idx": 31}, {"begin": 49265, "end": 50361, "idx": 32}, {"begin": 50363, "end": 50902, "idx": 33}], "Head": [{"begin": 1358, "end": 1372, "n": "1", "idx": 0}, {"begin": 7131, "end": 7158, "n": "2.1", "idx": 1}, {"begin": 9421, "end": 9456, "n": "2.2", "idx": 2}, {"begin": 11426, "end": 11443, "n": "3", "idx": 3}, {"begin": 12519, "end": 12521, "idx": 4}, {"begin": 12664, "end": 12667, "idx": 5}, {"begin": 13498, "end": 13534, "n": "3.2", "idx": 6}, {"begin": 15963, "end": 15999, "n": "3.3", "idx": 7}, {"begin": 16742, "end": 16744, "idx": 8}, {"begin": 16835, "end": 16837, "idx": 9}, {"begin": 16985, "end": 16988, "idx": 10}, {"begin": 17808, "end": 17830, "n": "4", "idx": 11}, {"begin": 18532, "end": 18561, "n": "4.1", "idx": 12}, {"begin": 19455, "end": 19498, "n": "4.2", "idx": 13}, {"begin": 21031, "end": 21053, "n": "4.3", "idx": 14}, {"begin": 23591, "end": 23620, "n": "4.4", "idx": 15}, {"begin": 25702, "end": 25718, "idx": 16}, {"begin": 25721, "end": 25727, "idx": 17}, {"begin": 27059, "end": 27082, "n": "5.1.4", "idx": 18}, {"begin": 28247, "end": 28270, "n": "5.2", "idx": 19}, {"begin": 28273, "end": 28298, "n": "5.2.1", "idx": 20}, {"begin": 30054, "end": 30082, "n": "5.2.2", "idx": 21}, {"begin": 33403, "end": 33421, "n": "5.3", "idx": 22}, {"begin": 34999, "end": 35056, "n": "5.3.1", "idx": 23}, {"begin": 35059, "end": 35094, "n": "5.3.2", "idx": 24}, {"begin": 35097, "end": 35116, "n": "5.3.3", "idx": 25}, {"begin": 36394, "end": 36417, "n": "5.4", "idx": 26}, {"begin": 38661, "end": 38691, "n": "5.5", "idx": 27}, {"begin": 41622, "end": 41634, "n": "6", "idx": 28}, {"begin": 42653, "end": 42668, "idx": 29}, {"begin": 44477, "end": 44495, "idx": 30}, {"begin": 49265, "end": 49285, "idx": 31}, {"begin": 50363, "end": 50381, "idx": 32}], "Paragraph": [{"begin": 98, "end": 1347, "idx": 0}, {"begin": 1373, "end": 1580, "idx": 1}, {"begin": 1581, "end": 1966, "idx": 2}, {"begin": 1967, "end": 3143, "idx": 3}, {"begin": 3144, "end": 3821, "idx": 4}, {"begin": 3822, "end": 4562, "idx": 5}, {"begin": 4563, "end": 5017, "idx": 6}, {"begin": 5018, "end": 5732, "idx": 7}, {"begin": 5733, "end": 7114, "idx": 8}, {"begin": 7115, "end": 7129, "idx": 9}, {"begin": 7159, "end": 7249, "idx": 10}, {"begin": 7285, "end": 7549, "idx": 11}, {"begin": 7577, "end": 7830, "idx": 12}, {"begin": 7831, "end": 8265, "idx": 13}, {"begin": 8343, "end": 8391, "idx": 14}, {"begin": 8453, "end": 8612, "idx": 15}, {"begin": 8666, "end": 8712, "idx": 16}, {"begin": 8713, "end": 8716, "idx": 17}, {"begin": 8744, "end": 8779, "idx": 18}, {"begin": 8828, "end": 8850, "idx": 19}, {"begin": 9050, "end": 9215, "idx": 20}, {"begin": 9216, "end": 9419, "idx": 21}, {"begin": 9457, "end": 10591, "idx": 22}, {"begin": 10592, "end": 11424, "idx": 23}, {"begin": 11444, "end": 12098, "idx": 24}, {"begin": 12137, "end": 12308, "idx": 25}, {"begin": 12458, "end": 12517, "idx": 26}, {"begin": 12522, "end": 12572, "idx": 27}, {"begin": 12575, "end": 12662, "idx": 28}, {"begin": 12668, "end": 13496, "idx": 29}, {"begin": 13535, "end": 13838, "idx": 30}, {"begin": 13910, "end": 14150, "idx": 31}, {"begin": 14151, "end": 14908, "idx": 32}, {"begin": 15054, "end": 15200, "idx": 33}, {"begin": 15254, "end": 15310, "idx": 34}, {"begin": 15311, "end": 15427, "idx": 35}, {"begin": 15487, "end": 15961, "idx": 36}, {"begin": 16000, "end": 16387, "idx": 37}, {"begin": 16427, "end": 16498, "idx": 38}, {"begin": 16602, "end": 16654, "idx": 39}, {"begin": 16655, "end": 16657, "idx": 40}, {"begin": 16658, "end": 16680, "idx": 41}, {"begin": 16681, "end": 16683, "idx": 42}, {"begin": 16684, "end": 16740, "idx": 43}, {"begin": 16745, "end": 16795, "idx": 44}, {"begin": 16798, "end": 16833, "idx": 45}, {"begin": 16838, "end": 16862, "idx": 46}, {"begin": 16863, "end": 16887, "idx": 47}, {"begin": 16888, "end": 16891, "idx": 48}, {"begin": 16892, "end": 16931, "idx": 49}, {"begin": 16932, "end": 16935, "idx": 50}, {"begin": 16936, "end": 16947, "idx": 51}, {"begin": 16948, "end": 16983, "idx": 52}, {"begin": 16989, "end": 17191, "idx": 53}, {"begin": 17252, "end": 17446, "idx": 54}, {"begin": 17503, "end": 17806, "idx": 55}, {"begin": 17831, "end": 18530, "idx": 56}, {"begin": 18562, "end": 18747, "idx": 57}, {"begin": 18748, "end": 19102, "idx": 58}, {"begin": 19103, "end": 19290, "idx": 59}, {"begin": 19350, "end": 19453, "idx": 60}, {"begin": 19499, "end": 19846, "idx": 61}, {"begin": 19847, "end": 19966, "idx": 62}, {"begin": 20020, "end": 20161, "idx": 63}, {"begin": 20162, "end": 21029, "idx": 64}, {"begin": 21054, "end": 21439, "idx": 65}, {"begin": 21463, "end": 21804, "idx": 66}, {"begin": 21805, "end": 21902, "idx": 67}, {"begin": 21951, "end": 22349, "idx": 68}, {"begin": 22459, "end": 22732, "idx": 69}, {"begin": 22733, "end": 22806, "idx": 70}, {"begin": 22807, "end": 22983, "idx": 71}, {"begin": 23050, "end": 23092, "idx": 72}, {"begin": 23093, "end": 23589, "idx": 73}, {"begin": 23621, "end": 24134, "idx": 74}, {"begin": 24135, "end": 24864, "idx": 75}, {"begin": 24865, "end": 25700, "idx": 76}, {"begin": 25863, "end": 25950, "idx": 77}, {"begin": 25951, "end": 26360, "idx": 78}, {"begin": 26361, "end": 27057, "idx": 79}, {"begin": 27083, "end": 28245, "idx": 80}, {"begin": 28299, "end": 30052, "idx": 81}, {"begin": 30083, "end": 30753, "idx": 82}, {"begin": 30754, "end": 32403, "idx": 83}, {"begin": 32404, "end": 33154, "idx": 84}, {"begin": 33155, "end": 33401, "idx": 85}, {"begin": 33422, "end": 34997, "idx": 86}, {"begin": 35117, "end": 36392, "idx": 87}, {"begin": 36418, "end": 37981, "idx": 88}, {"begin": 37982, "end": 38659, "idx": 89}, {"begin": 38692, "end": 38859, "idx": 90}, {"begin": 38901, "end": 39528, "idx": 91}, {"begin": 39586, "end": 39914, "idx": 92}, {"begin": 39915, "end": 40049, "idx": 93}, {"begin": 40114, "end": 40791, "idx": 94}, {"begin": 40792, "end": 41620, "idx": 95}, {"begin": 41635, "end": 42651, "idx": 96}, {"begin": 42669, "end": 43031, "idx": 97}, {"begin": 43128, "end": 43139, "idx": 98}, {"begin": 43140, "end": 43277, "idx": 99}, {"begin": 43368, "end": 43380, "idx": 100}, {"begin": 43471, "end": 43491, "idx": 101}, {"begin": 43492, "end": 43787, "idx": 102}, {"begin": 43853, "end": 43947, "idx": 103}, {"begin": 44006, "end": 44165, "idx": 104}, {"begin": 44236, "end": 44309, "idx": 105}, {"begin": 44365, "end": 44475, "idx": 106}, {"begin": 44496, "end": 44657, "idx": 107}, {"begin": 44745, "end": 44806, "idx": 108}, {"begin": 44906, "end": 44973, "idx": 109}, {"begin": 45064, "end": 45183, "idx": 110}, {"begin": 45473, "end": 45497, "idx": 111}, {"begin": 45499, "end": 45623, "idx": 112}, {"begin": 45763, "end": 46187, "idx": 113}, {"begin": 46225, "end": 46450, "idx": 114}, {"begin": 46555, "end": 46738, "idx": 115}, {"begin": 46805, "end": 46834, "idx": 116}, {"begin": 46835, "end": 47078, "idx": 117}, {"begin": 47079, "end": 47124, "idx": 118}, {"begin": 47303, "end": 47337, "idx": 119}, {"begin": 47513, "end": 47546, "idx": 120}, {"begin": 47698, "end": 48186, "idx": 121}, {"begin": 48187, "end": 48232, "idx": 122}, {"begin": 48247, "end": 48397, "idx": 123}, {"begin": 48450, "end": 48492, "idx": 124}, {"begin": 48493, "end": 48563, "idx": 125}, {"begin": 48646, "end": 48686, "idx": 126}, {"begin": 48738, "end": 48746, "idx": 127}, {"begin": 48800, "end": 48900, "idx": 128}, {"begin": 49018, "end": 49129, "idx": 129}, {"begin": 49130, "end": 49263, "idx": 130}, {"begin": 49286, "end": 49352, "idx": 131}, {"begin": 49409, "end": 49540, "idx": 132}, {"begin": 49597, "end": 49834, "idx": 133}, {"begin": 49897, "end": 49976, "idx": 134}, {"begin": 49977, "end": 50075, "idx": 135}, {"begin": 50141, "end": 50361, "idx": 136}, {"begin": 50382, "end": 50438, "idx": 137}, {"begin": 50439, "end": 50630, "idx": 138}, {"begin": 50635, "end": 50901, "idx": 139}], "ReferenceToBib": [{"begin": 1559, "end": 1563, "target": "#b10", "idx": 0}, {"begin": 1564, "end": 1567, "target": "#b15", "idx": 1}, {"begin": 1568, "end": 1571, "target": "#b16", "idx": 2}, {"begin": 1572, "end": 1575, "target": "#b24", "idx": 3}, {"begin": 1576, "end": 1579, "target": "#b39", "idx": 4}, {"begin": 1788, "end": 1791, "target": "#b5", "idx": 5}, {"begin": 1792, "end": 1794, "target": "#b8", "idx": 6}, {"begin": 1795, "end": 1798, "target": "#b25", "idx": 7}, {"begin": 1827, "end": 1831, "target": "#b36", "idx": 8}, {"begin": 1832, "end": 1835, "target": "#b43", "idx": 9}, {"begin": 1949, "end": 1953, "target": "#b16", "idx": 10}, {"begin": 1954, "end": 1957, "target": "#b19", "idx": 11}, {"begin": 1958, "end": 1961, "target": "#b20", "idx": 12}, {"begin": 1962, "end": 1965, "target": "#b24", "idx": 13}, {"begin": 2028, "end": 2032, "target": "#b18", "idx": 14}, {"begin": 2437, "end": 2441, "target": "#b10", "idx": 15}, {"begin": 2442, "end": 2445, "target": "#b12", "idx": 16}, {"begin": 2446, "end": 2449, "target": "#b13", "idx": 17}, {"begin": 2450, "end": 2453, "target": "#b19", "idx": 18}, {"begin": 2454, "end": 2457, "target": "#b20", "idx": 19}, {"begin": 2458, "end": 2461, "target": "#b42", "idx": 20}, {"begin": 2522, "end": 2526, "target": "#b12", "idx": 21}, {"begin": 2527, "end": 2530, "target": "#b20", "idx": 22}, {"begin": 2855, "end": 2859, "target": "#b12", "idx": 23}, {"begin": 2860, "end": 2863, "target": "#b19", "idx": 24}, {"begin": 2864, "end": 2867, "target": "#b22", "idx": 25}, {"begin": 3138, "end": 3142, "target": "#b20", "idx": 26}, {"begin": 3217, "end": 3221, "target": "#b13", "idx": 27}, {"begin": 3222, "end": 3225, "target": "#b19", "idx": 28}, {"begin": 3226, "end": 3229, "target": "#b20", "idx": 29}, {"begin": 3230, "end": 3233, "target": "#b42", "idx": 30}, {"begin": 3236, "end": 3239, "target": "#b0", "idx": 31}, {"begin": 3486, "end": 3489, "target": "#b1", "idx": 32}, {"begin": 3693, "end": 3697, "target": "#b11", "idx": 33}, {"begin": 3711, "end": 3715, "target": "#b12", "idx": 34}, {"begin": 7443, "end": 7447, "target": "#b30", "idx": 35}, {"begin": 7723, "end": 7726, "target": "#b3", "idx": 36}, {"begin": 7865, "end": 7868, "target": "#b7", "idx": 37}, {"begin": 8038, "end": 8042, "target": "#b9", "idx": 38}, {"begin": 8085, "end": 8089, "target": "#b32", "idx": 39}, {"begin": 8113, "end": 8116, "target": "#b6", "idx": 40}, {"begin": 8542, "end": 8546, "target": "#b32", "idx": 41}, {"begin": 8753, "end": 8757, "target": "#b11", "idx": 42}, {"begin": 9414, "end": 9418, "target": "#b20", "idx": 43}, {"begin": 9460, "end": 9464, "target": "#b29", "idx": 44}, {"begin": 9721, "end": 9725, "target": "#b23", "idx": 45}, {"begin": 9746, "end": 9750, "target": "#b28", "idx": 46}, {"begin": 9769, "end": 9773, "target": "#b14", "idx": 47}, {"begin": 9855, "end": 9859, "target": "#b20", "idx": 48}, {"begin": 9860, "end": 9863, "target": "#b37", "idx": 49}, {"begin": 9864, "end": 9867, "target": "#b40", "idx": 50}, {"begin": 9868, "end": 9871, "target": "#b42", "idx": 51}, {"begin": 9988, "end": 9992, "target": "#b19", "idx": 52}, {"begin": 10135, "end": 10139, "target": "#b22", "idx": 53}, {"begin": 10369, "end": 10373, "target": "#b12", "idx": 54}, {"begin": 10508, "end": 10512, "target": "#b11", "idx": 55}, {"begin": 11976, "end": 11991, "idx": 56}, {"begin": 12636, "end": 12639, "target": "#b8", "idx": 57}, {"begin": 12757, "end": 12760, "target": "#b6", "idx": 58}, {"begin": 13063, "end": 13067, "target": "#b20", "idx": 59}, {"begin": 13962, "end": 13965, "target": "#b7", "idx": 60}, {"begin": 14404, "end": 14408, "target": "#b9", "idx": 61}, {"begin": 16126, "end": 16130, "target": "#b27", "idx": 62}, {"begin": 16926, "end": 16930, "target": "#b10", "idx": 63}, {"begin": 17112, "end": 17116, "target": "#b31", "idx": 64}, {"begin": 17414, "end": 17418, "target": "#b14", "idx": 65}, {"begin": 17518, "end": 17522, "target": "#b15", "idx": 66}, {"begin": 18790, "end": 18794, "target": "#b11", "idx": 67}, {"begin": 18808, "end": 18812, "target": "#b12", "idx": 68}, {"begin": 19629, "end": 19633, "target": "#b44", "idx": 69}, {"begin": 19865, "end": 19869, "target": "#b44", "idx": 70}, {"begin": 20858, "end": 20862, "target": "#b44", "idx": 71}, {"begin": 20882, "end": 20886, "target": "#b11", "idx": 72}, {"begin": 21568, "end": 21572, "target": "#b13", "idx": 73}, {"begin": 21573, "end": 21576, "target": "#b20", "idx": 74}, {"begin": 21577, "end": 21580, "target": "#b34", "idx": 75}, {"begin": 21581, "end": 21584, "target": "#b35", "idx": 76}, {"begin": 21585, "end": 21588, "target": "#b41", "idx": 77}, {"begin": 21593, "end": 21596, "target": "#b0", "idx": 78}, {"begin": 21597, "end": 21600, "target": "#b20", "idx": 79}, {"begin": 21601, "end": 21604, "target": "#b35", "idx": 80}, {"begin": 21605, "end": 21608, "target": "#b41", "idx": 81}, {"begin": 21810, "end": 21814, "target": "#b9", "idx": 82}, {"begin": 22040, "end": 22044, "target": "#b21", "idx": 83}, {"begin": 23410, "end": 23414, "target": "#b12", "idx": 84}, {"begin": 23892, "end": 23896, "target": "#b24", "idx": 85}, {"begin": 23949, "end": 23953, "target": "#b26", "idx": 86}, {"begin": 24250, "end": 24254, "target": "#b17", "idx": 87}, {"begin": 24315, "end": 24318, "target": "#b2", "idx": 88}, {"begin": 24367, "end": 24370, "target": "#b1", "idx": 89}, {"begin": 25522, "end": 25526, "target": "#b24", "idx": 90}, {"begin": 25534, "end": 25538, "target": "#b38", "idx": 91}, {"begin": 25549, "end": 25553, "target": "#b12", "idx": 92}, {"begin": 25563, "end": 25567, "target": "#b33", "idx": 93}, {"begin": 25581, "end": 25585, "target": "#b11", "idx": 94}, {"begin": 25945, "end": 25949, "target": "#b24", "idx": 95}, {"begin": 25957, "end": 25961, "target": "#b38", "idx": 96}, {"begin": 26011, "end": 26015, "target": "#b12", "idx": 97}, {"begin": 26122, "end": 26126, "target": "#b9", "idx": 98}, {"begin": 26162, "end": 26166, "target": "#b11", "idx": 99}, {"begin": 26260, "end": 26264, "target": "#b33", "idx": 100}, {"begin": 26356, "end": 26359, "target": "#b4", "idx": 101}, {"begin": 26742, "end": 26746, "target": "#b33", "idx": 102}, {"begin": 26840, "end": 26844, "target": "#b24", "idx": 103}, {"begin": 26934, "end": 26938, "target": "#b12", "idx": 104}, {"begin": 28069, "end": 28073, "target": "#b33", "idx": 105}, {"begin": 36628, "end": 36632, "target": "#b18", "idx": 106}, {"begin": 39418, "end": 39421, "target": "#b4", "idx": 107}, {"begin": 39422, "end": 39425, "target": "#b33", "idx": 108}, {"begin": 39843, "end": 39847, "target": "#b33", "idx": 109}, {"begin": 40381, "end": 40385, "target": "#b31", "idx": 110}, {"begin": 40386, "end": 40389, "target": "#b15", "idx": 111}, {"begin": 40390, "end": 40392, "target": "#b7", "idx": 112}, {"begin": 40393, "end": 40395, "target": "#b3", "idx": 113}, {"begin": 40396, "end": 40398, "target": "#b1", "idx": 114}, {"begin": 44031, "end": 44035, "target": "#b26", "idx": 115}, {"begin": 44075, "end": 44079, "target": "#b23", "idx": 116}, {"begin": 44110, "end": 44114, "target": "#b24", "idx": 117}, {"begin": 44156, "end": 44160, "target": "#b26", "idx": 118}, {"begin": 44388, "end": 44392, "target": "#b29", "idx": 119}, {"begin": 44430, "end": 44434, "target": "#b24", "idx": 120}, {"begin": 46277, "end": 46281, "target": "#b37", "idx": 121}, {"begin": 46584, "end": 46588, "target": "#b33", "idx": 122}, {"begin": 46687, "end": 46691, "target": "#b34", "idx": 123}, {"begin": 46696, "end": 46700, "target": "#b37", "idx": 124}, {"begin": 46853, "end": 46857, "target": "#b40", "idx": 125}, {"begin": 46948, "end": 46952, "target": "#b24", "idx": 126}, {"begin": 47004, "end": 47008, "target": "#b23", "idx": 127}, {"begin": 49075, "end": 49079, "target": "#b33", "idx": 128}, {"begin": 50625, "end": 50628, "target": "#b4", "idx": 129}, {"begin": 50815, "end": 50818, "target": "#b4", "idx": 130}], "ReferenceString": [{"begin": 51333, "end": 51560, "id": "b0", "idx": 0}, {"begin": 51562, "end": 51824, "id": "b1", "idx": 1}, {"begin": 51828, "end": 52024, "id": "b2", "idx": 2}, {"begin": 52028, "end": 52161, "id": "b3", "idx": 3}, {"begin": 52165, "end": 52411, "id": "b4", "idx": 4}, {"begin": 52415, "end": 52669, "id": "b5", "idx": 5}, {"begin": 52673, "end": 52886, "id": "b6", "idx": 6}, {"begin": 52890, "end": 53058, "id": "b7", "idx": 7}, {"begin": 53062, "end": 53360, "id": "b8", "idx": 8}, {"begin": 53364, "end": 53497, "id": "b9", "idx": 9}, {"begin": 53501, "end": 53692, "id": "b10", "idx": 10}, {"begin": 53696, "end": 53933, "id": "b11", "idx": 11}, {"begin": 53937, "end": 54144, "id": "b12", "idx": 12}, {"begin": 54148, "end": 54298, "id": "b13", "idx": 13}, {"begin": 54302, "end": 54441, "id": "b14", "idx": 14}, {"begin": 54445, "end": 54680, "id": "b15", "idx": 15}, {"begin": 54684, "end": 54942, "id": "b16", "idx": 16}, {"begin": 54946, "end": 55104, "id": "b17", "idx": 17}, {"begin": 55108, "end": 55324, "id": "b18", "idx": 18}, {"begin": 55328, "end": 55535, "id": "b19", "idx": 19}, {"begin": 55539, "end": 55668, "id": "b20", "idx": 20}, {"begin": 55672, "end": 55820, "id": "b21", "idx": 21}, {"begin": 55824, "end": 55992, "id": "b22", "idx": 22}, {"begin": 55996, "end": 56191, "id": "b23", "idx": 23}, {"begin": 56195, "end": 56398, "id": "b24", "idx": 24}, {"begin": 56402, "end": 56613, "id": "b25", "idx": 25}, {"begin": 56617, "end": 56754, "id": "b26", "idx": 26}, {"begin": 56758, "end": 56860, "id": "b27", "idx": 27}, {"begin": 56864, "end": 57056, "id": "b28", "idx": 28}, {"begin": 57060, "end": 57339, "id": "b29", "idx": 29}, {"begin": 57343, "end": 57460, "id": "b30", "idx": 30}, {"begin": 57464, "end": 57602, "id": "b31", "idx": 31}, {"begin": 57606, "end": 57764, "id": "b32", "idx": 32}, {"begin": 57768, "end": 57872, "id": "b33", "idx": 33}, {"begin": 57876, "end": 57960, "id": "b34", "idx": 34}, {"begin": 57964, "end": 58069, "id": "b35", "idx": 35}, {"begin": 58073, "end": 58186, "id": "b36", "idx": 36}, {"begin": 58190, "end": 58451, "id": "b37", "idx": 37}, {"begin": 58455, "end": 58595, "id": "b38", "idx": 38}, {"begin": 58599, "end": 58795, "id": "b39", "idx": 39}, {"begin": 58799, "end": 58979, "id": "b40", "idx": 40}, {"begin": 58983, "end": 59171, "id": "b41", "idx": 41}, {"begin": 59175, "end": 59357, "id": "b42", "idx": 42}, {"begin": 59361, "end": 59525, "id": "b43", "idx": 43}, {"begin": 59529, "end": 59672, "id": "b44", "idx": 44}], "Sentence": [{"begin": 98, "end": 268, "idx": 0}, {"begin": 269, "end": 415, "idx": 1}, {"begin": 416, "end": 531, "idx": 2}, {"begin": 532, "end": 661, "idx": 3}, {"begin": 662, "end": 736, "idx": 4}, {"begin": 737, "end": 801, "idx": 5}, {"begin": 802, "end": 934, "idx": 6}, {"begin": 935, "end": 1045, "idx": 7}, {"begin": 1046, "end": 1231, "idx": 8}, {"begin": 1232, "end": 1347, "idx": 9}, {"begin": 1373, "end": 1580, "idx": 10}, {"begin": 1581, "end": 1687, "idx": 11}, {"begin": 1688, "end": 1836, "idx": 12}, {"begin": 1837, "end": 1966, "idx": 13}, {"begin": 1967, "end": 2303, "idx": 14}, {"begin": 2304, "end": 2462, "idx": 15}, {"begin": 2463, "end": 2531, "idx": 16}, {"begin": 2532, "end": 2716, "idx": 17}, {"begin": 2717, "end": 2769, "idx": 18}, {"begin": 2770, "end": 2951, "idx": 19}, {"begin": 2952, "end": 3143, "idx": 20}, {"begin": 3144, "end": 3240, "idx": 21}, {"begin": 3241, "end": 3270, "idx": 22}, {"begin": 3271, "end": 3387, "idx": 23}, {"begin": 3388, "end": 3490, "idx": 24}, {"begin": 3491, "end": 3528, "idx": 25}, {"begin": 3529, "end": 3615, "idx": 26}, {"begin": 3616, "end": 3716, "idx": 27}, {"begin": 3717, "end": 3821, "idx": 28}, {"begin": 3822, "end": 3913, "idx": 29}, {"begin": 3914, "end": 4065, "idx": 30}, {"begin": 4066, "end": 4175, "idx": 31}, {"begin": 4176, "end": 4304, "idx": 32}, {"begin": 4305, "end": 4481, "idx": 33}, {"begin": 4482, "end": 4562, "idx": 34}, {"begin": 4563, "end": 4615, "idx": 35}, {"begin": 4616, "end": 4826, "idx": 36}, {"begin": 4827, "end": 4918, "idx": 37}, {"begin": 4919, "end": 5017, "idx": 38}, {"begin": 5018, "end": 5195, "idx": 39}, {"begin": 5196, "end": 5279, "idx": 40}, {"begin": 5280, "end": 5374, "idx": 41}, {"begin": 5375, "end": 5578, "idx": 42}, {"begin": 5579, "end": 5732, "idx": 43}, {"begin": 5733, "end": 5858, "idx": 44}, {"begin": 5859, "end": 5998, "idx": 45}, {"begin": 5999, "end": 6069, "idx": 46}, {"begin": 6070, "end": 6179, "idx": 47}, {"begin": 6180, "end": 6409, "idx": 48}, {"begin": 6410, "end": 6550, "idx": 49}, {"begin": 6551, "end": 6625, "idx": 50}, {"begin": 6626, "end": 6793, "idx": 51}, {"begin": 6794, "end": 6820, "idx": 52}, {"begin": 6821, "end": 7009, "idx": 53}, {"begin": 7010, "end": 7114, "idx": 54}, {"begin": 7115, "end": 7129, "idx": 55}, {"begin": 7159, "end": 7249, "idx": 56}, {"begin": 7285, "end": 7377, "idx": 57}, {"begin": 7378, "end": 7549, "idx": 58}, {"begin": 7577, "end": 7601, "idx": 59}, {"begin": 7602, "end": 7727, "idx": 60}, {"begin": 7728, "end": 7830, "idx": 61}, {"begin": 7831, "end": 7960, "idx": 62}, {"begin": 7961, "end": 8117, "idx": 63}, {"begin": 8118, "end": 8265, "idx": 64}, {"begin": 8343, "end": 8391, "idx": 65}, {"begin": 8453, "end": 8495, "idx": 66}, {"begin": 8496, "end": 8612, "idx": 67}, {"begin": 8666, "end": 8712, "idx": 68}, {"begin": 8713, "end": 8716, "idx": 69}, {"begin": 8744, "end": 8779, "idx": 70}, {"begin": 8828, "end": 8850, "idx": 71}, {"begin": 9050, "end": 9097, "idx": 72}, {"begin": 9098, "end": 9215, "idx": 73}, {"begin": 9216, "end": 9419, "idx": 74}, {"begin": 9457, "end": 9560, "idx": 75}, {"begin": 9561, "end": 9726, "idx": 76}, {"begin": 9727, "end": 9784, "idx": 77}, {"begin": 9785, "end": 9872, "idx": 78}, {"begin": 9873, "end": 9993, "idx": 79}, {"begin": 9994, "end": 10140, "idx": 80}, {"begin": 10141, "end": 10225, "idx": 81}, {"begin": 10226, "end": 10374, "idx": 82}, {"begin": 10375, "end": 10455, "idx": 83}, {"begin": 10456, "end": 10591, "idx": 84}, {"begin": 10592, "end": 10733, "idx": 85}, {"begin": 10734, "end": 10871, "idx": 86}, {"begin": 10872, "end": 10964, "idx": 87}, {"begin": 10965, "end": 11164, "idx": 88}, {"begin": 11165, "end": 11290, "idx": 89}, {"begin": 11291, "end": 11358, "idx": 90}, {"begin": 11359, "end": 11424, "idx": 91}, {"begin": 11444, "end": 11520, "idx": 92}, {"begin": 11521, "end": 11634, "idx": 93}, {"begin": 11635, "end": 11867, "idx": 94}, {"begin": 11868, "end": 11954, "idx": 95}, {"begin": 11955, "end": 12098, "idx": 96}, {"begin": 12137, "end": 12174, "idx": 97}, {"begin": 12175, "end": 12231, "idx": 98}, {"begin": 12232, "end": 12308, "idx": 99}, {"begin": 12458, "end": 12517, "idx": 100}, {"begin": 12522, "end": 12572, "idx": 101}, {"begin": 12575, "end": 12601, "idx": 102}, {"begin": 12602, "end": 12640, "idx": 103}, {"begin": 12641, "end": 12662, "idx": 104}, {"begin": 12668, "end": 12703, "idx": 105}, {"begin": 12704, "end": 12721, "idx": 106}, {"begin": 12722, "end": 12744, "idx": 107}, {"begin": 12745, "end": 12887, "idx": 108}, {"begin": 12888, "end": 13141, "idx": 109}, {"begin": 13142, "end": 13311, "idx": 110}, {"begin": 13312, "end": 13496, "idx": 111}, {"begin": 13535, "end": 13662, "idx": 112}, {"begin": 13663, "end": 13768, "idx": 113}, {"begin": 13769, "end": 13838, "idx": 114}, {"begin": 13910, "end": 13966, "idx": 115}, {"begin": 13967, "end": 14150, "idx": 116}, {"begin": 14151, "end": 14238, "idx": 117}, {"begin": 14239, "end": 14339, "idx": 118}, {"begin": 14340, "end": 14455, "idx": 119}, {"begin": 14456, "end": 14495, "idx": 120}, {"begin": 14496, "end": 14584, "idx": 121}, {"begin": 14585, "end": 14727, "idx": 122}, {"begin": 14728, "end": 14805, "idx": 123}, {"begin": 14806, "end": 14859, "idx": 124}, {"begin": 14860, "end": 14908, "idx": 125}, {"begin": 15054, "end": 15200, "idx": 126}, {"begin": 15254, "end": 15310, "idx": 127}, {"begin": 15311, "end": 15427, "idx": 128}, {"begin": 15487, "end": 15594, "idx": 129}, {"begin": 15595, "end": 15684, "idx": 130}, {"begin": 15685, "end": 15744, "idx": 131}, {"begin": 15745, "end": 15961, "idx": 132}, {"begin": 16000, "end": 16131, "idx": 133}, {"begin": 16132, "end": 16300, "idx": 134}, {"begin": 16301, "end": 16387, "idx": 135}, {"begin": 16427, "end": 16498, "idx": 136}, {"begin": 16602, "end": 16654, "idx": 137}, {"begin": 16655, "end": 16657, "idx": 138}, {"begin": 16658, "end": 16680, "idx": 139}, {"begin": 16681, "end": 16683, "idx": 140}, {"begin": 16684, "end": 16740, "idx": 141}, {"begin": 16745, "end": 16795, "idx": 142}, {"begin": 16798, "end": 16833, "idx": 143}, {"begin": 16838, "end": 16853, "idx": 144}, {"begin": 16854, "end": 16862, "idx": 145}, {"begin": 16863, "end": 16887, "idx": 146}, {"begin": 16888, "end": 16891, "idx": 147}, {"begin": 16892, "end": 16931, "idx": 148}, {"begin": 16932, "end": 16935, "idx": 149}, {"begin": 16936, "end": 16947, "idx": 150}, {"begin": 16948, "end": 16983, "idx": 151}, {"begin": 16989, "end": 17032, "idx": 152}, {"begin": 17033, "end": 17117, "idx": 153}, {"begin": 17118, "end": 17191, "idx": 154}, {"begin": 17252, "end": 17303, "idx": 155}, {"begin": 17304, "end": 17446, "idx": 156}, {"begin": 17503, "end": 17643, "idx": 157}, {"begin": 17644, "end": 17806, "idx": 158}, {"begin": 17831, "end": 17895, "idx": 159}, {"begin": 17896, "end": 18029, "idx": 160}, {"begin": 18030, "end": 18114, "idx": 161}, {"begin": 18115, "end": 18225, "idx": 162}, {"begin": 18226, "end": 18453, "idx": 163}, {"begin": 18454, "end": 18530, "idx": 164}, {"begin": 18562, "end": 18679, "idx": 165}, {"begin": 18680, "end": 18747, "idx": 166}, {"begin": 18748, "end": 18893, "idx": 167}, {"begin": 18894, "end": 19031, "idx": 168}, {"begin": 19032, "end": 19102, "idx": 169}, {"begin": 19103, "end": 19210, "idx": 170}, {"begin": 19211, "end": 19290, "idx": 171}, {"begin": 19350, "end": 19453, "idx": 172}, {"begin": 19499, "end": 19697, "idx": 173}, {"begin": 19698, "end": 19809, "idx": 174}, {"begin": 19810, "end": 19846, "idx": 175}, {"begin": 19847, "end": 19871, "idx": 176}, {"begin": 19872, "end": 19966, "idx": 177}, {"begin": 20020, "end": 20161, "idx": 178}, {"begin": 20162, "end": 20337, "idx": 179}, {"begin": 20338, "end": 20436, "idx": 180}, {"begin": 20437, "end": 20547, "idx": 181}, {"begin": 20548, "end": 20584, "idx": 182}, {"begin": 20585, "end": 20687, "idx": 183}, {"begin": 20688, "end": 20828, "idx": 184}, {"begin": 20829, "end": 21029, "idx": 185}, {"begin": 21054, "end": 21131, "idx": 186}, {"begin": 21132, "end": 21255, "idx": 187}, {"begin": 21256, "end": 21333, "idx": 188}, {"begin": 21334, "end": 21337, "idx": 189}, {"begin": 21338, "end": 21439, "idx": 190}, {"begin": 21463, "end": 21623, "idx": 191}, {"begin": 21624, "end": 21804, "idx": 192}, {"begin": 21805, "end": 21902, "idx": 193}, {"begin": 21951, "end": 22349, "idx": 194}, {"begin": 22459, "end": 22732, "idx": 195}, {"begin": 22733, "end": 22806, "idx": 196}, {"begin": 22807, "end": 22872, "idx": 197}, {"begin": 22873, "end": 22983, "idx": 198}, {"begin": 23050, "end": 23092, "idx": 199}, {"begin": 23093, "end": 23232, "idx": 200}, {"begin": 23233, "end": 23345, "idx": 201}, {"begin": 23346, "end": 23420, "idx": 202}, {"begin": 23421, "end": 23487, "idx": 203}, {"begin": 23488, "end": 23589, "idx": 204}, {"begin": 23621, "end": 23777, "idx": 205}, {"begin": 23778, "end": 23977, "idx": 206}, {"begin": 23978, "end": 24134, "idx": 207}, {"begin": 24135, "end": 24140, "idx": 208}, {"begin": 24141, "end": 24150, "idx": 209}, {"begin": 24151, "end": 24371, "idx": 210}, {"begin": 24372, "end": 24443, "idx": 211}, {"begin": 24444, "end": 24608, "idx": 212}, {"begin": 24609, "end": 24790, "idx": 213}, {"begin": 24791, "end": 24864, "idx": 214}, {"begin": 24865, "end": 25026, "idx": 215}, {"begin": 25027, "end": 25269, "idx": 216}, {"begin": 25270, "end": 25318, "idx": 217}, {"begin": 25319, "end": 25425, "idx": 218}, {"begin": 25426, "end": 25431, "idx": 219}, {"begin": 25432, "end": 25452, "idx": 220}, {"begin": 25453, "end": 25586, "idx": 221}, {"begin": 25587, "end": 25700, "idx": 222}, {"begin": 25863, "end": 25950, "idx": 223}, {"begin": 25951, "end": 26001, "idx": 224}, {"begin": 26002, "end": 26068, "idx": 225}, {"begin": 26069, "end": 26167, "idx": 226}, {"begin": 26168, "end": 26360, "idx": 227}, {"begin": 26361, "end": 26489, "idx": 228}, {"begin": 26490, "end": 26555, "idx": 229}, {"begin": 26556, "end": 26726, "idx": 230}, {"begin": 26727, "end": 27057, "idx": 231}, {"begin": 27083, "end": 27195, "idx": 232}, {"begin": 27196, "end": 27493, "idx": 233}, {"begin": 27494, "end": 27619, "idx": 234}, {"begin": 27620, "end": 27671, "idx": 235}, {"begin": 27672, "end": 27875, "idx": 236}, {"begin": 27876, "end": 28024, "idx": 237}, {"begin": 28025, "end": 28245, "idx": 238}, {"begin": 28299, "end": 28446, "idx": 239}, {"begin": 28447, "end": 28555, "idx": 240}, {"begin": 28556, "end": 28605, "idx": 241}, {"begin": 28606, "end": 28692, "idx": 242}, {"begin": 28693, "end": 28793, "idx": 243}, {"begin": 28794, "end": 28919, "idx": 244}, {"begin": 28920, "end": 28993, "idx": 245}, {"begin": 28994, "end": 29143, "idx": 246}, {"begin": 29144, "end": 29194, "idx": 247}, {"begin": 29195, "end": 29296, "idx": 248}, {"begin": 29297, "end": 29454, "idx": 249}, {"begin": 29455, "end": 29670, "idx": 250}, {"begin": 29671, "end": 29781, "idx": 251}, {"begin": 29782, "end": 29916, "idx": 252}, {"begin": 29917, "end": 30052, "idx": 253}, {"begin": 30083, "end": 30296, "idx": 254}, {"begin": 30297, "end": 30327, "idx": 255}, {"begin": 30328, "end": 30397, "idx": 256}, {"begin": 30398, "end": 30501, "idx": 257}, {"begin": 30502, "end": 30597, "idx": 258}, {"begin": 30598, "end": 30692, "idx": 259}, {"begin": 30693, "end": 30753, "idx": 260}, {"begin": 30754, "end": 30762, "idx": 261}, {"begin": 30763, "end": 30927, "idx": 262}, {"begin": 30928, "end": 31103, "idx": 263}, {"begin": 31104, "end": 31276, "idx": 264}, {"begin": 31277, "end": 31499, "idx": 265}, {"begin": 31500, "end": 31688, "idx": 266}, {"begin": 31689, "end": 31829, "idx": 267}, {"begin": 31830, "end": 32028, "idx": 268}, {"begin": 32029, "end": 32172, "idx": 269}, {"begin": 32173, "end": 32305, "idx": 270}, {"begin": 32306, "end": 32403, "idx": 271}, {"begin": 32404, "end": 32484, "idx": 272}, {"begin": 32485, "end": 32577, "idx": 273}, {"begin": 32578, "end": 32681, "idx": 274}, {"begin": 32682, "end": 32814, "idx": 275}, {"begin": 32815, "end": 32892, "idx": 276}, {"begin": 32893, "end": 33053, "idx": 277}, {"begin": 33054, "end": 33154, "idx": 278}, {"begin": 33155, "end": 33232, "idx": 279}, {"begin": 33233, "end": 33401, "idx": 280}, {"begin": 33422, "end": 33517, "idx": 281}, {"begin": 33518, "end": 33673, "idx": 282}, {"begin": 33674, "end": 33770, "idx": 283}, {"begin": 33771, "end": 33810, "idx": 284}, {"begin": 33811, "end": 33923, "idx": 285}, {"begin": 33924, "end": 33963, "idx": 286}, {"begin": 33964, "end": 34004, "idx": 287}, {"begin": 34005, "end": 34194, "idx": 288}, {"begin": 34195, "end": 34323, "idx": 289}, {"begin": 34324, "end": 34445, "idx": 290}, {"begin": 34446, "end": 34491, "idx": 291}, {"begin": 34492, "end": 34727, "idx": 292}, {"begin": 34728, "end": 34874, "idx": 293}, {"begin": 34875, "end": 34997, "idx": 294}, {"begin": 35117, "end": 35254, "idx": 295}, {"begin": 35255, "end": 35335, "idx": 296}, {"begin": 35336, "end": 35392, "idx": 297}, {"begin": 35393, "end": 35475, "idx": 298}, {"begin": 35476, "end": 35642, "idx": 299}, {"begin": 35643, "end": 35686, "idx": 300}, {"begin": 35687, "end": 35850, "idx": 301}, {"begin": 35851, "end": 36136, "idx": 302}, {"begin": 36137, "end": 36144, "idx": 303}, {"begin": 36145, "end": 36200, "idx": 304}, {"begin": 36201, "end": 36300, "idx": 305}, {"begin": 36301, "end": 36392, "idx": 306}, {"begin": 36418, "end": 36489, "idx": 307}, {"begin": 36490, "end": 36634, "idx": 308}, {"begin": 36635, "end": 36705, "idx": 309}, {"begin": 36706, "end": 36810, "idx": 310}, {"begin": 36811, "end": 37065, "idx": 311}, {"begin": 37066, "end": 37161, "idx": 312}, {"begin": 37162, "end": 37250, "idx": 313}, {"begin": 37251, "end": 37337, "idx": 314}, {"begin": 37338, "end": 37519, "idx": 315}, {"begin": 37520, "end": 37578, "idx": 316}, {"begin": 37579, "end": 37752, "idx": 317}, {"begin": 37753, "end": 37864, "idx": 318}, {"begin": 37865, "end": 37981, "idx": 319}, {"begin": 37982, "end": 38139, "idx": 320}, {"begin": 38140, "end": 38270, "idx": 321}, {"begin": 38271, "end": 38447, "idx": 322}, {"begin": 38448, "end": 38659, "idx": 323}, {"begin": 38692, "end": 38804, "idx": 324}, {"begin": 38805, "end": 38859, "idx": 325}, {"begin": 38901, "end": 39081, "idx": 326}, {"begin": 39082, "end": 39179, "idx": 327}, {"begin": 39180, "end": 39310, "idx": 328}, {"begin": 39311, "end": 39426, "idx": 329}, {"begin": 39427, "end": 39528, "idx": 330}, {"begin": 39586, "end": 39643, "idx": 331}, {"begin": 39644, "end": 39830, "idx": 332}, {"begin": 39831, "end": 39914, "idx": 333}, {"begin": 39915, "end": 40049, "idx": 334}, {"begin": 40114, "end": 40223, "idx": 335}, {"begin": 40224, "end": 40282, "idx": 336}, {"begin": 40283, "end": 40399, "idx": 337}, {"begin": 40400, "end": 40486, "idx": 338}, {"begin": 40487, "end": 40586, "idx": 339}, {"begin": 40587, "end": 40791, "idx": 340}, {"begin": 40792, "end": 40824, "idx": 341}, {"begin": 40825, "end": 41039, "idx": 342}, {"begin": 41040, "end": 41409, "idx": 343}, {"begin": 41410, "end": 41478, "idx": 344}, {"begin": 41479, "end": 41620, "idx": 345}, {"begin": 41635, "end": 41753, "idx": 346}, {"begin": 41754, "end": 41863, "idx": 347}, {"begin": 41864, "end": 41977, "idx": 348}, {"begin": 41978, "end": 42066, "idx": 349}, {"begin": 42067, "end": 42162, "idx": 350}, {"begin": 42163, "end": 42344, "idx": 351}, {"begin": 42345, "end": 42542, "idx": 352}, {"begin": 42543, "end": 42651, "idx": 353}, {"begin": 42669, "end": 42729, "idx": 354}, {"begin": 42730, "end": 42889, "idx": 355}, {"begin": 42890, "end": 42951, "idx": 356}, {"begin": 42952, "end": 42984, "idx": 357}, {"begin": 42985, "end": 43031, "idx": 358}, {"begin": 43128, "end": 43139, "idx": 359}, {"begin": 43140, "end": 43277, "idx": 360}, {"begin": 43368, "end": 43380, "idx": 361}, {"begin": 43471, "end": 43491, "idx": 362}, {"begin": 43492, "end": 43585, "idx": 363}, {"begin": 43586, "end": 43748, "idx": 364}, {"begin": 43749, "end": 43787, "idx": 365}, {"begin": 43853, "end": 43896, "idx": 366}, {"begin": 43897, "end": 43947, "idx": 367}, {"begin": 44006, "end": 44080, "idx": 368}, {"begin": 44081, "end": 44165, "idx": 369}, {"begin": 44236, "end": 44309, "idx": 370}, {"begin": 44365, "end": 44475, "idx": 371}, {"begin": 44496, "end": 44502, "idx": 372}, {"begin": 44503, "end": 44657, "idx": 373}, {"begin": 44745, "end": 44806, "idx": 374}, {"begin": 44906, "end": 44957, "idx": 375}, {"begin": 44958, "end": 44973, "idx": 376}, {"begin": 45064, "end": 45183, "idx": 377}, {"begin": 45473, "end": 45497, "idx": 378}, {"begin": 45499, "end": 45623, "idx": 379}, {"begin": 45763, "end": 45904, "idx": 380}, {"begin": 45905, "end": 46086, "idx": 381}, {"begin": 46087, "end": 46187, "idx": 382}, {"begin": 46225, "end": 46258, "idx": 383}, {"begin": 46259, "end": 46330, "idx": 384}, {"begin": 46331, "end": 46450, "idx": 385}, {"begin": 46555, "end": 46589, "idx": 386}, {"begin": 46590, "end": 46666, "idx": 387}, {"begin": 46667, "end": 46738, "idx": 388}, {"begin": 46805, "end": 46834, "idx": 389}, {"begin": 46835, "end": 46887, "idx": 390}, {"begin": 46888, "end": 47009, "idx": 391}, {"begin": 47010, "end": 47078, "idx": 392}, {"begin": 47079, "end": 47124, "idx": 393}, {"begin": 47303, "end": 47337, "idx": 394}, {"begin": 47513, "end": 47546, "idx": 395}, {"begin": 47698, "end": 47936, "idx": 396}, {"begin": 47937, "end": 48067, "idx": 397}, {"begin": 48068, "end": 48186, "idx": 398}, {"begin": 48187, "end": 48232, "idx": 399}, {"begin": 48247, "end": 48315, "idx": 400}, {"begin": 48316, "end": 48382, "idx": 401}, {"begin": 48383, "end": 48397, "idx": 402}, {"begin": 48450, "end": 48492, "idx": 403}, {"begin": 48493, "end": 48563, "idx": 404}, {"begin": 48646, "end": 48686, "idx": 405}, {"begin": 48738, "end": 48746, "idx": 406}, {"begin": 48800, "end": 48900, "idx": 407}, {"begin": 49018, "end": 49129, "idx": 408}, {"begin": 49130, "end": 49175, "idx": 409}, {"begin": 49176, "end": 49263, "idx": 410}, {"begin": 49286, "end": 49352, "idx": 411}, {"begin": 49409, "end": 49449, "idx": 412}, {"begin": 49450, "end": 49540, "idx": 413}, {"begin": 49597, "end": 49632, "idx": 414}, {"begin": 49633, "end": 49728, "idx": 415}, {"begin": 49729, "end": 49834, "idx": 416}, {"begin": 49897, "end": 49976, "idx": 417}, {"begin": 49977, "end": 50075, "idx": 418}, {"begin": 50141, "end": 50277, "idx": 419}, {"begin": 50278, "end": 50361, "idx": 420}, {"begin": 50382, "end": 50388, "idx": 421}, {"begin": 50389, "end": 50438, "idx": 422}, {"begin": 50439, "end": 50520, "idx": 423}, {"begin": 50521, "end": 50561, "idx": 424}, {"begin": 50562, "end": 50630, "idx": 425}, {"begin": 50635, "end": 50779, "idx": 426}, {"begin": 50780, "end": 50785, "idx": 427}, {"begin": 50786, "end": 50901, "idx": 428}], "ReferenceToFigure": [{"begin": 14453, "end": 14454, "target": "#fig_4", "idx": 0}, {"begin": 15308, "end": 15309, "target": "#fig_4", "idx": 1}, {"begin": 29305, "end": 29306, "target": "#fig_5", "idx": 2}, {"begin": 30338, "end": 30339, "target": "#fig_8", "idx": 3}, {"begin": 30349, "end": 30350, "idx": 4}, {"begin": 32418, "end": 32419, "target": "#fig_8", "idx": 5}, {"begin": 33160, "end": 33161, "target": "#fig_8", "idx": 6}, {"begin": 36142, "end": 36143, "idx": 7}, {"begin": 36487, "end": 36488, "idx": 8}, {"begin": 37999, "end": 38000, "idx": 9}, {"begin": 40822, "end": 40823, "target": "#fig_10", "idx": 10}], "Abstract": [{"begin": 88, "end": 1347, "idx": 0}], "SectionFootnote": [{"begin": 50904, "end": 51316, "idx": 0}], "Footnote": [{"begin": 50915, "end": 51005, "id": "foot_0", "idx": 0}, {"begin": 51006, "end": 51106, "id": "foot_1", "n": "1", "idx": 1}, {"begin": 51107, "end": 51316, "id": "foot_2", "n": "2", "idx": 2}]}}