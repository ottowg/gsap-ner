{"text": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning\n\nAbstract:\nRecent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.\n\n\n1 Introduction\nA multitude of datasets and models have been developed in natural language processing for a wide variety of tasks and applications. However, a large proportion of these have focused on English. Many works have contributed resources for other languages, developing specialized models for each language of interest is not scalable, not to mention difficult for low resource languages where labeled data is exceptionally scarce.\nRecent work in multilingual NLP shows promise for incorporating many languages into one architecture. For example, the mBART (Liu et al., 2020) model trains on twenty five different languages and can be finetuned for various different tasks. For translation, mBART was finetuned on bitext (bilingual finetuning). However, while mBART was trained on a variety of languages, the multilingual nature of the pretraining is not used during finetuning. Finetuning on bitext to translate from one language to another does not leverage the full capacity of the multilingual pretraining. Instead, we propose multilingual finetuning of pretrained models, and we demonstrate large improvements compared to bilingual finetuning.\nPrevious work (Aharoni et al., 2019; Arivazhagan et al., 2019b; Zhang et al., 2020) has explored multilingual translation by training multiple directions within the same model from scratch, but this approach faces challenges for mid to low resource languages. In lower resource scenarios, bitext data is usually unavailable in large quantities, making it challenging to train from scratch. In contrast, monolingual data exists even for low resource languages, particularly in resources such as Wikipedia or Commoncrawl, a version of the web. Thus, leveraging this monolingual data through pretraining can provide a much stronger starting point for low resource machine translation tasks.\nHowever, unlike training a multilingual model from scratch, pretrained models are limited to the choices made during pretraining. For example, mBART was only trained on 25 languages, so finetuning to translate on a model not part of these 25 languages is not possible. Thus, people are restricted to the languages selected to train the initial model, as it is incredibly computationally intensive to retrain from scratch. In this work, we show that existing pretrained models, such as mBART (Liu et al., 2020) can be extended to additional languages. We demonstrate by doubling the number of languages supported by mBART -to 50 -without loss of performance on the original 25 languages and without starting from scratch. This allows languages to be added flexibly, while preserving the broader utility of the pretrained model, as it can be used for tasks beyond translation.\nFurther, working in a multilingual setting remains challenging, as various different datasets, evaluation settings, and preprocessing such as tokenization are used. Benchmarks for sentence embeddings (Hu et al., 2020), natural language inference (Conneau et al., 2018), and question answering (Lewis et al., 2019b) exist, but there is not yet a setting for machine translation. To this end, we contribute the ML50 benchmark, a dataset of 50 languages with publicly available training and evaluation sets, including high, mid, and extremely low resource directions. We will open source this benchmark for the community.\nWe make three main contributions:\n\u2022 An effective and novel approach for multilingual translation models with multilingual pretraining (with monolingual data) followed by multilingual finetuning (with parallel data).\nIn the Many-to-English setting, multilingual finetuning achieves a 3.6 BLEU improvement over bilingual finetuning, and 2.6 BLEU improvement compared to multilingual models trained from scratch. On average, combining Many-to-English and English-to-Many, multilingual finetuning improves 1 BLEU points over the strongest baseline.\n\u2022 We show that existing pretrained models, such as mBART, can be extended to incorporate additional languages without training from scratch and without performance loss on the original languages. We release mBART50 for the community to use, which has double the number of languages of the original mBART.\n\u2022 To facilitate reproducible research on multilingual translation with representative challenges of the real world, we create the ML50 benchmark covering high, mid, and low resource languages and consisting of 230M bitext.\n2 Related work\n\n2.1 Multilingual Denoising Pretraining\nThis work is related to recent progress of pretraining techniques for NLP applications (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Song et al., 2019; Lewis et al., 2019a).\nIn particular, recent works explored pre-training on multilingual unlabeled corpus (Lample and Conneau, 2019; Conneau et al., 2019; Liu et al., 2020; Tran et al., 2020), and significantly improved the performance of fine-tuning on machine translation between two languages. We extend Liu et al. (2020) by allowing fine-tuning in multilingual settings.\n\n2.2 Multilingual Neural Machine Translation\nTraining a universal translation system between multiple languages (Firat et al., 2016; Johnson et al., 2017) has shown enormous improvement for translating low-resource languages (Gu et al., 2018), and even enabling zero-shot translation (Gu et al., 2019; Arivazhagan et al., 2019a).  Arivazhagan et al. (2019b) indicates that it is essential to train gigantic models with enough capacity to fully leverage massive multilingual corpora.\nA closely related concurrent work, Siddhant et al. (2020) shows it is possible to train a multilingual system jointly with monolingual datasets based on Song et al. (2019). It naturally enables translation for languages without parallel data. In contrast, this work focuses on fine-tuning multilingual translation systems given a pre-trained model.\n\n3 Multilingual Translation from Denoising Pretraining\nWe briefly describe the pretrained multilingual BART model and present multilingual finetuning, a technique to convert pretrained models into multilingual machine translation systems.\nmBART multilingual BART (mBART) (Liu et al., 2020) is a sequence-to-sequence generative pretraining scheme. The model incorporates N languages by concatenating data:D = {D 1 , ..., D N }\nwhere each D i is a collection of monolingual documents in language i. mBART is trained as a denoising autoencoder, training to predict the original text X given g(X) where g is a noising function that corrupts text. We maximize L \u03b8 :L \u03b8 = D i \u2208D x\u2208D i log P (x|g(x); \u03b8) ,\nwhere x is an instance in language i and the distribution P is defined by the seq-to-seq model. This model is pretrained using two types of noise in grandom span masking and order permutation -as described in (Liu et al., 2020).\n\n3.1 Multilingual Finetuning\nTo leverage multilingual pretraining to create translation systems, previous work (Liu et al., 2020) used mBART as a starting point and then performed bilingual finetuning. Concretely, the seq-to-seq model was finetuned on language i to language j translation. However, bilingual finetuning does not leverage the full capacity of multilingual pretraining. Recent work on multilingual translation (Aharoni et al., 2019; Arivazhagan et al., 2019b) displays that strong translation models can be created by doing multilingual training rather than using bilingual tranining. Instead of training a model from language i to language j, a model is trained to translate N languages to N other languages. Thus, we propose to do multilingual finetuning (ML-FT) to adapt pretrained models to become multilingual models. This procedure creates one model capable of translating many languages to many other languages, which has efficiency and storage maintenance benefits. Further, multilingual finetuning retains several benefits of multilingual translation models in general, for example allowing languages of similar family to benefit each other.\nTo perform multilingual finetuning, we collect bitexts of different language pairs (i, j) into a collection B i,j = {(x i , y j )} for each direction (i, j). Following mBART (Liu et al., 2020), we augment each bitext pair (x i , y j ) by adding a source language token and a target language token at the beginning of x and y respectively to form a target language token augmented pair (x , y ). We then initialize a transformer based seq-to-seq model by the pretained mBART, and provide the multilingual bitexts B = i,j B i,j to finetune the pretrained model.\n\nMultilingual Translation Model Variants\nWe explore 3 configurations to create different versions of multilingual translation models: Many-to-one (N \u2192 1), one-to-Many (1 \u2192 N ), and Many-to-Many (N \u2194 N ) via a pivot language. Concretely, the Many-to-one model encodes N languages and decodes to English, while the one-to-Many model encodes English and decodes into N languages. Finally, the Many-to-Many model encodes and decodes N languages. We follow (Arivazhagan et al., 2019b) and use pivot data through English to create Many-to-Many models.\nTemperature Sampling When training multilingual models with many languages, the training dataset sizes are imbalanced as different languages have different quantities of bitext. Thus, we train with temperature upsampling, which upsamples lower resource pairs so that the high resource languages do not dominate the training data. We follow Arivazhagan et al. (2019b) and use the following temperature based sampling function with temperature T to sample data for each direction:p i,j \u221d |B i,j | i,j |B i,j | 1/T\n4 Results from Multilingual Finetuning on 25 Languages\nWe first examine the impact of multilingual finetuning directly on existing pretrained models. We present results on the 25 languages included in mBART, using the existing mBART model. First, we describe three strong baselines: bilingual finetuning, bilingual translation models from scratch, and multilingual translation models from scratch. Then, we describe our experimental setting. Finally, we present results on 25 languages, showing that on average, multilingual finetuning improves 0.2 BLEU over the strongest baseline -1.0 BLEU point improvement over the strongest to-English baseline while \u22120.63 difference to the strongest from-English baseline.\n\n4.1 Baselines\nWe compare our proposed multilingual finetuning to three strong baselines: bilingual training from scratch, bilingual finetuning, and multilingual models trained from scratch.\n\nBilingual Trained from Scratch (BL-Scratch)\nWe train bilingual translation models with standard Transformer (Vaswani et al., 2017) models 1 for translation into and from English to 49 languages.\nFor directions with more than 1 million bitext training data (de, cs, fr, ja, es, ru, pl, zh, fi, lv, lt, and hi ), we train Transformer Big models 2 as there is more data to benefit from additional model capacity. For directions with more than 10 million bitext training data (de, cs, fr, ja, es, ru, pl, and zh), we train  We compare to bilingual finetuning (BL-FT) and multilingual translation from scratch (ML-SC). We perform multilingual finetuning on the existing mBART model. On average, multilingual finetuning (ML-FT) improves 1.0 BLEU in Many-to-one (N\u21921), \u22120.77 BLEU in one-to-Many (1\u2192N), and \u22120.77 and \u22121.85 BLEU for to-English and from-English respectively in Many-to-Many (N\u2194N) settings compared to the strongest baselines ML-SC many-to-one, BL-FT, and ML-SC many-to-one and BL-FT finetuning (combined baselines for ML-FT many-to-many) respectively.\nTransformer Large models 3 as there is even more data to benefit from additional model capacity. The best performing bilingual model is selected as the Bilingual Train from Scratch baseline.\nBilingual Finetuning (BL-FT) Bilingual finetuning adapts the mBART model into bilingual machine translation models by training for longer on translation bitext. For each language direction, we follow Liu et al. (2020) and finetune for 40K updates to obtain the Bilingual Finetuning baseline.\n\nMultilingual Trained from Scratch (ML-SC)\nWe train 3 different multlilingual models from scratch: Many-to-one (N\u21921), one-to-Many (1\u2192N), and Many-to-Many (N\u2194N) with English as pivot. We train for 500K updates and sweep through different batch sizes, learning rates, and upsampling temperature for best performing multilingual model on validation, using 32 GPUs for each training instance. Following Arivazhagan et al. (2019b), we train with temperature upsampling.\n\n4.2 Evaluation and Generation\nWe evaluate performance with tokenized BLEU, following the tokenization in mBART (Liu et al., 2020). To generate, we decode using beam search with beam size N = 5 with length penalty= 1.0 on the validation set. We do not perform checkpoint averaging. To select the best performing model in a sweep, we compare BLEU on the validation set.\n\n4.3 Performance on 25 Languages\nWe first evaluate our proposed multilingual finetuning technique on 25 languages using the existing mBART model. We compare bilingual finetuning from mBART (BL-FT), multilingual training from scratch (ML-SC), and multilingual finetuning (ML-FT) by quantifying the BLEU improvement over the bilingual training from scratch baseline. Results are displayed in Table 1, separated into three settings: Many-to-one (N\u21921), one-to-Many (1\u2192N),\nand Many-to-Many (N\u2194N).\nPerformance of Multilingual Finetuning Compared to the BL-FT and ML-SC baselines, multilingual finetuning has consistently stronger results in the Many-to-one setting, translating from 25 different languages into English. The improvement is 7.9 BLEU points stronger than the bilingual from scratch baseline, and 1.0 BLEU points stronger than the the strongest baseline, ML-SC.\nHowever, in the one-to-Many setting, improvement of all multilingual methods against bilingual baselines is lower across the board. We hypothesize this is due to the challenge of needing to decode into many different languages (additional analysis is presented in Section 6.1). Multilingual finetuning method is 3 BLEU points stronger than the bilingual from scratch baseline; it is also comparable to the strongest baseline -bilingual finetuning with \u22120.6 BLEU difference on average.\nFinally, in the Many-to-Many setting, improvement of all many-to-many multilingual methods against bilingual baselines is lower across the board. Again we hypothesize this is due to the challenge of decoding into many different languages including English (additional analysis is presented in Section 6.1). Multilingual finetuning method is 3.98 BLEU points stronger than the bilingual from scratch baseline for translation from and into English combined. Overall, it is lower than the strongest from-English and into-English baselines combined with \u22121.3 BLEU difference on average.\n\nPerformance by Resource Level\nComparing the languages by resource level, we see that the improvement from multilingual training is more significant as the quantity of training bitext decreases. For example, in the multilingual finetuning (ML-FT) Many-to-one setting, improvement over bilingual from scratch is 4.4 BLEU points for languages with more than 10M bitext, but is 18.0 BLEU points for languages with 7K-30K available bitext. The trend is less consistent in the one-to-Many setting, but low resource languages still see improvements. For example, with multilingual finetuning (ML-FT), improvement over bilingual from scratch is 2.2 BLEU for languages with more than 10M bitext, but 7.6 BLEU for languages with 7K-30K available bitext.\n\n5 Results from Multilingual Finetuning on 50 Languages\nMultilingual finetuning showed strong improvements on 25 languages in the Many-to-one setting and we subsequently extend to incorporate a greater number of languages -50 instead of 25. However, the number of languages possible is limited by the initial selection of languages in mBART. To remedy this, we show that the number of languages in mBART can be easily extended with additional pretraining. Second, we build the ML50 benchmark, to standardize training data, evaluation data, and evaluation procedure across 50 different languages. Finally, we display results of multilingual finetuning from mBART on 50 languages and show strong improvements over the baselines.\n\n5.1 Doubling the Languages in mBART\nWe describe how we extend existing pretrained models to incorporate a greater number of languages. This technique allows existing models to be used on new languages, rather than needing to restart a computationally intensive pretraining method from scratch.\nCreating mBART50 While multilingual pretrained models have shown strong performance in a variety of tasks (Liu et al., 2020; Conneau et al., 2019), they remain limited as they are trained on a fixed number of languages. For example, mBART was trained on 25 languages, all fairly high resource. Pretraining fully from scratch is computationally intensive -mBART trained for 2.5 weeks on 256 Nvidia V100 GPUs (Liu et al., 2020). However, there are hundreds of different languages in the world, so restarting pretraining from scratch to add any of them to mBART would be difficult. Instead, we take the existing mBART model, trained on 25 languages, and show that it can be extend to more than 50 languages. We take the public available pretrained mBART model 4 which was pretrained on 25 languages and extend its embedding layers with randomly initialized vectors for an extra set of 25 language tokens. We then combine the monolingual data of original 25 languages and the new 25 languages together to continue pretraining this extended MBART model. We will release the mBART50 model as a general purpose multilingual pretrained model, which will be useful Data size Languages 10M+ German, Czech, French, Japanese, Spanish, Russian, Polish, Chinese 1M -10M Finnish, Latvian, Lithuanian, Hindi, Estonian 100k to 1M Tamil, Romanian, Pashto, Sinhala, Malayalam, Dutch, Nepali, Italian, Arabic, Korean, Hebrew, Turkish, Khmer, Farsi, Vietnamese, Croatian, Ukrainian 10K to 100K Thai, Indonesian, Swedish, Portuguese, Xhosa, Afrikaans, Kazakh, Urdu, Macedonian, Telugu, Slovenian, Burmese, Georgia 10K-Marathi, Gujarati, Mongolian, Azerbaijani, Bengali for a variety of generation tasks beyond machine translation.\n\nData and Training Details\nWe use the mBART.cc25 checkpoint (Liu et al., 2020) available in the fairseq library (Ott et al., 2019) to continue the pretraining process. We use the monolingual data from XLMR (Conneau et al., 2019) to extend the pretraining to a set of 25 languages in addition to the 25 languages mBART model. To be consistent mBART, we reuse its 250K sentencepiece (Kudo and Richardson, 2018) model which was trained using monolingual data for 100 languages from XLMR, and thus already supports languages beyond the original 25 mBART was trained on. For pre-training, we train mBART50 for an additional 300K updates with a batch size of 1700 tokens. The sizes of the monolingual data for the additional 50 languages is provided in the appendix.\n\n5.2 ML50 Benchmark\nTo demonstrate the impact of multilingual finetuning on additional languages, we create the ML50 Benchmark. ML50 standardizes the training and evaluation schemes across 50 different languages, from extremely low resource languages like Xhosa and Gujarati to high resource languages like French and German. The full list of languages is shown in Table 3. We group the languages into five categories based on the amount of available training data: more than 10M pairs (8 languages), 1M to 10M pairs (5 languages), 100k to 1M pairs (17 languages), 10K to 100K pairs (13 languages), and finally, less than 10K pairs of training data (5 languages). ML50 includes languages in N language families, from Germanic and Romance languages to Indic and African ones. Many additional languages we contribute are lower resource, compared to the languages in the original mBART.\nTraining Data We gather parallel data between English and 49 other languages to form ML50, to enable the training of machine translation models. We select these 49 languages based on the amount of parallel and monolingual data to cover languages with different amount of resources and under different language families. The quantity of available monolingual data is relevant for pretraining, so we want to ensure there is a sufficient amount. All of the data is publicly available, such as WMT, IWSLT, WAT, TED, and other published research works. For training data, each language pair can include multiple sources. We simply concatenate them together and remove duplicated source-target sentence pairs for each language pair. We use fasttext (Joulin et al., 2017) to perform language identification on both source and target sentences, and we remove sentences pairs if either source or target sentence is not predicted as expected language. We further filter out training data that match to any source or target side sentences in evaluation datasets. Compared to other datasets such as OPUS100, the ML50 benchmark contains around 4 times more training data. The full list of languages, data sources, and amount of resulting data can be found in Table 6 in the Appendix.\nEvaluation Data To ensure high quality evaluation of languages covered in ML50, we include publicly available, widely used evaluation sets. We source these evaluation datasets from translation workshops such as WMT, IWSLT, WAT, and other published research works. We follow the evaluation protocol, including tokenization, used for each of these evaluation sets, to ensure our results are comparable with existing work. We release these scripts to make it easier for others. scratch We compare to bilingual finetuning (BL-FT) and multilingual translation from scratch (ML-SC). On average, multilingual finetuning (ML-FT) improves 2.61 BLEU in Many-to-one (N\u21921), \u22120.47 BLEU in one-to-Many (1\u2192N), and \u22120.15 and \u22120.35 BLEU for to-English and from-English respectively in Many-to-Many (N\u2194N) settings compared to the strongest baselines ML-SC many-to-one, BL-FT, and ML-SC many-to-one and BL-FT finetuning (combined baselines for ML-FT many-to-many) respectively.\nother datasets such as OPUS100, we choose to use high quality existing evaluation datasets rather than use part of the training data as evaluation. This is because training data, particularly for low resource languages, is often very noisy and unreliable.\n\n5.3 Performance on 50 Languages\nWe evaluate the performance of mBART50 on the ML50 Benchmark. We compare to the same baselines -bilingual finetuning, bilingual training from scratch, and multilingual training from scratch. Results are displayed in Table 4.\nIn the Many-to-One setting averaged across all languages, multilingual finetuning improves over the strongest baseline, multilingual many-to-many from scratch, by 2.5 BLEU points. For lower resource language pairs, the improvement is much more significant. For example, the improvement for languages with 4K-10K training data is 4.8 BLEU points over the strongest baseline, and the improvement for languages with 10K-100K train-ing data is 4+ BLEU over the strongest baseline.\nFor One-to-Many, the performance of all methods -bilingual finetuning, multilingual from scratch, and multilingual finetuning -is similar. On average, all models have around 5.7 to 7 BLEU points improvement over bilingual baselines.\nFinally, in Many-to-Many, multilingual finetuning achieves 0.8 improvement in the to-English direction over the strongest baseline. In the from-English direction, the performance of Many-to-Many from multilingual finetuning is similar to multilingual from scratch, both around 5.5 to 6 BLEU improvement over bilingual baselines.\n\n5.4 Comparison to Bilingual Finetuning\nWe examine the performance of our proposed multilingual finetuning method compared to bilingual finetuning. Current work shows that strong translation models can be created by finetuning pretrained models to bilingual translation models. However, this means that a separate model would need to be  created for each translation direction of interest, which creates a large quantity of models that need to be finetuned. In contrast, multilingual finetuning allows a multitude of directions to be captured within one model.\nHowever, multilingual finetuning would mean that the same model capacity must model many directions rather than just one, which could decrease performance. In Figure 1, we analyze the improvement of multilingual finetuning over the bilingual finetuning. On the left, we compare the Many-toone setting translating into English, and on the right we compare the one-to-Many setting translating out of English to many different languages.\nIn the Many-to-one setting, every language pair except one is improved by multilingual finetuning. Some low resource languages see substantial improvement of 10+ BLEU points, with the largest improvement being over 15 BLEU improvement. On average, multilingual finetuning improves 12.3 BLEU across all directions into English. In the oneto-Many setting, performance is about the same between multilingual finetuning and bilingual finetuning, with the average improvement at 6.3 BLEU across all directions out of English comparing to bilingual baselines.\n\n6 Discussion\n\n\n6.1 Challenges of one-to-Many\nIn the Many-to-one setting, where models must encode various different languages and decode into English, large improvements are seen when doing multilingual modeling. Previous work has similarly observed this improvement (Arivazhagan et al., 2019b) in multilingual training from scratch, as multilingual modeling increases the quantity of target-side English data seen by the model. For example, compared to bilingual finetuning, our mul-tilingual finetuning model is exposed to English target side data from 50 different language pairs. However, in the one-to-Many setting and the Many-to-Many setting, models must decode into 50 different languages. This is a difficult decoding challenge, as a strong conditional language model must be learned for each language. While pretraining exposes the model to monolingual data, the quantity of monolingual data varies for each language. For lower resource languages, such as Gujarati or Xhosa, the quantity of monolingual data available even through online resources such as Commoncrawl, remains limited. Other work (Arivazhagan et al., 2019b) observes similar trends in performance of one-to-Many.\nOverall, we find that multilingual finetuning performs better than any of our assessed baselinesbilingual training from scratch, bilingual finetuning, and multilingual training from scratch -when averaged across the Many-to-one and one-to-Many directions. It is important to note that this effect mainly comes from the strong improvement of the Many-to-one setting, and all approaches have similar performance in the one-to-Many setting.\n\n6.2 Comparison of mBART50 on 25 Languages\nWe show that the mBART model can be extended from 25 languages to 50 languages without starting from scratch. In this section, we evaluate if adding additional languages is harmful for performance on the original 25 languages. As the model remains the same size but has more to model, it could have reduced capacity for the original 25 languages, but we do not see any reduction in performance. Results are shown in Figure 2. For each language, we plot the performance when doing bilingual finetuning with mBART25 and mBART50. We show that performance is almost exactly the same with both models, indicating that the number of languages can be doubled without loss of performance.\n\n7 Conclusion\nWe demonstrate that multilingual neural machine translation models can be created from pretrained models such as mBART. Previous work using pretrained models focused only on bilingual finetuning, and work in multilingual translation trained only from scratch. While using pretrained models could limit the number of languages possible, we show that mBART can be extended to double the number of original languages, without loss of performance on the original languages. We release mBART50 for the community as a strong generative denoising pretrained model in 50 different languages. Further, to train and evaluate on 50 languages, we develop and release the ML50 benchmark. In conclusion, we show that by performing multilingual finetuning, strong improvements of over 2 BLEU points can be achieved in the Many-to-one setting. Overall, averaging across the Many-to-one and one-to-Many directions, our proposed multilingual finetuning strategy outperforms all baselines.\nA Appendices Table 6: ML50 Benchmark dataset stats. For each language, we list the size of training data after the filtering steps, the source of training/evaluation data, and the size of evaluation data. We notice that part of the available dataset are missing due to human error for a few language pairs. We mark these languages with asterisk and we will release next version of the ML50 benchmark data to include the missing data.\n\nFootnotes:\n1: 5 layers with 512 embedding dimension,\n2048: FFN embedding dimension, and 8 heads for both encoder and decoder 2 6 layers with 1024 embedding dimension, 4096 FFN embedding dimension, and 16 heads for both encoder and decoder\n3: 12 layers with 1024 embedding dimension,\n4096: FFN embedding dimension, and 16 heads for both encoder and decoder\n4: https://github.com/pytorch/fairseq/ tree/master/examples/mbart\n\nReferences:\n\n- Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), pages 3874-3884, Minneapolis, Minnesota. Association for Computational Linguistics.- Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee Aharoni, Melvin Johnson, and Wolfgang Macherey. 2019a. The missing ingredient in zero- shot neural machine translation. arXiv preprint arXiv:1903.07091.\n\n- Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019b. Massively multilingual neural machine translation in the wild: Findings and chal- lenges. arXiv preprint arXiv:1907.05019.\n\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\n- Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad- ina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluat- ing cross-lingual sentence representations. arXiv preprint arXiv:1809.05053.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In North American Association for Com- putational Linguistics (NAACL).\n\n- Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In NAACL.\n\n- Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018. Universal neural machine translation for extremely low resource languages. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 344-354, New Orleans, Louisiana. As- sociation for Computational Linguistics.\n\n- Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic- tor OK Li. 2019. Improved zero-shot neural ma- chine translation via ignoring spurious correlations. arXiv preprint arXiv:1906.01181.\n\n- Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra- ham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generaliza- tion. arXiv preprint arXiv:2003.11080.\n\n- Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2017. Googles multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351.\n\n- Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427-431. Association for Computational Linguistics.\n\n- Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.\n\n- Guillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. arXiv preprint arXiv:1901.07291.\n\n- Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019a. Bart: Denoising sequence-to-sequence pre-training for natural language generation, trans- lation, and comprehension. arXiv preprint arXiv:1910.13461.\n\n- Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019b. Mlqa: Eval- uating cross-lingual extractive question answering. arXiv preprint arXiv:1910.07475.\n\n- Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\n- Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. FAIRSEQ: A fast, extensible toolkit for sequence modeling. In North American Association for Computational Linguistics (NAACL): System Demonstrations.\n\n- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In North American Association for Com- putational Linguistics (NAACL).\n\n- Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. 2018. Improving language un- derstanding with unsupervised learning. Technical report, OpenAI.\n\n- Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Ari- vazhagan, and Yonghui Wu. 2020. Leveraging monolingual data with self-supervision for multi- lingual neural machine translation. arXiv preprint arXiv:2005.04816.\n\n- Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. In International Conference on Machine Learning (ICML).\n\n- Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020. Cross-lingual retrieval for iterative self-supervised training. arXiv preprint arXiv:2006.09526.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems.\n\n- Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. arXiv preprint arXiv:2004.11867.\n\n", "annotations": {"Abstract": [{"begin": 82, "end": 1489, "idx": 0}], "Head": [{"begin": 1492, "end": 1506, "n": "1", "idx": 0}, {"begin": 5921, "end": 5959, "n": "2.1", "idx": 1}, {"begin": 6524, "end": 6567, "n": "2.2", "idx": 2}, {"begin": 7356, "end": 7409, "n": "3", "idx": 3}, {"begin": 8284, "end": 8311, "n": "3.1", "idx": 4}, {"begin": 10010, "end": 10049, "idx": 5}, {"begin": 11780, "end": 11793, "n": "4.1", "idx": 6}, {"begin": 11971, "end": 12014, "idx": 7}, {"begin": 13514, "end": 13555, "idx": 8}, {"begin": 13979, "end": 14008, "n": "4.2", "idx": 9}, {"begin": 14348, "end": 14379, "n": "4.3", "idx": 10}, {"begin": 16285, "end": 16314, "idx": 11}, {"begin": 17030, "end": 17084, "n": "5", "idx": 12}, {"begin": 17757, "end": 17792, "n": "5.1", "idx": 13}, {"begin": 19761, "end": 19786, "idx": 14}, {"begin": 20522, "end": 20540, "n": "5.2", "idx": 15}, {"begin": 23892, "end": 23923, "n": "5.3", "idx": 16}, {"begin": 25189, "end": 25227, "n": "5.4", "idx": 17}, {"begin": 26739, "end": 26751, "n": "6", "idx": 18}, {"begin": 26754, "end": 26783, "n": "6.1", "idx": 19}, {"begin": 28368, "end": 28409, "n": "6.2", "idx": 20}, {"begin": 29092, "end": 29104, "n": "7", "idx": 21}], "ReferenceToBib": [{"begin": 2058, "end": 2076, "target": "#b16", "idx": 0}, {"begin": 2664, "end": 2686, "target": "#b0", "idx": 1}, {"begin": 2687, "end": 2713, "target": "#b2", "idx": 2}, {"begin": 2714, "end": 2733, "target": "#b25", "idx": 3}, {"begin": 3829, "end": 3847, "target": "#b16", "idx": 4}, {"begin": 4413, "end": 4430, "target": "#b9", "idx": 5}, {"begin": 4459, "end": 4481, "target": "#b4", "idx": 6}, {"begin": 4506, "end": 4527, "target": "#b15", "idx": 7}, {"begin": 6047, "end": 6068, "target": "#b19", "idx": 8}, {"begin": 6069, "end": 6090, "target": "#b20", "idx": 9}, {"begin": 6091, "end": 6111, "target": "#b5", "idx": 10}, {"begin": 6112, "end": 6129, "target": "#b17", "idx": 11}, {"begin": 6130, "end": 6148, "target": "#b22", "idx": 12}, {"begin": 6149, "end": 6169, "target": "#b14", "idx": 13}, {"begin": 6254, "end": 6280, "target": "#b13", "idx": 14}, {"begin": 6281, "end": 6302, "target": "#b3", "idx": 15}, {"begin": 6303, "end": 6320, "target": "#b16", "idx": 16}, {"begin": 6321, "end": 6339, "target": "#b23", "idx": 17}, {"begin": 6455, "end": 6472, "target": "#b16", "idx": 18}, {"begin": 6635, "end": 6655, "target": "#b6", "idx": 19}, {"begin": 6656, "end": 6677, "target": "#b10", "idx": 20}, {"begin": 6748, "end": 6765, "target": "#b7", "idx": 21}, {"begin": 6807, "end": 6824, "target": "#b8", "idx": 22}, {"begin": 6825, "end": 6851, "target": "#b1", "idx": 23}, {"begin": 6854, "end": 6880, "target": "#b2", "idx": 24}, {"begin": 7041, "end": 7063, "target": "#b21", "idx": 25}, {"begin": 7159, "end": 7177, "target": "#b22", "idx": 26}, {"begin": 7626, "end": 7644, "target": "#b16", "idx": 27}, {"begin": 8263, "end": 8281, "target": "#b16", "idx": 28}, {"begin": 8394, "end": 8412, "target": "#b16", "idx": 29}, {"begin": 8708, "end": 8730, "target": "#b0", "idx": 30}, {"begin": 8731, "end": 8757, "target": "#b2", "idx": 31}, {"begin": 9623, "end": 9641, "target": "#b16", "idx": 32}, {"begin": 10461, "end": 10488, "target": "#b2", "idx": 33}, {"begin": 10895, "end": 10921, "target": "#b2", "idx": 34}, {"begin": 12079, "end": 12100, "target": "#b24", "idx": 35}, {"begin": 13421, "end": 13438, "target": "#b16", "idx": 36}, {"begin": 13912, "end": 13938, "target": "#b2", "idx": 37}, {"begin": 14090, "end": 14108, "target": "#b16", "idx": 38}, {"begin": 18157, "end": 18175, "target": "#b16", "idx": 39}, {"begin": 18176, "end": 18197, "target": "#b3", "idx": 40}, {"begin": 18458, "end": 18476, "target": "#b16", "idx": 41}, {"begin": 19820, "end": 19838, "target": "#b16", "idx": 42}, {"begin": 19872, "end": 19890, "target": "#b18", "idx": 43}, {"begin": 19966, "end": 19988, "target": "#b3", "idx": 44}, {"begin": 20141, "end": 20168, "target": "#b12", "idx": 45}, {"begin": 22148, "end": 22169, "target": "#b11", "idx": 46}, {"begin": 27006, "end": 27033, "target": "#b2", "idx": 47}, {"begin": 27846, "end": 27873, "target": "#b2", "idx": 48}], "ReferenceToFootnote": [{"begin": 12109, "end": 12110, "target": "#foot_0", "idx": 0}, {"begin": 13055, "end": 13056, "target": "#foot_2", "idx": 1}], "SectionFootnote": [{"begin": 30511, "end": 30932, "idx": 0}], "ReferenceString": [{"begin": 30949, "end": 31321, "id": "b0", "idx": 0}, {"begin": 31323, "end": 31526, "id": "b1", "idx": 1}, {"begin": 31530, "end": 31811, "id": "b2", "idx": 2}, {"begin": 31815, "end": 32086, "id": "b3", "idx": 3}, {"begin": 32090, "end": 32307, "id": "b4", "idx": 4}, {"begin": 32311, "end": 32537, "id": "b5", "idx": 5}, {"begin": 32541, "end": 32689, "id": "b6", "idx": 6}, {"begin": 32693, "end": 33090, "id": "b7", "idx": 7}, {"begin": 33094, "end": 33273, "id": "b8", "idx": 8}, {"begin": 33277, "end": 33511, "id": "b9", "idx": 9}, {"begin": 33515, "end": 33836, "id": "b10", "idx": 10}, {"begin": 33840, "end": 34159, "id": "b11", "idx": 11}, {"begin": 34163, "end": 34501, "id": "b12", "idx": 12}, {"begin": 34505, "end": 34623, "id": "b13", "idx": 13}, {"begin": 34627, "end": 34916, "id": "b14", "idx": 14}, {"begin": 34920, "end": 35102, "id": "b15", "idx": 15}, {"begin": 35106, "end": 35328, "id": "b16", "idx": 16}, {"begin": 35332, "end": 35567, "id": "b17", "idx": 17}, {"begin": 35571, "end": 35836, "id": "b18", "idx": 18}, {"begin": 35840, "end": 36070, "id": "b19", "idx": 19}, {"begin": 36074, "end": 36237, "id": "b20", "idx": 20}, {"begin": 36241, "end": 36494, "id": "b21", "idx": 21}, {"begin": 36498, "end": 36691, "id": "b22", "idx": 22}, {"begin": 36695, "end": 36845, "id": "b23", "idx": 23}, {"begin": 36849, "end": 37062, "id": "b24", "idx": 24}, {"begin": 37066, "end": 37251, "id": "b25", "idx": 25}], "ReferenceToTable": [{"begin": 14743, "end": 14744, "target": "#tab_0", "idx": 0}, {"begin": 20892, "end": 20893, "target": "#tab_2", "idx": 1}, {"begin": 22657, "end": 22658, "idx": 2}, {"begin": 24146, "end": 24147, "target": "#tab_3", "idx": 3}], "Footnote": [{"begin": 30522, "end": 30563, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 30564, "end": 30749, "id": "foot_1", "n": "2048", "idx": 1}, {"begin": 30750, "end": 30793, "id": "foot_2", "n": "3", "idx": 2}, {"begin": 30794, "end": 30866, "id": "foot_3", "n": "4096", "idx": 3}, {"begin": 30867, "end": 30932, "id": "foot_4", "n": "4", "idx": 4}], "Paragraph": [{"begin": 92, "end": 1489, "idx": 0}, {"begin": 1507, "end": 1932, "idx": 1}, {"begin": 1933, "end": 2649, "idx": 2}, {"begin": 2650, "end": 3337, "idx": 3}, {"begin": 3338, "end": 4212, "idx": 4}, {"begin": 4213, "end": 4831, "idx": 5}, {"begin": 4832, "end": 4865, "idx": 6}, {"begin": 4866, "end": 5047, "idx": 7}, {"begin": 5048, "end": 5376, "idx": 8}, {"begin": 5377, "end": 5681, "idx": 9}, {"begin": 5682, "end": 5904, "idx": 10}, {"begin": 5905, "end": 5919, "idx": 11}, {"begin": 5960, "end": 6170, "idx": 12}, {"begin": 6171, "end": 6522, "idx": 13}, {"begin": 6568, "end": 7005, "idx": 14}, {"begin": 7006, "end": 7354, "idx": 15}, {"begin": 7410, "end": 7593, "idx": 16}, {"begin": 7594, "end": 7759, "idx": 17}, {"begin": 7781, "end": 8015, "idx": 18}, {"begin": 8054, "end": 8282, "idx": 19}, {"begin": 8312, "end": 9448, "idx": 20}, {"begin": 9449, "end": 10008, "idx": 21}, {"begin": 10050, "end": 10554, "idx": 22}, {"begin": 10555, "end": 11033, "idx": 23}, {"begin": 11067, "end": 11121, "idx": 24}, {"begin": 11122, "end": 11778, "idx": 25}, {"begin": 11794, "end": 11969, "idx": 26}, {"begin": 12015, "end": 12165, "idx": 27}, {"begin": 12166, "end": 13029, "idx": 28}, {"begin": 13030, "end": 13220, "idx": 29}, {"begin": 13221, "end": 13512, "idx": 30}, {"begin": 13556, "end": 13977, "idx": 31}, {"begin": 14009, "end": 14346, "idx": 32}, {"begin": 14380, "end": 14814, "idx": 33}, {"begin": 14815, "end": 14838, "idx": 34}, {"begin": 14839, "end": 15215, "idx": 35}, {"begin": 15216, "end": 15700, "idx": 36}, {"begin": 15701, "end": 16283, "idx": 37}, {"begin": 16315, "end": 17028, "idx": 38}, {"begin": 17085, "end": 17755, "idx": 39}, {"begin": 17793, "end": 18050, "idx": 40}, {"begin": 18051, "end": 19759, "idx": 41}, {"begin": 19787, "end": 20520, "idx": 42}, {"begin": 20541, "end": 21404, "idx": 43}, {"begin": 21405, "end": 22675, "idx": 44}, {"begin": 22676, "end": 23634, "idx": 45}, {"begin": 23635, "end": 23890, "idx": 46}, {"begin": 23924, "end": 24148, "idx": 47}, {"begin": 24149, "end": 24625, "idx": 48}, {"begin": 24626, "end": 24858, "idx": 49}, {"begin": 24859, "end": 25187, "idx": 50}, {"begin": 25228, "end": 25748, "idx": 51}, {"begin": 25749, "end": 26183, "idx": 52}, {"begin": 26184, "end": 26737, "idx": 53}, {"begin": 26784, "end": 27928, "idx": 54}, {"begin": 27929, "end": 28366, "idx": 55}, {"begin": 28410, "end": 29090, "idx": 56}, {"begin": 29105, "end": 30075, "idx": 57}, {"begin": 30076, "end": 30509, "idx": 58}], "SectionHeader": [{"begin": 0, "end": 1489, "idx": 0}], "SectionReference": [{"begin": 30934, "end": 37253, "idx": 0}], "Sentence": [{"begin": 92, "end": 239, "idx": 0}, {"begin": 240, "end": 371, "idx": 1}, {"begin": 372, "end": 478, "idx": 2}, {"begin": 479, "end": 587, "idx": 3}, {"begin": 588, "end": 840, "idx": 4}, {"begin": 841, "end": 959, "idx": 5}, {"begin": 960, "end": 1070, "idx": 6}, {"begin": 1071, "end": 1240, "idx": 7}, {"begin": 1241, "end": 1489, "idx": 8}, {"begin": 1507, "end": 1638, "idx": 9}, {"begin": 1639, "end": 1700, "idx": 10}, {"begin": 1701, "end": 1932, "idx": 11}, {"begin": 1933, "end": 2034, "idx": 12}, {"begin": 2035, "end": 2174, "idx": 13}, {"begin": 2175, "end": 2245, "idx": 14}, {"begin": 2246, "end": 2379, "idx": 15}, {"begin": 2380, "end": 2511, "idx": 16}, {"begin": 2512, "end": 2649, "idx": 17}, {"begin": 2650, "end": 2909, "idx": 18}, {"begin": 2910, "end": 3039, "idx": 19}, {"begin": 3040, "end": 3191, "idx": 20}, {"begin": 3192, "end": 3337, "idx": 21}, {"begin": 3338, "end": 3467, "idx": 22}, {"begin": 3468, "end": 3606, "idx": 23}, {"begin": 3607, "end": 3759, "idx": 24}, {"begin": 3760, "end": 3888, "idx": 25}, {"begin": 3889, "end": 4058, "idx": 26}, {"begin": 4059, "end": 4212, "idx": 27}, {"begin": 4213, "end": 4377, "idx": 28}, {"begin": 4378, "end": 4590, "idx": 29}, {"begin": 4591, "end": 4777, "idx": 30}, {"begin": 4778, "end": 4831, "idx": 31}, {"begin": 4832, "end": 4865, "idx": 32}, {"begin": 4866, "end": 5047, "idx": 33}, {"begin": 5048, "end": 5241, "idx": 34}, {"begin": 5242, "end": 5376, "idx": 35}, {"begin": 5377, "end": 5572, "idx": 36}, {"begin": 5573, "end": 5681, "idx": 37}, {"begin": 5682, "end": 5904, "idx": 38}, {"begin": 5905, "end": 5919, "idx": 39}, {"begin": 5960, "end": 6170, "idx": 40}, {"begin": 6171, "end": 6444, "idx": 41}, {"begin": 6445, "end": 6522, "idx": 42}, {"begin": 6568, "end": 6852, "idx": 43}, {"begin": 6853, "end": 7005, "idx": 44}, {"begin": 7006, "end": 7178, "idx": 45}, {"begin": 7179, "end": 7248, "idx": 46}, {"begin": 7249, "end": 7354, "idx": 47}, {"begin": 7410, "end": 7593, "idx": 48}, {"begin": 7594, "end": 7701, "idx": 49}, {"begin": 7702, "end": 7759, "idx": 50}, {"begin": 7781, "end": 7997, "idx": 51}, {"begin": 7998, "end": 8015, "idx": 52}, {"begin": 8054, "end": 8149, "idx": 53}, {"begin": 8150, "end": 8282, "idx": 54}, {"begin": 8312, "end": 8484, "idx": 55}, {"begin": 8485, "end": 8572, "idx": 56}, {"begin": 8573, "end": 8667, "idx": 57}, {"begin": 8668, "end": 8882, "idx": 58}, {"begin": 8883, "end": 9007, "idx": 59}, {"begin": 9008, "end": 9120, "idx": 60}, {"begin": 9121, "end": 9271, "idx": 61}, {"begin": 9272, "end": 9448, "idx": 62}, {"begin": 9449, "end": 9606, "idx": 63}, {"begin": 9607, "end": 9843, "idx": 64}, {"begin": 9844, "end": 10008, "idx": 65}, {"begin": 10050, "end": 10233, "idx": 66}, {"begin": 10234, "end": 10385, "idx": 67}, {"begin": 10386, "end": 10450, "idx": 68}, {"begin": 10451, "end": 10554, "idx": 69}, {"begin": 10555, "end": 10732, "idx": 70}, {"begin": 10733, "end": 10884, "idx": 71}, {"begin": 10885, "end": 11033, "idx": 72}, {"begin": 11067, "end": 11121, "idx": 73}, {"begin": 11122, "end": 11216, "idx": 74}, {"begin": 11217, "end": 11306, "idx": 75}, {"begin": 11307, "end": 11464, "idx": 76}, {"begin": 11465, "end": 11508, "idx": 77}, {"begin": 11509, "end": 11778, "idx": 78}, {"begin": 11794, "end": 11969, "idx": 79}, {"begin": 12015, "end": 12165, "idx": 80}, {"begin": 12166, "end": 12380, "idx": 81}, {"begin": 12381, "end": 12584, "idx": 82}, {"begin": 12585, "end": 12648, "idx": 83}, {"begin": 12649, "end": 12738, "idx": 84}, {"begin": 12739, "end": 13029, "idx": 85}, {"begin": 13030, "end": 13126, "idx": 86}, {"begin": 13127, "end": 13220, "idx": 87}, {"begin": 13221, "end": 13381, "idx": 88}, {"begin": 13382, "end": 13512, "idx": 89}, {"begin": 13556, "end": 13695, "idx": 90}, {"begin": 13696, "end": 13901, "idx": 91}, {"begin": 13902, "end": 13977, "idx": 92}, {"begin": 14009, "end": 14109, "idx": 93}, {"begin": 14110, "end": 14219, "idx": 94}, {"begin": 14220, "end": 14259, "idx": 95}, {"begin": 14260, "end": 14346, "idx": 96}, {"begin": 14380, "end": 14492, "idx": 97}, {"begin": 14493, "end": 14711, "idx": 98}, {"begin": 14712, "end": 14814, "idx": 99}, {"begin": 14815, "end": 14838, "idx": 100}, {"begin": 14839, "end": 15060, "idx": 101}, {"begin": 15061, "end": 15215, "idx": 102}, {"begin": 15216, "end": 15347, "idx": 103}, {"begin": 15348, "end": 15493, "idx": 104}, {"begin": 15494, "end": 15700, "idx": 105}, {"begin": 15701, "end": 15846, "idx": 106}, {"begin": 15847, "end": 16007, "idx": 107}, {"begin": 16008, "end": 16156, "idx": 108}, {"begin": 16157, "end": 16283, "idx": 109}, {"begin": 16315, "end": 16478, "idx": 110}, {"begin": 16479, "end": 16719, "idx": 111}, {"begin": 16720, "end": 16827, "idx": 112}, {"begin": 16828, "end": 17028, "idx": 113}, {"begin": 17085, "end": 17269, "idx": 114}, {"begin": 17270, "end": 17370, "idx": 115}, {"begin": 17371, "end": 17484, "idx": 116}, {"begin": 17485, "end": 17624, "idx": 117}, {"begin": 17625, "end": 17755, "idx": 118}, {"begin": 17793, "end": 17891, "idx": 119}, {"begin": 17892, "end": 18050, "idx": 120}, {"begin": 18051, "end": 18270, "idx": 121}, {"begin": 18271, "end": 18344, "idx": 122}, {"begin": 18345, "end": 18477, "idx": 123}, {"begin": 18478, "end": 18629, "idx": 124}, {"begin": 18630, "end": 18755, "idx": 125}, {"begin": 18756, "end": 18952, "idx": 126}, {"begin": 18953, "end": 19099, "idx": 127}, {"begin": 19100, "end": 19759, "idx": 128}, {"begin": 19787, "end": 19808, "idx": 129}, {"begin": 19809, "end": 19927, "idx": 130}, {"begin": 19928, "end": 20084, "idx": 131}, {"begin": 20085, "end": 20325, "idx": 132}, {"begin": 20326, "end": 20425, "idx": 133}, {"begin": 20426, "end": 20520, "idx": 134}, {"begin": 20541, "end": 20648, "idx": 135}, {"begin": 20649, "end": 20846, "idx": 136}, {"begin": 20847, "end": 20894, "idx": 137}, {"begin": 20895, "end": 21184, "idx": 138}, {"begin": 21185, "end": 21295, "idx": 139}, {"begin": 21296, "end": 21404, "idx": 140}, {"begin": 21405, "end": 21549, "idx": 141}, {"begin": 21550, "end": 21724, "idx": 142}, {"begin": 21725, "end": 21847, "idx": 143}, {"begin": 21848, "end": 21952, "idx": 144}, {"begin": 21953, "end": 22020, "idx": 145}, {"begin": 22021, "end": 22131, "idx": 146}, {"begin": 22132, "end": 22346, "idx": 147}, {"begin": 22347, "end": 22456, "idx": 148}, {"begin": 22457, "end": 22563, "idx": 149}, {"begin": 22564, "end": 22675, "idx": 150}, {"begin": 22676, "end": 22815, "idx": 151}, {"begin": 22816, "end": 22939, "idx": 152}, {"begin": 22940, "end": 23095, "idx": 153}, {"begin": 23096, "end": 23150, "idx": 154}, {"begin": 23151, "end": 23252, "idx": 155}, {"begin": 23253, "end": 23634, "idx": 156}, {"begin": 23635, "end": 23782, "idx": 157}, {"begin": 23783, "end": 23890, "idx": 158}, {"begin": 23924, "end": 23985, "idx": 159}, {"begin": 23986, "end": 24114, "idx": 160}, {"begin": 24115, "end": 24148, "idx": 161}, {"begin": 24149, "end": 24328, "idx": 162}, {"begin": 24329, "end": 24405, "idx": 163}, {"begin": 24406, "end": 24625, "idx": 164}, {"begin": 24626, "end": 24764, "idx": 165}, {"begin": 24765, "end": 24858, "idx": 166}, {"begin": 24859, "end": 24990, "idx": 167}, {"begin": 24991, "end": 25187, "idx": 168}, {"begin": 25228, "end": 25335, "idx": 169}, {"begin": 25336, "end": 25465, "idx": 170}, {"begin": 25466, "end": 25645, "idx": 171}, {"begin": 25646, "end": 25748, "idx": 172}, {"begin": 25749, "end": 25904, "idx": 173}, {"begin": 25905, "end": 26002, "idx": 174}, {"begin": 26003, "end": 26183, "idx": 175}, {"begin": 26184, "end": 26282, "idx": 176}, {"begin": 26283, "end": 26419, "idx": 177}, {"begin": 26420, "end": 26510, "idx": 178}, {"begin": 26511, "end": 26737, "idx": 179}, {"begin": 26784, "end": 26951, "idx": 180}, {"begin": 26952, "end": 27167, "idx": 181}, {"begin": 27168, "end": 27322, "idx": 182}, {"begin": 27323, "end": 27436, "idx": 183}, {"begin": 27437, "end": 27550, "idx": 184}, {"begin": 27551, "end": 27666, "idx": 185}, {"begin": 27667, "end": 27834, "idx": 186}, {"begin": 27835, "end": 27928, "idx": 187}, {"begin": 27929, "end": 28184, "idx": 188}, {"begin": 28185, "end": 28366, "idx": 189}, {"begin": 28410, "end": 28519, "idx": 190}, {"begin": 28520, "end": 28636, "idx": 191}, {"begin": 28637, "end": 28804, "idx": 192}, {"begin": 28805, "end": 28835, "idx": 193}, {"begin": 28836, "end": 28936, "idx": 194}, {"begin": 28937, "end": 29090, "idx": 195}, {"begin": 29105, "end": 29224, "idx": 196}, {"begin": 29225, "end": 29364, "idx": 197}, {"begin": 29365, "end": 29574, "idx": 198}, {"begin": 29575, "end": 29688, "idx": 199}, {"begin": 29689, "end": 29779, "idx": 200}, {"begin": 29780, "end": 29932, "idx": 201}, {"begin": 29933, "end": 30075, "idx": 202}, {"begin": 30076, "end": 30127, "idx": 203}, {"begin": 30128, "end": 30280, "idx": 204}, {"begin": 30281, "end": 30382, "idx": 205}, {"begin": 30383, "end": 30509, "idx": 206}], "ReferenceToFigure": [{"begin": 25915, "end": 25916, "target": "#fig_1", "idx": 0}, {"begin": 28833, "end": 28834, "target": "#fig_2", "idx": 1}], "Div": [{"begin": 92, "end": 1489, "idx": 0}, {"begin": 1492, "end": 5919, "idx": 1}, {"begin": 5921, "end": 6522, "idx": 2}, {"begin": 6524, "end": 7354, "idx": 3}, {"begin": 7356, "end": 8282, "idx": 4}, {"begin": 8284, "end": 10008, "idx": 5}, {"begin": 10010, "end": 11778, "idx": 6}, {"begin": 11780, "end": 11969, "idx": 7}, {"begin": 11971, "end": 13512, "idx": 8}, {"begin": 13514, "end": 13977, "idx": 9}, {"begin": 13979, "end": 14346, "idx": 10}, {"begin": 14348, "end": 16283, "idx": 11}, {"begin": 16285, "end": 17028, "idx": 12}, {"begin": 17030, "end": 17755, "idx": 13}, {"begin": 17757, "end": 19759, "idx": 14}, {"begin": 19761, "end": 20520, "idx": 15}, {"begin": 20522, "end": 23890, "idx": 16}, {"begin": 23892, "end": 25187, "idx": 17}, {"begin": 25189, "end": 26737, "idx": 18}, {"begin": 26739, "end": 26752, "idx": 19}, {"begin": 26754, "end": 28366, "idx": 20}, {"begin": 28368, "end": 29090, "idx": 21}, {"begin": 29092, "end": 30509, "idx": 22}], "SectionMain": [{"begin": 1489, "end": 30509, "idx": 0}]}}