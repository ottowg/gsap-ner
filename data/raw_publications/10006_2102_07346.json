{"text": "ON THE THEORY OF IMPLICIT DEEP LEARNING: GLOBAL CONVERGENCE WITH IMPLICIT LAYERS\n\nAbstract:\nA deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation.\nIn this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.\n\nMain:\n\n\n\n1 INTRODUCTION\nA feedforward deep neural network consists of a stack of H layers, where H is the depth of the network. The value for the depth H is typically a hyperparameter and is chosen by network designers (e.g., ResNet-101 in He et al. 2016). Each layer computes some transformation of the output of the previous layer. Surprisingly, several recent studies achieved results competitive with the state-ofthe-art performances by using the same transformation for each layer with weight tying (Dabre & Fujita, 2019; Bai et al., 2019b; Dehghani et al., 2019). In general terms, the output of the l-th layer with weight tying can be written by z (l) = h(z (l\u22121) ; x, \u03b8) for l = 1, 2, . . . , H \u2212 1,\nwhere x is the input to the neural network, z (l) is the output of the l-th layer (with z (0) = x), \u03b8 represents the trainable parameters that are shared among different layers (i.e., weight tying), and z (l\u22121) \u2192 h(z (l\u22121) ; x, \u03b8) is some continuous function that transforms z (l\u22121) given x and \u03b8. With weight tying, the memory requirement does not increase as the depth H increases in the forward pass. However, the efficient backward pass to compute gradients for training the network usually requires to store the values of the intermediate layers. Accordingly, the overall computational requirement typically increases as the finite depth H increases even with weight tying.\nInstead of using a finite depth H, Bai et al. (2019a) recently introduced the deep equilibrium model that is equivalent to running an infinitely deep feedforward network with weight tying. Instead of running the layer-by-layer computation in equation ( 1), the deep equilibrium model uses rootfinding to directly compute a fixed point z * = lim l\u2192\u221e z (l) , where the limit can be ensured to exist by a choice of h. We can train the deep equilibrium model with gradient-based optimization by analytically backpropagating through the fixed point using implicit differentiation (e.g., Griewank & Walther, 2008; Bell & Burke, 2008; Christianson, 1994). With numerical experiments, Bai et al. (2019a) showed that the deep equilibrium model can improve performance over previous state-ofthe-art models while significantly reducing memory consumption.\nDespite the remarkable performances of deep equilibrium models, our theoretical understanding of its properties is yet limited. Indeed, immense efforts are still underway to mathematically understand deep linear networks, which have finite values for the depth H without weight tying (Saxe et al., 2014; Kawaguchi, 2016; Hardt & Ma, 2017; Laurent & Brecht, 2018; Arora et al., 2018; Bartlett et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020b). In deep linear networks, the function h at each layer is linear in \u03b8 and linear in x; i.e., the map (x, \u03b8) \u2192 h(z (l\u22121) ; x, \u03b8) is bilinear. Despite this linearity, several key properties of deep learning are still present in deep linear networks. For example, the gradient dynamics is nonlinear and the objective function is nonconvex. Accordingly, understanding gradient dynamics of deep linear networks is considered to be a valuable step towards the mathematical understanding of deep neural networks (Saxe et al., 2014; Arora et al., 2018; 2019a).\nIn this paper, inspired by the previous studies of deep linear networks, we initiate a theoretical study of gradient dynamics of deep equilibrium linear models as a step towards theoretically understanding general deep equilibrium models. As we shall see in Section 2, the function h at each layer is nonlinear in \u03b8 for deep equilibrium linear models, whereas it is linear for deep linear networks. This additional nonlinearity is essential to enforce the existence of the fixed point z * . The additional nonlinearity, the infinite depth, and weight tying are the three key proprieties of deep equilibrium linear models that are absent in deep linear networks. Because of these three differences, we cannot rely on the previous proofs and results in the literature of deep linear networks. Furthermore, we analyze gradient dynamics, whereas Kawaguchi ( 2016 Accordingly, we employ different approaches in our analysis and derive qualitatively and quantitatively different results when compared with previous studies. In Section 2, we provide theoretical and numerical observations that further motivate us to study deep equilibrium linear models. In Section 3, we mathematically prove convergence of gradient dynamics to global minima and the exact relationship between the gradient dynamics of deep equilibrium linear models and that of the adaptive trust region method. Section 5 gives a review of related literature, which strengthens the main motivation of this paper along with the above discussion (in Section 1). Finally, Section 6 presents concluding remarks on our results, the limitation of this study, and future research directions.\n\n2 PRELIMINARIES\nWe begin by defining the notation. We are given a training dataset ((x i , y i )) n i=1 of n samples where x i \u2208 X \u2286 R mx and y i \u2208 Y \u2286 R my are the i-th input and the i-th target output, respectively. We would like to learn a hypothesis (or predictor) from a parametric family H = {f \u03b8 : R mx \u2192 R my | \u03b8 \u2208 \u0398} by minimizing the objective function L (called the empirical loss) over \u03b8 \u2208 \u0398: L(\u03b8) = n i=1 (f \u03b8 (x i ), y i ), where \u03b8 is the parameter vector and : R my \u00d7 Y \u2192 R \u22650 is the loss function that measures the difference between the prediction f \u03b8 (x i ) and the target y i for each sample. For example, when the parametric family of interest is the class of linear models as H = {x \u2192 W \u03c6(x) | W \u2208 R my\u00d7m }, the objective function L can be rewritten as:L 0 (W ) = n i=1 (W \u03c6(x i ), y i ),\nwhere the feature map \u03c6 is an arbitrary fixed function that is allowed to be nonlinear and is chosen by model designers to transforms an input x \u2208 R mx into the desired features \u03c6(x) \u2208 R m . We use vec(W ) \u2208 R mym to represent the standard vectorization of a matrix W \u2208 R my\u00d7m .\nInstead of linear models, our interest in this paper lies on deep equilibrium models. The output z * of the last hidden layer of a deep equilibrium model is defined byz * = lim l\u2192\u221e z (l) = lim l\u2192\u221e h(z (l\u22121) ; x, \u03b8) = h(z * ; x, \u03b8),\nwhere the last equality follows from the continuity of z \u2192 h(z; x, \u03b8) (i.e., the limit commutes with the continuous function). Thus, z * can be computed by solving the equation z * = h(z * ; x, \u03b8) without running the infinitely deep layer-by-layer computation. The gradients with respect to parameters are computed analytically via backpropagation through z * using implicit differentiation.\n\n2.1 DEEP EQUILIBRIUM LINEAR MODELS\nA deep equilibrium linear model is an instance of the family of deep equilibrium models and is defined by setting the function h at each layer as follows:\nh(z (l\u22121) ; x, \u03b8) = \u03b3\u03c3(A)z (l\u22121) + \u03c6(x),\nwhere \u03b8 = (A, B) with two trainable parameter matrices A \u2208 R m\u00d7m and B \u2208 R my\u00d7m . Along with a positive real number \u03b3 \u2208 (0, 1), the nonlinear function \u03c3 is used to ensure the existence of the fixed point and is defined by \u03c3(A) ij = exp(Aij ) m k=1 exp(A kj ) . The class of deep equilibrium linear models is given by H = {x \u2192 B lim l\u2192\u221e z (l) (x, A) | A \u2208 R m\u00d7m , B \u2208 R my\u00d7m }, where z (l) (x, A) = \u03b3\u03c3(A)z (l\u22121) + \u03c6(x). Therefore, the objective function for deep equilibrium linear models can be written asL(A, B) = n i=1\nB lim l\u2192\u221e z (l) (x i , A) , y i .\n(5)\nThe outputs of deep equilibrium linear models f \u03b8 (x) = B lim l\u2192\u221e z (l) (x, A) are nonlinear and non-multilinear in the optimization variable A. This is in contrast to linear models and deep linear networks. From the optimization viewpoint, linear models W \u03c6(x) are called linear because they are linear in the optimization variables W . Deep linear networks 1) x are multilinear in the optimization variables (W (1) , W (2) , . . . , W (H) ) (this holds also when we replace x by \u03c6(x)). This difference creates a challenge in the analysis of deep equilibrium linear models.W (H) W (H\u22121) \u2022 \u2022 \u2022 W (\nFollowing previous works on gradient dynamics of different machine learning models (Saxe et al., 2014; Ji & Telgarsky, 2020), we consider the process of learning deep equilibrium linear models via gradient flow:d dt A t = \u2212 \u2202L \u2202A (A t , B t ), d dt B t = \u2212 \u2202L \u2202B (A t , B t ), \u2200t \u2265 0,\nwhere (A t , B t ) represents the model parameters at time t with an arbitrary initialization (A 0 , B 0 ). Throughout this paper, a feature map \u03c6 and a real number \u03b3 \u2208 (0, 1) are given and arbitrary (except in experimental observations) and we omit their universal quantifiers for the purpose of brevity.\n\n2.2 PRELIMINARY OBSERVATION FOR ADDITIONAL MOTIVATION\nOur analysis is chiefly motivated as a step towards mathematically understanding general deep equilibrium models (as discussed in Sections 1 and 5). In addition to the main motivation, this section provides supplementary motivations through theoretical and numerical preliminary observations.\nIn general deep equilibrium models, the limit, lim l\u2192\u221e z (l) , is not ensured to exist (see Appendix C). In this view, the class of deep equilibrium linear models is one instance where the limit is guaranteed to exist for any values of model parameters as stated in Proposition 1:Proposition 1. Given any (x, A), the sequence (z (l) (x, A)) l in Euclidean space R m converges.\nProof. We use the nonlinearity \u03c3 to ensure the convergence in our proof in Appendix A.5.\nProposition 1 shows that we can indeed define the deep equilibrium linear model with lim l\u2192\u221e z (l) = z * (x, A). Therefore, understanding this model is a sensible starting point for theory of general deep equilibrium models.\nAs our analysis has been mainly motivated for theory, it would be of additional value to discuss whether the model would also make sense in practice, at least potentially in the future. Consider an (unknown) underling data distribution P (x, y) = P (y|x)P (x). Intuitively, if the mean of the P (y|x) is approximately given by a (true unknown) deep equilibrium linear model, then it would make sense to use the parametric family of deep equilibrium linear models to have the inductive bias in practice.\nTo confirm this intuition, we conducted numerical simulations. To generate datasets, we first drew uniformly at random 200 input images for input data points x i from a standard image dataset -CIFAR-10, CIFAR-100 or Kuzushiji-MNIST (Krizhevsky & Hinton, 2009; Clanuwat et al., 2019).\nWe then generated targets asy i = B * (lim l\u2192\u221e z (l) (x i , A * )) + \u03b4 i where \u03b4 i i.i.d.\n\u223c N (0, 1). Each entry of the true (unknown) matrices A * and B * was independently drawn from the standard normal distribution. For each dataset generated in this way, we used stochastic gradient descent (SGD) to train linear models, fully-connected feedforward deep neural networks with ReLU nonlinearity As can be seen, all models preformed approximately the same at initial points, but deep equilibrium linear models outperformed both linear models and DNNs in test errors after training, confirming our intuition above. Moreover, we confirmed qualitatively same behaviors with four more datasets as well as for DNNs with and without bias terms in Appendix D. These observations additionally motivated us to study deep equilibrium linear models to obtain our main results in the next section. The purpose of these experiments is to provide a secondary motivation for our theoretical analyses.\n\n3 MAIN RESULTS\nIn this section, we establish mathematical properties of gradient dynamics for deep equilibrium linear models by directly analyzing its trajectories. We prove linear convergence to global minimum in Section 3.1 and further analyze the dynamics from the viewpoint of trust region in Section 3.2.\n\n3.1 CONVERGENCE ANALYSIS\nWe begin in Section 3.1.1 with a presentation of the concept of the Polyak-\u0141ojasiewicz (PL) inequality and additional notation. The PL inequality is used to regularize the choice of the loss functions in our main convergence theorem for a general class of losses in Section 3.1.2. We conclude in Section 3.1.3 by providing concrete examples of the convergence theorem with the square loss and the logistic loss, where the PL inequality is no longer required as the PL inequality is proven to be satisfied by these loss functions.\n\n3.1.1 THE POLYAK-\u0141OJASIEWICZ INEQUALITY AND ADDITIONAL NOTATION\nIn our context, the notion of the PL inequality is formally defined as follows:\nDefinition 1. The function L 0 is said to satisfy the Polyak-\u0141ojasiewicz (PL) inequality with radius R \u2208 (0, \u221e] and parameter \u03ba >0 if 1 2 \u2207L vec 0 (vec(W )) 2 2 \u2265 \u03ba(L vec 0 (vec(W )) \u2212 L * 0,R ) for all W 1 < R, where L vec 0 (vec(\u2022)) := L 0 (\u2022) and L * 0,R := inf W : W 1<R L 0 (W ).\nWith any radius R > 0 sufficiently large (such that it covers the domain of L 0 ), Definition 1 becomes equivalent to the definition of the PL inequality in the optimization literature (e.g., Polyak, 1963; Karimi et al., 2016). See Appendix C for additional explanations on the equivalence. In general, the non-convex objective function L of deep equilibrium linear models does not satisfy the PL inequality. Therefore, we cannot assume the inequality on L. However, in order to obtain linear convergence for a general class of the loss functions , we need some assumption on : otherwise, we can choose a loss to violate the convergence. Accordingly, we will regularize the choice of the loss through the PL inequality on the functionL 0 : W \u2192 n i=1 (W \u03c6(x i ), y i ).\nThe PL inequality with a radius R \u2208 (0, \u221e] (Definition 1) leads to the notion of the global minimum value in the domain corresponding to the radius in our analysis:L * R = inf A\u2208R m\u00d7m ,B\u2208B R L(A, B), where B R = {B \u2208 R my\u00d7m | B 1 < (1 \u2212 \u03b3)R}. With R = \u221e, this recovers the global minimum value L * in the unconstrained domain as L * R = L * := inf A\u2208R m\u00d7m ,B\u2208R my \u00d7m L(A, B). Furthermore, if a global minimum (A * , B * ) \u2208 R m\u00d7m \u00d7 R my\u00d7m exists, there exists R < \u221e such that for any R \u2208 [ R, \u221e), we have B * \u2208 B R and thus L * R = L * .\nIn other words, if a global minimum exists, using a (sufficiently large) finite radiusR < \u221e suffices to obtain L * R = L * . We close this subsection by introducing additional notation. For a real symmetric matrix M , we use \u03bb min (M ) to represent its smallest eigenvalue. For an arbitrary matrix M \u2208 R d\u00d7d , we let rank(M ) be its rank, M p be its matrix norm induced by the vector p-norm, \u03c3 min (M ) be its smallest singular value (i.e., the min(d, d )-th largest singular value), M * j be its j-th column vector in R d , and M i * be its i-th row vector in R d . For d \u2208 N >0 , we denote by I d the identify matrix in R d\u00d7d . We define the Jacobian matrix J k,t \u2208 R m\u00d7m of the vector-valued functionA * k \u2192 \u03c3(A) * k by (J k,t ) ij = \u2202\u03c3(A) ik\n\u2202A jk | A=At for all t \u2265 0 and k = 1, . . . , m. Finally, we define the feature matrix \u03a6 \u2208 R m\u00d7n by \u03a6 ki = \u03c6(x i ) k for k = 1, . . . , m and i = 1, . . . , n .\n\n3.1.2 MAIN CONVERGENCE THEOREM\nUsing the PL inequality only on the loss function through L 0 (Definition 1), we present our main theorem -a guarantee on linear convergence to global minimum for the gradient dynamics of the non-convex objective L for deep equilibrium linear models: Theorem 1. Let : R my \u00d7 Y \u2192 R \u22650 be arbitrary such that the function q \u2192 (q, y i ) is differentiable for any i \u2208 {1, . . . , n} (with an arbitrary m y \u2208 N >0 and an arbitrary Y). Then, for any T > 0, R \u2208 (0, \u221e] and \u03ba > 0 such that B t 1 < (1 \u2212 \u03b3)R for all t \u2208 [0, T ] and L 0 satisfies the PL inequality with the radius R and the parameter \u03ba, the following holds:L(A T , B T ) \u2264 L * R + L(A 0 , B 0 ) \u2212 L * 0,R e \u22122\u03ba\u03bb T T ,\nwhere \u03bb T := inf t\u2208[0,T ] \u03bb min (D t ) > 0 and D t is a positive definite matrix defined byD t := m k=1 (U \u2212 t ) * k (U \u22121 t ) k * \u2297 I my + \u03b3 2 B t U \u22121 t J k,t J k,t U \u2212T t B t ,with U t := I m \u2212 \u03b3\u03c3(A t ). Furthermore, \u03bb T \u2265 1 m(1+\u03b3) 2 for any T \u2265 0 (lim T \u2192\u221e \u03bb T \u2265 1 m(1+\u03b3) 2 ).\nProof. The additional nonlinearity \u03c3 creates a complex interaction among m hidden neurons. This interaction is difficult to be factorized out for the gradients of L with respect to A. This is different from but analogous to the challenge to deal with nonlinear activations in the loss landscape of (nonoverparameterized) deep nonlinear networks, for which previous works have made assumptions of sparse connections to factorize the interaction (Kawaguchi et al., 2019). In contrast, we do not rely on sparse connections. Instead, we observe that although it is difficult to factorize this complex interaction (due to the nonlinearity \u03c3) in the space of loss landscape, we can factorize it in the space of gradient dynamics. See Appendix A.1 for the proof overview and the complete proof.\nTheorem 1 shows that in the worst case for \u03bb T , the optimality gap decreases exponentially towards zero asL(A T , B T ) \u2212 L * R \u2264 C 0 e \u2212 2\u03ba mL(A T , B T ) \u2212 L * R \u2264 for any T \u2265 m(1 + \u03b3) 2 2\u03ba log L(A 0 , B 0 ) \u2212 L * 0,R .\nTheorem 1 also states that the rate of convergence improves further depending on the quality of the matrix D t (defined in equation ( 8)) in terms of its smallest eigenvalue over the particular trajectory (A t , B t ) up to the specific time t \u2264 T ; i.e., \u03bb T = inf t\u2208[0,T ] \u03bb min (D t ). This opens up the direction of future work for further improvement of the convergence rate through the design of initialization (A 0 , B 0 ) to maximize \u03bb T for trajectories generated from a specific initialization scheme.\n\n3.1.3 EXAMPLES: SQUARE LOSS AND LOGISTIC LOSS\nThe main convergence theorem in the previous subsection is stated for any radius R \u2208 (0, \u221e] and parameter \u03ba > 0 that satisfy the conditions on B t 1 and the PL inequality (see Theorem 1). The values of these variables are not completely specified there as they depend on the choice of the loss functions . In this subsection, we show that these values can be specified further and the condition on PL inequality can be discarded by considering a specific choice of loss functions .\nIn particular, by using the square loss for , we prove that we can set R = \u221e and \u03ba = 2\u03c3 min (\u03a6) 2 : Corollary 1. Let (q, y i ) = q \u2212 y i 2 2 where y i \u2208 R my for i = 1, 2, . . . , n (with an arbitrary m y \u2208 N >0 ). Assume that rank(\u03a6) = min(n, m). Then for any T > 0,L(A T , B T ) \u2264 L * + (L(A 0 , B 0 ) \u2212 L * 0 ) e \u22124\u03c3min(\u03a6) 2 \u03bb T T , where \u03c3 min (\u03a6) > 0, L * 0 := inf W \u2208R my \u00d7m L 0 (W ), and \u03bb T := inf t\u2208[0,T ] \u03bb min (D t ) \u2265 1 m(1+\u03b3) 2 .\nProof. This statement follows from Theorem 1. The conditions on B t 1 and the PL inequality (in Theorem 1) are now discarded by using the property of the square loss . See Appendix A.3 for the complete proof.\nIn Corollary 1, the global linear convergence is established for the square loss without the notion of the radius R as we set R = \u221e. Even with the square loss, the objective function L is non-convex. Despite the non-convexity, Corollary 1 shows that for any desired accuracy > 0,L(A T , B T ) \u2212 L * \u2264 for any T \u2265 m(1 + \u03b3) 2 4\u03c3 min (\u03a6) 2 log L(A 0 , B 0 ) \u2212 L * 0 . ()\nCorollary 1 allows both cases of m \u2264 n and m > n. In the case of over-parameterization m > n, the covariance matrix \u03a6\u03a6 \u2208 R m\u00d7m (or XX with \u03c6(x) = x) is always rank deficient because rank(\u03a6\u03a6 ) = rank(\u03a6) \u2264 n < m. This implies that the Hessian of L 0 is always rank deficient, because the Hessian of L 0 is \u2207 2 L vec 0 (vec(W )) = 2[\u03a6\u03a6 \u2297 I my ] \u2208 R mym\u00d7mym (see Appendix A.3 for its derivation) and because rank([\u03a6\u03a6 \u2297 I my ]) = rank(\u03a6\u03a6 ) rank(I my ) \u2264 m y n < m y m. Since the strong convexity on a twice differentiable function requires its Hessian to be of full rank, this means that the objective L 0 for linear models is not strongly convex in the case of overparameterization m > n. Nevertheless, we establish the linear convergence to global minimum for deep equilibrium linear models in Corollary 1 for both cases of m > n and m \u2264 n by using Theorem 1.\nFor the logistic loss for , the following corollary proves the global convergence at a linear rate:Corollary 2. Let (q, y i ) = \u2212y i log( 1 1+e \u2212q ) \u2212 (1 \u2212 y i ) log(1 \u2212 1 1+e \u2212q ) + \u03c4 q 2\n2 with an arbitrary \u03c4 \u2265 0 where y i \u2208 {0, 1} for i = 1, 2, . . . , n. Assume that rank(\u03a6) = m. Then for any T > 0 and R \u2208 (0, \u221e] such that B t 1 < (1 \u2212 \u03b3)R for all t \u2208 [0, T ], the following holds:L(A T , B T ) \u2264 L * R + L(A 0 , B 0 ) \u2212 L * 0,R e \u22122(2\u03c4 +\u03c1(R))\u03c3min(\u03a6) 2 \u03bb T T , where \u03c3 min (\u03a6) > 0, \u03bb T := inf t\u2208[0,T ] \u03bb min (D t ) \u2265 1 m(1+\u03b3) 2 , and \u03c1(R) := inf W : W 1 <R, i\u2208{1,...,n} 1 1 + e \u2212W \u03c6(xi) 1 \u2212 1 1 + e \u2212W \u03c6(xi) \u2265 0.\nProof. This statement follows from Theorem 1 by proving that the condition on PL inequality is satisfied with the parameter \u03ba = (2\u03c4 + \u03c1(R))\u03c3 min (\u03a6) 2 . See Appendix A.4 for the complete proof.\nIn Corollary 2, we can also set R = \u221e to remove the notion of the radius R from the statement of the global convergence for the logistic loss. By setting R = \u221e, Corollary 2 states that for any T > 0, L(A T , B T ) \u2264 L * + (L(A 0 , B 0 ) \u2212 L * 0 ) e \u22124\u03c4 \u03c3min(\u03a6) 2 \u03bb T T , for the logistic loss. For any \u03c4 > 0, this implies that for any desired accuracy > 0,L(A T , B T ) \u2212 L * \u2264 for any T \u2265 m(1 + \u03b3) 2 4\u03c4 \u03c3 min (\u03a6) 2 log L(A 0 , B 0 ) \u2212 L * 0 .\nIn practice, we may want to set \u03c4 > 0 to regularize the parameters (for generalization) and to ensure the existence of global minima (for optimization and identifiability). That is, if we set \u03c4 = 0 instead, the global minima may not exist in any bounded space, due to the property of the logistic loss. This is consistent with Corollary 2 in that if \u03c4 = 0, equation ( 11) does not hold and we must consider the convergence to the global minimum value L * R defined in a bounded domain with a radius R < \u221e. In the case of \u03c4 = 0 and R < \u221e, Corollary 2 implies that for desired accuracy > 0,L(A T , B T ) \u2212 L * R \u2264 for any T \u2265 m(1 + \u03b3) 2 2\u03c1(R)\u03c3 min (\u03a6) 2 log L(A 0 , B 0 ) \u2212 L * 0,R ,\nwhere we have \u03c1(R) > 0 because R < \u221e. Therefore, Corollary 2 establish the linear convergence to global minimum with both cases of \u03c4 > 0 and \u03c4 = 0 for the logistic loss.\n\n3.2 UNDERSTANDING DYNAMICS THROUGH TRUST REGION NEWTON METHOD\nIn this subsection, we analyze the dynamics of deep equilibrium linear models in the space of the hypothesis,f \u03b8t : x \u2192 B t lim l\u2192\u221e z (l) (x, A t ) .\nFor any functions g and \u1e21 with a domain X \u2286 R mx , we write g = \u1e21 if g(x) = \u1e21(x) for all x \u2208 X .\nThe following theorem shows that the dynamics of deep equilibrium linear models f \u03b8t can be written as d dt f \u03b8t = 1 \u03b4t V t \u03c6 where 1 \u03b4t is scalar and V t follows the dynamics of a trust region Newton method of shallow models with the (non-standard) adaptive trust region V t . This suggests potential benefits of deep equilibrium linear models in two aspects: when compared to shallow models, it can sometimes accelerate optimization via the effect of the implicit trust region method (but not necessarily as the trust region method does not necessarily accelerate optimization) and induces novel implicit bias for generalization via the non-standard implicit trust region V t . Theorem 2. Let : R my \u00d7 Y \u2192 R \u22650 be arbitrary such that the function q \u2192 (q, y i ) is differentiable for any i \u2208 {1, . . . , n} with m y = 1 and (an arbitrary Y). Then for any time t \u2265 0, there exist a real number \u03b4t > 0 such that for any \u03b4 t \u2208 (0, \u03b4t ],d dt f \u03b8t = 1 \u03b4 t V t \u03c6, vec(V t ) \u2208 argmin v\u2208Vt L t 0 (v),\nwhereV t := {v \u2208 R m : v Gt \u2264 \u03b4 t d dt vec(B t U \u22121 t ) Gt }, G t := U t S \u22121 t \u2212 \u03b4 t F t U t 0,L t 0 (v) := L vec 0 (vec(B t U \u22121 t )) + \u2207L vec 0 (vec(B t U \u22121 t )) v + 1 2 v \u2207 2 L vec 0 (vec(B t U \u22121 t ))v.\nHere,F t := n i=1 \u2207 2 i (f \u03b8t (x i ))(lim l\u2192\u221e z (l) (x i , A t ))(lim l\u2192\u221e z (l) (x i , A t )) with i (q) := (q, y i ) and S t := I m + \u03b3 2 diag(v S t ) with v S t \u2208 R m and (v S t ) k := J k,t (B t U \u22121 t ) 2 2 \u2200k.\nProof. This is proven with the Karush-Kuhn-Tucker (KKT) conditions for the constrained optimization problem:minimize v\u2208Vt L t 0 (v). See Appendix A.2.\nWhen many global minima exist, a difference in the gradient dynamics can lead to a significant discrepancy in the learned models: i.e., two different gradient dynamics can find significantly different global minima with different behaviors for generalization and test accuracies (Kawaguchi et al., 2017). In machine learning, this is an important phenomenon called implicit bias -inductive bias induced implicitly through gradient dynamics -and is the subject of an emerging active research area (Gunasekar et al., 2017; Soudry et al., 2018; Gunasekar et al., 2018; Woodworth et al., 2020; Moroshko et al., 2020).\n\nPublished as a conference paper at ICLR 2021\nAs can be seen in Theorem 2, the gradient dynamics of deep equilibrium linear models f \u03b8t differs from that of linear models W t \u03c6 with any adaptive learning rates, fixed preconditioners, and existing variants of Newton methods. This is consistent with our experiments in Section 2.2 and Appendix D where the dynamics of deep equilibrium linear models resulted in the learned predictors with higher test accuracies, when compared to linear models with any learning rates. In this regard, Theorem 2 provides a partial explanation (and a starting point of the theory) for the observed generalization behaviors, whereas Theorem 1 (with Corollaries 1 and 2) provides the theory for the global convergence observed in the experiments.\nTheorem 2, along with our experimental results, suggests the importance of theoretically understanding implicit bias of the dynamics with the time-dependent trust region. In Appendix B, we show that Theorem 2 suggests a new type of implicit bias towards a simple function as a result of infinite depth, whereas understanding this implicit bias in more details is left as an open problem for future work.\n\n4 EXPERIMENTS\nIn this section, we conduct experiments to further verify and demonstrate our theory. To compare with the previous findings, we use the same synthetic data as that in the previous work (Zou et al., 2020b) : i.e., we randomly generate x i \u2208 R 10 from the standard normal distribution and set y i = \u2212x i + 0.1\u03c2 i for all i \u2208 {1, 2, . . . , n} with n = 1000, where \u03c2 i is independently generated by the standard normal distribution. We set \u03c6(x) = x and use the square loss (q, y i ) = q \u2212 y i 2 2 . As in the previous work, we consider random initialization and identity initialization (Zou et al., 2020b) and report the results in Figure 2 (a). As can be seen in the figure, deep equilibrium linear models converges to the global minimum value with all initialization and random trials, whereas linear ResNet converges to a suboptimal value with identity initialization. This is consistent with our theory for deep equilibrium linear models and the previous work for ResNet (Zou et al., 2020b).\nWe repeated the same experiment by generating (x i ) k independently from the uniform distribution of the interval [\u22121, 1] instead for all i \u2208 {1, . . . , n} and k \u2208 {1, . . . , m} with n = 1000 and m = 10. Figure 2 (b) shows the results of this experiment with the uniform distribution and confirm the global convergence of deep equilibrium linear models again with all initialization and random trials. In this case, linear ResNet with identity initialization also converged to the global minimum value. These observations are consistent with Corollary 1 where deep equilibrium linear models are guaranteed to converge to the global minimum value without any condition on the initialization.\nWe now consider the rate of the global convergence. In Corollary 1, we can set \u03bb T = 1 m(1+\u03b3) 2 to get a guarantee for the global linear convergence rate for all initializations in theory. However, in practice, this is a pessimistic convergence rate and we may want to choose \u03bb T depending on a initialization. To demonstrate this, using the same data as that in Figure 2 Deep networks are also linearized implicitly in the neural tangent kernel (NTK) regime with significant over-parameterization m n (Yehudai & Shamir, 2019; Lee et al., 2019). By significantly increasing model parameters (or more concretely the width m), we can ensure deep features or corresponding NTK to stay nearly the same during training. In other words, deep networks in this regime are approximately linear models with random features corresponding to the NTK at random initialization. Because of this implicit linearization, deep networks in the NTK regime are shown to achieve globally minimum training errors by interpolating all training data points (Zou et al., 2020a; Li & Liang, 2018; Jacot et al., 2018; Du et al., 2019; 2018; Chizat et al., 2019; Arora et al., 2019b; Allen-Zhu et al., 2019; Lee et al., 2019; Fang et al., 2020; Montanari & Zhong, 2020).\nThese previous studies have significantly advanced our theoretical understanding of deep learning through the study of deep linear networks and implicitly linearized deep networks in the NTK regime. In this context, this paper is expected to contribute to the theoretical advancement through the study of a new and significantly different type of deep models -deep equilibrium linear models. In deep equilibrium linear models, the function at each layer A \u2192 h(z (l\u22121) ; x, \u03b8) is nonlinear due to the additional nonlinearity \u03c3: A \u2192 h(z (l\u22121) ; x, \u03b8) := \u03b3\u03c3(A)z (l\u22121) + \u03c6(x). In contrast, for deep linear networks, the function at each layer W (l) \u2192 h (l) (z (l\u22121) ; x, W (l) ) := W (l) z (l\u22121) is linear (it is linear also with skip connection). Furthermore, the nonlinearity \u03c3 is not an element-wise function, which poses an additional challenge in the mathematical analysis of deep equilibrium linear models. The nonlinearity \u03c3, the infinite depth, and weight tying in deep equilibrium linear models necessitated us to develop new approaches in our proofs. The differences in the models and proofs naturally led to qualitatively and quantitatively different results. For example, we do not require any of over-parameterization m n, interpolation of all training data points, and any assumptions mentioned above for deep linear networks.\nUnlike previous papers, we also related the dynamics of deep equilibrium linear models to that of a trust region Newton method of shallow models with G t -quadratic norm. This suggested potential benefits of deep equilibrium linear models. Our theory is consistent with our numerical observations.\n\n6 CONCLUSION\nFor deep equilibrium linear models, despite the non-convexity, we have rigorously proven convergence of gradient dynamics to global minima, at a linear rate, for a general class of loss functions, including the square loss and logistic loss. Moreover, we have proven the relationship between the gradient dynamics of deep equilibrium linear models and that of the adaptive trust region method. These results apply to models with any configuration on the width of hidden layers, the number of data points, and input/output dimensions, allowing rank-deficient covariance matrices as well as both under-parameterization and over-parameterization.\nThe crucial assumption for our analysis is the differentiability of the function q \u2192 (q, y i ), which is satisfied by standard loss functions, such as the square loss, the logistic loss, and the smoothed hinge loss (q, y i ) = (max{0, 1 \u2212 y i q}) k with k \u2265 2. However, it is not satisfied by the (non-smoothed) hinge loss (q, y i ) = max{0, 1 \u2212 y i q}, the treatment of which is left to future work. Future work also includes corresponding theoretical analyses with stochastic gradient descent.\nOur theoretical results (in Section 3) and numerical observations (in Section 2.2 and Appendix D) uncover the special properties of deep equilibrium linear models, providing a basis of future work for theoretical studies of implicit bias and for further empirical investigations of deep equilibrium models. In our proofs, the treatments of the additional nonlinearity \u03c3, the infinite depth, and weight tying are especially unique, and we expect our new proof techniques to be proven useful in further studies of gradient dynamics for deep models.\n\nA PROOFS\nIn this appendix, we complete the proofs of our theoretical results. We present the proofs of Theorem 1 in Appendix A.1, Theorem 2 in Appendix A.2, Corollary 1 in Appendix A.3, Corollary 2 in Appendix A.4, and Proposition 1 in Appendix A.5. We also provide a proof overview of Theorem 1 in the beginning of Appendix A.1.\nBefore starting our proofs, we first introduce additional notation used in the proofs and then discuss alternative proofs using the implicit function theorem to avoid relying on the convergence of Neumann series.\nAdditional notation. Given a scalar-valued function a \u2208 R and a matrix M \u2208 R d\u00d7d , we write\u2202a \u2202M = \uf8ee \uf8ef \uf8f0 \u2202a \u2202M11 \u2022 \u2022 \u2022 \u2202a \u2202M 1d . . . . . . . . . \u2202a \u2202M d1 \u2022 \u2022 \u2022 \u2202a \u2202M dd \uf8f9 \uf8fa \uf8fb \u2208 R d\u00d7d ,\nwhere M ij represents the (i, j)-th entry of the matrix M . Given a vector-valued function a \u2208 R d and a column vector b \u2208 R d , we write\u2202a \u2202b = \uf8ee \uf8ef \uf8ef \uf8f0 \u2202a1 \u2202b1 \u2022 \u2022 \u2022 \u2202a1 \u2202b d . . . . . . . . . \u2202a d \u2202b1 \u2022 \u2022 \u2022 \u2202a d \u2202b d \uf8f9 \uf8fa \uf8fa \uf8fb \u2208 R d\u00d7d ,\nwhere b i represents the i-th entry of the column vector b. Similarly, given a vector-valued function a \u2208 R d and a row vector b \u2208 R 1\u00d7d , we write\u2202a \u2202b = \uf8ee \uf8ef \uf8ef \uf8f0 \u2202a1 \u2202b11 \u2022 \u2022 \u2022 \u2202a1 \u2202b 1d . . . . . . . . . \u2202a d \u2202b11 \u2022 \u2022 \u2022 \u2202a d \u2202b 1d \uf8f9 \uf8fa \uf8fa \uf8fb \u2208 R d\u00d7d ,\nwhere b 1i represents the i-th entry of the row vector b. We use \u2207 A L to represent the map (A, B) \u2192 \u2202L \u2202A (A, B) (without the usual transpose used in vector calculus). Given a matrix M and a function \u03d5, we define \u2207 M \u03d5 similarly as the map M \u2192 \u2202\u03d5 \u2202M (M ). Our proofs also use the indicator function:1{i = k} = 1 if i = k 0 if i = k\nFinally, we recall the definition of the Kronecker product of two matrices: for matricesM \u2208 R d M \u00d7d M and M \u2208 R d M \u00d7d M , M \u2297 M = \uf8ee \uf8ef \uf8f0 M 11 M \u2022 \u2022 \u2022 M 1d M M . . . . . . . . . M d M 1 M \u2022 \u2022 \u2022 M d M d M M \uf8f9 \uf8fa \uf8fb \u2208 R d M d M \u00d7d M d M .\nOn alternative proofs using the implicit function theorem. In our default proofs, we utilize the Neumann series \u221e k=0 \u03b3 k \u03c3(A) k when deriving the formula of the gradients with respect to A. Instead of using the Neumann series, we can alternatively use the implicit function theorem to derive the formula of the gradients with respect to A. Specifically, in this alternative proof, we apply the implicit function theorem to the function \u03c8 defined by\u03c8(vec[A], z) = z \u2212 \u03b3\u03c3(A)z \u2212 \u03c6(x),\nwhere vec[A] and z \u2208 R m are independent variables of the function \u03c8: i.e., \u03c8(vec[A], z) is allowed to be nonzero. On the other hand, the vector z satisfying \u03c8(vec[A], z) = 0 is the fixed point z * = lim l\u2192\u221e z (l) based on equation (3). Therefore, by applying the implicit function theorem to the function \u03c8, it holds that if the the Jacobian matrix \u2202\u03c8(vec[A],z) \u2202z| z=z * is invertible, then \u2202z * \u2202 vec[A] = \u2212 \u2202\u03c8(vec[A], z) \u2202z z=z * \u22121 \u2202\u03c8(vec[A], z) \u2202 vec[A] z=z * . () Since \u2202\u03c8(vec[A],z) \u2202z z=z * = I \u2212 \u03b3\u03c3(A) is invertible, it holds that \u2202z * \u2202 vec[A] = \u2212 (I \u2212 \u03b3\u03c3(A)) \u22121 \u2202\u03c8(vec[A], z) \u2202 vec[A] z=z * . ()\nMoreover, since \u03c3(A)z \u2208 R m is a column vector,\u2202\u03c8(vec[A], z) \u2202 vec[A] z=z * = \u2212\u03b3 \u2202\u03c3(A)z \u2202 vec[A] z=z * = \u2212\u03b3 \u2202 vec[\u03c3(A)z] \u2202 vec[A] z=z * = \u2212\u03b3 \u2202[z \u2297 I m ] vec[\u03c3(A)] \u2202 vec[A] z=z * = \u2212\u03b3[(z * ) \u2297 I m ] \u2202 vec[\u03c3(A)] \u2202 vec[A] . ()\nCombining equations ( 15) and ( 16), we have\u2202z * \u2202 vec[A] = \u03b3 (I \u2212 \u03b3\u03c3(A)) \u22121 [(z * ) \u2297 I m ] \u2202 vec[\u03c3(A)] \u2202 vec[A] .\nIn our proofs, whenever we require the gradients with respect to A, we can directly use equation ( 17), instead of relying on the convergence of the Neumann series. For example, equation ( 21) in the proof of Theorem 1 is identical to equation ( 17) with additional multiplication of B q * : i.e., for the left hand side,B q * \u2202z * \u2202 vec[A] = \u2202B q * z * \u2202 vec[A] = \u2202B q * U \u22121 \u03c6(x)\n\u2202A and for the right hand side,\u03b3B q * (I \u2212 \u03b3\u03c3(A)) \u22121 [(z * ) \u2297 I m ] \u2202 vec[\u03c3(A)] \u2202 vec[A] = \u03b3 \u2202\u03c3(A) * 1 \u2202A * 1 (B q * U \u22121 ) \u03c6(x) (U \u2212 ) * 1 \u2022 \u2022 \u2022 \u2202\u03c3(A) * m \u2202A * m (B q * U \u22121 ) \u03c6(x) (U \u2212 ) * m A.1 PROOF OF THEOREM 1\nWe begin with a proof overview of Theorem 1. We first compute the derivatives of the output of deep equilibrium linear models with respect to the parameters A in Appendix A.1.1. Then using the derivatives, we rearrange the formula of \u2207 A L such that it is related to the formula of \u2207L 0 in Appendices A.1.1-A.1.3. Intuitively, we then want to understand \u2207 A L through the property of \u2207L 0 , similarly to the landscape analyses of deep linear networks by Kawaguchi (2016). However, we note there that the additional nonlinearity \u03c3 creates a complex interaction over the dimension m to prevent us from using such a proof approach. Instead, using the proven relation of \u2207 A L and \u2207L 0 from Appendices A.1.1-A.1.3, we directly analyze the trajectories of the dynamics over time t in Appendices A.1.4-A.1.5, which results in a partial factorization of the iteration over the dimension m. Using such a partial factorization, we derive the linear convergence rate in Appendices A.1.6-A.1.7 by using the PL inequality and the properties of induced norms.\nBefore getting into the details of the proof, we now briefly discuss the property of our proof in terms of the tightness of a bound. In the condition of B t 1 < (1 \u2212 \u03b3)R in the statement of Theorem 1, the quantity (1 \u2212 \u03b3) comes from the proof in Appendix A.1.7: i.e., it is the reciprocal of the quantity1 1\u2212\u03b3 in the upper bound of (I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 1 1\u2212\u03b3 .\nTherefore, a natural question is whether or not we can improve this bound further. This bound turns out to be tight based on the following lower bound. The matrix I m \u2212 \u03b3\u03c3(A) is a Z-matrix since off-diagonal entries are less than or equal to zero. Furthermore, I m \u2212 \u03b3\u03c3(A) is M -matrix since eigenvalues of I m \u2212 \u03b3\u03c3(A) are the eigenvalues of I m \u2212 \u03b3\u03c3(A) and the eigenvalues of I \u2212 \u03b3\u03c3(A) are lower bounded by 1 \u2212 \u03b3 > 0. This is because \u03c3(A) is a stochastic matrix with the largest eigenvalue being one. Moreover, in the proof in Appendix A.1.7, we showed that |I \u2212\u03b3\u03c3(A)| jj \u2212 i =j |I \u2212\u03b3\u03c3(A)| ij = 1\u2212\u03b3 for all j. Therefore, using the lower bound by Mora\u010da (2008), we have(I \u2212 \u03b3\u03c3(A)) \u22121 1 \u2265 1 max j (|I \u2212 \u03b3\u03c3(A)| jj \u2212 i =j |I \u2212 \u03b3\u03c3(A)| ij ) = 1 1 \u2212 \u03b3 ,\nwhich matches with the upper bound of (I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 1 1\u2212\u03b3 . Therefore, we cannot further improve the our bound on B t 1 in general without making some additional assumption.\n\nA.1.1 REARRANGING THE FORMULA OF \u2207 A L\nWe will use the following facts for matrix calculus (that can be derived by using definition of derivatives: e.g., see Barnes, 2006) :\u2202M \u22121 \u2202a = \u2212M \u22121 \u2202M \u2202a M \u22121 \u2202a M \u22121 b \u2202M = \u2212M \u2212 ab M \u2212 \u2202g(M ) \u2202a = i j \u2202g(M ) \u2202M ij \u2202M ij \u2202a \u2202g(a) \u2202M = \u2202g(a) \u2202a \u2202a \u2202M\nRecall that U = I \u2212 \u03b3\u03c3(A). From the above facts, given a function g, we have\u2202g(U ) \u2202A kl = m i=1 m j=1 \u2202g(U ) \u2202U ij \u2202U ij \u2202A kl = m i=1 m j=1 \u2202g(U ) \u2202U ij \u2202U ij \u2202\u03c3(A) ij \u2202\u03c3(A) ij \u2202A kl = \u2212\u03b3 m i=1 m j=1 \u2202g(U ) \u2202U ij \u2202\u03c3(A) ij \u2202A kl . ()\nUsing the quotient rule,\u2202\u03c3(A) ij \u2202A kl = \u2202 \u2202A kl exp(A ij ) t exp(A tj ) = ( \u2202 exp(Aij ) \u2202A kl )( t exp(A tj )) \u2212 exp(A ij )( \u2202 t exp(Atj ) \u2202A kl ) ( t exp(A tj )) 2 = 1{i = k}1{j = l} exp(A ij )( t exp(A tj )) \u2212 1{j = l} exp(A ij ) exp(A kj ) ( t exp(A tj )) 2 = 1{i = k}1{j = l} exp(A ij ) t exp(A tj ) \u2212 1{j = l} exp(A ij ) exp(A kj ) ( t exp(A tj )) 2 = 1{i = k}1{j = l}\u03c3(A) ij \u2212 1{j = l} exp(A ij ) t exp(A tj ) exp(A kj ) t exp(A tj ) = 1{j = l}1{i = k}\u03c3(A) ij \u2212 1{j = l}\u03c3(A) ij \u03c3(A) kj .\nThus, \u2202g(U )\u2202A kl = \u2212\u03b3 m i=1 \u2202g(U ) \u2202U il \u2202\u03c3(A) il \u2202A kl = \u2212\u03b3 \u2202g(U ) \u2202U * l \u2202\u03c3(A) * l \u2202A kl \u2208 R,\nwhere \u2202g(U ) \u2202U * l \u2208 R m\u00d71 and \u2202\u03c3(A) * l \u2202A kl \u2208 R m\u00d71 . This yields\u2202g(U ) \u2202A * l = \u2212\u03b3 \u2202g(U ) \u2202U * l \u2202\u03c3(A) * l \u2202A * l \u2208 R 1\u00d7m ,\nwhere \u2202\u03c3(A) * l \u2202A * l \u2208 R m\u00d7m . Now we want to set g(U ) to be the output of deep equilibrium linear models as g(U ) = B q * lim l\u2192\u221e z (l) (x, A) for each q \u2208 {1, . . . , m y }. To do this, we first simplify the formula of the output B q * lim l\u2192\u221e z (l) (x, A) using the following:(I m \u2212 \u03b3\u03c3(A)) l k=0 \u03b3 k \u03c3(A) k = I m \u2212 \u03b3\u03c3(A) + \u03b3\u03c3(A) \u2212 (\u03b3\u03c3(A)) 2 + (\u03b3\u03c3(A)) 2 \u2212 (\u03b3\u03c3(A)) 3 + \u2022 \u2022 \u2022 \u2212 (\u03b3\u03c3(A)) l+1 = I \u2212 (\u03b3\u03c3(A)) l+1 .\nTherefore,(I m \u2212 \u03b3\u03c3(A)) lim l\u2192\u221e z (l) (x, A) = lim l\u2192\u221e (I m \u2212 \u03b3\u03c3(A)) l k=0 \u03b3 k \u03c3(A) k \u03c6(x) = I m \u2212 lim l\u2192\u221e (\u03b3\u03c3(A)) l+1 \u03c6(x) = \u03c6(x)\nwhere the first line, the second line and the last line used the fact that \u03b3\u03c3(A) ij \u2265 0,\u03c3(A) 1 = max j i |\u03c3(A) ij | = 1,\nand hence \u03b3\u03c3(A) 1 < 1 for \u03b3 \u2208 (0, 1). This shows that B lim l\u2192\u221e z (l) (x, A) = BU \u22121 \u03c6(x), where the inverse U \u22121 exists as the corresponding Neumann series converges \u221e k=0 \u03b3 k \u03c3(A) k since \u03b3\u03c3(A) 1 < 1. Therefore, we can now set g(U ) = B q * lim l\u2192\u221e z (l) (x, A) = B q * U \u22121 \u03c6(x).\n\nThen, using\n\u2202a M \u22121 b \u2202M = \u2212M \u2212 ab M \u2212 , \u2202g(U ) \u2202U = \u2202B q * U \u22121 \u03c6(x) \u2202U = \u2212U \u2212 (B q * ) \u03c6(x) U \u2212 , which implies that \u2202B q * U \u22121 \u03c6(x) \u2202U * l = \u2212(U \u2212 (B q * ) \u03c6(x) U \u2212 ) * l = \u2212U \u2212 (B q * ) \u03c6(x) (U \u2212 ) * l \u2208 R m\u00d71 .Combining ( 18) and (20),\u2202g(U ) \u2202A * l = \u2202B q * U \u22121 \u03c6(x) \u2202A * l = \u2212\u03b3 \u2202g(U ) \u2202U * l \u2202\u03c3(A) * l \u2202A * l = \u03b3 U \u2212 (B q * ) \u03c6(x) (U \u2212 ) * l \u2202\u03c3(A) * l \u2202A * l = \u03b3((U \u2212 ) * l ) \u03c6(x)B q * U \u22121 \u2202\u03c3(A) * l \u2202A * l = \u03b3(U \u22121 ) l * \u03c6(x)B q * U \u22121 \u2202\u03c3(A) * l \u2202A * l \u2208 R 1\u00d7m ,\nwhere we used (U \u2212 ) * l = ((U \u22121 ) ) * l = ((U \u22121 ) l * ) and ((U \u2212 ) * l ) = (((U \u22121 ) l * ) ) = (U \u22121 ) l * . By taking transpose,\u2202B q * U \u22121 \u03c6(x) \u2202A * l = \u03b3 \u2202\u03c3(A) * l \u2202A * l (B q * U \u22121 ) \u03c6(x) (U \u2212 ) * l \u2208 R m\u00d71 .\nBy rearranging this to the matrix form,\u2202B q * U \u22121 \u03c6(x) \u2202A (21) = \u03b3 \u2202\u03c3(A) * 1 \u2202A * 1 (B q * U \u22121 ) \u03c6(x) (U \u2212 ) * 1 \u2022 \u2022 \u2022 \u2202\u03c3(A) * m \u2202A * m (B q * U \u22121 ) \u03c6(x) (U \u2212 ) * m ,\nwhere \u2202Bq * U \u22121 \u03c6(x) \u2202A \u2208 R m\u00d7m . Each entry of this matrix represents the derivatives of the model output with respect to the parameters A. We now use this to rearrange \u2207 A L(A, B) := \u2202L(A,B) \u2202A . We set \u0177iq = B q * U \u22121 \u03c6(x) and \u0177i = BU \u22121 \u03c6(x) and defineJ k := \u2202\u03c3(A) * k \u2202A * k \u2208 R m\u00d7m ,Q := n i=1 \u2202 (\u0177 i , y i ) \u2202 \u0177i \u03c6(x i ) \u2208 R my\u00d7m .\nThen, using the chain rule and the above formula of\u2202Bq * U \u22121 \u03c6(x) \u2202A , \u2202L(A, B) \u2202A = n i=1 my q=1 \u2202 (\u0177 i , y i ) \u2202 \u0177iq \u2202 \u0177iq \u2202A = \u03b3 n i=1 my q=1 \u2202 (\u0177 i , y i ) \u2202 \u0177iq J 1 (B q * U \u22121 ) \u03c6(x i ) (U \u2212 ) * 1 \u2022 \u2022 \u2022 J m (B q * U \u22121 ) \u03c6(x i ) (U \u2212 ) * m = \u03b3 n i=1 J 1 ( my q=1 (B q * U \u22121 ) \u2202 (\u0177i,yi) \u2202 \u0177iq )\u03c6(x i ) (U \u2212 ) * 1 \u2022 \u2022 \u2022 J m ( my q=1 (B q * U \u22121 ) \u2202 (\u0177i,yi) \u2202 \u0177iq )\u03c6(x i ) (U \u2212 ) * m = \u03b3 n i=1 J 1 ( my q=1 ((BU \u22121 ) ) * q \u2202 (\u0177i,yi) \u2202 \u0177iq )\u03c6(x i ) (U \u2212 ) * 1 \u2022 \u2022 \u2022 J m ( my q=1 ((BU \u22121 ) ) * q \u2202 (\u0177i,yi) \u2202 \u0177iq )\u03c6(x i ) (U \u2212 ) * m = \u03b3 n i=1 J 1 (BU \u22121 ) ( \u2202 (\u0177i,yi) \u2202 \u0177i ) \u03c6(x i ) (U \u2212 ) * 1 \u2022 \u2022 \u2022 J m (BU \u22121 ) ( \u2202 (\u0177i,yi) \u2202 \u0177i ) \u03c6(x i ) (U \u2212 ) * m = \u03b3 \u2202\u03c3(A) * 1 \u2202A * 1 (BU \u22121 ) Q(U \u2212 ) * 1 \u2022 \u2022 \u2022 \u2202\u03c3(A) * m \u2202A * m (BU \u22121 ) Q(U \u2212 ) * m\nSummarizing the above, we have that\u2207 A L(A, B) := \u2202L(A, B) \u2202A = \u03b3 J 1 (BU \u22121 ) Q(U \u2212 ) * 1 \u2022 \u2022 \u2022 J m (BU \u22121 ) Q(U \u2212 ) * m ,\nwhere\u2207 A L(A, B) \u2208 R m\u00d7m .\nA.1.2 REARRANGING THE FORMULA OF \u2207L 0\nIn order to relate L 0 to the gradient dynamics of L, we now rearrange the formula of \u2207L 0 . We set \u0177iq = W q * \u03c6(x i ) \u2208 R and \u0177i = W \u03c6(x i ) \u2208 R my for linear models. Then, by the chain rule,\u2202L 0 (W ) \u2202W = n i=1 my q=1 \u2202 (\u0177 i , y i ) \u2202 \u0177iq \u2202 \u0177iq \u2202W . Since \u2202 \u0177iq \u2202W k * = 1{k = q}\u03c6(x i ) , we have \u2202L 0 (W ) \u2202W k * = n i=1 \u2202 (\u0177 i , y i ) \u2202 \u0177ik \u2202 \u0177ik \u2202W k * = n i=1 \u2202 (\u0177 i , y i ) \u2202 \u0177ik \u03c6(x i ) .\nBy rearranging this into the matrix form,\u2202L 0 (W ) \u2202W = \uf8ee \uf8ef \uf8ef \uf8f0 n i=1 \u2202 (\u0177i,yi) \u2202 \u0177i1 \u03c6(x i ) . . . n i=1 \u2202 (\u0177i,yi) \u2202 \u0177imy \u03c6(x i ) \uf8f9 \uf8fa \uf8fa \uf8fb = n i=1 \uf8ee \uf8ef \uf8ef \uf8f0 \u2202 (\u0177i,yi) \u2202 \u0177i1 \u03c6(x i ) . . . \u2202 (\u0177i,yi) \u2202 \u0177imy \u03c6(x i ) \uf8f9 \uf8fa \uf8fa \uf8fb = n i=1 \uf8ee \uf8ef \uf8ef \uf8f0 \u2202 (\u0177i,yi) \u2202 \u0177i1 . . . \u2202 (\u0177i,yi) \u2202 \u0177imy \uf8f9 \uf8fa \uf8fa \uf8fb \u03c6(x i ) = n i=1 \u2202 (\u0177 i , y i ) \u2202 \u0177i \u03c6(x i ) \u2208 R my\u00d7m where \u2202 (\u0177i,yi) \u2202 \u0177i \u2208 R 1\u00d7my . Thus, \u2207L 0 (W ) := \u2202L(W ) \u2202W = n i=1 \u2202 (\u0177 i , y i ) \u2202 \u0177i \u03c6(x i ) \u2208 R my\u00d7m .\nA.1.3 COMBINING THE FORMULA OF \u2207 A L AND \u2207L 0\nCombining ( 22) and ( 23) by resolving the different definitions of \u0177i yields that\u2207 A L(A, B) (24) = \u03b3 J 1 (BU \u22121 ) \u2207L 0 (BU \u22121 )(U \u2212 ) * 1 \u2022 \u2022 \u2022 J m (BU \u22121 ) \u2207L 0 (BU \u22121 )(U \u2212 ) * m .\nHere, if there is no additional nonlinearity \u03c3, the matricesJ k = \u2202\u03c3(A) * k \u2202A * k\nbecome identity for all k. In that case, \u2207 A L(A, B) can be further simplified and factorize over m, which is desired for the analysis of gradient dynamics. However, due to the additional nonlinearity, we cannot factorize \u2207 A L(A, B) over m. One of the key techniques in our analysis is to keep this un-factorized \u2207 A L(A, B) and find a way to factorize it during the update of parameters (A t , B t ) in the gradient dynamics, as shown later in this proof. To do so, we now start considering the dynamics over time t.\nA.1.4 ANALYSING lim l\u2192\u221e z (l) (x, A t ) Now let us temporarily consider a gradient dynamics discretized by the Euler method asA t+1 = A t \u2212 \u03b1\u2207 A L(A t , B t ),\nwith some step size \u03b1 > 0. Then,lim l\u2192\u221e z (l) (x, A t+1 ) = (I m \u2212 \u03b3\u03c3(A t+1 )) \u22121 \u03c6(x) = (I m \u2212 \u03b3\u03c3(A t \u2212 \u03b1\u2207 A L(A t , B t ))) \u22121 \u03c6(x),\nwhere we used lim l\u2192\u221e z (l) (x, A t ) = U \u22121 t \u03c6(x) from Section A.1.1. By setting\u03d5 ij (\u03b1) = \u03c3(A \u2212 \u03b1\u2207 A L(A, B)) ij \u2208 R, \u03c3(A \u2212 \u03b1\u2207 A L(A, B)) ij = \u03d5 ij (\u03b1) = \u03d5 ij (0) + \u2202\u03d5 ij (0) \u2202\u03b1 \u03b1 + O(\u03b1 2 ).\n\nBy using the chain rule and setting\nM = A \u2212 \u03b1\u2207 A L(A, B) \u2208 R n\u00d7n , \u2202\u03d5 ij (\u03b1) \u2202\u03b1 = m k=1 m l=1 \u2202\u03c3(M ) ij \u2202M kl \u2202M kl \u2202\u03b1 = \u2212 m k=1 m l=1 [1{j = l}1{i = k}\u03c3(M ) ij \u2212 1{j = l}\u03c3(M ) ij \u03c3(M ) kj ] \u2207 A L(A, B) kl = \u2212 m k=1 [1{i = k}\u03c3(M ) ij \u2212 \u03c3(M ) ij \u03c3(M ) kj ]\u2207 A L(A, B) kj Therefore, \u2202\u03d5 ij (0) \u2202\u03b1 = \u2212 m k=1 [1{i = k}\u03c3(A) ij \u2212 \u03c3(A) ij \u03c3(A) kj ]\u2207 A L(A, B) kj = \u2212 m k=1 \u2202\u03c3(A) ij \u2202A kj \u2207 A L(A, B) kj = \u2212 \u2202\u03c3(A) ij \u2202A * j \u2207 A L(A, B) * j \u2208 R.Recalling the definition ofJ k := \u2202\u03c3(A) * k \u2202A * k \u2208 R m\u00d7m , \u2202\u03d5 * j (0) \u2202\u03b1 = \u2212 \u2202\u03c3(A) * j \u2202A * j \u2207 A L(A, B) * j = \u2212J j \u2207 A L(A, B) * j \u2208 R m\u00d71\nRearranging it into the matrix form,\u2202\u03d5(0) \u2202\u03b1 = \u2212 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] \u2208 R m\u00d7m\nPutting the above equations together,\u03c3(A \u2212 \u03b1\u2207 A L(A, B)) = \u03d5(0) + \u03b1 \u2202\u03d5(0) \u2202\u03b1 + O(\u03b1 2 ) = \u03c3(A) \u2212 \u03b1 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] + O(\u03b1 2 ).\nThus,[I m \u2212 \u03b3\u03c3(A \u2212 \u03b1\u2207 A L (A, B))] \u22121 = I m \u2212 \u03b3 \u03c3(A) \u2212 \u03b1 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] + O(\u03b1 2 ) \u22121 = I m \u2212 \u03b3\u03c3(A) + \u03b3\u03b1 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] + O(\u03b1 2 ) \u22121 = U + \u03b1\u03b3 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] + O(\u03b1 2 ) \u22121 . By setting M = [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] and \u03d5(\u03b1) = [U + \u03b1\u03b3M + o(\u03b1 2 )] \u22121 and by using \u2202 M \u22121 \u2202a = \u2212 M \u22121 \u2202 M \u2202a M \u22121 , [I \u2212 \u03b3\u03c3(A \u2212 \u03b1\u2207 A L(A, B))] \u22121 = [U + \u03b1\u03b3M + O(\u03b1 2 )] \u22121 = \u03d5(\u03b1) = \u03d5(0) + \u2202\u03d5(0) \u2202\u03b1 \u03b1 + O(\u03b1 2 ) = U \u22121 \u2212 \u03b1\u03b3U \u22121 M U \u22121 + 2\u03b1O(\u03b1) + O(\u03b1 2 ) = U \u22121 \u2212 \u03b1\u03b3U \u22121 M U \u22121 + O(\u03b1 2 )\nSummarizing above,[I m \u2212 \u03b3\u03c3(A \u2212 \u03b1\u2207 A L(A, B))] \u22121 (25) = U \u22121 \u2212 \u03b1\u03b3U \u22121 [J 1 \u2207 A L(A, B) * 1 \u2022 \u2022 \u2022 J m \u2207 A L(A, B) * m ] U \u22121 + O(\u03b1 2 ) A.1.\n\n5 PUTTING RESULTS TOGETHER FOR INDUCED DYNAMICS\nWe now consider the dynamics ofZ t := B t U \u22121 t in R my\u00d7m that is induced by the gradient dynamics of (A t , B t ): d dt A t = \u2212 \u2202L \u2202A (A t , B t ), d dt B t = \u2212 \u2202L \u2202B (A t , B t ), \u2200t \u2265 0.\nContinuing the previous subsection, we first consider the dynamics discretized by the Euler method:Z t+1 := B t+1 U \u22121 t+1 = [B t \u2212 \u03b1\u2207 B L(A t , B t )][I m \u2212 \u03b3\u03c3(A t \u2212 \u03b1\u2207 A L(A t , B t ))] \u22121\n, where \u03b1 > 0. Then, substituting (25) into the right-hand side of this equation,Z t+1 = B t+1 [U \u22121 t \u2212 \u03b1\u03b3U \u22121 t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t + O(\u03b1 2 )] = B t+1 U \u22121 t \u2212 \u03b1\u03b3B t+1 U \u22121 t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t + O(\u03b1 2 ). Using B t+1 = [B t \u2212 \u03b1\u2207 B L(A t , B t )],\nwe have 24), we have that\u03b1\u03b3B t+1 U \u22121 t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t = \u03b1\u03b3B t U \u22121 t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t + O(\u03b1 2 ) = \u03b1\u03b3Z t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t + O(\u03b1 2 ). Since (U \u2212 ) * k = ((U \u22121 ) ) * k = ((U \u22121 ) k * ) , U \u22121 = \uf8ee \uf8ef \uf8f0 (U \u22121 ) 1 * . . . (U \u22121 ) m * \uf8f9 \uf8fa \uf8fb and \u2207 A L(A, B) * k = \u2202L(A,B) \u2202A * k = \u03b3J k (BU \u22121 ) \u2207L 0 (BU \u22121 )(U \u2212 ) * k from (Z t [J 1,t \u2207 A L(A t , B t ) * 1 \u2022 \u2022 \u2022 J m,t \u2207 A L(A t , B t ) * m ] U \u22121 t = \u03b3Z t J 1,t J 1,t Z t \u2207L 0 (Z t )(U \u2212 t ) * 1 \u2022 \u2022 \u2022 J m,t J m,t Z t \u2207L 0 (Z t )(U \u2212 t ) * m U \u22121 t = \u03b3Z t J 1,t J 1,t Z t \u2207L 0 (Z t )(U \u2212 t ) * 1 \u2022 \u2022 \u2022 J m,t J m,t Z t \u2207L 0 (Z t )(U \u2212 t ) * m \uf8ee \uf8ef \uf8f0 (U \u22121 t ) 1 * . . . (U \u22121 t ) m * \uf8f9 \uf8fa \uf8fb = m k=1 Z t J k,t J k,t Z t \u2207L 0 (Z t )((U \u22121 t ) k * ) (U \u22121 t ) k * = m k=1 Z t J k,t J k,t Z t \u2207L(Z t )(U \u2212 t ) * k (U \u22121 t ) k *\nOn the other hand, usingB t+1 = [B t \u2212 \u03b1\u2207 B L(A t , B t )] and \u2207 B L(A, B) := \u2202L(A,B) \u2202B = n i=1 \u2202 (\u0177i,yi) \u2202 \u0177i \u03c6(x i ) U \u2212 = \u2207L 0 (BU \u22121 )U \u2212 , we have B t+1 U \u22121 t = Z t \u2212 \u03b1\u2207L 0 (Z t )U \u2212 t U \u22121 t . Summarizing these equations by noticing U \u2212 U \u22121 = m k=1 (U \u2212 ) * k (U \u22121 ) k * yields that Z t+1 = Z t \u2212 \u03b1\u2207L 0 (Z t ) m k=1 (U \u2212 t ) * k (U \u22121 t ) k * \u2212 \u03b1\u03b3 2 m k=1 Z t J k,t J k,t Z t \u2207L 0 (Z t )(U \u2212 t ) * k (U \u22121 t ) k * + O(\u03b1 2 ) = Z t \u2212 \u03b1 m k=1 \u2207L 0 (Z t )(U \u2212 t ) * k (U \u22121 t ) k * + \u03b3 2 Z t J k,t J k,t Z t \u2207L 0 (Z t )(U \u2212 t ) * k (U \u22121 t ) k * + O(\u03b1 2 ) = Z t \u2212 \u03b1 m k=1 (I my + \u03b3 2 Z t J k,t J k,t Z t )\u2207L 0 (Z t )(U \u2212 t ) * k (U \u22121 t ) k * + O(\u03b1 2 )\nBy vectorizing both sides,vec(Z t+1 ) = vec(Z t ) \u2212 \u03b1 m k=1 vec (I my + \u03b3 2 Z t J k,t J k,t Z t )\u2207L 0 (Z t )(U \u2212 t ) * k (U \u22121 t ) k * + O(\u03b1 2 ) = vec(Z t ) \u2212 \u03b1 m k=1 [(U \u2212 t ) * k (U \u22121 t ) k * \u2297 (I my + \u03b3 2 Z t J k,t J k,t Z t )] vec(\u2207L 0 (Z t )) + O(\u03b1 2 )\nBy definingD t := m k=1 [(U \u2212 t ) * k (U \u22121 t ) k * \u2297 (I my + \u03b3 2 Z t J k,t J k,t Z t )],\nwe havevec(Z t+1 ) = vec(Z t ) \u2212 \u03b1D t vec(\u2207L 0 (Z t )) + O(\u03b1 2 ). This implies that vec(Z t+1 ) \u2212 vec(Z t ) \u03b1 = \u2212D t vec(\u2207L 0 (Z t )) + O(\u03b1),\nwhere \u03b1 > 0. By recalling the definition of the Euler method and defining Z(t) = Z t , we can rewrite this as vec(Z(t + \u03b1)) \u2212 vec(Z(t)) \u03b1 = \u2212D t vec(\u2207L 0 (Z t )) + O(\u03b1).\nBy taking the limit for \u03b1 \u2192 0 and going back to continuous-time dynamics, this implies thatd dt vec(Z t ) = \u2212D t vec(\u2207L 0 (Z t )).\nHere, we note that the complex interaction over m due to the nonlinearity is factorized out into the matrix D t . Furthermore, the interaction within the matrix D t has more structures when compared with that in the gradients themselves from (24). For example, unlike the gradients, the interaction over m even within D t can be factorized out in the case of m y = 1 as:D = m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 I my + \u03b3 2 ZJ k J k Z = m k=1 1 + \u03b3 2 ZJ k J k Z (U \u2212 ) * k (U \u22121 ) k * = U \u2212 diag \uf8eb \uf8ec \uf8ed \uf8ee \uf8ef \uf8f0 1 + \u03b3 2 ZJ 1 J 1 Z . . . 1 + \u03b3 2 ZJ m J m Z \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 U \u22121 = U \u2212 \uf8eb \uf8ec \uf8edIm + diag \uf8eb \uf8ec \uf8ed \uf8ee \uf8ef \uf8f0 \u03b3 2 ZJ 1 J 1 Z . . . \u03b3 2 ZJ m J m Z \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f8 U \u22121 .\nAlthough we do not assume m y = 1, this illustrates the additional structure well.\n\nA.1.6 ANALYSIS OF THE MATRIX D t\nFrom the definition of D t , we have thatD t = m k=1 (U \u2212 t ) * k (U \u22121 t ) k * \u2297 I my + \u03b3 2 Z t J k,t J k,t Z t = m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 I my + m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t = m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 I my + m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t = U \u2212 U \u22121 \u2297 I my + m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t (27)\nSince U \u2212 U \u22121 is positive definite, I my is positive definite, and a Kronecker product of two positive definite matrices is positive definite (since the eigenvalues of Kronecker product are the products of eigenvalues of the two matrices), we haveU \u2212 U \u22121 \u2297 I my 0. (28) Since (U \u2212 ) * k (U \u22121 ) k * is positive semidefinite, \u03b3 2 Z t J k,t J k,t Z t is positive semidefinite,\n\nand a\nKronecker product of two positive semidefinite matrices is positive semidefinite (since the eigenvalues of Kronecker product are the products of eigenvalues of the two matrices), we have(U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t 0.\nSince a sum of positive semidefinite matrices is positive semidefinite (from the definition of positive semi-definiteness:x M k x \u2265 0 \u21d2 x ( k M k )x = k x M k x \u2265 0), m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t 0. ()\nSince a sum of a positive definite matrix and positive semidefinite matrix is positive definite (from the definition of positive definiteness and positive definiteness:(x M 1 x > 0 \u2227 x M 2 x) \u21d2 x (M 1 + M 2 )x = x M 1 x + x M 2 x > 0), D t = m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 I my + \u03b3 2 Z t J k,t J k,t Z t = U \u2212 U \u22121 \u2297 I my + m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 \u03b3 2 Z t J k,t J k,t Z t 0.\nTherefore, D t is a positive definite matrix for any t and hence\u03bb T := inf t\u2208[0,T ] \u03bb min (D t ) > 0.\nA.1.7 CONVERGENCE RATE VIA POLYAK-\u0141OJASIEWICZ INEQUALITY AND NORM BOUNDS Let R \u2208 (0, \u221e] and T > 0 be arbitrary. By taking derivative of L 0 (Z t ) \u2212 L * 0,R with respect to time t withZ t := B t U \u22121 t , d dt L 0 (Z t ) \u2212 L * 0,R = \uf8eb \uf8ed my i=1 m j=1 dL 0 dW (Z t ) ij d dt (Z t ) ij \uf8f6 \uf8f8 \u2212 d dt L * , = my i=1 m j=1 dL 0 dW (Z t ) ij d dt (Z t ) ij\nwhere we used the chain rule and the fact that d dt L * 0,R = 0. By using the vectorization notation with\u2207L 0 (Z t ) = dL0 dW (Z t ), d dt L 0 (Z t ) \u2212 L * 0,R = vec [\u2207L 0 (Z t )] vec d dt (Z t ) ,\nBy using ( 26) for the equation ofvec d dt (Z t ) , d dt L 0 (Z t ) \u2212 L * 0,R = \u2212 vec [\u2207L 0 (Z t )] D t vec[\u2207L 0 (Z t )] \u2264 \u2212\u03bb min (D t ) vec [\u2207L 0 (Z t )] 2 2 = \u2212\u03bb min (D t ) \u2207L 0 (Z t ) 2 F\nUsing the condition that \u2207L 0 satisfies the the Polyak-\u0141ojasiewicz inequality with radius R, if Z t 1 < R, then we have that for all t \u2208 [0, T ],d dt L 0 (Z t ) \u2212 L * 0,R \u2264 \u22122\u03ba\u03bb min (D t )(L 0 (Z t ) \u2212 L * 0,R ) \u2264 \u22122\u03ba\u03bb T (L 0 (Z t ) \u2212 L * 0,R ). By solving the differential equation, this implies that if Z t 1 < R, L 0 (Z T ) \u2212 L * 0,R \u2264 L 0 (Z 0 ) \u2212 L * 0,R e \u22122\u03ba\u03bb T T , Since L(A t , B t ) = L 0 (Z t ), if Z t 1 < R, L(A T , B T ) \u2264 L * 0,R + (L(A 0 , B 0 ) \u2212 L * 0,R )e \u22122\u03ba\u03bb T T . ()\nWe now complete the proof of the first part of the desired statement by showing that B1 < (1 \u2212 \u03b3)R implies Z t 1 < R. With Z = BU \u22121 , since any induced operator norm is a submultiplicative matrix norm, Z 1 = B(I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 B 1 (I m \u2212 \u03b3\u03c3(A)) \u22121 1 . We can then rewrite (I m \u2212 \u03b3\u03c3(A)) \u22121 1 = ((I m \u2212 \u03b3\u03c3(A)) \u22121 ) \u221e = (I m \u2212 \u03b3\u03c3(A) ) \u22121 \u221e .\nHere, the matrix I m \u2212\u03b3\u03c3(A) is strictly diagonally dominant: i.e., |I m \u2212\u03b3\u03c3(A) | ii > j =i |I m \u2212 \u03b3\u03c3(A)| ij for any i. This can be shown as follows: for any j,1 > \u03b3 \u21d0\u21d2 1 > \u03b3 i \u03c3(A) ij \u21d0\u21d2 1 > \u03b3\u03c3(A) jj + i =j \u03b3\u03c3(A) ij \u21d0\u21d2 1 \u2212 \u03b3\u03c3(A) jj > i =j \u03b3\u03c3(A) ij \u21d0\u21d2 |I m \u2212 \u03b3\u03c3(A)| jj > i =j | \u2212 \u03b3\u03c3(A)| ij \u21d0\u21d2 |I m \u2212 \u03b3\u03c3(A)| jj > i =j |I m \u2212 \u03b3\u03c3(A)| ij \u21d0\u21d2 |I m \u2212 \u03b3\u03c3(A) | jj > i =j |I m \u2212 \u03b3\u03c3(A) | ji This calculation also shows that |I m \u2212\u03b3\u03c3(A)| jj \u2212 i =j |I m \u2212\u03b3\u03c3(A)| ij = 1\u2212\n\u03b3 for all j. Thus, using the Ahlberg-Nilson-Varah bound for the strictly diagonally dominant matrix (Ahlberg & Nilson, 1963; Varah, 1975; Mora\u010da, 2008), we have(I m \u2212 \u03b3\u03c3(A) ) \u22121 \u221e \u2264 1 min j (|I m \u2212 \u03b3\u03c3(A)| jj \u2212 i =j |I m \u2212 \u03b3\u03c3(A)| ij ) = 1 1 \u2212 \u03b3 .\nPublished as a conference paper at ICLR 2021 By taking transpose,(I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 1 1 \u2212 \u03b3 .\nSummarizing above,Z 1 = B(I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 B 1 1 1 \u2212 \u03b3 . Therefore, if B 1 < R(1 \u2212 \u03b3), then Z 1 = B(I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 B 1 1 1\u2212\u03b3 < R, as desired. Combining this with (31) implies that if B 1 < R(1 \u2212 \u03b3), L(A t , B t ) \u2264 L * 0,R + (L(A 0 , B 0 ) \u2212 L * 0,R )e \u22122\u03ba\u03bb T T . Recall that L * 0,R = inf W : W 1<R L 0 (W ) and L * R = inf A\u2208R m\u00d7m ,B\u2208B R L(A, B) where B R = {B \u2208 R my\u00d7m | B 1 < (1 \u2212 \u03b3)R}. Here, B \u2208 B R implies that Z 1 = BU \u22121 1 \u2264 B 1 U \u22121 1 < (1 \u2212 \u03b3)R U \u22121 1 \u2264 R, using the above upper bond U \u22121 1 = (I m \u2212 \u03b3\u03c3(A)) \u22121 1 \u2264 1 1\u2212\u03b3 . Since L(A, B) = L 0 (Z) with Z = BU \u22121 , this implies that L * 0,R \u2264 L * R and thus L(A t , B t ) \u2264 L * 0,R + (L(A 0 , B 0 ) \u2212 L * 0,R )e \u22122\u03ba\u03bb T T \u2264 L * R + (L(A 0 , B 0 ) \u2212 L * 0,R )e \u22122\u03ba\u03bb T T .\nThis completes the first part of the desired statement of Theorem 1.\nThe remaining task is to lower bound \u03bb T , which is completed as follows: for any (A, B),\u03bb min (D) = min v: v =1 v Dv = min v: v =1 v m k=1 [(U \u2212 t ) * k (U \u22121 t ) k * \u2297 (I my + \u03b3 2 Z t J k,t J k,t Z t )] v \u2265 min v: v =1 v U \u2212 U \u22121 \u2297 I my v = \u03bb min ( U \u2212 U \u22121 \u2297 I my ) = \u03bb min (U \u2212 U \u22121 ) = \u03c3 2 min (U \u22121 ) = 1 U 2 2 \u2265 1 m U 2 1 (32)\nwhere the third line follows from ( 27)-( 29), the fifth line follows from the property of Kronecker product (the eigenvalues of Kronecker product of two matrices are the products of eigenvalues of the two matrices), and the last inequality follows from the relation between the spectral norm and the norm \u2022 1 . We now compute U 1 as: for any (A, B),U 1 = I m \u2212 \u03b3\u03c3(A) 1 = max j i |(I m \u2212 \u03b3\u03c3(A)) ij | = max j i |(I m ) ij \u2212 \u03b3\u03c3(A) ij | = max j |(I m ) jj \u2212 \u03b3\u03c3(A) jj | + i =j |(I m ) ij \u2212 \u03b3\u03c3(A) ij | = max j |1 \u2212 \u03b3\u03c3(A) jj | + i =j | \u2212 \u03b3\u03c3(A) ij | = max j 1 \u2212 \u03b3\u03c3(A) jj + i =j \u03b3\u03c3(A) ij = max j 1 + \u03b3 \uf8eb \uf8ed \uf8eb \uf8ed i =j \u03c3(A) ij \uf8f6 \uf8f8 \u2212 \u03c3(A) jj \uf8f6 \uf8f8 \u2264 1 + \u03b3.\nBy substituting this into (32), we have that for any (A, B) (and hence for any t),\u03bb min (D) \u2265 1 m(1 + \u03b3) 2 . ()\nThis completes the proof for both the first and second parts of the statement of Theorem 1.\n\nA.2 PROOF OF THEOREM 2\nWe first show that with \u03b4 t > 0 sufficiently small, we have G t 0. Recall thatG t = U t S \u22121 t \u2212 \u03b4 t F t U t = U t S \u22121 t U t \u2212 \u03b4 t U t F t U t . Thus, with \u03b4 t > 0 sufficiently small, for any v = 0, v Gv = v U t S \u22121 t U t v \u2212 \u03b4 t v U t F t U t v, which is dominated by the first term v U t S \u22121 t U t v if the matrix U t S \u22121 t U t is positive definite. Since S t := I m + \u03b3 2 diag(v S t ) with v S t \u2208 R m and (v S t ) k := J k,t (B t U \u22121 t ) 2 2 for k = 1, 2, .\n. . , m, the matrix U t S \u22121 t U t is positive definite. Thus, with \u03b4 t > 0 sufficiently small, v Gv is dominated by the first term, which is positive (since U t S \u22121 t U t is positive definite), and thus we have G t 0. Then we observe that the output ofargmin v: v G t \u2264\u03b4t d dt vec(BtU \u22121 t ) G t L t 0 (v)\nis the set of solutions of the following constrained optimization problem:minimize v L t 0 (v) s.t. v 2 Gt \u2212 \u03b4 2 t d dt vec(B t U \u22121 t ) 2 Gt \u2264 0.\nSince this optimization problem is convex, one of the sufficient conditions for global optimality is the KKT condition with a multiplier \u00b5 \u2208 R:\u2207L t 0 (v) + 2\u00b5G t v = 0 \u00b5 \u2265 0 \u00b5 v 2 Gt \u2212 \u03b4 2 t d dt vec(B t U \u22121 t ) 2 Gt = 0.\nTherefore, the desired statement is obtained if the above KKT condition is satisfied by v = \u03b4 t d dt vec(B t U \u22121 t ) with some multiplier \u00b5. The rest of this proof shows that the KKT condition is satisfied by setting v = \u03b4 t d dt vec(B t U \u22121 t ) and \u00b5 = 1 2\u03b4t . With this choice, the last two conditions of the KKT condition hold, since\u00b5 = 1 2\u03b4 t \u2265 0,v 2 Gt \u2212 \u03b4 2 t d dt vec(B t U \u22121 t ) 2 Gt = \u03b4 2 t d dt vec(B t U \u22121 t ) 2 Gt \u2212 \u03b4 2 t d dt vec(B t U \u22121 t ) 2 Gt = 0.\nThe remaining task is to show that\u2207L t 0 (v) + 2\u00b5G t v = 0 with v = \u03b4 t d dt vec(B t U \u22121 t ) and \u00b5 = 1 2\u03b4t . From the definition of L t 0 , \u2207L t 0 (v) + 2\u00b5G t v = \u2207L vec 0 (vec(B t U \u22121 t )) + \u2207 2 L vec 0 (vec(B t U \u22121 t ))v + 2\u00b5G t v.\nWe now compute and \u2207L vec 0 and \u2207 2 L vec 0 . Since \u2207L 0 (W ) := \u2202L0(W )\u2202W = n i=1 \u2202 (\u0177i,yi) \u2202 \u0177i x i , vec(\u2207L 0 (W )) = n i=1 vec I my \u2202 (\u0177 i , y i ) \u2202 \u0177i \u03c6(x i ) = n i=1 [\u03c6(x i ) \u2297 I my ] \u2202 (\u0177 i , y i ) \u2202 \u0177i , where \u0177i := W \u03c6(x i ) = [\u03c6(x i ) \u2297 I my ] vec[W ].\nTherefore,\u2207L vec 0 (vec(W )) = vec(\u2207L 0 (W )) = n i=1 [\u03c6(x i ) \u2297 I my ] \u2202 (\u0177 i , y i ) \u2202 \u0177i .\nFor the Hessian,\u2207 2 L vec 0 (vec(W )) = \u2202 \u2202 vec(W ) \u2207L vec 0 (vec(W )) = n i=1 [\u03c6(x i ) \u2297 I my ] \u2202 \u2202 \u0177i \u2202 (\u0177 i , y i ) \u2202 \u0177i \u2202 \u0177i \u2202 vec(W ) = n i=1 [\u03c6(x i ) \u2297 I my ] \u2202 \u2202 \u0177i \u2202 (\u0177 i , y i ) \u2202 \u0177i [\u03c6(x i ) \u2297 I my ] By defining i (z) = (z, y i ) and \u2207 2 i (z) = \u2202 \u2202z \u2202 i(z) \u2202z , \u2207 2 L vec 0 (vec(W )) = n i=1 [\u03c6(x i ) \u2297 I my ]\u2207 2 i (W \u03c6(x i ))[\u03c6(x i ) \u2297 I my ].\nSince we have that(I m \u2212 \u03b3\u03c3(A)) l k=0 \u03b3 k \u03c3(A) k = I m \u2212 \u03b3\u03c3(A) + \u03b3\u03c3(A) \u2212 (\u03b3\u03c3(A)) 2 + (\u03b3\u03c3(A)) 2 \u2212 (\u03b3\u03c3(A)) 3 + \u2022 \u2022 \u2022 \u2212 (\u03b3\u03c3(A)) l+1 = I \u2212 (\u03b3\u03c3(A)) l+1 ,\nwe can write:(I m \u2212 \u03b3\u03c3(A)) lim l\u2192\u221e z (l) (x, A) = lim l\u2192\u221e (I m \u2212 \u03b3\u03c3(A)) l k=0 \u03b3 k \u03c3(A) k \u03c6(x) = I m \u2212 lim l\u2192\u221e (\u03b3\u03c3(A)) l+1 \u03c6(x) = \u03c6(x),\nwhere we used the fact that \u03b3\u03c3(A) ij \u2265 0, \u03c3(A) 1 = max j i |\u03c3(A) ij | = 1, and thus \u03b3\u03c3(A) 1 < 1 for any \u03b3 \u2208 (0, 1). This shows that lim l\u2192\u221e z(l) (x, A) = z * (x, A) = U \u22121 \u03c6(x), from which we have \u03c6(x i ) = U z * (x i , A). Substituting \u03c6(x i ) = U z * (x i , A) into (35), \u2207 2 L vec 0 (vec(W )) = n i=1 [U z * (x i , A) \u2297 I my ]\u2207 2 i (W \u03c6(x i ))[z * (x i , A) U \u2297 I my ]. = n i=1 [U \u2297 I my ][z * (x i , A) \u2297 I my ]\u2207 2 i (W \u03c6(x i ))[z * (x i , A) \u2297 I my ][U \u2297 I my ]. = [U \u2297 I my ] n i=1 [z * (x i , A) \u2297 I my ]\u2207 2 i (W \u03c6(x i ))[z * (x i , A) \u2297 I my ] [U \u2297 I my ].\nIn the case of m y = 1, since I my = 1, \u2207 2 L vec 0 (vec(W )) is further simplified to:\u2207 2 L vec 0 (vec(W )) = U n i=1 \u2207 2 i (W \u03c6(x i ))z * (x i , A)z * (x i , A) U .\nTherefore,\u2207 2 L vec 0 (vec(B t U \u22121 t )) = U t F t U t ,F t = n i=1 \u2207 2 i (B t U \u22121 t \u03c6(x i ))z * (x i , A t )z * (x i , A t )\n.By plugging \u00b5 = 1 2\u03b4t and \u2207 2 L vec 0 (vec(B t U \u22121 t )) = U t F t U t into (34), \u2207L t 0 (v) + 2\u00b5G t v = \u2207L vec 0 (vec(B t U \u22121 t )) + U t F t U t v + 1 \u03b4 t U t S \u22121 t \u2212 \u03b4 t F t U t v = \u2207L vec 0 (vec(B t U \u22121 t )) + 1 \u03b4 t U t S \u22121 t U t v. By using v = \u03b4 t d dt vec(B t U \u22121 t ) , \u2207L t 0 (v) + 2\u00b5G t v = \u2207L vec 0 (vec(B t U \u22121 t )) + U t S \u22121 t U t d dt vec(B t U \u22121 t ) . By plugging (26) into d dt vec(B t U \u22121 t ) with Z t = B t U \u22121 t , \u2207L t 0 (v) + 2\u00b5G t v = \u2207L vec 0 (vec(B t U \u22121 t )) \u2212 U t S \u22121 t U t D t vec(\u2207L 0 (Z t )).\nRecall thatD t = m k=1 [(U \u2212 t ) * k (U \u22121 t ) k * \u2297 (I my + \u03b3 2 Z t J k,t J k,t Z t )].\nIn the case of m y = 1, the matrix D t can be simplified as:D = m k=1 (U \u2212 ) * k (U \u22121 ) k * \u2297 I my + \u03b3 2 ZJ k J k Z = m k=1 1 + \u03b3 2 ZJ k J k Z (U \u2212 ) * k (U \u22121 ) k * = U \u2212 diag \uf8eb \uf8ec \uf8ed \uf8ee \uf8ef \uf8f0 1 + \u03b3 2 ZJ 1 J 1 Z . . . 1 + \u03b3 2 ZJ m J m Z \uf8f9 \uf8fa \uf8fb \uf8f6 \uf8f7 \uf8f8 U \u22121 = U \u2212 SU \u22121 .\nPlugging this into the above equation for D t ,\u2207L t 0 (v) + 2\u00b5G t v = \u2207L vec 0 (vec(B t U \u22121 t )) \u2212 U t S \u22121 t U t U \u2212 t S t U \u22121 t vec(\u2207L 0 (Z t )) = \u2207L vec 0 (vec(B t U \u22121 t )) \u2212 \u2207L vec 0 (vec(B t U \u22121 t )) = 0.\nTherefore, the constrained optimization problem at time t is solved byv = \u03b4 t d dt vec(B t U \u22121 t ) , which implies that d dt vec(B t U \u22121 t ) = 1 \u03b4 t vec(V t ), vec(V t ) \u2208 argmin v: v G t \u2264\u03b4t d dt vec(BtU \u22121 t ) G t L t 0 (v),\nBy multiplying \u03c6(x) \u2297 I my to each side of the equation, we haved dt [\u03c6(x) \u2297 I my ] vec(B t U \u22121 t ) = B t lim l\u2192\u221e z (l) (x, A t ) , 1 \u03b4 t [\u03c6(x) \u2297 I my ] vec(V t ) = 1 \u03b4 t V t \u03c6(x), yielding that d dt B t lim l\u2192\u221e z (l) (x, A t ) = 1 \u03b4 t V t \u03c6(x).\nThis proves the desired statement of Theorem 2.\n\nA.3 PROOF OF COROLLARY 1\nThe assumption rank(\u03a6) = min(n, m) implies that \u03c3 min (\u03a6) > 0. Moreover, the square loss satisfies the assumption of the differentiability. Thus, Theorem 1 implies the statement of this corollary if L 0 with the square loss satisfies the Polyak-\u0141ojasiewicz inequality for any W \u2208 R my\u00d7m with parameter \u03ba = 2\u03c3 min (\u03a6) 2 . This is to be shown in the rest of this proof. By setting \u03d5 = L 0 in Definition 1, we have \u2207\u03d5 vec (vec(q)) 2 2 = \u2207L 0 (W ) 2 F . With the square loss, we can writeL 0 (W ) = n i=1 W \u03c6(x i ) \u2212 y i 2 2 = W \u03a6 \u2212 Y 2\nF where \u03a6 \u2208 R m\u00d7n and Y \u2208 R my\u00d7n with \u03a6 ji = \u03c6(x i ) j and Y ji = (y i ) j . Thus,\u2207L 0 (W ) = 2(W \u03a6 \u2212 Y )\u03a6 \u2208 R my\u00d7m .\nWe first consider the case of m \u2264 n. In this case, we consider the vectorization L vec 0 (vec(W )) = L 0 (W ) and derive the gradient with respect to vec(W ):\u2207L vec 0 (vec(W )) = 2 vec((W \u03a6 \u2212 Y )\u03a6 ) = 2[\u03a6 \u2297 I my ] vec(W \u03a6 \u2212 Y ).\nThen, the Hessian can be easily computed as\u2207 2 L vec 0 (vec(W )) = 2[\u03a6 \u2297 I my ][\u03a6 \u2297 I my ] = 2[\u03a6\u03a6 \u2297 I my ]\n, where I my is the identity matrix of size m y by m y . Since the singular values of Kronecker product of the two matrices is the product of singular values of each matrix, we have \u2207 2 L vec 0 (vec(W )) 2\u03c3 min (\u03a6) 2 I mym , where we used the fact that m \u2264 n in this case. Since W is arbitrary, this implies that L vec 0 is strongly convex with parameter 2\u03c3 min (\u03a6) 2 > 0 in R my\u00d7m . Since a strongly convex function with some parameter satisfies the Polyak-\u0141ojasiewicz inequality with the same parameter (Karimi et al., 2016), this implies that L vec 0 (and hence L 0 ) satisfies the Polyak-\u0141ojasiewicz inequality with parameter 2\u03c3 min (\u03a6) 2 > 0 in R my\u00d7m in the case of m \u2264 n.\nWe now consider the remaining case of m > n. In this case, using the singular value decomposition of\u03a6 = U \u03a3V , 1 2 \u2207L 0 (W ) 2 F = 2 \u03a6(W \u03a6 \u2212 Y ) 2 F = 2 U \u03a3V (W \u03a6 \u2212 Y ) 2 F = 2 \u03a3V (W \u03a6 \u2212 Y ) 2 F \u2265 2\u03c3 min (\u03a6) 2 V (W \u03a6 \u2212 Y ) 2 F = 2\u03c3 min (\u03a6) 2 L 0 (W ) \u2265 2\u03c3 min (\u03a6) 2 (L 0 (W ) \u2212 L * * 0 )\nfor any L * * 0 \u2265 0, where the first line uses q 2 F = q 2 F , the second line uses the singular value decomposition, and the third and fourth line uses the fact that U and V are orthonormal matrices. The forth line uses the fact that m > n in this case. Therefore, since W is arbitrary, we have shown that L 0 satisfies the Polyak-\u0141ojasiewicz inequality for any W \u2208 R my\u00d7m with parameter \u03ba = 2\u03c3 min (\u03a6) 2 in both cases of m > n and m \u2264 n.\n\nA.4 PROOF OF COROLLARY 2\nThe assumption rank(\u03a6) = m implies that \u03c3 min (\u03a6) > 0. Moreover, the logistic loss satisfies the assumption of the differentiability. Thus, Theorem 1 implies the statement of this corollary if L 0 with the logistic loss satisfies the Polyak-\u0141ojasiewicz inequality with the given radius R \u2208 (0, \u221e] and the parameter \u03ba = (2\u03c4 + \u03c1(R))\u03c3 min (\u03a6) 2 where \u03c1(R) depends on R. Let R \u2208 (0, \u221e] be given. Note that we have \u03c1(R) > 0 if R < \u221e, and \u03c1(R) = 0 if R = \u221e. If (R, \u03c4 ) = (\u221e, 0), then 2\u03c4 + \u03c1(R) = 0 for which the statement of this corollary trivially holds (since the bound does not decrease). Therefore, we focus on the remaining case of (R, \u03c4 ) = (\u221e, 0). Since (R, \u03c4 ) = (\u221e, 0), we have 2\u03c4 + \u03c1(R) > 0. We first compute the Hessian with respect to W as:\u2207 2 L 0 (W ) = n i=1 pi (1 \u2212 pi )\u03c6(x i )\u03c6(x i ) + 2\u03c4 n i=1 \u03c6(x i )\u03c6(x i ) , where pi = 1 1+e \u2212W \u03c6(x i ) . Therefore, v \u2207 2 L 0 (W )v = n i=1 pi (1 \u2212 pi )v \u03c6(x i )\u03c6(x i ) v + 2\u03c4 v n i=1 \u03c6(x i )\u03c6(x i ) v \u2265 \u03c1(R) n i=1 v \u03c6(x i )\u03c6(x i ) v + 2\u03c4 v n i=1 \u03c6(x i )\u03c6(x i ) v = (2\u03c4 + \u03c1(R))v \u03a6\u03a6 v \u2265 (2\u03c4 + \u03c1(R))\u03c3 min (\u03a6) 2 > 0.\nTherefore, L 0 is strongly convex with parameter (2\u03c4 + \u03c1(R))\u03c3 min (\u03a6) 2 > 0. Since a strongly convex function with a parameter satisfies the Polyak-\u0141ojasiewicz inequality with the same parameter (Karimi et al., 2016), this implies that L 0 satisfies the Polyak-\u0141ojasiewicz inequality with the given radius R \u2208 (0, \u221e] and parameter (2\u03c4 + \u03c1(R))\u03c3 min (\u03a6) 2 > 0. Since R \u2208 (0, \u221e] is arbitrary, this implies the statement of this corollary by Theorem 1.\nA.5 PROOF OF PROPOSITION 1\nLet (x, A) be given. By repeatedly applying the definition of z (l) (x, A), we obtainz (l) (x, A) = \u03b3\u03c3(A)z (l\u22121) + \u03c6(x) = \u03b3\u03c3(A)(\u03b3\u03c3(A)z (l\u22122) + \u03c6(x)) + \u03c6(x) = l k=0 \u03b3 k \u03c3(A) k \u03c6(x),\nwhere \u03c3(A) k represents the matrix multiplications of k copies of the matrix \u03c3(A) with \u03c3(A) 0 = I m . In general, if \u03c3 is identity, this sequence does not converge. However, with our definition of \u03c3, we have \u03c3(A) 1 = maxj i |\u03c3(A) ij | = 1.\nTherefore, \u03b3\u03c3(A) 1 = \u03b3 \u03c3(A) 1 = \u03b3 < 1 for any \u03b3 \u2208 (0, 1). Since an induced matrix norm is sub-multiplicative, this implies that\u03b3 k \u03c3(A) k 1 \u2264 k i=1 \u03b3\u03c3(A) 1 = \u03b3 k .\nIn other words, each term \u03b3 k \u03c3(A) k 1 in the series \u221e k=0 \u03b3 k \u03c3(A) k 1 is bounded by \u03b3 k . Since the series \u221e k=0 \u03b3 k converges in R with \u03b3 \u2208 (0, 1), this implies, by the comparison test, that the series\u221e k=0 \u03b3 k \u03c3(A) k\n1 converges in R. Thus, the sequence (l k=0 \u03b3 k \u03c3(A) k 1 ) l is a Cauchy sequence. By defining s l = l k=0 \u03b3 k \u03c3(A) k , we have s l \u2212 s l 1 = l k=l +1 \u03b3 k \u03c3(A) k 1 \u2264 l k=l +1 \u03b3 k \u03c3(A) k\n1 for any l > l by the triangle inequality of the (matrix) norm. Since ( l k=0 \u03b3 k \u03c3(A) k 1 ) l is a Cauchy sequence, this inequality implies that ( l k=0 \u03b3 k \u03c3(A) k ) l is a Cauchy sequence (in a Banach space (R m\u00d7m , \u2022 1 ), which is isometric to R mm under \u2022 1 ). Thus, the sequence ( l k=0 \u03b3 k \u03c3(A) k ) l converges. From (36), this implies that the sequence (z (l) (x, A)) l converges.\n\nB ON THE IMPLICIT BIAS\nIn this section, we show that Theorem 2 suggests an implicit bias towards a simple function as a result of infinite depth, whereas understanding this bias in more details is left as an open problem. This section focuses on the case of the square loss (q, y i ) = q \u2212 y i 2 2 with m y = 1. By solving vec(V t ) \u2208 argmin v\u2208V L t 0 (v) in Theorem 2 for the direction of the Newton method, Theorem 2 implies thatV t = \u2212r t (I m \u2212 \u03b3\u03c3(A t )) \u22121 \u2208 R 1\u00d7m ,\n(37) where r t \u2208 R m is an error vector with each entry being a function of the residuals f \u03b8t (x i ) \u2212 y i .\nSince \u03c3(A t ) is a positive matrix and is a left stochastic matrix due to the nonlinearity \u03c3, the Perron-Frobenius theorem (Perron, 1907; Frobenius, 1912) ensures that the largest eigenvalue of \u03c3(A t ) is one, any other eigenvalue in absolute value is strictly smaller than one, and any left eigenvector corresponding the largest eigenvalue is the vector \u03b71 = \u03b7[1, 1, . . . , 1] \u2208 R m where \u03b6 \u2208 R is some scalar. Thus, the largest eigenvalue of the matrix (I m \u2212 \u03b3\u03c3(A t )) \u22121 is 1 1\u2212\u03b3 , any other eigenvalue is in the form of 1 1\u2212\u03bb k \u03b3 with |\u03bb k | < 1, and any left eigenvector corresponding the largest eigenvalue is \u03b71 \u2208 R m . By decomposing the error vector as r t = P 1 r t + (1 \u2212 P 1 )r t , this implies that vec(V t ) = V t = 1 1\u2212\u03b3 P 1 r t + g \u03b3 ((1 \u2212 P 1 )r t ) \u2208 R m , where P 1 = 1 m 11 is a projection onto the column space of 1 = [1, 1, . . . , 1] \u2208 R m , and g \u03b3 is a function such that for any q in its domain, g \u03b3 (q) < c for all \u03b3 \u2208 (0, 1) with some constant c in \u03b3.\nIn other words, vec(V t ) in Theorem 2 can be decomposed into the two terms: 1 1\u2212\u03b3 P 1 r t (the projection of the error vector onto the column space of 1) and g \u03b3 ((1 \u2212 P 1 )r t ) (a function of the projection of the error vector onto the null space space of 1). Here, as \u03b3 \u2192 1, 1 1\u2212\u03b3 P 1 r t \u2192 \u221e and g \u03b3 ((1 \u2212 P 1 )r t ) < c. This implies that with \u03b3 < 1 sufficiently large, the first term 1 1\u2212\u03b3 P 1 r t dominates the second term g \u03b3 ((1 \u2212 P 1 )r t ).\nSince (P 1 r t ) \u03c6(x) = \u03bct m 1 \u03c6(x) with \u03bct = m k=1 (r t ) k \u2208 R, this implies that with \u03b3 < 1 sufficiently large, the dynamics of deep equilibrium linear models d dt f \u03b8t = 1 \u03b4t V t \u03c6 learns a simple shallow function \u03bcT m 1 \u03c6(x) first before learning more complicated components of the functions through g \u03b3 ((1 \u2212 P 1 )r t ), where \u03bcT = T 0 \u03bct dt \u2208 R. Here, \u03bcT m 1 \u03c6(x) is a simple average model that averages over the features 1 m 1 \u03c6(x) and multiplies it by a scaler \u03bct . Moreover, large \u03b3 < 1 means that we have large effective depth or large weighting for deeper layers since we have a shallow model with \u03b3 = 0 and \u03b3 is a discount factor of the infinite depth.\n\nC ADDITIONAL DISCUSSION\nOn existence of the limit. When Bai et al. (2019a) introduced general deep equilibrium models, they hypothesized that the limit lim l\u2192\u221e z (l) exists for several choices of h, and provided numerical results to support this hypothesis. In general deep equilibrium models, depending on the values of model parameters, the limit is not ensured to exists. For example, when h increases the norm of the output at every layer, then it is easy to see that the sequence diverge or explode. This is also true when we set h to be that of deep equilibrium models without the nonlinearity \u03c3 (or equivalently redefining \u03c3 to be the identity function): if the operator norms on A are not bounded by one, then the sequence can diverge in general. In other words, in general, some trajectory of gradient dynamics may potentially violate the assumption of the existence of the limit when learning models. In this view, the class of deep equilibrium linear models is one instance of general deep equilibrium models where the limit is guaranteed to exist for any values of model parameters as stated in Proposition 1. On the PL inequality. With R sufficiently large, the definition of the PL inequality in this paper is simply a rearrangement in the form where L 0 is allowed to take matrices as its inputs throughL vec 0 (vec(\u2022)) = L 0 (\u2022),\nwhere vec(M ) represents the standard vectorization of a matrix M . See (Polyak, 1963; Karimi et al., 2016) for more detailed explanations of the PL inequality. On the reditus R for the logistic loss. As shown in Section 3.1.3, we can use R = \u221e for the square loss and the logistic loss, in order to get a prior guarantee for the global linear convergence in theory. In practice, for the logistic loss, we may want to choose R depending on the different scenarios, because of the following observation.For the logistic loss, we would like to set the radius R to be large so that the trajectory on B is bounded as B t 1 < (1 \u2212 \u03b3)R for all t \u2208 [0, T ] and the global minimum value on the constrained domain to decrease: i.e., L * R \u2192 L * as R \u2192 \u221e. However, unlike in the case of the squared loss, the convergence rate decreases as we increase R in the case of the logistic loss, because \u03c1(R) decreases as R increases. This does not pose an issue because we can always pick R < \u221e so that for any t > 0 and T > 0, we have \u03c1(R) > c \u03c1 for some constant c \u03c1 > 0. Moreover, this tradeoff does not appear for the square loss: i.e., we can set R = \u221e for the square loss without decreasing the convergence rate. We can also avoid this tradeoff for the logistic loss by simply setting R = \u221e and \u03c4 > 0. On previous work without implicit linearization. In Section 5, we discussed the previous work on deep neural networks with implicit linearization via significant over-parameterization. Kawaguchi & Huang (2019) observed that we can also use the implicit linearization with mild over-parameterization by controlling learning rates to guarantee global convergence and generalization performances at the same time. On the other hand, there is another line of previous work where deep nonlinear neural networks are studied without any (implicit or explicit) linearization and without any strong assumptions; e.g., see the previous work by Shamir ( 2018 Whereas the conclusions of these previous studies without strong assumptions can be directly applicable to practical settings, their conclusions are not as strong as those of previous studies with strong assumptions (e.g., implicit linearization via significant over-parameterization) as expected. The direct practical applicability, however, comes with the benefit of being able to assist the progress of practical methods (Verma et al., 2019; Jagtap et al., 2020b; a).\n\nD EXPERIMENTS\nThe purpose of our experiments is to provide a secondary motivation for our theoretical analyses, instead of claiming the immediate benefits of using deep equilibrium linear models.\n\nD.1 EXPERIMENTAL SETUP\nFor data generation and all models, we set \u03c6(x) = x. Therefore, we have m = m x . Data. To generate datasets, we first drew uniformly at random 200 input images from a standard image dataset - CIFAR-10 (Krizhevsky & Hinton, 2009), CIFAR-100 (Krizhevsky & Hinton, 2009) or Kuzushiji-MNIST (Clanuwat et al., 2019) -as pre input data points x pre i \u2208 R m x . Out of 200 images, 100 images to be used for training were drawn from a train dataset and the 100 other images to be used for testing were drawn from the corresponding test dataset. Then, the input data pints x i \u2208 R mx with m x = 150 were generated as x i = Rx pre i where each entry of a matrix R \u2208 R mx\u00d7m x was set to \u03b4/ \u221a m x with \u03b4 i.i.d.\n\u223c N (0, 1) and was fixed over the indices i. We then generated the targets asy i = B * (lim l\u2192\u221e z (l) (x i , A * )) + \u03b4 i \u2208 R with \u03b3 = 0.8 where \u03b4 i i.i.d.\n\u223c N (0, 1). Each entry of the true (unknown) matrices A * \u2208 R 1\u00d7m and B * \u2208 R m\u00d7m was independently drawn from the standard normal distribution.\nModel. For DNNs, we used ReLU activation and W (l) \u2208 R m\u00d7m for l = 1, 2 . . . , H \u2212 1 (W (H) \u2208 R 1\u00d7m ). Each entry of the weight matrices W (l) for DNNs was initialized to \u03b4/ \u221a m where \u03b4 i.i.d.\n\u223c N (0, 1) for all l = 1, 2, . . . , H. Similarly, for deep equilibrium linear models, each entry of A and B was initialized to \u03b4/ \u221a m where \u03b4 i.i.d.\n\u223c N (0, 1). Linear models were initialized to represent the exact same functions as those of initial deep equilibrium linear models: i.e., W 0 = B 0 U \u22121 0 . We used \u03b3 = 0.8 for deep equilibrium linear models.\nTraining. For each dataset, we used stochastic gradient descent (SGD) to train linear models, deep equilibrium linear models, and fully-connected feedforward deep neural networks (DNNs). We used the square loss (q, y i ) = q \u2212 y i 2 2 . We fixed the mini-batch size to be 64 and the momentum coefficient to be 0.8. Under this setting, linear models are known to find a minimum norm solution (with extra elements from initialization) (Gunasekar et al., 2017; Poggio et al., 2017). Similarly, DNNs have been empirically observed to have implicit regularization effects (although the most well studied setting is with the loss functions with exponential tails) (e.g., see discussions in Poggio et al., 2017; Moroshko et al., 2020; Woodworth et al., 2020). In order to minimize the effect of learning rates on our conclusion, we conducted experiments with all the values of learning rates from the choices of 0.01, 0.005, 0.001, 0.0005, 0.0001 and 0.00005, and reported the results with both the worst cases and the best cases separately for each model (and each depth H for DNNs).\nAll experiments were implemented in PyTorch (Paszke et al., 2019).\n\nD.2 ADDITIONAL EXPERIMENTS\nIn this subsection, we report additional experimental results.\nAdditional datasets. We repeated the same experiments as those for Figure 1 with four additional datasets -modified MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), SEMEION (Srl & Brescia, 1994), and Fashion-MNIST (Xiao et al., 2017). We report the result of this experiment in Figure 4. As can be seen from Figures 4, we confirmed qualitatively the same observations as in Figure 1 : i.e., all models preformed approximately the same at initial points, but deep equilibrium linear models outperformed both linear models and nonlinear DNNs in test errors after training.\nDNNs without bias terms. In Figures 1-4, the results of DNNs are reported with bias terms. To consider the effect of discarding bias term, we also repeated the same experiments with DNNs without bias term and reported the results in Figure 5. As can be seen from Figures 5, we confirmed qualitatively the same observations: i.e., deep equilibrium linear models outperformed nonlinear DNNs in test errors.\nDNNs with deeper networks. To consider the effect of deeper networks, we also repeated the same experiments with deeper DNNs with depth H = 10, 100 and 200, and we reported the results in Figures 6-7. As can be seen from Figures 6-7, we again confirmed qualitatively the same observations: i.e., deep equilibrium linear models outperformed nonlinear DNNs in test errors, although DNNs can reduce training errors faster than deep equilibrium linear models. We experienced gradient explosion and gradient vanishing for DNNs with depth H = 100 and H = 200.\nLarger datasets. In Figures 1, we used only 200 data points so that we can observe the effect of inductive bias and overfitting phenomena under a small number of data points. If we use a large number of data points, it is expected that the benefit of the inductive bias with deep equilibrium linear models tends to become less noticeable because using a large number of data points can reduce the degree of overfitting for all models, including linear models and DNNs. However, we repeated the same experiments with all data points of each datasets: for example, we use 60000 training data points and 10000 test data points for MNIST. Figure 8 reports the results where the values are shown with the best learning rates for each model from the set of learning rates S LR = {0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005} (in terms of the final test errors at epoch = 100). As can be seen in the figure, deep equilibrium linear models outperformed both linear models and nonlinear DNNs in test errors.\nLogistic loss and theoretical bounds. In Corollary 2, we can set \u03bb T = 1 m(1+\u03b3) 2 to get a guarantee for the global linear convergence rate for any initialization in theory. However, in practice, this is a pessimistic convergence rate and we may want to choose \u03bb T depending on initializations. To demonstrate this, Figure 3 reports the numerical training trajectory along with theoretical upper bounds with initialization-independent \u03bb T = 1 m(1+\u03b3) 2 and initialization-dependent \u03bb T = inf t\u2208[0,T ] \u03bb min (D t ). As can be seen in Figure 3, the theoretical upper bound with initialization-dependent \u03bb T demonstrates a faster convergence rate.\n\nFootnotes:\n\nReferences:\n\n- J Harold Ahlberg and Edwin N Nilson. Convergence properties of the spline fit. Journal of the Society for Industrial and Applied Mathematics, 11(1):95-104, 1963.- Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter- ized neural networks, going beyond two layers. In Advances in neural information processing systems, pp. 6158-6169, 2019.\n\n- Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, 2018.\n\n- Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de- scent for deep linear neural networks. In International Conference on Learning Representations, 2019a. Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019b.\n\n- Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, pp. 690-701, 2019a.\n\n- Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In Inter- national Conference on Learning Representations, 2019b.\n\n- Randal J Barnes. Matrix differentiation. Springs Journal, pp. 1-9, 2006.\n\n- Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initializa- tion efficiently learns positive-definite linear transformations by deep residual networks. Neural computation, 31(3):477-502, 2019.\n\n- Bradley M Bell and James V Burke. Algorithmic differentiation of implicit functions and optimal values. In Advances in Automatic Differentiation, pp. 67-77. Springer, 2008.\n\n- Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.\n\n- Bruce Christianson. Reverse accumulation and attractive fixed points. Optimization Methods and Software, 3(4):311-326, 1994.\n\n- Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. In NeurIPS Creativity Workshop 2019, 2019.\n\n- Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6292- 6299, 2019.\n\n- Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019.\n\n- Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In International Conference on Machine Learning, pp. 1655-1664, 2019.\n\n- Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pp. 1675- 1685, 2019.\n\n- Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.\n\n- Cong Fang, Jason D Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field framework for over-parameterized deep neural networks. arXiv preprint arXiv:2007.01452, 2020.\n\n- Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\n- Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems, pp. 8572-8583, 2019.\n\n- Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157- 8166, 2018.\n\n- Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant. Adding one neuron can eliminate all bad local minima. In Advances in Neural Information Processing Systems, 2018.\n\n- Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: mem- orization and generalization under lazy training. preprint arXiv:2007.12826, 2020.\n\n- Nenad Mora\u010da. Bounds for norms of the matrix inverse and the smallest singular value. Linear algebra and its applications, 429(10):2589-2601, 2008.\n\n- Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D Lee, Nathan Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. arXiv preprint arXiv:2007.06738, 2020.\n\n- Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011.\n\n- Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on Machine Learning, pp. 4790-4799. PMLR, 2019.\n\n- Quynh Nguyen. A note on connectivity of sublevel sets in deep learning. arXiv preprint arXiv:2101.08576, 2021.\n\n- Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. In Advances in neural information processing systems, pp. 8026-8037, 2019.\n\n- Oskar Perron. Zur theorie der matrices. Mathematische Annalen, 64(2):248-263, 1907. Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173, 2017.\n\n- Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.\n\n- Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy- namics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014.\n\n- Ohad Shamir. Are ResNets provably better than linear predictors? In Advances in Neural Informa- tion Processing Systems, to appear, 2018.\n\n- Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im- plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19 (1):2822-2878, 2018.\n\n- B Tactile Srl and Italy Brescia. Semeion handwritten digit data set. Semeion Research Center of Sciences of Communication, Rome, Italy, 1994.\n\n- James M Varah. A lower bound for the smallest singular value of a matrix. Linear Algebra and its Applications, 11(1):3-5, 1975.\n\n- Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang. Graphmix: Regularized training of graph neural networks for semi-supervised learning. arXiv preprint arXiv:1909.11715, 2019.\n\n- Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277, 2020.\n\n- Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark- ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\n- Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. In Advances in Neural Information Processing Systems, pp. 6598-6608, 2019.\n\n- Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over- parameterized deep ReLU networks. Machine Learning, 109(3):467-492, 2020a.\n\n- Difan Zou, Philip M Long, and Quanquan Gu. On the global convergence of training deep linear resnets. In International Conference on Learning Representations, 2020b.\n\n", "annotations": {"SectionMain": [{"begin": 1315, "end": 80767, "idx": 0}], "ReferenceToFormula": [{"begin": 2949, "end": 2950, "target": "#formula_0", "idx": 0}, {"begin": 5409, "end": 5413, "idx": 1}, {"begin": 18352, "end": 18353, "target": "#formula_14", "idx": 2}, {"begin": 22760, "end": 22762, "target": "#formula_23", "idx": 3}, {"begin": 36319, "end": 36321, "target": "#formula_38", "idx": 4}, {"begin": 36329, "end": 36331, "target": "#formula_40", "idx": 5}, {"begin": 36512, "end": 36514, "target": "#formula_42", "idx": 6}, {"begin": 36602, "end": 36604, "idx": 7}, {"begin": 36659, "end": 36661, "target": "#formula_42", "idx": 8}, {"begin": 41775, "end": 41777, "target": "#formula_48", "idx": 9}, {"begin": 44575, "end": 44577, "target": "#formula_63", "idx": 10}, {"begin": 44585, "end": 44587, "target": "#formula_66", "idx": 11}, {"begin": 48199, "end": 48201, "idx": 12}, {"begin": 53620, "end": 53622, "target": "#formula_87", "idx": 13}, {"begin": 56606, "end": 56608, "idx": 14}, {"begin": 56612, "end": 56614, "target": "#formula_92", "idx": 15}, {"begin": 74100, "end": 74104, "idx": 16}], "SectionReference": [{"begin": 80781, "end": 88786, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1315, "idx": 0}], "Div": [{"begin": 92, "end": 1307, "idx": 0}, {"begin": 1318, "end": 6200, "idx": 1}, {"begin": 6202, "end": 7914, "idx": 2}, {"begin": 7916, "end": 9894, "idx": 3}, {"begin": 9896, "end": 12707, "idx": 4}, {"begin": 12709, "end": 13018, "idx": 5}, {"begin": 13020, "end": 13574, "idx": 6}, {"begin": 13576, "end": 16218, "idx": 7}, {"begin": 16220, "end": 18729, "idx": 8}, {"begin": 18731, "end": 23243, "idx": 9}, {"begin": 23245, "end": 25736, "idx": 10}, {"begin": 25738, "end": 26916, "idx": 11}, {"begin": 26918, "end": 31495, "idx": 12}, {"begin": 31497, "end": 33196, "idx": 13}, {"begin": 33198, "end": 39348, "idx": 14}, {"begin": 39350, "end": 41545, "idx": 15}, {"begin": 41547, "end": 45838, "idx": 16}, {"begin": 45840, "end": 47405, "idx": 17}, {"begin": 47407, "end": 51312, "idx": 18}, {"begin": 51314, "end": 52106, "idx": 19}, {"begin": 52108, "end": 57415, "idx": 20}, {"begin": 57417, "end": 62771, "idx": 21}, {"begin": 62773, "end": 65191, "idx": 22}, {"begin": 65193, "end": 68135, "idx": 23}, {"begin": 68137, "end": 70819, "idx": 24}, {"begin": 70821, "end": 74575, "idx": 25}, {"begin": 74577, "end": 74772, "idx": 26}, {"begin": 74774, "end": 77496, "idx": 27}, {"begin": 77498, "end": 80767, "idx": 28}], "Head": [{"begin": 1318, "end": 1332, "n": "1", "idx": 0}, {"begin": 6202, "end": 6217, "n": "2", "idx": 1}, {"begin": 7916, "end": 7950, "n": "2.1", "idx": 2}, {"begin": 9896, "end": 9949, "n": "2.2", "idx": 3}, {"begin": 12709, "end": 12723, "n": "3", "idx": 4}, {"begin": 13020, "end": 13044, "n": "3.1", "idx": 5}, {"begin": 13576, "end": 13639, "n": "3.1.1", "idx": 6}, {"begin": 16220, "end": 16250, "n": "3.1.2", "idx": 7}, {"begin": 18731, "end": 18776, "n": "3.1.3", "idx": 8}, {"begin": 23245, "end": 23306, "n": "3.2", "idx": 9}, {"begin": 25738, "end": 25782, "idx": 10}, {"begin": 26918, "end": 26931, "n": "4", "idx": 11}, {"begin": 31497, "end": 31509, "n": "6", "idx": 12}, {"begin": 33198, "end": 33206, "idx": 13}, {"begin": 39350, "end": 39388, "idx": 14}, {"begin": 41547, "end": 41558, "idx": 15}, {"begin": 45840, "end": 45875, "idx": 16}, {"begin": 47407, "end": 47454, "n": "5", "idx": 17}, {"begin": 51314, "end": 51346, "idx": 18}, {"begin": 52108, "end": 52113, "idx": 19}, {"begin": 57417, "end": 57439, "idx": 20}, {"begin": 62773, "end": 62797, "idx": 21}, {"begin": 65193, "end": 65217, "idx": 22}, {"begin": 68137, "end": 68159, "idx": 23}, {"begin": 70821, "end": 70844, "idx": 24}, {"begin": 74577, "end": 74590, "idx": 25}, {"begin": 74774, "end": 74796, "idx": 26}, {"begin": 77498, "end": 77524, "idx": 27}], "Paragraph": [{"begin": 92, "end": 411, "idx": 0}, {"begin": 412, "end": 1307, "idx": 1}, {"begin": 1333, "end": 2016, "idx": 2}, {"begin": 2017, "end": 2695, "idx": 3}, {"begin": 2696, "end": 3540, "idx": 4}, {"begin": 3541, "end": 4554, "idx": 5}, {"begin": 4555, "end": 6200, "idx": 6}, {"begin": 6218, "end": 6976, "idx": 7}, {"begin": 7012, "end": 7290, "idx": 8}, {"begin": 7291, "end": 7458, "idx": 9}, {"begin": 7523, "end": 7914, "idx": 10}, {"begin": 7951, "end": 8105, "idx": 11}, {"begin": 8106, "end": 8146, "idx": 12}, {"begin": 8147, "end": 8652, "idx": 13}, {"begin": 8668, "end": 8701, "idx": 14}, {"begin": 8702, "end": 8705, "idx": 15}, {"begin": 8706, "end": 9280, "idx": 16}, {"begin": 9304, "end": 9515, "idx": 17}, {"begin": 9589, "end": 9894, "idx": 18}, {"begin": 9950, "end": 10242, "idx": 19}, {"begin": 10243, "end": 10523, "idx": 20}, {"begin": 10620, "end": 10708, "idx": 21}, {"begin": 10709, "end": 10933, "idx": 22}, {"begin": 10934, "end": 11436, "idx": 23}, {"begin": 11437, "end": 11720, "idx": 24}, {"begin": 11721, "end": 11749, "idx": 25}, {"begin": 11811, "end": 12707, "idx": 26}, {"begin": 12724, "end": 13018, "idx": 27}, {"begin": 13045, "end": 13574, "idx": 28}, {"begin": 13640, "end": 13719, "idx": 29}, {"begin": 13720, "end": 13849, "idx": 30}, {"begin": 14005, "end": 14739, "idx": 31}, {"begin": 14774, "end": 14938, "idx": 32}, {"begin": 15312, "end": 16015, "idx": 33}, {"begin": 16058, "end": 16218, "idx": 34}, {"begin": 16251, "end": 16865, "idx": 35}, {"begin": 16926, "end": 17017, "idx": 36}, {"begin": 17207, "end": 17994, "idx": 37}, {"begin": 17995, "end": 18102, "idx": 38}, {"begin": 18218, "end": 18729, "idx": 39}, {"begin": 18777, "end": 19258, "idx": 40}, {"begin": 19259, "end": 19526, "idx": 41}, {"begin": 19702, "end": 19910, "idx": 42}, {"begin": 19911, "end": 20190, "idx": 43}, {"begin": 20279, "end": 21135, "idx": 44}, {"begin": 21136, "end": 21235, "idx": 45}, {"begin": 21325, "end": 21522, "idx": 46}, {"begin": 21754, "end": 21947, "idx": 47}, {"begin": 21948, "end": 22304, "idx": 48}, {"begin": 22392, "end": 22980, "idx": 49}, {"begin": 23074, "end": 23243, "idx": 50}, {"begin": 23307, "end": 23416, "idx": 51}, {"begin": 23457, "end": 23553, "idx": 52}, {"begin": 23554, "end": 24488, "idx": 53}, {"begin": 24548, "end": 24553, "idx": 54}, {"begin": 24757, "end": 24762, "idx": 55}, {"begin": 24972, "end": 25080, "idx": 56}, {"begin": 25123, "end": 25736, "idx": 57}, {"begin": 25783, "end": 26512, "idx": 58}, {"begin": 26513, "end": 26916, "idx": 59}, {"begin": 26932, "end": 27924, "idx": 60}, {"begin": 27925, "end": 28618, "idx": 61}, {"begin": 28619, "end": 29860, "idx": 62}, {"begin": 29861, "end": 31197, "idx": 63}, {"begin": 31198, "end": 31495, "idx": 64}, {"begin": 31510, "end": 32153, "idx": 65}, {"begin": 32154, "end": 32649, "idx": 66}, {"begin": 32650, "end": 33196, "idx": 67}, {"begin": 33207, "end": 33527, "idx": 68}, {"begin": 33528, "end": 33740, "idx": 69}, {"begin": 33741, "end": 33832, "idx": 70}, {"begin": 33927, "end": 34064, "idx": 71}, {"begin": 34164, "end": 34311, "idx": 72}, {"begin": 34415, "end": 34715, "idx": 73}, {"begin": 34748, "end": 34836, "idx": 74}, {"begin": 34983, "end": 35432, "idx": 75}, {"begin": 35466, "end": 35831, "idx": 76}, {"begin": 36073, "end": 36120, "idx": 77}, {"begin": 36297, "end": 36341, "idx": 78}, {"begin": 36413, "end": 36734, "idx": 79}, {"begin": 36795, "end": 36826, "idx": 80}, {"begin": 37012, "end": 38058, "idx": 81}, {"begin": 38059, "end": 38363, "idx": 82}, {"begin": 38420, "end": 39089, "idx": 83}, {"begin": 39168, "end": 39348, "idx": 84}, {"begin": 39389, "end": 39523, "idx": 85}, {"begin": 39642, "end": 39718, "idx": 86}, {"begin": 39877, "end": 39901, "idx": 87}, {"begin": 40372, "end": 40384, "idx": 88}, {"begin": 40469, "end": 40538, "idx": 89}, {"begin": 40598, "end": 40880, "idx": 90}, {"begin": 41011, "end": 41021, "idx": 91}, {"begin": 41142, "end": 41230, "idx": 92}, {"begin": 41263, "end": 41545, "idx": 93}, {"begin": 41763, "end": 41788, "idx": 94}, {"begin": 42020, "end": 42153, "idx": 95}, {"begin": 42238, "end": 42277, "idx": 96}, {"begin": 42408, "end": 42666, "idx": 97}, {"begin": 42749, "end": 42800, "idx": 98}, {"begin": 43488, "end": 43523, "idx": 99}, {"begin": 43612, "end": 43617, "idx": 100}, {"begin": 43639, "end": 43676, "idx": 101}, {"begin": 43677, "end": 43870, "idx": 102}, {"begin": 44075, "end": 44116, "idx": 103}, {"begin": 44517, "end": 44562, "idx": 104}, {"begin": 44563, "end": 44645, "idx": 105}, {"begin": 44748, "end": 44808, "idx": 106}, {"begin": 44831, "end": 45349, "idx": 107}, {"begin": 45350, "end": 45476, "idx": 108}, {"begin": 45510, "end": 45542, "idx": 109}, {"begin": 45645, "end": 45727, "idx": 110}, {"begin": 46275, "end": 46302, "idx": 111}, {"begin": 46418, "end": 46454, "idx": 112}, {"begin": 46524, "end": 46561, "idx": 113}, {"begin": 46682, "end": 46687, "idx": 114}, {"begin": 47266, "end": 47284, "idx": 115}, {"begin": 47455, "end": 47486, "idx": 116}, {"begin": 47646, "end": 47745, "idx": 117}, {"begin": 47837, "end": 47918, "idx": 118}, {"begin": 48191, "end": 48216, "idx": 119}, {"begin": 49124, "end": 49148, "idx": 120}, {"begin": 49783, "end": 49809, "idx": 121}, {"begin": 50042, "end": 50053, "idx": 122}, {"begin": 50132, "end": 50139, "idx": 123}, {"begin": 50274, "end": 50443, "idx": 124}, {"begin": 50444, "end": 50535, "idx": 125}, {"begin": 50575, "end": 50945, "idx": 126}, {"begin": 51230, "end": 51312, "idx": 127}, {"begin": 51347, "end": 51388, "idx": 128}, {"begin": 51730, "end": 51978, "idx": 129}, {"begin": 52114, "end": 52300, "idx": 130}, {"begin": 52352, "end": 52474, "idx": 131}, {"begin": 52580, "end": 52748, "idx": 132}, {"begin": 52962, "end": 53026, "idx": 133}, {"begin": 53064, "end": 53248, "idx": 134}, {"begin": 53411, "end": 53516, "idx": 135}, {"begin": 53609, "end": 53643, "idx": 136}, {"begin": 53800, "end": 53945, "idx": 137}, {"begin": 54289, "end": 54375, "idx": 138}, {"begin": 54632, "end": 54791, "idx": 139}, {"begin": 55088, "end": 55248, "idx": 140}, {"begin": 55334, "end": 55399, "idx": 141}, {"begin": 55430, "end": 55448, "idx": 142}, {"begin": 56167, "end": 56235, "idx": 143}, {"begin": 56236, "end": 56325, "idx": 144}, {"begin": 56570, "end": 56920, "idx": 145}, {"begin": 57212, "end": 57294, "idx": 146}, {"begin": 57324, "end": 57415, "idx": 147}, {"begin": 57440, "end": 57518, "idx": 148}, {"begin": 57907, "end": 58161, "idx": 149}, {"begin": 58215, "end": 58289, "idx": 150}, {"begin": 58362, "end": 58505, "idx": 151}, {"begin": 58585, "end": 58923, "idx": 152}, {"begin": 59055, "end": 59089, "idx": 153}, {"begin": 59292, "end": 59364, "idx": 154}, {"begin": 59555, "end": 59565, "idx": 155}, {"begin": 59649, "end": 59665, "idx": 156}, {"begin": 60005, "end": 60023, "idx": 157}, {"begin": 60154, "end": 60167, "idx": 158}, {"begin": 60289, "end": 60430, "idx": 159}, {"begin": 60854, "end": 60941, "idx": 160}, {"begin": 61021, "end": 61031, "idx": 161}, {"begin": 61148, "end": 61149, "idx": 162}, {"begin": 61680, "end": 61691, "idx": 163}, {"begin": 61769, "end": 61829, "idx": 164}, {"begin": 62034, "end": 62081, "idx": 165}, {"begin": 62248, "end": 62318, "idx": 166}, {"begin": 62477, "end": 62541, "idx": 167}, {"begin": 62724, "end": 62771, "idx": 168}, {"begin": 62798, "end": 63282, "idx": 169}, {"begin": 63331, "end": 63413, "idx": 170}, {"begin": 63449, "end": 63607, "idx": 171}, {"begin": 63678, "end": 63721, "idx": 172}, {"begin": 63785, "end": 64463, "idx": 173}, {"begin": 64464, "end": 64564, "idx": 174}, {"begin": 64752, "end": 65191, "idx": 175}, {"begin": 65218, "end": 65965, "idx": 176}, {"begin": 66279, "end": 66727, "idx": 177}, {"begin": 66728, "end": 66754, "idx": 178}, {"begin": 66755, "end": 66840, "idx": 179}, {"begin": 66936, "end": 67156, "idx": 180}, {"begin": 67176, "end": 67303, "idx": 181}, {"begin": 67340, "end": 67544, "idx": 182}, {"begin": 67561, "end": 67599, "idx": 183}, {"begin": 67747, "end": 68135, "idx": 184}, {"begin": 68160, "end": 68568, "idx": 185}, {"begin": 68609, "end": 68718, "idx": 186}, {"begin": 68719, "end": 69700, "idx": 187}, {"begin": 69701, "end": 70153, "idx": 188}, {"begin": 70154, "end": 70819, "idx": 189}, {"begin": 70845, "end": 72139, "idx": 190}, {"begin": 72167, "end": 74575, "idx": 191}, {"begin": 74591, "end": 74772, "idx": 192}, {"begin": 74797, "end": 75496, "idx": 193}, {"begin": 75497, "end": 75574, "idx": 194}, {"begin": 75653, "end": 75797, "idx": 195}, {"begin": 75798, "end": 75991, "idx": 196}, {"begin": 75992, "end": 76141, "idx": 197}, {"begin": 76142, "end": 76351, "idx": 198}, {"begin": 76352, "end": 77429, "idx": 199}, {"begin": 77430, "end": 77496, "idx": 200}, {"begin": 77525, "end": 77587, "idx": 201}, {"begin": 77588, "end": 78165, "idx": 202}, {"begin": 78166, "end": 78570, "idx": 203}, {"begin": 78571, "end": 79124, "idx": 204}, {"begin": 79125, "end": 80123, "idx": 205}, {"begin": 80124, "end": 80767, "idx": 206}], "ReferenceToBib": [{"begin": 1535, "end": 1564, "idx": 0}, {"begin": 1813, "end": 1835, "target": "#b12", "idx": 1}, {"begin": 1836, "end": 1854, "target": "#b5", "idx": 2}, {"begin": 1855, "end": 1877, "target": "#b13", "idx": 3}, {"begin": 2731, "end": 2749, "target": "#b4", "idx": 4}, {"begin": 3278, "end": 3303, "idx": 5}, {"begin": 3304, "end": 3323, "target": "#b8", "idx": 6}, {"begin": 3324, "end": 3343, "target": "#b10", "idx": 7}, {"begin": 3373, "end": 3391, "target": "#b4", "idx": 8}, {"begin": 3825, "end": 3844, "target": "#b31", "idx": 9}, {"begin": 3845, "end": 3861, "idx": 10}, {"begin": 3862, "end": 3879, "idx": 11}, {"begin": 3880, "end": 3903, "idx": 12}, {"begin": 3904, "end": 3923, "target": "#b2", "idx": 13}, {"begin": 3924, "end": 3946, "target": "#b7", "idx": 14}, {"begin": 3947, "end": 3961, "target": "#b14", "idx": 15}, {"begin": 3962, "end": 3982, "idx": 16}, {"begin": 3983, "end": 4001, "idx": 17}, {"begin": 4507, "end": 4526, "target": "#b31", "idx": 18}, {"begin": 4527, "end": 4546, "target": "#b2", "idx": 19}, {"begin": 4547, "end": 4553, "target": "#b4", "idx": 20}, {"begin": 9387, "end": 9406, "target": "#b31", "idx": 21}, {"begin": 9407, "end": 9428, "idx": 22}, {"begin": 11669, "end": 11696, "idx": 23}, {"begin": 11697, "end": 11719, "target": "#b11", "idx": 24}, {"begin": 14190, "end": 14210, "idx": 25}, {"begin": 14211, "end": 14231, "idx": 26}, {"begin": 17651, "end": 17675, "target": "#b36", "idx": 27}, {"begin": 25402, "end": 25426, "target": "#b29", "idx": 28}, {"begin": 25619, "end": 25643, "idx": 29}, {"begin": 25644, "end": 25664, "target": "#b33", "idx": 30}, {"begin": 25665, "end": 25688, "target": "#b33", "idx": 31}, {"begin": 25689, "end": 25712, "target": "#b37", "idx": 32}, {"begin": 25713, "end": 25735, "target": "#b24", "idx": 33}, {"begin": 27117, "end": 27136, "idx": 34}, {"begin": 27515, "end": 27534, "idx": 35}, {"begin": 27904, "end": 27923, "idx": 36}, {"begin": 29121, "end": 29145, "target": "#b39", "idx": 37}, {"begin": 29146, "end": 29163, "target": "#b19", "idx": 38}, {"begin": 29651, "end": 29670, "target": "#b40", "idx": 39}, {"begin": 29671, "end": 29688, "target": "#b20", "idx": 40}, {"begin": 29689, "end": 29708, "idx": 41}, {"begin": 29709, "end": 29725, "idx": 42}, {"begin": 29726, "end": 29731, "target": "#b32", "idx": 43}, {"begin": 29732, "end": 29752, "target": "#b9", "idx": 44}, {"begin": 29753, "end": 29773, "idx": 45}, {"begin": 29774, "end": 29797, "target": "#b1", "idx": 46}, {"begin": 29798, "end": 29815, "target": "#b19", "idx": 47}, {"begin": 29816, "end": 29834, "target": "#b17", "idx": 48}, {"begin": 29835, "end": 29859, "target": "#b22", "idx": 49}, {"begin": 39067, "end": 39080, "target": "#b23", "idx": 50}, {"begin": 39508, "end": 39521, "target": "#b6", "idx": 51}, {"begin": 55188, "end": 55212, "target": "#b0", "idx": 52}, {"begin": 55213, "end": 55225, "target": "#b35", "idx": 53}, {"begin": 55226, "end": 55239, "target": "#b23", "idx": 54}, {"begin": 64290, "end": 64311, "idx": 55}, {"begin": 66474, "end": 66495, "idx": 56}, {"begin": 68842, "end": 68856, "target": "#b29", "idx": 57}, {"begin": 68857, "end": 68873, "idx": 58}, {"begin": 70877, "end": 70895, "target": "#b4", "idx": 59}, {"begin": 72239, "end": 72253, "target": "#b30", "idx": 60}, {"begin": 72254, "end": 72274, "idx": 61}, {"begin": 74529, "end": 74549, "target": "#b36", "idx": 62}, {"begin": 74550, "end": 74571, "idx": 63}, {"begin": 74572, "end": 74574, "idx": 64}, {"begin": 74990, "end": 75026, "idx": 65}, {"begin": 75028, "end": 75065, "idx": 66}, {"begin": 75085, "end": 75108, "target": "#b11", "idx": 67}, {"begin": 76785, "end": 76809, "idx": 68}, {"begin": 76810, "end": 76830, "target": "#b29", "idx": 69}, {"begin": 77036, "end": 77056, "target": "#b29", "idx": 70}, {"begin": 77057, "end": 77079, "target": "#b24", "idx": 71}, {"begin": 77080, "end": 77103, "target": "#b37", "idx": 72}, {"begin": 77474, "end": 77495, "target": "#b28", "idx": 73}, {"begin": 77704, "end": 77730, "idx": 74}, {"begin": 77737, "end": 77758, "target": "#b25", "idx": 75}, {"begin": 77768, "end": 77789, "target": "#b34", "idx": 76}, {"begin": 77809, "end": 77828, "target": "#b38", "idx": 77}], "Sentence": [{"begin": 92, "end": 236, "idx": 0}, {"begin": 237, "end": 411, "idx": 1}, {"begin": 412, "end": 609, "idx": 2}, {"begin": 610, "end": 834, "idx": 3}, {"begin": 835, "end": 997, "idx": 4}, {"begin": 998, "end": 1178, "idx": 5}, {"begin": 1179, "end": 1307, "idx": 6}, {"begin": 1333, "end": 1436, "idx": 7}, {"begin": 1437, "end": 1565, "idx": 8}, {"begin": 1566, "end": 1642, "idx": 9}, {"begin": 1643, "end": 1878, "idx": 10}, {"begin": 1879, "end": 2007, "idx": 11}, {"begin": 2008, "end": 2016, "idx": 12}, {"begin": 2017, "end": 2314, "idx": 13}, {"begin": 2315, "end": 2420, "idx": 14}, {"begin": 2421, "end": 2568, "idx": 15}, {"begin": 2569, "end": 2695, "idx": 16}, {"begin": 2696, "end": 2884, "idx": 17}, {"begin": 2885, "end": 3110, "idx": 18}, {"begin": 3111, "end": 3344, "idx": 19}, {"begin": 3345, "end": 3540, "idx": 20}, {"begin": 3541, "end": 3668, "idx": 21}, {"begin": 3669, "end": 4002, "idx": 22}, {"begin": 4003, "end": 4142, "idx": 23}, {"begin": 4143, "end": 4249, "idx": 24}, {"begin": 4250, "end": 4338, "idx": 25}, {"begin": 4339, "end": 4554, "idx": 26}, {"begin": 4555, "end": 4793, "idx": 27}, {"begin": 4794, "end": 4953, "idx": 28}, {"begin": 4954, "end": 5045, "idx": 29}, {"begin": 5046, "end": 5216, "idx": 30}, {"begin": 5217, "end": 5345, "idx": 31}, {"begin": 5346, "end": 5572, "idx": 32}, {"begin": 5573, "end": 5702, "idx": 33}, {"begin": 5703, "end": 5927, "idx": 34}, {"begin": 5928, "end": 6075, "idx": 35}, {"begin": 6076, "end": 6200, "idx": 36}, {"begin": 6218, "end": 6252, "idx": 37}, {"begin": 6253, "end": 6419, "idx": 38}, {"begin": 6420, "end": 6813, "idx": 39}, {"begin": 6814, "end": 6976, "idx": 40}, {"begin": 7012, "end": 7202, "idx": 41}, {"begin": 7203, "end": 7290, "idx": 42}, {"begin": 7291, "end": 7376, "idx": 43}, {"begin": 7377, "end": 7458, "idx": 44}, {"begin": 7523, "end": 7649, "idx": 45}, {"begin": 7650, "end": 7783, "idx": 46}, {"begin": 7784, "end": 7914, "idx": 47}, {"begin": 7951, "end": 8105, "idx": 48}, {"begin": 8106, "end": 8146, "idx": 49}, {"begin": 8147, "end": 8228, "idx": 50}, {"begin": 8229, "end": 8407, "idx": 51}, {"begin": 8408, "end": 8565, "idx": 52}, {"begin": 8566, "end": 8652, "idx": 53}, {"begin": 8668, "end": 8701, "idx": 54}, {"begin": 8702, "end": 8705, "idx": 55}, {"begin": 8706, "end": 8913, "idx": 56}, {"begin": 8914, "end": 9138, "idx": 57}, {"begin": 9139, "end": 9193, "idx": 58}, {"begin": 9194, "end": 9280, "idx": 59}, {"begin": 9304, "end": 9515, "idx": 60}, {"begin": 9589, "end": 9696, "idx": 61}, {"begin": 9697, "end": 9894, "idx": 62}, {"begin": 9950, "end": 10098, "idx": 63}, {"begin": 10099, "end": 10242, "idx": 64}, {"begin": 10243, "end": 10347, "idx": 65}, {"begin": 10348, "end": 10523, "idx": 66}, {"begin": 10620, "end": 10626, "idx": 67}, {"begin": 10627, "end": 10708, "idx": 68}, {"begin": 10709, "end": 10821, "idx": 69}, {"begin": 10822, "end": 10933, "idx": 70}, {"begin": 10934, "end": 11119, "idx": 71}, {"begin": 11120, "end": 11194, "idx": 72}, {"begin": 11195, "end": 11436, "idx": 73}, {"begin": 11437, "end": 11499, "idx": 74}, {"begin": 11500, "end": 11720, "idx": 75}, {"begin": 11721, "end": 11749, "idx": 76}, {"begin": 11811, "end": 11822, "idx": 77}, {"begin": 11823, "end": 11939, "idx": 78}, {"begin": 11940, "end": 12335, "idx": 79}, {"begin": 12336, "end": 12607, "idx": 80}, {"begin": 12608, "end": 12707, "idx": 81}, {"begin": 12724, "end": 12873, "idx": 82}, {"begin": 12874, "end": 13018, "idx": 83}, {"begin": 13045, "end": 13070, "idx": 84}, {"begin": 13071, "end": 13172, "idx": 85}, {"begin": 13173, "end": 13325, "idx": 86}, {"begin": 13326, "end": 13354, "idx": 87}, {"begin": 13355, "end": 13574, "idx": 88}, {"begin": 13640, "end": 13719, "idx": 89}, {"begin": 13720, "end": 13733, "idx": 90}, {"begin": 13734, "end": 13849, "idx": 91}, {"begin": 14005, "end": 14232, "idx": 92}, {"begin": 14233, "end": 14295, "idx": 93}, {"begin": 14296, "end": 14413, "idx": 94}, {"begin": 14414, "end": 14642, "idx": 95}, {"begin": 14643, "end": 14739, "idx": 96}, {"begin": 14774, "end": 14938, "idx": 97}, {"begin": 15312, "end": 15436, "idx": 98}, {"begin": 15437, "end": 15497, "idx": 99}, {"begin": 15498, "end": 15585, "idx": 100}, {"begin": 15586, "end": 15878, "idx": 101}, {"begin": 15879, "end": 15941, "idx": 102}, {"begin": 15942, "end": 16015, "idx": 103}, {"begin": 16058, "end": 16101, "idx": 104}, {"begin": 16102, "end": 16106, "idx": 105}, {"begin": 16107, "end": 16191, "idx": 106}, {"begin": 16192, "end": 16212, "idx": 107}, {"begin": 16213, "end": 16218, "idx": 108}, {"begin": 16251, "end": 16512, "idx": 109}, {"begin": 16513, "end": 16624, "idx": 110}, {"begin": 16625, "end": 16680, "idx": 111}, {"begin": 16681, "end": 16865, "idx": 112}, {"begin": 16926, "end": 17017, "idx": 113}, {"begin": 17207, "end": 17213, "idx": 114}, {"begin": 17214, "end": 17297, "idx": 115}, {"begin": 17298, "end": 17676, "idx": 116}, {"begin": 17677, "end": 17727, "idx": 117}, {"begin": 17728, "end": 17930, "idx": 118}, {"begin": 17931, "end": 17994, "idx": 119}, {"begin": 17995, "end": 18102, "idx": 120}, {"begin": 18218, "end": 18506, "idx": 121}, {"begin": 18507, "end": 18729, "idx": 122}, {"begin": 18777, "end": 18964, "idx": 123}, {"begin": 18965, "end": 19082, "idx": 124}, {"begin": 19083, "end": 19258, "idx": 125}, {"begin": 19259, "end": 19371, "idx": 126}, {"begin": 19372, "end": 19436, "idx": 127}, {"begin": 19437, "end": 19473, "idx": 128}, {"begin": 19474, "end": 19506, "idx": 129}, {"begin": 19507, "end": 19526, "idx": 130}, {"begin": 19702, "end": 19708, "idx": 131}, {"begin": 19709, "end": 19747, "idx": 132}, {"begin": 19748, "end": 19869, "idx": 133}, {"begin": 19870, "end": 19910, "idx": 134}, {"begin": 19911, "end": 20043, "idx": 135}, {"begin": 20044, "end": 20110, "idx": 136}, {"begin": 20111, "end": 20190, "idx": 137}, {"begin": 20279, "end": 20328, "idx": 138}, {"begin": 20329, "end": 20489, "idx": 139}, {"begin": 20490, "end": 20742, "idx": 140}, {"begin": 20743, "end": 20963, "idx": 141}, {"begin": 20964, "end": 21135, "idx": 142}, {"begin": 21136, "end": 21235, "idx": 143}, {"begin": 21325, "end": 21389, "idx": 144}, {"begin": 21390, "end": 21419, "idx": 145}, {"begin": 21420, "end": 21522, "idx": 146}, {"begin": 21754, "end": 21760, "idx": 147}, {"begin": 21761, "end": 21906, "idx": 148}, {"begin": 21907, "end": 21947, "idx": 149}, {"begin": 21948, "end": 22090, "idx": 150}, {"begin": 22091, "end": 22241, "idx": 151}, {"begin": 22242, "end": 22304, "idx": 152}, {"begin": 22392, "end": 22564, "idx": 153}, {"begin": 22565, "end": 22694, "idx": 154}, {"begin": 22695, "end": 22897, "idx": 155}, {"begin": 22898, "end": 22980, "idx": 156}, {"begin": 23074, "end": 23111, "idx": 157}, {"begin": 23112, "end": 23243, "idx": 158}, {"begin": 23307, "end": 23416, "idx": 159}, {"begin": 23457, "end": 23553, "idx": 160}, {"begin": 23554, "end": 23831, "idx": 161}, {"begin": 23832, "end": 24233, "idx": 162}, {"begin": 24234, "end": 24356, "idx": 163}, {"begin": 24357, "end": 24396, "idx": 164}, {"begin": 24397, "end": 24488, "idx": 165}, {"begin": 24548, "end": 24553, "idx": 166}, {"begin": 24757, "end": 24762, "idx": 167}, {"begin": 24972, "end": 24978, "idx": 168}, {"begin": 24979, "end": 25080, "idx": 169}, {"begin": 25123, "end": 25427, "idx": 170}, {"begin": 25428, "end": 25736, "idx": 171}, {"begin": 25783, "end": 26011, "idx": 172}, {"begin": 26012, "end": 26254, "idx": 173}, {"begin": 26255, "end": 26512, "idx": 174}, {"begin": 26513, "end": 26683, "idx": 175}, {"begin": 26684, "end": 26916, "idx": 176}, {"begin": 26932, "end": 27017, "idx": 177}, {"begin": 27018, "end": 27267, "idx": 178}, {"begin": 27268, "end": 27361, "idx": 179}, {"begin": 27362, "end": 27427, "idx": 180}, {"begin": 27428, "end": 27574, "idx": 181}, {"begin": 27575, "end": 27800, "idx": 182}, {"begin": 27801, "end": 27924, "idx": 183}, {"begin": 27925, "end": 28077, "idx": 184}, {"begin": 28078, "end": 28100, "idx": 185}, {"begin": 28101, "end": 28329, "idx": 186}, {"begin": 28330, "end": 28430, "idx": 187}, {"begin": 28431, "end": 28618, "idx": 188}, {"begin": 28619, "end": 28670, "idx": 189}, {"begin": 28671, "end": 28807, "idx": 190}, {"begin": 28808, "end": 28929, "idx": 191}, {"begin": 28930, "end": 29164, "idx": 192}, {"begin": 29165, "end": 29333, "idx": 193}, {"begin": 29334, "end": 29482, "idx": 194}, {"begin": 29483, "end": 29860, "idx": 195}, {"begin": 29861, "end": 30059, "idx": 196}, {"begin": 30060, "end": 30252, "idx": 197}, {"begin": 30253, "end": 30433, "idx": 198}, {"begin": 30434, "end": 30604, "idx": 199}, {"begin": 30605, "end": 30769, "idx": 200}, {"begin": 30770, "end": 30917, "idx": 201}, {"begin": 30918, "end": 31027, "idx": 202}, {"begin": 31028, "end": 31197, "idx": 203}, {"begin": 31198, "end": 31368, "idx": 204}, {"begin": 31369, "end": 31437, "idx": 205}, {"begin": 31438, "end": 31495, "idx": 206}, {"begin": 31510, "end": 31751, "idx": 207}, {"begin": 31752, "end": 31903, "idx": 208}, {"begin": 31904, "end": 32153, "idx": 209}, {"begin": 32154, "end": 32554, "idx": 210}, {"begin": 32555, "end": 32649, "idx": 211}, {"begin": 32650, "end": 32956, "idx": 212}, {"begin": 32957, "end": 33196, "idx": 213}, {"begin": 33207, "end": 33275, "idx": 214}, {"begin": 33276, "end": 33447, "idx": 215}, {"begin": 33448, "end": 33527, "idx": 216}, {"begin": 33528, "end": 33740, "idx": 217}, {"begin": 33741, "end": 33761, "idx": 218}, {"begin": 33762, "end": 33832, "idx": 219}, {"begin": 33927, "end": 33986, "idx": 220}, {"begin": 33987, "end": 34064, "idx": 221}, {"begin": 34164, "end": 34223, "idx": 222}, {"begin": 34224, "end": 34311, "idx": 223}, {"begin": 34415, "end": 34472, "idx": 224}, {"begin": 34473, "end": 34583, "idx": 225}, {"begin": 34584, "end": 34671, "idx": 226}, {"begin": 34672, "end": 34715, "idx": 227}, {"begin": 34748, "end": 34836, "idx": 228}, {"begin": 34983, "end": 35041, "idx": 229}, {"begin": 35042, "end": 35432, "idx": 230}, {"begin": 35466, "end": 35580, "idx": 231}, {"begin": 35581, "end": 35702, "idx": 232}, {"begin": 35703, "end": 35831, "idx": 233}, {"begin": 36073, "end": 36120, "idx": 234}, {"begin": 36297, "end": 36341, "idx": 235}, {"begin": 36413, "end": 36577, "idx": 236}, {"begin": 36578, "end": 36734, "idx": 237}, {"begin": 36795, "end": 36826, "idx": 238}, {"begin": 37012, "end": 37056, "idx": 239}, {"begin": 37057, "end": 37189, "idx": 240}, {"begin": 37190, "end": 37325, "idx": 241}, {"begin": 37326, "end": 37483, "idx": 242}, {"begin": 37484, "end": 37640, "idx": 243}, {"begin": 37641, "end": 37722, "idx": 244}, {"begin": 37723, "end": 37814, "idx": 245}, {"begin": 37815, "end": 37894, "idx": 246}, {"begin": 37895, "end": 38058, "idx": 247}, {"begin": 38059, "end": 38191, "idx": 248}, {"begin": 38192, "end": 38320, "idx": 249}, {"begin": 38321, "end": 38363, "idx": 250}, {"begin": 38420, "end": 38502, "idx": 251}, {"begin": 38503, "end": 38571, "idx": 252}, {"begin": 38572, "end": 38667, "idx": 253}, {"begin": 38668, "end": 38921, "idx": 254}, {"begin": 38922, "end": 38963, "idx": 255}, {"begin": 38964, "end": 39030, "idx": 256}, {"begin": 39031, "end": 39089, "idx": 257}, {"begin": 39168, "end": 39234, "idx": 258}, {"begin": 39235, "end": 39348, "idx": 259}, {"begin": 39389, "end": 39523, "idx": 260}, {"begin": 39642, "end": 39668, "idx": 261}, {"begin": 39669, "end": 39718, "idx": 262}, {"begin": 39877, "end": 39901, "idx": 263}, {"begin": 40372, "end": 40384, "idx": 264}, {"begin": 40469, "end": 40526, "idx": 265}, {"begin": 40527, "end": 40538, "idx": 266}, {"begin": 40598, "end": 40630, "idx": 267}, {"begin": 40631, "end": 40767, "idx": 268}, {"begin": 40768, "end": 40776, "idx": 269}, {"begin": 40777, "end": 40880, "idx": 270}, {"begin": 41011, "end": 41021, "idx": 271}, {"begin": 41142, "end": 41230, "idx": 272}, {"begin": 41263, "end": 41300, "idx": 273}, {"begin": 41301, "end": 41465, "idx": 274}, {"begin": 41466, "end": 41545, "idx": 275}, {"begin": 41763, "end": 41788, "idx": 276}, {"begin": 42020, "end": 42132, "idx": 277}, {"begin": 42133, "end": 42153, "idx": 278}, {"begin": 42238, "end": 42277, "idx": 279}, {"begin": 42408, "end": 42442, "idx": 280}, {"begin": 42443, "end": 42606, "idx": 281}, {"begin": 42607, "end": 42666, "idx": 282}, {"begin": 42749, "end": 42800, "idx": 283}, {"begin": 43488, "end": 43523, "idx": 284}, {"begin": 43612, "end": 43617, "idx": 285}, {"begin": 43639, "end": 43644, "idx": 286}, {"begin": 43645, "end": 43676, "idx": 287}, {"begin": 43677, "end": 43769, "idx": 288}, {"begin": 43770, "end": 43845, "idx": 289}, {"begin": 43846, "end": 43870, "idx": 290}, {"begin": 44075, "end": 44116, "idx": 291}, {"begin": 44517, "end": 44522, "idx": 292}, {"begin": 44523, "end": 44562, "idx": 293}, {"begin": 44563, "end": 44645, "idx": 294}, {"begin": 44748, "end": 44808, "idx": 295}, {"begin": 44831, "end": 44857, "idx": 296}, {"begin": 44858, "end": 44987, "idx": 297}, {"begin": 44988, "end": 45072, "idx": 298}, {"begin": 45073, "end": 45288, "idx": 299}, {"begin": 45289, "end": 45349, "idx": 300}, {"begin": 45350, "end": 45355, "idx": 301}, {"begin": 45356, "end": 45476, "idx": 302}, {"begin": 45510, "end": 45536, "idx": 303}, {"begin": 45537, "end": 45542, "idx": 304}, {"begin": 45645, "end": 45716, "idx": 305}, {"begin": 45717, "end": 45727, "idx": 306}, {"begin": 46275, "end": 46302, "idx": 307}, {"begin": 46418, "end": 46454, "idx": 308}, {"begin": 46524, "end": 46561, "idx": 309}, {"begin": 46682, "end": 46687, "idx": 310}, {"begin": 47266, "end": 47284, "idx": 311}, {"begin": 47455, "end": 47486, "idx": 312}, {"begin": 47646, "end": 47745, "idx": 313}, {"begin": 47837, "end": 47851, "idx": 314}, {"begin": 47852, "end": 47918, "idx": 315}, {"begin": 48191, "end": 48216, "idx": 316}, {"begin": 49124, "end": 49148, "idx": 317}, {"begin": 49783, "end": 49809, "idx": 318}, {"begin": 50042, "end": 50053, "idx": 319}, {"begin": 50132, "end": 50139, "idx": 320}, {"begin": 50274, "end": 50443, "idx": 321}, {"begin": 50444, "end": 50535, "idx": 322}, {"begin": 50575, "end": 50688, "idx": 323}, {"begin": 50689, "end": 50822, "idx": 324}, {"begin": 50823, "end": 50945, "idx": 325}, {"begin": 51230, "end": 51312, "idx": 326}, {"begin": 51347, "end": 51388, "idx": 327}, {"begin": 51730, "end": 51978, "idx": 328}, {"begin": 52114, "end": 52300, "idx": 329}, {"begin": 52352, "end": 52474, "idx": 330}, {"begin": 52580, "end": 52748, "idx": 331}, {"begin": 52962, "end": 53026, "idx": 332}, {"begin": 53064, "end": 53175, "idx": 333}, {"begin": 53176, "end": 53248, "idx": 334}, {"begin": 53411, "end": 53475, "idx": 335}, {"begin": 53476, "end": 53516, "idx": 336}, {"begin": 53609, "end": 53643, "idx": 337}, {"begin": 53800, "end": 53945, "idx": 338}, {"begin": 54289, "end": 54375, "idx": 339}, {"begin": 54632, "end": 54750, "idx": 340}, {"begin": 54751, "end": 54791, "idx": 341}, {"begin": 55088, "end": 55100, "idx": 342}, {"begin": 55101, "end": 55248, "idx": 343}, {"begin": 55334, "end": 55399, "idx": 344}, {"begin": 55430, "end": 55448, "idx": 345}, {"begin": 56167, "end": 56235, "idx": 346}, {"begin": 56236, "end": 56325, "idx": 347}, {"begin": 56570, "end": 56881, "idx": 348}, {"begin": 56882, "end": 56920, "idx": 349}, {"begin": 57212, "end": 57294, "idx": 350}, {"begin": 57324, "end": 57415, "idx": 351}, {"begin": 57440, "end": 57518, "idx": 352}, {"begin": 57907, "end": 57910, "idx": 353}, {"begin": 57911, "end": 57963, "idx": 354}, {"begin": 57964, "end": 58161, "idx": 355}, {"begin": 58215, "end": 58289, "idx": 356}, {"begin": 58362, "end": 58505, "idx": 357}, {"begin": 58585, "end": 58726, "idx": 358}, {"begin": 58727, "end": 58848, "idx": 359}, {"begin": 58849, "end": 58923, "idx": 360}, {"begin": 59055, "end": 59089, "idx": 361}, {"begin": 59292, "end": 59337, "idx": 362}, {"begin": 59338, "end": 59364, "idx": 363}, {"begin": 59555, "end": 59565, "idx": 364}, {"begin": 59649, "end": 59665, "idx": 365}, {"begin": 60005, "end": 60023, "idx": 366}, {"begin": 60154, "end": 60167, "idx": 367}, {"begin": 60289, "end": 60404, "idx": 368}, {"begin": 60405, "end": 60430, "idx": 369}, {"begin": 60854, "end": 60941, "idx": 370}, {"begin": 61021, "end": 61031, "idx": 371}, {"begin": 61148, "end": 61149, "idx": 372}, {"begin": 61680, "end": 61691, "idx": 373}, {"begin": 61769, "end": 61829, "idx": 374}, {"begin": 62034, "end": 62081, "idx": 375}, {"begin": 62248, "end": 62318, "idx": 376}, {"begin": 62477, "end": 62541, "idx": 377}, {"begin": 62724, "end": 62771, "idx": 378}, {"begin": 62798, "end": 62860, "idx": 379}, {"begin": 62861, "end": 62937, "idx": 380}, {"begin": 62938, "end": 63118, "idx": 381}, {"begin": 63119, "end": 63165, "idx": 382}, {"begin": 63166, "end": 63247, "idx": 383}, {"begin": 63248, "end": 63282, "idx": 384}, {"begin": 63331, "end": 63407, "idx": 385}, {"begin": 63408, "end": 63413, "idx": 386}, {"begin": 63449, "end": 63485, "idx": 387}, {"begin": 63486, "end": 63607, "idx": 388}, {"begin": 63678, "end": 63721, "idx": 389}, {"begin": 63785, "end": 63841, "idx": 390}, {"begin": 63842, "end": 64057, "idx": 391}, {"begin": 64058, "end": 64168, "idx": 392}, {"begin": 64169, "end": 64463, "idx": 393}, {"begin": 64464, "end": 64508, "idx": 394}, {"begin": 64509, "end": 64564, "idx": 395}, {"begin": 64752, "end": 64952, "idx": 396}, {"begin": 64953, "end": 65006, "idx": 397}, {"begin": 65007, "end": 65191, "idx": 398}, {"begin": 65218, "end": 65272, "idx": 399}, {"begin": 65273, "end": 65351, "idx": 400}, {"begin": 65352, "end": 65609, "idx": 401}, {"begin": 65610, "end": 65669, "idx": 402}, {"begin": 65670, "end": 65804, "idx": 403}, {"begin": 65805, "end": 65867, "idx": 404}, {"begin": 65868, "end": 65965, "idx": 405}, {"begin": 66279, "end": 66727, "idx": 406}, {"begin": 66728, "end": 66754, "idx": 407}, {"begin": 66755, "end": 66775, "idx": 408}, {"begin": 66776, "end": 66840, "idx": 409}, {"begin": 66936, "end": 67037, "idx": 410}, {"begin": 67038, "end": 67100, "idx": 411}, {"begin": 67101, "end": 67156, "idx": 412}, {"begin": 67176, "end": 67233, "idx": 413}, {"begin": 67234, "end": 67303, "idx": 414}, {"begin": 67340, "end": 67431, "idx": 415}, {"begin": 67432, "end": 67544, "idx": 416}, {"begin": 67561, "end": 67599, "idx": 417}, {"begin": 67747, "end": 67811, "idx": 418}, {"begin": 67812, "end": 68012, "idx": 419}, {"begin": 68013, "end": 68065, "idx": 420}, {"begin": 68066, "end": 68135, "idx": 421}, {"begin": 68160, "end": 68358, "idx": 422}, {"begin": 68359, "end": 68448, "idx": 423}, {"begin": 68449, "end": 68568, "idx": 424}, {"begin": 68609, "end": 68718, "idx": 425}, {"begin": 68719, "end": 69092, "idx": 426}, {"begin": 69093, "end": 69131, "idx": 427}, {"begin": 69132, "end": 69347, "idx": 428}, {"begin": 69348, "end": 69572, "idx": 429}, {"begin": 69573, "end": 69700, "idx": 430}, {"begin": 69701, "end": 69963, "idx": 431}, {"begin": 69964, "end": 70027, "idx": 432}, {"begin": 70028, "end": 70153, "idx": 433}, {"begin": 70154, "end": 70628, "idx": 434}, {"begin": 70629, "end": 70819, "idx": 435}, {"begin": 70845, "end": 70871, "idx": 436}, {"begin": 70872, "end": 71078, "idx": 437}, {"begin": 71079, "end": 71195, "idx": 438}, {"begin": 71196, "end": 71325, "idx": 439}, {"begin": 71326, "end": 71575, "idx": 440}, {"begin": 71576, "end": 71731, "idx": 441}, {"begin": 71732, "end": 71942, "idx": 442}, {"begin": 71943, "end": 71964, "idx": 443}, {"begin": 71965, "end": 72139, "idx": 444}, {"begin": 72167, "end": 72234, "idx": 445}, {"begin": 72235, "end": 72327, "idx": 446}, {"begin": 72328, "end": 72367, "idx": 447}, {"begin": 72368, "end": 72394, "idx": 448}, {"begin": 72395, "end": 72533, "idx": 449}, {"begin": 72534, "end": 72912, "idx": 450}, {"begin": 72913, "end": 73082, "idx": 451}, {"begin": 73083, "end": 73222, "idx": 452}, {"begin": 73223, "end": 73367, "idx": 453}, {"begin": 73368, "end": 73505, "idx": 454}, {"begin": 73506, "end": 73641, "idx": 455}, {"begin": 73642, "end": 73867, "idx": 456}, {"begin": 73868, "end": 74402, "idx": 457}, {"begin": 74403, "end": 74575, "idx": 458}, {"begin": 74591, "end": 74772, "idx": 459}, {"begin": 74797, "end": 74849, "idx": 460}, {"begin": 74850, "end": 74878, "idx": 461}, {"begin": 74879, "end": 74884, "idx": 462}, {"begin": 74885, "end": 75152, "idx": 463}, {"begin": 75153, "end": 75334, "idx": 464}, {"begin": 75335, "end": 75496, "idx": 465}, {"begin": 75497, "end": 75541, "idx": 466}, {"begin": 75542, "end": 75574, "idx": 467}, {"begin": 75653, "end": 75664, "idx": 468}, {"begin": 75665, "end": 75797, "idx": 469}, {"begin": 75798, "end": 75804, "idx": 470}, {"begin": 75805, "end": 75875, "idx": 471}, {"begin": 75876, "end": 75901, "idx": 472}, {"begin": 75902, "end": 75991, "idx": 473}, {"begin": 75992, "end": 76026, "idx": 474}, {"begin": 76027, "end": 76141, "idx": 475}, {"begin": 76142, "end": 76153, "idx": 476}, {"begin": 76154, "end": 76299, "idx": 477}, {"begin": 76300, "end": 76351, "idx": 478}, {"begin": 76352, "end": 76361, "idx": 479}, {"begin": 76362, "end": 76538, "idx": 480}, {"begin": 76539, "end": 76588, "idx": 481}, {"begin": 76589, "end": 76666, "idx": 482}, {"begin": 76667, "end": 76831, "idx": 483}, {"begin": 76832, "end": 77104, "idx": 484}, {"begin": 77105, "end": 77429, "idx": 485}, {"begin": 77430, "end": 77496, "idx": 486}, {"begin": 77525, "end": 77587, "idx": 487}, {"begin": 77588, "end": 77608, "idx": 488}, {"begin": 77609, "end": 77829, "idx": 489}, {"begin": 77830, "end": 77882, "idx": 490}, {"begin": 77883, "end": 78165, "idx": 491}, {"begin": 78166, "end": 78190, "idx": 492}, {"begin": 78191, "end": 78256, "idx": 493}, {"begin": 78257, "end": 78408, "idx": 494}, {"begin": 78409, "end": 78570, "idx": 495}, {"begin": 78571, "end": 78597, "idx": 496}, {"begin": 78598, "end": 78771, "idx": 497}, {"begin": 78772, "end": 79026, "idx": 498}, {"begin": 79027, "end": 79124, "idx": 499}, {"begin": 79125, "end": 79141, "idx": 500}, {"begin": 79142, "end": 79299, "idx": 501}, {"begin": 79300, "end": 79593, "idx": 502}, {"begin": 79594, "end": 79759, "idx": 503}, {"begin": 79760, "end": 79904, "idx": 504}, {"begin": 79905, "end": 79995, "idx": 505}, {"begin": 79996, "end": 80123, "idx": 506}, {"begin": 80124, "end": 80161, "idx": 507}, {"begin": 80162, "end": 80297, "idx": 508}, {"begin": 80298, "end": 80418, "idx": 509}, {"begin": 80419, "end": 80637, "idx": 510}, {"begin": 80638, "end": 80767, "idx": 511}], "ReferenceToFigure": [{"begin": 27568, "end": 27569, "target": "#fig_3", "idx": 0}, {"begin": 28989, "end": 28990, "target": "#fig_3", "idx": 1}, {"begin": 77662, "end": 77663, "target": "#fig_1", "idx": 2}, {"begin": 77880, "end": 77881, "idx": 3}, {"begin": 77976, "end": 77977, "target": "#fig_1", "idx": 4}, {"begin": 78202, "end": 78205, "target": "#fig_1", "idx": 5}, {"begin": 78406, "end": 78407, "idx": 6}, {"begin": 78767, "end": 78770, "idx": 7}, {"begin": 78800, "end": 78803, "idx": 8}, {"begin": 79767, "end": 79768, "idx": 9}, {"begin": 80447, "end": 80448, "idx": 10}, {"begin": 80663, "end": 80664, "idx": 11}], "Abstract": [{"begin": 82, "end": 1307, "idx": 0}], "SectionFootnote": [{"begin": 80769, "end": 80779, "idx": 0}], "ReferenceString": [{"begin": 80796, "end": 80957, "id": "b0", "idx": 0}, {"begin": 80959, "end": 81174, "id": "b1", "idx": 1}, {"begin": 81178, "end": 81361, "id": "b2", "idx": 2}, {"begin": 81365, "end": 81772, "id": "b3", "idx": 3}, {"begin": 81776, "end": 81922, "id": "b4", "idx": 4}, {"begin": 81926, "end": 82079, "id": "b5", "idx": 5}, {"begin": 82083, "end": 82155, "id": "b6", "idx": 6}, {"begin": 82159, "end": 82389, "id": "b7", "idx": 7}, {"begin": 82393, "end": 82565, "id": "b8", "idx": 8}, {"begin": 82569, "end": 82741, "id": "b9", "idx": 9}, {"begin": 82745, "end": 82869, "id": "b10", "idx": 10}, {"begin": 82873, "end": 83061, "id": "b11", "idx": 11}, {"begin": 83065, "end": 83269, "id": "b12", "idx": 12}, {"begin": 83273, "end": 83443, "id": "b13", "idx": 13}, {"begin": 83447, "end": 83609, "id": "b14", "idx": 14}, {"begin": 83613, "end": 83806, "id": "b15", "idx": 15}, {"begin": 83810, "end": 83975, "id": "b16", "idx": 16}, {"begin": 83979, "end": 84163, "id": "b17", "idx": 17}, {"begin": 84167, "end": 84333, "id": "b18", "idx": 18}, {"begin": 84337, "end": 84614, "id": "b19", "idx": 19}, {"begin": 84618, "end": 84817, "id": "b20", "idx": 20}, {"begin": 84821, "end": 84986, "id": "b21", "idx": 21}, {"begin": 84990, "end": 85167, "id": "b22", "idx": 22}, {"begin": 85171, "end": 85318, "id": "b23", "idx": 23}, {"begin": 85322, "end": 85547, "id": "b24", "idx": 24}, {"begin": 85551, "end": 85776, "id": "b25", "idx": 25}, {"begin": 85780, "end": 85914, "id": "b26", "idx": 26}, {"begin": 85918, "end": 86028, "id": "b27", "idx": 27}, {"begin": 86032, "end": 86330, "id": "b28", "idx": 28}, {"begin": 86334, "end": 86652, "id": "b29", "idx": 29}, {"begin": 86656, "end": 86807, "id": "b30", "idx": 30}, {"begin": 86811, "end": 87015, "id": "b31", "idx": 31}, {"begin": 87019, "end": 87156, "id": "b32", "idx": 32}, {"begin": 87160, "end": 87369, "id": "b33", "idx": 33}, {"begin": 87373, "end": 87514, "id": "b34", "idx": 34}, {"begin": 87518, "end": 87645, "id": "b35", "idx": 35}, {"begin": 87649, "end": 87867, "id": "b36", "idx": 36}, {"begin": 87871, "end": 88088, "id": "b37", "idx": 37}, {"begin": 88092, "end": 88260, "id": "b38", "idx": 38}, {"begin": 88264, "end": 88452, "id": "b39", "idx": 39}, {"begin": 88456, "end": 88615, "id": "b40", "idx": 40}, {"begin": 88619, "end": 88784, "id": "b41", "idx": 41}]}}