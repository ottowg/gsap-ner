{"text": "A comprehensive study on the prediction reliability of graph neural networks for virtual screening\n\nAbstract:\nPrediction models based on deep neural networks are increasingly gaining attention for fast and accurate virtual screening systems. For decision makings in virtual screening, researchers find it useful to interpret an output of classification system as probability, since such interpretation allows them to filter out more desirable compounds. However, probabilistic interpretation cannot be correct for models that hold over-parameterization problems or inappropriate regularizations, leading to unreliable prediction and decision making. In this regard, we concern the reliability of neural prediction models on molecular properties, especially when models are trained with sparse data points and imbalanced distributions. This work aims to propose guidelines for training reliable models, we thus provide methodological details and ablation studies on the following train principles. We investigate the effects of model architectures, regularization methods, and loss functions on the prediction performance and reliability of classification results. Moreover, we evaluate prediction reliability of models on\n\nMain:\n\n\n\nIntroduction\nRecent advancements in deep learning 1 have opened the door to enjoying a variety of unmet molecular applications. 3] [4] [5] In contrast to using structural or physicochemical descriptors, such as Morgan fingerprints, 6 neural models employ unleashed structural inputs (e.g simplified input molecular line-entry system; SMILES and molecular graph), map them to hidden representations, and make predictions.\nFor the purpose, convolutional/recurrent neural networks [7] [8] [9] [10] and graph neural networks 11, 12 have been applied for processing SMILES and molecular graph inputs, respectively.  [23] [24] Albeit with great success, there are key challenges in developing accurate and reliable prediction models arisen from the nature of statistical learning. Since modern neural networks consist of a large number of parameters, the performance of neural models significantly deteriorates unless a large amount of data is secured. 25, 26 Furthermore, they are prone to make over-confident predictions, in that predictive output is higher than true accuracy. 27 For example of binary classification problems, the final output of neural networks is produced by a sigmoid activation and is bounded from zero to one. Hence ones tend to interpret the final output as a probability of belonging to a target class. If an output of a perfectly calibrated model is 0.8, then ones will interpret that the predictive label is positive with 80% probability of correctness. Such probabilistic interpretation enables ones to rely on the final model output for selecting compounds expected more likely to belong to target class. However, over-confident model's actual accuracy may be lower than 80% for predictions with an output probability value of 0.8, and such discrepancy may eventually interrupt the robust decision making.\nTherefore, a lot of attempts in vision recognition and language understanding have been made to enhance the reliability as well as performance of model predictions. 28, 29 For that purpose, regularization methods, 30 data augmentations 31 and advanced learning algorithms 32, 33 have been adopted. Previous works 13, 14, 23 shed light on needs for reliable-AI by studying uncertainty estimation for prediction tasks and chemical reaction planning. However, to the best of our knowledge, there is no research with thorough ablation studies that comprehensively study the affect of various factors -model architectures, regularizations, and learning and inference algorithms -on prediction reliability. This motivates us to start this work.\nIn particular, models that speak their results in the language of probability allows us to choose more desirable compounds in virtual screening, which will then be taken into account for experimental validation. One common approach is to select samples with high predictive output (sometimes referred to as confidence). Stokes et al. 34 screened compounds from drug repurposing hub library by using the prediction score of the ensemble model and experimentally validated their efficacy. In order to enhance success rate of virtual screening, however, ones may need to evaluate whether the model gives true probability of correct prediction, i.e. the relationship between true accuracy and prediction confidence. We point out that current evaluations are limited to validating model performance and providing averaged scores on entire data points of test sets, however, does not evaluate the model in terms of prediction reliability.\nIn this work, we present a comprehensive study on the reliability of prediction models based on graph neural network 2, 12, 35, 36 in classification tasks. We focus on how to assess and improve prediction reliability in order for successful virtual screening with probabilstic interpretation of final outputs to be possible. The rest of paper firstly provides preliminaries on strategies to evaluate prediction reliability, i.e. calibration curve, expected calibration error, and entropy histogram. Then, we briefly introduce methods in our scope -graph convolutional network and its augmentations, regularizations, and focal loss. Numerical experiments investigate the affect of model architectures, regularizations, and also their implications on virtual screening. Our study leaves lessons that relevant model capacity and appropriate regularizations is key to achieve high success rate in screening more desirable compounds with prediction probability.\n\nPreliminaries on prediction reliability\nWe elaborate the methods to evaluate prediction reliability. Let us write our model f produces an output pi = f (x i ) for a given input x i . Then, a predictive label \u0177i is determined by the threshold-based estimator:\u0177i = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 1 if pi > \u03b4 0 otherwise, ()\nwhere \u03b4 is the threshold, and 0.5 is usually chosen. If ones would like to interpret the final output pi as a true confidence (or probability) of correct prediction p i , a model should be perfectly calibrated. As proposed in Guo et al., a perfect calibration of models can be defined as follows:P ( \u0176 = y| P = p) = p, \u2200p \u2208 [0, 1]. ()\nThey also defined the term expected calibration error (ECE),ECE = E P [|P ( \u0176 = y| P = p) \u2212 p|],\nwhich can be interpreted as the gap between true and model's confidence. We will introduce the empirical ECE estimator later.\nIn order to evaluate the reliability (calibration performance) of models with empirical data points, we utilize calibration curve, expected calibration error (ECE), and entropy histogram. If we divide the predictive results into the total M number of bins (intervals), then the accuracy and the confidence of predictions in the m-th bin B m is given byacc(B m ) = 1 |B m | i\u2208Bm I(\u0177 i = y i ),\nandconf(B m ) = 1 |B m | i\u2208Bm pi ,\nwhere|BECE = M m=1 |B m | n |acc(B m ) \u2212 conf(B m )|.\nLastly, we also provide the distribution (histogram) of predictive entropy, which is defined asH(p) = \u2212p log p \u2212 (1 \u2212 p) log(1 \u2212 p), \u2200p \u2208 (0, 1),\nNote that predictive entropy represents the amount of information lacks in predictions, in other words, predictive uncertainty. That being said, if a model does not have enough information on samples, predictions will show high predictive entropy. But, over-confident models tend to show large amount of zero entropy predictions and vice versa, as shown in our experimental demonstration. We note that predictive entropy is maximum at p = 0.5 and minimum at p = 0.0 or 1.0.\n\nMethods\nIn this section, we describe the methods -model architectures, regularization methods, and loss functions -whose effects on prediction performance and reliability are investigated with numerical experiments.\n\nModel architectures\nWe express molecular graphs with undirected graph G(X, A), where X \u2208 R N \u00d7d is a set of N node features, and A \u2208 R N \u00d7N is an adjacency matrix. Note that we consider connectivity between nodes only, i.e. A ij \u2208 0, 1 for all node pairs (i, j). Graph neural networks (GNNs)\nfor graph-level prediction tasks consist of three parts: i) an encoder featurizes input node information, ii) a readout summarizes node features and produces graph features, and iii) a predictor maps graph features to target property values. Among the various GNN variants, we consider a graph convolutional network (GCN) 2, 35 as a baseline and its advancements augmented with self-attention mechanism in node and/or graph featurizations.\nA simple expression on node featurizations in GCN is given byHl+1 = ReLU(AH l W l ),\nwhere H l \u2208 R N \u00d7d l is a set of node features, which have d l dimension for the l-th graph convolution layer, H 0 = X, W l \u2208 R d l \u00d7d l+1 is a weight parameter, and ReLU is rectifier linear unit (ReLU) activation. Graph convolution layer can be improved by applying attention mechanism 37 that computes attention coefficients between a set of query and key node feature pairs. By following the analogy in graph attention network (GAT), 36 graph attention layer updates node features byHl+1 i = ReLU( j\u2208N i \u03b1 l ij H l j W l ),\nwhere N i denotes a set of adjacent nodes and i-th node itself, H l i denotes i-th node feature and \u03b1 l ij = f (H l i W l , H l j W l ) \u2208 R is attention coefficient whose query and key vectors are H l i W l and H l j W l respectively. We adopt the self-attention mechanism 37 to compute the attention coefficient between adjacent node features:\u03b1 ij = tanh( (H l i W l )W l a (H l j W l ) T \u221a d l+1\n), (10)   where A readout layer aggregates a set of node features and returns a graph feature vector z l \u2208 R d l g . We added subscript g (graph) to z, W and d for weight parameters in readout layers, to emphasize that they are different set of weight parameters to convolution layers.W l a \u2208 R d l \u00d7d l is a\nThe most basic operation that satisfy permutation invariance for aggregation is summationreadoutz l g = sigmoid( N i=1 H l i W l g ),\nwhere W l g \u2208 R d l \u00d7d l g is a weight parameter. Beyond summarizing node features with equal weights, it would be more powerful to aggregate node features with different importances. For this purpose, we adopt self-attention in the readout step again, as proposed in Lee et al.:39 z l g = sigmoid( N i=1 \u03b1 l i H l i W l g ),\nwhere the attention coefficient \u03b1 l i is given by\u03b1 l i = N \u00d7 softmax( 1(H l i W l g ) T d l g ),\nwhere 1 \u2208 R d l g is a vector whose elements are one. This attention readout computes similarity between the one-vector (query vector) and the l-th node features (key vectors), and uses resulting coefficient for linear-combination of node features (value vectors). To aggregate with appropriate summary statistics, we scale the attention coefficient with the number of node features N after applying softmax activation. We experimentally found that this scaling allows the outputs of the attention readout to be distinguishable for given two different graphs. We discuss this fact in supplementary information. We use the concatenation of all the outputs of the l-th readout layers for l \u2208 [1, ..., L], as proposed in Xu et al.:40 z G = CONCAT([z 1 , ..., z L ]). ()\nwhere L is the number of node embedding layers. Since the outputs of l-th graph convolution layer can be thought as the l-hop substructure of center nodes, this concatenation enables the predictor to use the hierarchical structures of input graphs. A linear classifier computes the final output by using a graph feature inputp = sigmoid(z T g W c + b c ),\nwhere W c and b c are weight and bias parameters for the classifier.\n\nRegularizations\nRegularizing neural networks is obviously important to prevent over-fitting problem, which degrades prediction performance. Furthermore, they can lead to obtain well-calibrated high prediction probability. In this section, we introduce regularization methods widely used in modern neural networks and our experimental investigation as well.\nDropout 30 is one of the most popular regularization methods. Its first proposal interpreted the effect of dropout as preventing models to be dependent on specific input or hidden features. Furthermore, Gal and Ghahramani 41 proposed Monte Carlo-dropout (MC-DO), approximate Bayesian inference method with dropout variational posterior. Its predictive distribution is given by the MC sampling of outputs produced by model parameters with stochastic dropout masks. In our experiments, we both investigate the effect of standard dropout (DO), which does not use stochastic dropout mask in test phase, andMC-dropout .\nLabel smoothing (LS) 42 is a simple regularization method that add a small uniform perturbations to each c-th class label y i,c for the input x i . The perturbed labels of the c-th class y LS i,c for the input x i is given byy LS i,c = y i,c (1 \u2212 \u03b1 LS ) + \u03b1 LS C , \u2200c \u2208 1, ..., C\nwhere \u03b1 LS is the amount of perturbation, and C is the number of classes. Note that all the experiments in this study are binary classification, i.e. C = 2.\nThe learning objective of training with LS is given byL LS (y, p; \u03b1 LS ) = L BCE (y LS , p). ()\nwhere 43 is a regularization method to penalize over-confident predictions, by introducing the predictive entropy H(p) as a penalty term, like the way in L2-weight decay. The learning objective with ERL is given byL BCE (y, p) = n i=1 \u2212y i log pi \u2212 (1 \u2212 y i ) log(1 \u2212 pi ) is binary cross-entropy (BCE) loss. Entropy regularization (ERL)L ERL (y, p; \u03b2) = L BCE (y, p) \u2212 \u03b2 n i=1 H(p i ), ()\nwhere \u03b2 is a hyper-parameter that controls the amount of predictive entropy penalty.\n\nFocal loss\nFocal loss (FL) 44 is a well-known loss function for detecting rare samples in imbalanced data distribution by penalizing predictions of high output probability . While the learning objective with ERL is given by the summation of original loss function (i.e. BCE) and its regularization term, the learning objective with the FL is simply given by:L FL (y, p; \u03b3 FL ) = n i=1 \u2212y i (1 \u2212 pi ) \u03b3 FL log pi \u2212 (1 \u2212 y i )p \u03b3 FL i log(1 \u2212 pi ),\nwithout any additional penalty, where weights depend on the output of the neural network.\n\u03b3 FL > 0 is a hyperparameter that controls an extent that the over-confidence is penalized.\nThe factor (1\u2212p i ) \u03b3 FL in the first term of R.H.S. reduces log pi significantly for large value of p i (near to 1). On the other hand, p\u03b3 FL i in the 2nd term of R.H.S reduces log(1\u2212p i ) significantly for small value of p i (near to 0). As a result, training with the focal loss penalizes the overconfident predictions by enforcing the output to be less confident (output p i are not near to either 0 or 1).\nAs proposed in Lin et al., 44 we performed the experiments with weighted focal loss (WFL), given byL WFL (y, p; \u03b1 FL , \u03b3 FL ) = n i=1 \u2212\u03b1 FL y i (1 \u2212 pi ) \u03b3 FL log pi \u2212 (1 \u2212 \u03b1 FL )(1 \u2212 y i )p \u03b3 FL i log(1 \u2212 pi ), ()\nwhere \u03b1 FL and (1 \u2212 \u03b1 FL ) are hyper-parameters that role as weight factors for the prediction losses on positive and negative samples, respectively.\n\nInterpretation of the effect of cost-sensitive learning\nIn this section, we describe how cost-sensitive learning (i.e. LS, ERL, and FL) is interpreted as a regularization of probability estimator. To this end, we conclude that cost-sensitive learning could not provide well-calibrated results, but biased probability estimation\nThe learning objective of training with LS can be rewritten asL LS (y, p; \u03b1 LS ) = L BCE (y, p) + \u03b2 n i=1 KL[U(y i ) P (y i |x i )] + const.,\nwhere KL[p q] is the Kullback-Leibler (KL) divergence between two distributions p and q, and U(y) denotes the uniform distribution. It is straightforward to show eqn. 21 becomes equivalent to eqn. 17. By using the definition of KL divergence, the penalty term (the second term of R.H.S.) in eqn. 21 is given byKL[U(y i ) P (y i |x i )] = E U (y i ) [log U(y i ) P (y i |x i ) ] = \u2212 C c=1 1 C log pi,c + 1 C log 1 C . ()\nIf we let \u03b2 in 21 as the constant multiple of \u03b1 LS , it concludes the proof.\nSimilarly, the learning objective of training with ERL can be rewritten asL ERL (y, p; \u03b2) = L BCE (y, p) + \u03b2 n i=1 KL[P \u03b8 (y|x) U(y)] + const.,\nand the penalty term also can be rewritten asKL[ P (y i |x i ) U(y i )] = E P (y i |x i ) [log P (y i |x i ) U(y i ) ] = + C c=1 pi,c log pi,c \u2212 pi,c log 1 C .\nSince n i=1 C c=1 pi,c log C = n log C is constant, we confirm that eqn. 18 is equivalent to eqn. 23.\nWe can understand that LS and ERL penalize confident predictions by enforcing the prediction distribution to the uniform distribution. The key difference between LS and ERL is that the former and the latter minimize the forward and reverse KL-divergences, respectively.\nAs a result, LS penalizes all predictions with equal weight (i.e. 1/C), on the other hand, ERL penalizes over-confident predictions with larger weight (i.e. pi,c ).\nLastly, we interpret the learning objective of FL as the BCE with asymmetric entropy regularization. For our understanding, we use the approximate relation(1\u2212 p) \u03b3 FL \u2248 1\u2212\u03b3 FL p,\nand we can rewrite the FL as follows:L FL (y, p; \u03b3 FL ) \u2248 \u2212 n i=1 y i log pi \u2212 (1 \u2212 y i ) log(1 \u2212 pi ) \u2212 \u03b3 FL {\u2212y i pi log pi \u2212 (1 \u2212 y i )(1 \u2212 pi ) log(1 \u2212 pi )} = L BCE (y, p) \u2212 \u03b3 FL n i=1 H asym (y i , pi ),\nwhere asymmetric entropy is defined asH asym (y i , pi ) = \u2212y i pi log pi \u2212 (1 \u2212 y i )(1 \u2212 pi ) log(1 \u2212 pi )\n. We can understand that maximizing the asymmetric entropy discourages over-confident prediction on the given true labels, while maximizing the standard entropy (ERL) penalizes regardless of labels.\nThe above learning algorithms have their learning objective as a form of L BCE (y, p) + \u03b2f (p), a summation of BCE and predictive probability regularizer. Theory of logistic regression reveals that minimizing BCE gives asymptotic convergence of the model output to p * = P (Y = 1|X) -the probability of observing positive sample given input random variable X -as a number of training (empirical) samples increases. On the other hand, cost-sensitive learnings introduce additive probability regularizer and enforce predictive distribution to be similar with uniform distribution. It can help to alleviate over-confident prediction since it maximizes predictive entropy, but does not guarantee the convergence of output to unbiased probability estimation. Previous works 29, [42] [43] [44] [45] in other domains empirically show that cost-sensitive learning can improve predictive performance and/or reliability. We show yet undiscovered results in molecular property prediction tasks, emphasizing the importance of appropriate regularizers for well-calibrated probability estimation.\n\nExperiments\nDataset -BACE, BBBP, HIV We used the three datasets -BACE, BBBP and HIV sets -which are widely used in machine learning applications of property predictions. The BACE dataset provides qualitative (binary label) binding results for set of human inhibitors of human beta-secretase 1.\nThe BBBP dataset includes binary labels on the blood-brain barrier permeability properties for chemical compounds. The HIV dataset gives binary labels on the ability to inhibit HIV replication. We obtained input and label pairs from the MoleculeNet homepage. 5 aining scheme Since the number of data points are small, the models were evaluated by averaging the results of five-fold experiments; five sets of train-test split were made with five different random seeds. We regularized the model by using L2-weight decay with coefficient 10 \u22124 \u00d7\n(1 \u2212 p do ) where p do is the dropout probability. For clear visualization, we show calibration curves, entropy histogram, and output probability histograms each of which is obtained by using the first random seed.\n\nEffect of model architectures on prediction performance and reliability\nFirstly, we investigate the effect of model architectures (parameterizations) on both prediction performance and reliability. While related researches thrive in computer vision and natural language understanding fields, this important question still has not been answered\nwith well-designed ablation study in molecular applications. Thus, we aim to answer the following question: \"Does the recently invented neural model designs -graph attention network and attention readout -show their promising effect on molecular property prediction?\"\nIn Figure 1, we summarize the prediction performance and reliability of the four different models -'GCN with summation readout (GCN+Sum)', 'GCN with attention readout (GCN+Attn)', 'GAT with summation readout (GAT+sum)', and 'GAT with attention readout (GAT+Attn)'. GCN and GAT are used for node featurization before readout. We found that the 'GCN+Attn' model shows the best prediction performance for the three datasets (tasks). Observing the results, GAT seems to degrade both prediction performance and reliability; the usage of GAT significantly harmed ECE, in particular.\nIn order to provide interval-wise information of predictions in addition to the results averaged on the entire test set, we visualize the calibration curves and the entropy histograms in 2. As high ECE values highlight, using GAT significantly enlarged the variance in accuracy  Also, the predictive entropy of the GAT models is located near 0.0 more frequently compared to the GCN models. With such evidence, we can conclude that GAT models are prone to over-confidence problems. This observation tells us that probabilistic interpretation of GAT model's output wouldn't be feasible unless the model is calibrated. Thus, for reliable virtual screening, it is necessary to calibrate the GAT model. We note that the large variance in predictive accuracy for the BACE and the BBBP tasks might have arisen due to the small size of the datasets.\nWe note that the GAT models shows better prediction performance than GCN models for regression tasks with a large amount of data samples -unlike for aforementioned classification tasks -as shown in Figure 9 and Table 3 in supplementary information. We conjecture that the small number of data samples and imbalanced distribution of classification datasets made GAT models to perform worse, as GAT models consist of more parameters than GCN models. For the next following ablation studies, testing the effect of regularization methods and focal loss, we set 'GCN+Attn' as the baseline model which shows the best performance and reliability results.\n\nEffect of regularizations on prediction performance and reliability\nNext, we investigate the effect of well-known regularization methods on enhancing the reliability of our baseline model, i.e. 'GCN+Attn'. We adopted a number of regularization methods -standard dropout (DO), Monte-Carlo dropout (MC-DO), label smoothing (LS), and entropy regularization (ERL). The hyper-parameters for each method are included in the implementation detail section. Figure 3 summarizes the prediction performance (accuracy, AUROC, and F1-score) and reliability (ECE) of the models implementing above methods All the models have resulted in similar prediction performance. On the other hand, prediction reliability widely varied depending on the regularization methods: applying DO and MC-DO have improved the reliability, while LS and ERL have made it worsen. We found that MC-\n\nEffect of focal loss on prediction performance and reliability\nA lot of public datasets are imbalanced in that the number of samples from majority and minority class are largely different. For example, the ratio of active and inactive compounds in the HIV dataset is 3:97. The true distribution might be similarly imbalanced in nature;\nthere are much less bio-active compounds than inactive compounds. Focal loss 44 has been well-known for treating imbalanced datasets, especially that of image datasets. However, to the best of our knowledge, there is no previous work studying the predictive reliability of models adopting focal loss. Thus, we investigate the effects of focal loss on predictive performance and reliability for the HIV activity detection task.\nAs shown in eqn. 20, weight factor larger than 0.5 gives larger penalty to misclassification of true positive samples, and weight factor smaller than 0.5 does the same to true negative samples. As a result, using larger weight factor would encourage correct classification of true positive samples, yet misleading some negative samples to be classified as false positives. In other words, larger the weight factor, more the samples would be detected as positives; this would result in higher recall and lower precision values.\nSuch prediction is confirmed through our experiments. For models trained in different set of \u03b1 FL and \u03b3 FL , Figure 5 shows the prediction performance (accuracy, precision, recall, and F1-score) and the prediction reliability (ECE and OCE) results, and Figure 6 shows corresponding calibration curves, entropy histograms, and output probability histograms.\nVarying the weight factor \u03b1 FL did significantly affect precision and recall. As expected, larger \u03b1 FL gave the model lower precision and higher recall in overall. In fact, F1-score, the harmonic mean of precision and recall, found best at \u03b1 FL = 0.75. Now we assess and analyze the effect of focal loss on prediction performance and reliability. We could observe that focal loss did not improve the prediction performance, and even damaged the prediction reliability in all of our test cases except \u03b1 = 0.1 and \u03b3 = 1.0 case.\nWe suspect that such detrimental effect of focal loss arises from the same reason ERL harms reliability; focal loss and ERL both push the predictive distribution to a uniform distribution by strongly penalizing high confidence predictions. Since LS, ERL, and FL regularize predictive distribution itself, the predictive distribution cannot possibly estimate the true distribution without bias.\n\nReliability of models in virtual screening scenario\nLastly, we aim to imitate/cover a real-world virtual screening scenario -where screening library can be largely discrepant from training data distribution -by training models with DUD-E database 48 and testing the models on the ChEMBL database. 49 Such experimental strategy is elaborated in the second experimental section of Ryu et al.. 13 Due to an inherent discrepancy between training and test data distribution, uncertainty of the prediction would be unavoidably higher in virtual screening situation. Thus, over-confident predictions would be exceptionally harmful, and predicting labels with correct probability estimation becomes significantly important to achieve high success rates.\nWe trained models by using the EGFR/VGFR2/ABL1 sets in the DUD-E database. For each training, we built four different models -baseline, MC-DO, LS, and ERL (same models in the second experiments). Then, we obtained the predictive probability of compounds associated to the EGFR/VGFR2/ABL1 sets in the ChEMBL database, where labels are given by negative log of half-maximum inhibitory concentration (pIC50) value. In order to set/view our problem of virtual screening as classification tasks, we let the label of compounds as negative (zero) if pIC50 is smaller than 7.0, and positive (one) otherwise. In other words, we attempted to find the compounds whose pIC50 is larger than 7.0 with our model trained with the DUD-E dataset. More details on training procedure and datasets are provided in supplementary information.\nWe sorted the compounds by output probability in descending order, and screened the top K%. Figure 7 summarizes the success rate -the precision of prediction, or the ratio between a number of true positive compounds and a number of screened compounds -with respect to K value, for each model. If the models were well-calibrated, when we choose compounds of higher output probability, the success rate would be higher. For MC-DO model, the higher the output probability criteria became, the higher the success rate we achieved. However, the other models -LS and ERL -did not show such behaviour, providing considerably low success rate for screened top 5-10% compounds. In that sense, we suggest MC-DO model as an appropriate model for virtual screening.\nFor more detailed analysis, we visualize the histogram of true positive, false positive, true negative, and false negative predictions of the four different models in Figure 10, 11 and 12 in supplementary information. Since the regularizers of LS and ERL forces the predictive distribution similar to the uniform distribution, the models located a large amount of false positive compounds near 1.0. We conjecture that such penalties lead to relatively low success rate for screening with probability criteria due to the biased probability estimation as described in the method section.\n\nRemarks\nSo far, we have presented a number of experimental results and analysis on prediction reliability of graph neural networks. We conclude our experimental analysis with the following remarks.\nRemark 1. \"Modest model capacity is necessary for reliable and accurate predictions.\" Attention mechanism is now widely adopted for neural networks in various domains, and graph convolutional network (GCN) is not an exception. Accordingly, GAT -an attention adopted version of GCN -can easily be regarded as an advanced model for all-time purpose. However, we found out that GAT sometimes causes over-fitting and provides less reliable predictions, probably due to over-parameterization. In the supporting information section, we provide the results of additional experiment where the models were show high output probabilities. However, our study reveals that the improvement of either precision or recall, as well as F1-score, was mainly determined by the weight factor \u03b1 FL . Also, high \u03b3 FL resulted in poor prediction reliability due to the nature of cost-sensitive learning.\nOur findings highlight that weighted cross entropy, which is equivalent to the focal loss of \u03b1 FL = 0.5 and \u03b3 FL > 0.0, would be effective for handling imbalanced data.\nWe believe that our findings give valuable lessons on developing virtual screening systems in different purposes. When ones desire to discover i) as many true positive samples as possible or ii) as less failures as possible, a model of i) high recall (few false negatives) or ii) high precision (few false positives) would be favorable for each scenario. For example of toxicity prediction systems, models providing high recall performance can greatly reduce possible failures in clinical trials. In order to achieve either high precision or high recall model that allows reliable prediction, our study recommends to: \"Do not penalize output probability, but use different weight factor.\"\n\nConclusion\nIn this paper, we have presented the comprehensive study on the performance and reliability of graph neural networks in binary classification tasks of molecular properties. We followed the language of probability to describe the prediction reliability and assessed the reliability across models developed with various model architectures, regularizations, and loss functions.\nWe concerned inevitable challenges in molecular applications, i.e. deficient and imbalanced data situation, and suggested a guide to achieve a model as reliable as possible -\"Use modest model capacity, appropriate regularization and loss function, and learning and inference algorithm from Bayesian learning.\" Beyond our scope, we propose the following future research directions that expected to accomplish accurate and reliable prediction models.\n\u2022 There might be room for improvement in better model architectures for molecular graphs. For example, it would be valuable to study the usefulness of recent advancements in node pooling methods 50, 51 that reduce the dimensionality of node features.\nWhile pooling is a common practice in convolutional neural networks for computer vision tasks, current graph neural networks based on message passing framework (such as GCNs) do not reduce the node feature dimensionality. Instead, graph neural networks simply aggregate all the node features, which sometimes result in producing graph features that lack node information. Hence, to enable better graph representation learning, we are keen to find an effective method to summarize statistics of node features: knowhows borrowed from convolutional neural networks (e.g. downsampling) might stand a chance. Eventually, it could leverage better predictions with less parameters.\n\u2022 More precise Bayesian learning algorithms would improve prediction reliability. Previous researches 13, 14 and this work have adopted MC-DO for approximate Bayesian inference due to the intractability in computing exact posterior distribution. Since the uncertainty is estimated by the variance of predictive distribution, and predictive distribution is inferred by posterior distribution, it is noteworthy to investigate the efficacy of advanced Bayesian learning methods in learning posterior distribution. We believe that recent researches in Bayesian learning community [52] [53] [54] [55] [56] can give fruitful hints for better Bayesian learning and reliable predictions.\n8] [59] [60] [61] Unsupervised representation learning has the virtue of label-free learning, and algorithms such as contrastive learning 62 facilitate obtaining representations useful for down-stream prediction tasks. 63 Since we can easily find abundant structural data of drug-like compounds from public chemical database, 49, 64 such unsupervised pre-training methods can give an apt opportunity to develop models in data-efficient manners.\nConsequently, we believe that our study will widen the opportunity of neural models in chemistry researches via reliable AI systems.\n\nFootnotes:\n\nReferences:\n\n- LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. nature 2015, 521, 436.- Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru- Guzik, A.; Adams, R. P. Convolutional networks on graphs for learning molecular fin- gerprints. Advances in neural information processing systems. 2015; pp 2224-2232.\n\n- Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; Riley, P. Molecular graph convolu- tions: moving beyond fingerprints. Journal of computer-aided molecular design 2016, 30, 595-608.\n\n- Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G. E. Neural message passing for quantum chemistry. Proceedings of the 34th International Conference on Machine Learning-Volume 70. 2017; pp 1263-1272.\n\n- Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: a benchmark for molecular machine learning. Chemical science 2018, 9, 513-530.\n\n- Rogers, D.; Hahn, M. Extended-connectivity fingerprints. Journal of chemical informa- tion and modeling 2010, 50, 742-754.\n\n- Krizhevsky, A.; Sutskever, I.; Hinton, G. E. Imagenet classification with deep convo- lutional neural networks. Advances in neural information processing systems. 2012; pp 1097-1105.\n\n- Kim, Y. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 2014,\n\n- Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural computation 1997, 9, 1735-1780.\n\n- Cho, K.; Van Merri\u00ebnboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 2014,\n\n- Scarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks 2008, 20, 61-80.\n\n- Battaglia, P. W.; Hamrick, J. B.; Bapst, V.; Sanchez-Gonzalez, A.; Zambaldi, V.; Ma- linowski, M.; Tacchetti, A.; Raposo, D.; Santoro, A.; Faulkner, R., et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261 2018,\n\n- Ryu, S.; Kwon, Y.; Kim, W. Y. A Bayesian graph convolutional network for reliable prediction of molecular properties with uncertainty quantification. Chemical Science 2019, 10, 8438-8446.\n\n- Zhang, Y., et al. Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning. Chemical Science 2019, 10, 8154-8163.\n\n- Segler, M. H.; Kogej, T.; Tyrchan, C.; Waller, M. P. Generating focused molecule li- braries for drug discovery with recurrent neural networks. ACS central science 2017, 4, 120-131.\n\n- G\u00f3mez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hern\u00e1ndez-Lobato, J. M.; S\u00e1nchez- Lengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous represen- tation of molecules. ACS central science 2018, 4, 268-276.\n\n- De Cao, N.; Kipf, T. MolGAN: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973 2018,\n\n- Sanchez-Lengeling, B.; Aspuru-Guzik, A. Inverse molecular design using machine learn- ing: Generative models for matter engineering. Science 2018, 361, 360-365.\n\n- Zhavoronkov, A.; Ivanenkov, Y. A.; Aliper, A.; Veselov, M. S.; Aladinskiy, V. A.; Al- adinskaya, A. V.; Terentiev, V. A.; Polykovskiy, D. A.; Kuznetsov, M. D.; Asadulaev, A., et al. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Na- ture biotechnology 2019, 37, 1038-1040.\n\n- Hong, S. H.; Ryu, S.; Lim, J.; Kim, W. Y. Molecular Generative Model Based On Adversarially Regularized Autoencoder. Journal of Chemical Information and Modeling 2019,\n\n- Segler, M. H.; Preuss, M.; Waller, M. P. Planning chemical syntheses with deep neural networks and symbolic AI. Nature 2018, 555, 604.\n\n- Coley, C. W.; Jin, W.; Rogers, L.; Jamison, T. F.; Jaakkola, T. S.; Green, W. H.; Barzi- lay, R.; Jensen, K. F. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science 2019, 10, 370-377.\n\n- Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Hunter, C. A.; Bekas, C.; Lee, A. A. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS central science 2019, 5, 1572-1583.\n\n- Dai, H.; Li, C.; Coley, C.; Dai, B.; Song, L. Retrosynthesis Prediction with Conditional Graph Logic Network. Advances in Neural Information Processing Systems. 2019; pp 8870-8880.\n\n- Vapnik, V. The nature of statistical learning theory; Springer science & business media, 2013.\n\n- Zhang, C.; Bengio, S.; Hardt, M.; Recht, B.; Vinyals, O. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 2016,\n\n- Guo, C.; Pleiss, G.; Sun, Y.; Weinberger, K. Q. On calibration of modern neural net- works. Proceedings of the 34th International Conference on Machine Learning-Volume 70. 2017; pp 1321-1330.\n\n- Snoek, J.; Ovadia, Y.; Fertig, E.; Lakshminarayanan, B.; Nowozin, S.; Sculley, D.; Dil- lon, J.; Ren, J.; Nado, Z. Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems. 2019; pp 13969-13980.\n\n- Thulasidasan, S.; Chennupati, G.; Bilmes, J. A.; Bhattacharya, T.; Michalak, S. On\n\n- He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. Pro- ceedings of the IEEE conference on computer vision and pattern recognition. 2016; pp 770-778.\n\n- Lee, J.; Lee, Y.; Kim, J.; Kosiorek, A.; Choi, S.; Teh, Y. W. Set Transformer: A Frame- work for Attention-based Permutation-Invariant Neural Networks. International Con- ference on Machine Learning. 2019; pp 3744-3753.\n\n- Xu, K.; Hu, W.; Leskovec, J.; Jegelka, S. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826 2018,\n\n- Gal, Y.; Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. international conference on machine learning. 2016; pp 1050-1059.\n\n- Szegedy, C.; Ioffe, S.; Vanhoucke, V.; Alemi, A. A. Inception-v4, inception-resnet and the impact of residual connections on learning. Thirty-first AAAI conference on artificial intelligence. 2017.\n\n- Pereyra, G.; Tucker, G.; Chorowski, J.; Kaiser, L.; Hinton, G. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548 2017,\n\n- Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detec- tion. Proceedings of the IEEE international conference on computer vision. 2017; pp 2980-2988.\n\n- M\u00fcller, R.; Kornblith, S.; Hinton, G. E. When does label smoothing help? Advances in Neural Information Processing Systems. 2019; pp 4696-4705.\n\n- Loshchilov, I.; Hutter, F. Decoupled weight decay regularization. 2018,\n\n- Kingma, D. P.; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 2014,\n\n- Mysinger, M. M.; Carchia, M.; Irwin, J. J.; Shoichet, B. K. Directory of useful decoys, enhanced (DUD-E): better ligands and decoys for better benchmarking. Journal of medicinal chemistry 2012, 55, 6582-6594.\n\n- Gaulton, A.; Bellis, L. J.; Bento, A. P.; Chambers, J.; Davies, M.; Hersey, A.; Light, Y.; McGlinchey, S.; Michalovich, D.; Al-Lazikani, B., et al. ChEMBL: a large-scale bioac- tivity database for drug discovery. Nucleic acids research 2012, 40, D1100-D1107.\n\n- Ying, Z.; You, J.; Morris, C.; Ren, X.; Hamilton, W.; Leskovec, J. Hierarchical graph representation learning with differentiable pooling. Advances in neural information pro- cessing systems. 2018; pp 4800-4810.\n\n- Lee, J.; Lee, I.; Kang, J. Self-attention graph pooling. arXiv preprint arXiv:1904.08082 2019,\n\n- Mandt, S.; Hoffman, M. D.; Blei, D. M. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research 2017, 18, 4873-4907.\n\n- Mobiny, A.; Nguyen, H. V.; Moulik, S.; Garg, N.; Wu, C. C. DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks. arXiv preprint arXiv:1906.04569 2019,\n\n- Osawa, K.; Swaroop, S.; Khan, M. E. E.; Jain, A.; Eschenhagen, R.; Turner, R. E.; Yokota, R. Practical deep learning with bayesian principles. Advances in Neural Infor- mation Processing Systems. 2019; pp 4289-4301.\n\n- Maddox, W. J.; Izmailov, P.; Garipov, T.; Vetrov, D. P.; Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems. 2019; pp 13132-13143.\n\n- Wilson, A. G.; Izmailov, P. Bayesian Deep Learning and a Probabilistic Perspective of Generalization. arXiv preprint arXiv:2002.08791 2020,\n\n- Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805 2018,\n\n- He, K.; Girshick, R.; Doll\u00e1r, P. Rethinking imagenet pre-training. Proceedings of the IEEE International Conference on Computer Vision. 2019; pp 4918-4927.\n\n- Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V.; Leskovec, J. Pre-training graph neural networks. arXiv preprint arXiv:1905.12265 2019,\n\n- Hendrycks, D.; Lee, K.; Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960 2019,\n\n- Hendrycks, D.; Mazeika, M.; Kadavath, S.; Song, D. Using self-supervised learning can improve model robustness and uncertainty. Advances in Neural Information Processing Systems. 2019; pp 15637-15648.\n\n- Oord, A. v. d.; Li, Y.; Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 2018,\n\n- H\u00e9naff, O. J.; Razavi, A.; Doersch, C.; Eslami, S.; Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272 2019,\n\n- Irwin, J. J.; Shoichet, B. K. ZINC-a free database of commercially available compounds for virtual screening. Journal of chemical information and modeling 2005, 45, 177-182.\n\n", "annotations": {"ReferenceToTable": [{"begin": 21955, "end": 21956, "target": "#tab_4", "idx": 0}], "SectionMain": [{"begin": 1229, "end": 33681, "idx": 0}], "SectionReference": [{"begin": 33695, "end": 43676, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 1229, "idx": 0}], "Div": [{"begin": 110, "end": 1221, "idx": 0}, {"begin": 1232, "end": 5691, "idx": 1}, {"begin": 5693, "end": 7657, "idx": 2}, {"begin": 7659, "end": 7874, "idx": 3}, {"begin": 7876, "end": 11675, "idx": 4}, {"begin": 11677, "end": 13656, "idx": 5}, {"begin": 13658, "end": 15062, "idx": 6}, {"begin": 15064, "end": 18651, "idx": 7}, {"begin": 18653, "end": 19705, "idx": 8}, {"begin": 19707, "end": 22385, "idx": 9}, {"begin": 22387, "end": 23247, "idx": 10}, {"begin": 23249, "end": 25815, "idx": 11}, {"begin": 25817, "end": 28722, "idx": 12}, {"begin": 28724, "end": 30660, "idx": 13}, {"begin": 30662, "end": 33681, "idx": 14}], "Head": [{"begin": 1232, "end": 1244, "idx": 0}, {"begin": 5693, "end": 5732, "idx": 1}, {"begin": 7659, "end": 7666, "idx": 2}, {"begin": 7876, "end": 7895, "idx": 3}, {"begin": 11677, "end": 11692, "idx": 4}, {"begin": 13658, "end": 13668, "idx": 5}, {"begin": 15064, "end": 15119, "idx": 6}, {"begin": 18653, "end": 18664, "idx": 7}, {"begin": 19707, "end": 19778, "idx": 8}, {"begin": 22387, "end": 22454, "idx": 9}, {"begin": 23249, "end": 23311, "idx": 10}, {"begin": 25817, "end": 25868, "idx": 11}, {"begin": 28724, "end": 28731, "idx": 12}, {"begin": 30662, "end": 30672, "idx": 13}], "Paragraph": [{"begin": 110, "end": 1221, "idx": 0}, {"begin": 1245, "end": 1652, "idx": 1}, {"begin": 1653, "end": 3062, "idx": 2}, {"begin": 3063, "end": 3801, "idx": 3}, {"begin": 3802, "end": 4734, "idx": 4}, {"begin": 4735, "end": 5691, "idx": 5}, {"begin": 5733, "end": 5951, "idx": 6}, {"begin": 5998, "end": 6294, "idx": 7}, {"begin": 6333, "end": 6393, "idx": 8}, {"begin": 6430, "end": 6555, "idx": 9}, {"begin": 6556, "end": 6908, "idx": 10}, {"begin": 6949, "end": 6952, "idx": 11}, {"begin": 6984, "end": 6989, "idx": 12}, {"begin": 7038, "end": 7133, "idx": 13}, {"begin": 7184, "end": 7657, "idx": 14}, {"begin": 7667, "end": 7874, "idx": 15}, {"begin": 7896, "end": 8167, "idx": 16}, {"begin": 8168, "end": 8607, "idx": 17}, {"begin": 8608, "end": 8669, "idx": 18}, {"begin": 8693, "end": 9179, "idx": 19}, {"begin": 9220, "end": 9564, "idx": 20}, {"begin": 9618, "end": 9903, "idx": 21}, {"begin": 9927, "end": 10023, "idx": 22}, {"begin": 10061, "end": 10340, "idx": 23}, {"begin": 10387, "end": 10436, "idx": 24}, {"begin": 10484, "end": 11212, "idx": 25}, {"begin": 11251, "end": 11576, "idx": 26}, {"begin": 11607, "end": 11675, "idx": 27}, {"begin": 11693, "end": 12033, "idx": 28}, {"begin": 12034, "end": 12636, "idx": 29}, {"begin": 12649, "end": 12874, "idx": 30}, {"begin": 12929, "end": 13085, "idx": 31}, {"begin": 13086, "end": 13140, "idx": 32}, {"begin": 13182, "end": 13396, "idx": 33}, {"begin": 13572, "end": 13656, "idx": 34}, {"begin": 13669, "end": 14016, "idx": 35}, {"begin": 14105, "end": 14194, "idx": 36}, {"begin": 14195, "end": 14286, "idx": 37}, {"begin": 14287, "end": 14697, "idx": 38}, {"begin": 14698, "end": 14797, "idx": 39}, {"begin": 14913, "end": 15062, "idx": 40}, {"begin": 15120, "end": 15391, "idx": 41}, {"begin": 15392, "end": 15454, "idx": 42}, {"begin": 15534, "end": 15844, "idx": 43}, {"begin": 15954, "end": 16030, "idx": 44}, {"begin": 16031, "end": 16105, "idx": 45}, {"begin": 16175, "end": 16220, "idx": 46}, {"begin": 16335, "end": 16436, "idx": 47}, {"begin": 16437, "end": 16706, "idx": 48}, {"begin": 16707, "end": 16871, "idx": 49}, {"begin": 16872, "end": 17027, "idx": 50}, {"begin": 17051, "end": 17088, "idx": 51}, {"begin": 17261, "end": 17299, "idx": 52}, {"begin": 17370, "end": 17568, "idx": 53}, {"begin": 17569, "end": 18651, "idx": 54}, {"begin": 18665, "end": 18946, "idx": 55}, {"begin": 18947, "end": 19490, "idx": 56}, {"begin": 19491, "end": 19705, "idx": 57}, {"begin": 19779, "end": 20050, "idx": 58}, {"begin": 20051, "end": 20318, "idx": 59}, {"begin": 20319, "end": 20895, "idx": 60}, {"begin": 20896, "end": 21737, "idx": 61}, {"begin": 21738, "end": 22385, "idx": 62}, {"begin": 22455, "end": 23247, "idx": 63}, {"begin": 23312, "end": 23584, "idx": 64}, {"begin": 23585, "end": 24011, "idx": 65}, {"begin": 24012, "end": 24538, "idx": 66}, {"begin": 24539, "end": 24895, "idx": 67}, {"begin": 24896, "end": 25421, "idx": 68}, {"begin": 25422, "end": 25815, "idx": 69}, {"begin": 25869, "end": 26562, "idx": 70}, {"begin": 26563, "end": 27382, "idx": 71}, {"begin": 27383, "end": 28136, "idx": 72}, {"begin": 28137, "end": 28722, "idx": 73}, {"begin": 28732, "end": 28921, "idx": 74}, {"begin": 28922, "end": 29802, "idx": 75}, {"begin": 29803, "end": 29971, "idx": 76}, {"begin": 29972, "end": 30660, "idx": 77}, {"begin": 30673, "end": 31048, "idx": 78}, {"begin": 31049, "end": 31497, "idx": 79}, {"begin": 31498, "end": 31748, "idx": 80}, {"begin": 31749, "end": 32423, "idx": 81}, {"begin": 32424, "end": 33103, "idx": 82}, {"begin": 33104, "end": 33548, "idx": 83}, {"begin": 33549, "end": 33681, "idx": 84}], "ReferenceToBib": [{"begin": 1282, "end": 1283, "target": "#b0", "idx": 0}, {"begin": 1363, "end": 1366, "target": "#b3", "idx": 1}, {"begin": 1367, "end": 1370, "target": "#b4", "idx": 2}, {"begin": 1464, "end": 1465, "target": "#b5", "idx": 3}, {"begin": 1710, "end": 1713, "target": "#b6", "idx": 4}, {"begin": 1714, "end": 1717, "target": "#b7", "idx": 5}, {"begin": 1718, "end": 1721, "target": "#b8", "idx": 6}, {"begin": 1722, "end": 1726, "target": "#b9", "idx": 7}, {"begin": 1753, "end": 1756, "target": "#b10", "idx": 8}, {"begin": 1757, "end": 1759, "target": "#b11", "idx": 9}, {"begin": 1843, "end": 1847, "target": "#b22", "idx": 10}, {"begin": 1848, "end": 1852, "target": "#b23", "idx": 11}, {"begin": 2179, "end": 2182, "target": "#b24", "idx": 12}, {"begin": 2183, "end": 2185, "target": "#b25", "idx": 13}, {"begin": 2306, "end": 2308, "target": "#b26", "idx": 14}, {"begin": 3228, "end": 3231, "target": "#b27", "idx": 15}, {"begin": 3232, "end": 3234, "target": "#b28", "idx": 16}, {"begin": 3335, "end": 3338, "idx": 17}, {"begin": 3339, "end": 3341, "idx": 18}, {"begin": 3376, "end": 3379, "target": "#b12", "idx": 19}, {"begin": 3380, "end": 3383, "target": "#b13", "idx": 20}, {"begin": 3384, "end": 3386, "target": "#b22", "idx": 21}, {"begin": 4136, "end": 4138, "idx": 22}, {"begin": 4852, "end": 4854, "target": "#b1", "idx": 23}, {"begin": 4855, "end": 4858, "target": "#b11", "idx": 24}, {"begin": 4859, "end": 4862, "idx": 25}, {"begin": 4863, "end": 4865, "idx": 26}, {"begin": 8490, "end": 8492, "target": "#b1", "idx": 27}, {"begin": 8493, "end": 8495, "idx": 28}, {"begin": 12670, "end": 12672, "target": "#b33", "idx": 29}, {"begin": 13188, "end": 13190, "target": "#b34", "idx": 30}, {"begin": 13685, "end": 13687, "target": "#b35", "idx": 31}, {"begin": 14725, "end": 14727, "target": "#b35", "idx": 32}, {"begin": 18338, "end": 18341, "target": "#b28", "idx": 33}, {"begin": 18342, "end": 18346, "target": "#b33", "idx": 34}, {"begin": 18347, "end": 18351, "target": "#b34", "idx": 35}, {"begin": 18352, "end": 18356, "target": "#b35", "idx": 36}, {"begin": 18357, "end": 18361, "target": "#b36", "idx": 37}, {"begin": 19206, "end": 19207, "target": "#b4", "idx": 38}, {"begin": 23662, "end": 23664, "target": "#b35", "idx": 39}, {"begin": 26064, "end": 26066, "target": "#b39", "idx": 40}, {"begin": 26114, "end": 26116, "target": "#b40", "idx": 41}, {"begin": 26208, "end": 26210, "target": "#b12", "idx": 42}, {"begin": 31693, "end": 31696, "target": "#b41", "idx": 43}, {"begin": 31697, "end": 31699, "target": "#b42", "idx": 44}, {"begin": 32526, "end": 32529, "target": "#b12", "idx": 45}, {"begin": 32530, "end": 32532, "target": "#b13", "idx": 46}, {"begin": 33000, "end": 33004, "target": "#b43", "idx": 47}, {"begin": 33005, "end": 33009, "target": "#b44", "idx": 48}, {"begin": 33010, "end": 33014, "target": "#b45", "idx": 49}, {"begin": 33015, "end": 33019, "target": "#b46", "idx": 50}, {"begin": 33020, "end": 33024, "target": "#b47", "idx": 51}, {"begin": 33107, "end": 33111, "target": "#b50", "idx": 52}, {"begin": 33112, "end": 33116, "target": "#b51", "idx": 53}, {"begin": 33117, "end": 33121, "target": "#b52", "idx": 54}, {"begin": 33323, "end": 33325, "target": "#b54", "idx": 55}, {"begin": 33430, "end": 33433, "target": "#b40", "idx": 56}, {"begin": 33434, "end": 33436, "target": "#b55", "idx": 57}], "Sentence": [{"begin": 110, "end": 241, "idx": 0}, {"begin": 242, "end": 453, "idx": 1}, {"begin": 454, "end": 649, "idx": 2}, {"begin": 650, "end": 834, "idx": 3}, {"begin": 835, "end": 996, "idx": 4}, {"begin": 997, "end": 1163, "idx": 5}, {"begin": 1164, "end": 1221, "idx": 6}, {"begin": 1245, "end": 1359, "idx": 7}, {"begin": 1360, "end": 1652, "idx": 8}, {"begin": 1653, "end": 1841, "idx": 9}, {"begin": 1842, "end": 2006, "idx": 10}, {"begin": 2007, "end": 2185, "idx": 11}, {"begin": 2186, "end": 2308, "idx": 12}, {"begin": 2309, "end": 2460, "idx": 13}, {"begin": 2461, "end": 2555, "idx": 14}, {"begin": 2556, "end": 2708, "idx": 15}, {"begin": 2709, "end": 2861, "idx": 16}, {"begin": 2862, "end": 3062, "idx": 17}, {"begin": 3063, "end": 3234, "idx": 18}, {"begin": 3235, "end": 3360, "idx": 19}, {"begin": 3361, "end": 3510, "idx": 20}, {"begin": 3511, "end": 3763, "idx": 21}, {"begin": 3764, "end": 3801, "idx": 22}, {"begin": 3802, "end": 4013, "idx": 23}, {"begin": 4014, "end": 4121, "idx": 24}, {"begin": 4122, "end": 4288, "idx": 25}, {"begin": 4289, "end": 4513, "idx": 26}, {"begin": 4514, "end": 4734, "idx": 27}, {"begin": 4735, "end": 4890, "idx": 28}, {"begin": 4891, "end": 5059, "idx": 29}, {"begin": 5060, "end": 5233, "idx": 30}, {"begin": 5234, "end": 5366, "idx": 31}, {"begin": 5367, "end": 5502, "idx": 32}, {"begin": 5503, "end": 5691, "idx": 33}, {"begin": 5733, "end": 5793, "idx": 34}, {"begin": 5794, "end": 5875, "idx": 35}, {"begin": 5876, "end": 5951, "idx": 36}, {"begin": 5998, "end": 6050, "idx": 37}, {"begin": 6051, "end": 6208, "idx": 38}, {"begin": 6209, "end": 6294, "idx": 39}, {"begin": 6333, "end": 6393, "idx": 40}, {"begin": 6430, "end": 6502, "idx": 41}, {"begin": 6503, "end": 6555, "idx": 42}, {"begin": 6556, "end": 6743, "idx": 43}, {"begin": 6744, "end": 6908, "idx": 44}, {"begin": 6949, "end": 6952, "idx": 45}, {"begin": 6984, "end": 6989, "idx": 46}, {"begin": 7038, "end": 7133, "idx": 47}, {"begin": 7184, "end": 7311, "idx": 48}, {"begin": 7312, "end": 7431, "idx": 49}, {"begin": 7432, "end": 7572, "idx": 50}, {"begin": 7573, "end": 7657, "idx": 51}, {"begin": 7667, "end": 7874, "idx": 52}, {"begin": 7896, "end": 8039, "idx": 53}, {"begin": 8040, "end": 8099, "idx": 54}, {"begin": 8100, "end": 8138, "idx": 55}, {"begin": 8139, "end": 8167, "idx": 56}, {"begin": 8168, "end": 8409, "idx": 57}, {"begin": 8410, "end": 8607, "idx": 58}, {"begin": 8608, "end": 8669, "idx": 59}, {"begin": 8693, "end": 8907, "idx": 60}, {"begin": 8908, "end": 9070, "idx": 61}, {"begin": 9071, "end": 9179, "idx": 62}, {"begin": 9220, "end": 9454, "idx": 63}, {"begin": 9455, "end": 9564, "idx": 64}, {"begin": 9618, "end": 9734, "idx": 65}, {"begin": 9735, "end": 9903, "idx": 66}, {"begin": 9927, "end": 10023, "idx": 67}, {"begin": 10061, "end": 10110, "idx": 68}, {"begin": 10111, "end": 10244, "idx": 69}, {"begin": 10245, "end": 10340, "idx": 70}, {"begin": 10387, "end": 10436, "idx": 71}, {"begin": 10484, "end": 10537, "idx": 72}, {"begin": 10538, "end": 10748, "idx": 73}, {"begin": 10749, "end": 10903, "idx": 74}, {"begin": 10904, "end": 11043, "idx": 75}, {"begin": 11044, "end": 11094, "idx": 76}, {"begin": 11095, "end": 11212, "idx": 77}, {"begin": 11251, "end": 11298, "idx": 78}, {"begin": 11299, "end": 11499, "idx": 79}, {"begin": 11500, "end": 11576, "idx": 80}, {"begin": 11607, "end": 11675, "idx": 81}, {"begin": 11693, "end": 11816, "idx": 82}, {"begin": 11817, "end": 11898, "idx": 83}, {"begin": 11899, "end": 12033, "idx": 84}, {"begin": 12034, "end": 12095, "idx": 85}, {"begin": 12096, "end": 12223, "idx": 86}, {"begin": 12224, "end": 12370, "idx": 87}, {"begin": 12371, "end": 12497, "idx": 88}, {"begin": 12498, "end": 12636, "idx": 89}, {"begin": 12649, "end": 12796, "idx": 90}, {"begin": 12797, "end": 12874, "idx": 91}, {"begin": 12929, "end": 13002, "idx": 92}, {"begin": 13003, "end": 13078, "idx": 93}, {"begin": 13079, "end": 13085, "idx": 94}, {"begin": 13086, "end": 13140, "idx": 95}, {"begin": 13182, "end": 13352, "idx": 96}, {"begin": 13353, "end": 13396, "idx": 97}, {"begin": 13572, "end": 13656, "idx": 98}, {"begin": 13669, "end": 13831, "idx": 99}, {"begin": 13832, "end": 13927, "idx": 100}, {"begin": 13928, "end": 14016, "idx": 101}, {"begin": 14105, "end": 14194, "idx": 102}, {"begin": 14195, "end": 14286, "idx": 103}, {"begin": 14287, "end": 14404, "idx": 104}, {"begin": 14405, "end": 14526, "idx": 105}, {"begin": 14527, "end": 14697, "idx": 106}, {"begin": 14698, "end": 14797, "idx": 107}, {"begin": 14913, "end": 15062, "idx": 108}, {"begin": 15120, "end": 15182, "idx": 109}, {"begin": 15183, "end": 15260, "idx": 110}, {"begin": 15261, "end": 15391, "idx": 111}, {"begin": 15392, "end": 15454, "idx": 112}, {"begin": 15534, "end": 15665, "idx": 113}, {"begin": 15666, "end": 15700, "idx": 114}, {"begin": 15701, "end": 15734, "idx": 115}, {"begin": 15735, "end": 15829, "idx": 116}, {"begin": 15830, "end": 15844, "idx": 117}, {"begin": 15954, "end": 16030, "idx": 118}, {"begin": 16031, "end": 16105, "idx": 119}, {"begin": 16175, "end": 16220, "idx": 120}, {"begin": 16335, "end": 16407, "idx": 121}, {"begin": 16408, "end": 16432, "idx": 122}, {"begin": 16433, "end": 16436, "idx": 123}, {"begin": 16437, "end": 16571, "idx": 124}, {"begin": 16572, "end": 16706, "idx": 125}, {"begin": 16707, "end": 16772, "idx": 126}, {"begin": 16773, "end": 16863, "idx": 127}, {"begin": 16864, "end": 16871, "idx": 128}, {"begin": 16872, "end": 16972, "idx": 129}, {"begin": 16973, "end": 17027, "idx": 130}, {"begin": 17051, "end": 17088, "idx": 131}, {"begin": 17261, "end": 17299, "idx": 132}, {"begin": 17370, "end": 17568, "idx": 133}, {"begin": 17569, "end": 17723, "idx": 134}, {"begin": 17724, "end": 17983, "idx": 135}, {"begin": 17984, "end": 18147, "idx": 136}, {"begin": 18148, "end": 18322, "idx": 137}, {"begin": 18323, "end": 18479, "idx": 138}, {"begin": 18480, "end": 18651, "idx": 139}, {"begin": 18665, "end": 18822, "idx": 140}, {"begin": 18823, "end": 18946, "idx": 141}, {"begin": 18947, "end": 19061, "idx": 142}, {"begin": 19062, "end": 19140, "idx": 143}, {"begin": 19141, "end": 19207, "idx": 144}, {"begin": 19208, "end": 19415, "idx": 145}, {"begin": 19416, "end": 19490, "idx": 146}, {"begin": 19491, "end": 19541, "idx": 147}, {"begin": 19542, "end": 19705, "idx": 148}, {"begin": 19779, "end": 19904, "idx": 149}, {"begin": 19905, "end": 20050, "idx": 150}, {"begin": 20051, "end": 20111, "idx": 151}, {"begin": 20112, "end": 20318, "idx": 152}, {"begin": 20319, "end": 20583, "idx": 153}, {"begin": 20584, "end": 20643, "idx": 154}, {"begin": 20644, "end": 20748, "idx": 155}, {"begin": 20749, "end": 20895, "idx": 156}, {"begin": 20896, "end": 21285, "idx": 157}, {"begin": 21286, "end": 21376, "idx": 158}, {"begin": 21377, "end": 21511, "idx": 159}, {"begin": 21512, "end": 21593, "idx": 160}, {"begin": 21594, "end": 21737, "idx": 161}, {"begin": 21738, "end": 21986, "idx": 162}, {"begin": 21987, "end": 22185, "idx": 163}, {"begin": 22186, "end": 22385, "idx": 164}, {"begin": 22455, "end": 22592, "idx": 165}, {"begin": 22593, "end": 22747, "idx": 166}, {"begin": 22748, "end": 22835, "idx": 167}, {"begin": 22836, "end": 23041, "idx": 168}, {"begin": 23042, "end": 23229, "idx": 169}, {"begin": 23230, "end": 23247, "idx": 170}, {"begin": 23312, "end": 23437, "idx": 171}, {"begin": 23438, "end": 23521, "idx": 172}, {"begin": 23522, "end": 23584, "idx": 173}, {"begin": 23585, "end": 23650, "idx": 174}, {"begin": 23651, "end": 23753, "idx": 175}, {"begin": 23754, "end": 23885, "idx": 176}, {"begin": 23886, "end": 24011, "idx": 177}, {"begin": 24012, "end": 24028, "idx": 178}, {"begin": 24029, "end": 24205, "idx": 179}, {"begin": 24206, "end": 24384, "idx": 180}, {"begin": 24385, "end": 24538, "idx": 181}, {"begin": 24539, "end": 24592, "idx": 182}, {"begin": 24593, "end": 24895, "idx": 183}, {"begin": 24896, "end": 24973, "idx": 184}, {"begin": 24974, "end": 25059, "idx": 185}, {"begin": 25060, "end": 25148, "idx": 186}, {"begin": 25149, "end": 25242, "idx": 187}, {"begin": 25243, "end": 25421, "idx": 188}, {"begin": 25422, "end": 25661, "idx": 189}, {"begin": 25662, "end": 25815, "idx": 190}, {"begin": 25869, "end": 26116, "idx": 191}, {"begin": 26117, "end": 26376, "idx": 192}, {"begin": 26377, "end": 26562, "idx": 193}, {"begin": 26563, "end": 26637, "idx": 194}, {"begin": 26638, "end": 26758, "idx": 195}, {"begin": 26759, "end": 26974, "idx": 196}, {"begin": 26975, "end": 27162, "idx": 197}, {"begin": 27163, "end": 27291, "idx": 198}, {"begin": 27292, "end": 27382, "idx": 199}, {"begin": 27383, "end": 27675, "idx": 200}, {"begin": 27676, "end": 27800, "idx": 201}, {"begin": 27801, "end": 27909, "idx": 202}, {"begin": 27910, "end": 28051, "idx": 203}, {"begin": 28052, "end": 28136, "idx": 204}, {"begin": 28137, "end": 28354, "idx": 205}, {"begin": 28355, "end": 28535, "idx": 206}, {"begin": 28536, "end": 28722, "idx": 207}, {"begin": 28732, "end": 28855, "idx": 208}, {"begin": 28856, "end": 28921, "idx": 209}, {"begin": 28922, "end": 29007, "idx": 210}, {"begin": 29008, "end": 29148, "idx": 211}, {"begin": 29149, "end": 29269, "idx": 212}, {"begin": 29270, "end": 29409, "idx": 213}, {"begin": 29410, "end": 29550, "idx": 214}, {"begin": 29551, "end": 29700, "idx": 215}, {"begin": 29701, "end": 29802, "idx": 216}, {"begin": 29803, "end": 29971, "idx": 217}, {"begin": 29972, "end": 30085, "idx": 218}, {"begin": 30086, "end": 30326, "idx": 219}, {"begin": 30327, "end": 30468, "idx": 220}, {"begin": 30469, "end": 30660, "idx": 221}, {"begin": 30673, "end": 30845, "idx": 222}, {"begin": 30846, "end": 31048, "idx": 223}, {"begin": 31049, "end": 31358, "idx": 224}, {"begin": 31359, "end": 31497, "idx": 225}, {"begin": 31498, "end": 31587, "idx": 226}, {"begin": 31588, "end": 31748, "idx": 227}, {"begin": 31749, "end": 31970, "idx": 228}, {"begin": 31971, "end": 32120, "idx": 229}, {"begin": 32121, "end": 32316, "idx": 230}, {"begin": 32317, "end": 32352, "idx": 231}, {"begin": 32353, "end": 32423, "idx": 232}, {"begin": 32424, "end": 32505, "idx": 233}, {"begin": 32506, "end": 32669, "idx": 234}, {"begin": 32670, "end": 32934, "idx": 235}, {"begin": 32935, "end": 33103, "idx": 236}, {"begin": 33104, "end": 33325, "idx": 237}, {"begin": 33326, "end": 33548, "idx": 238}, {"begin": 33549, "end": 33681, "idx": 239}], "ReferenceToFigure": [{"begin": 20329, "end": 20330, "target": "#fig_1", "idx": 0}, {"begin": 21943, "end": 21944, "target": "#fig_8", "idx": 1}, {"begin": 22843, "end": 22844, "target": "#fig_3", "idx": 2}, {"begin": 24655, "end": 24656, "target": "#fig_5", "idx": 3}, {"begin": 24799, "end": 24800, "target": "#fig_6", "idx": 4}, {"begin": 27482, "end": 27483, "target": "#fig_7", "idx": 5}, {"begin": 28311, "end": 28313, "target": "#fig_9", "idx": 6}], "Abstract": [{"begin": 100, "end": 1221, "idx": 0}], "SectionFootnote": [{"begin": 33683, "end": 33693, "idx": 0}], "ReferenceString": [{"begin": 33710, "end": 33781, "id": "b0", "idx": 0}, {"begin": 33783, "end": 34034, "id": "b1", "idx": 1}, {"begin": 34038, "end": 34220, "id": "b2", "idx": 2}, {"begin": 34224, "end": 34439, "id": "b3", "idx": 3}, {"begin": 34443, "end": 34636, "id": "b4", "idx": 4}, {"begin": 34640, "end": 34762, "id": "b5", "idx": 5}, {"begin": 34766, "end": 34948, "id": "b6", "idx": 6}, {"begin": 34952, "end": 35055, "id": "b7", "idx": 7}, {"begin": 35059, "end": 35153, "id": "b8", "idx": 8}, {"begin": 35157, "end": 35384, "id": "b9", "idx": 9}, {"begin": 35388, "end": 35544, "id": "b10", "idx": 10}, {"begin": 35548, "end": 35809, "id": "b11", "idx": 11}, {"begin": 35813, "end": 36000, "id": "b12", "idx": 12}, {"begin": 36004, "end": 36176, "id": "b13", "idx": 13}, {"begin": 36180, "end": 36361, "id": "b14", "idx": 14}, {"begin": 36365, "end": 36674, "id": "b15", "idx": 15}, {"begin": 36678, "end": 36801, "id": "b16", "idx": 16}, {"begin": 36805, "end": 36965, "id": "b17", "idx": 17}, {"begin": 36969, "end": 37271, "id": "b18", "idx": 18}, {"begin": 37275, "end": 37442, "id": "b19", "idx": 19}, {"begin": 37446, "end": 37580, "id": "b20", "idx": 20}, {"begin": 37584, "end": 37817, "id": "b21", "idx": 21}, {"begin": 37821, "end": 38035, "id": "b22", "idx": 22}, {"begin": 38039, "end": 38219, "id": "b23", "idx": 23}, {"begin": 38223, "end": 38317, "id": "b24", "idx": 24}, {"begin": 38321, "end": 38479, "id": "b25", "idx": 25}, {"begin": 38483, "end": 38674, "id": "b26", "idx": 26}, {"begin": 38678, "end": 38960, "id": "b27", "idx": 27}, {"begin": 38964, "end": 39046, "id": "b28", "idx": 28}, {"begin": 39050, "end": 39230, "id": "b29", "idx": 29}, {"begin": 39234, "end": 39453, "id": "b30", "idx": 30}, {"begin": 39457, "end": 39576, "id": "b31", "idx": 31}, {"begin": 39580, "end": 39755, "id": "b32", "idx": 32}, {"begin": 39759, "end": 39956, "id": "b33", "idx": 33}, {"begin": 39960, "end": 40135, "id": "b34", "idx": 34}, {"begin": 40139, "end": 40324, "id": "b35", "idx": 35}, {"begin": 40328, "end": 40471, "id": "b36", "idx": 36}, {"begin": 40475, "end": 40546, "id": "b37", "idx": 37}, {"begin": 40550, "end": 40652, "id": "b38", "idx": 38}, {"begin": 40656, "end": 40864, "id": "b39", "idx": 39}, {"begin": 40868, "end": 41126, "id": "b40", "idx": 40}, {"begin": 41130, "end": 41341, "id": "b41", "idx": 41}, {"begin": 41345, "end": 41439, "id": "b42", "idx": 42}, {"begin": 41443, "end": 41606, "id": "b43", "idx": 43}, {"begin": 41610, "end": 41782, "id": "b44", "idx": 44}, {"begin": 41786, "end": 42001, "id": "b45", "idx": 45}, {"begin": 42005, "end": 42209, "id": "b46", "idx": 46}, {"begin": 42213, "end": 42352, "id": "b47", "idx": 47}, {"begin": 42356, "end": 42526, "id": "b48", "idx": 48}, {"begin": 42530, "end": 42685, "id": "b49", "idx": 49}, {"begin": 42689, "end": 42837, "id": "b50", "idx": 50}, {"begin": 42841, "end": 42979, "id": "b51", "idx": 51}, {"begin": 42983, "end": 43183, "id": "b52", "idx": 52}, {"begin": 43187, "end": 43320, "id": "b53", "idx": 53}, {"begin": 43324, "end": 43497, "id": "b54", "idx": 54}, {"begin": 43501, "end": 43674, "id": "b55", "idx": 55}]}}