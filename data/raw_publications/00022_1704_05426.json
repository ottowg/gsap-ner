{"text": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\n\nAbstract:\nThis paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement. This task will involve reading a line from a non-fiction article and writing three sentences that relate to it. The line will describe a situation or event. Using only this description and what you know about the world:\n\n\n1 Introduction\nMany of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success. While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to which current models extract reasonable representations of language meaning in these settings.\nThe task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences-like one of those in Figure 1 and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADIC-TION. Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting an effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics). In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, and lexical and syntactic ambiguity.\nAs the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways.\nMet my first girlfriend that way. The 8 million dollars for emergency housing was still not enough to solve the problem.\n\nFACE-TO-FACE contradiction\nNow, as children tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.\nLETTERS neutralN N N N\nAll of the children love working in their gardens.\nAt 8:34, the  First, the sentences in SNLI are derived from only a single text genre-image captions-and are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomena-like temporal reasoning (e.g., yesterday), belief (e.g., know), and modality (e.g., should)-rare enough to be irrelevant to task performance. Second, because of these issues, SNLI is not sufficiently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for fine-grained comparisons between strong models.\nThis paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English. While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.\nOur chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artificial neural network techniques have made it possible to train generalpurpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to NLU have involved models that are trained on data that closely resembles the target evaluation data, both in task and style. This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.\nWith this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains. The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard American English is used. All of the genres appear in the test and development sets, but only five are included in the training set. Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time.\nFigure 1: The main text of a prompt (truncated) that was presented to our annotators. This version is used for the written non-fiction genres.\n\n2 The Corpus\n\n\n2.1 Data Collection\nThe data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis. This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy.\nPremise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English. We selected nine sources from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod et al., 2000; Ide and Macleod, 2001; Ide and Suderman, 2006, downloaded 12/2016 1), balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators.\nOANC data constitutes the following nine genres: transcriptions from the Charlotte Narrative and Conversation Collection of two-sided, inperson conversations that took place in the early 2000s (FACE-TO-FACE); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s-early 2000s (LETTERS); the public re-port from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE) written between 1996-2000; transcriptions from University of Pennsylvania's Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM).\nFor our tenth genre, FICTION, we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning genres including mystery, humor, western, science fiction, and fantasy by authors Isaac Asimov, Agatha Christie, Ben Essex (Elliott Gesswell), Nick Name (Piotr Kowalczyk), Andre Norton, Lester del Ray, and Mike Shea.\nWe construct premise sentences from these ten source texts with minimal preprocessing; unique the sentences within genres, exclude very short sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists.\nAlthough SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution. SNLI can be appended and treated as an unusually large additional CAPTIONS genre, built on image captions from the Flickr30k corpus (Young et al., 2014).\n\nHypothesis Collection\nTo collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT), one which is necessarily false or inappropriate whenever the premise is true (CONTRADICTION), and one where neither condition applies (NEUTRAL). This method of data collection ensures that the three classes will be represented equally in the raw corpus. The prompts that surround each premise sentence during hypothesis collection are slightly tailored to fit the genre of that premise sentence. We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that fit the intended meanings of the three classes. There are five unique prompts in total: one for written non-fiction genres (SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL; Figure 1), one for spoken genres (TELEPHONE, FACE-TO-FACE), one for each of the less formal written genres (FICTION, LETTERS), and a specialized one for 9/11, tailored to fit its potentially emotional content. Each prompt is accompanied by example premises and hypothesis that are specific to each genre.\nBelow the instructions, we present three text fields-one for each label-followed by a field for reporting issues, and a link to the frequently asked questions (FAQ) page. We provide one FAQ page per prompt. FAQs are modeled on their SNLI counterparts (supplied by the authors of that work) and include additional curated examples, answers to genre-specific questions arising from our pilot phase, and information about logistical concerns like payment.\nFor both hypothesis collection and validation, we present prompts to annotators using Hybrid (gethybrid.io), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI. We used this platform to hire an organized group of workers. 387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors.\nValidation We perform an additional round of annotation on test and development examples to ensure accurate labelling. The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are pre-sented with pairs of sentences and asked to supply a single label (ENTAILMENT, CONTRADICTION, NEUTRAL) for the pair. Each pair is relabeled by four workers, yielding a total of five labels per example. Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.\nFor each validated sentence pair, we assign a gold label representing a majority vote between the initial label assigned to the pair by the original annotator, and the four additional labels assigned by validation annotators. A small number of examples did not receive a three-vote consensus on any one label. These examples are included in the distributed corpus, but are marked with '-' in the gold label field, and should not be used in standard evaluations. Table 2 shows summary statistics capturing the results of validation, alongside corresponding figures for SNLI. These statistics indicate that the labels included in MultiNLI are about as reliable as those included in SNLI, despite MultiNLI's more diverse text contents.\n\n2.2 The Resulting Corpus\nTable 1 shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences. Punctuation is often omitted. Hypotheses can rely heavily on knowledge about the world, and often don't correspond closely with their premises in syntactic structure.\nUnlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indefinitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017).\nThe corpus is available in two formats-tab separated text and JSON Lines (jsonl), following SNLI. For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified:\n\u2022 gold label: label used for classification.\nIn examples rejected during the validation process, the value of this field will be '-'.\n\u2022 sentence{1,2} parse: Each sentence  as parsed by the Stanford PCFG Parser 3.5.2  (Klein and Manning, 2003).\n\u2022 sentence{1,2} binary parse: parses in unlabeled binary-branching format.\n\u2022 label[1]: The label assigned during the creation of the sentence pair. In rare cases this may be different from gold label, if a consensus of annotators chose a different label during the validation phase.\n\u2022 label[2...5]: The four labels assigned during validation by individual annotators to each development and test example. These fields will be empty for training examples.\nThe current version of the corpus is freely available at nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modified and redistributed. The majority of the corpus is released under the OANC's license, which allows all content to be freely used, modified, and shared under permissive terms. The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses; the remaining works of fiction are in the public domain in the United States (but may be licensed differently elsewhere).\nPartition The distributed corpus comes with an explicit train/test/development split. The test and development sets contain 2,000 randomly selected examples each from each of the genres, resulting in a total of 20,000 examples per set. No premise sentence occurs in more than one set.\nStatistics Table 3 shows some additional statistics. Premise sentences in MultiNLI tend to be longer (max 401 words, mean 22.3 words) than their hypotheses (max 70 words), and much longer, on average, than premises in SNLI (mean 14.1 words); premises in MultiNLI also tend to be parsed as complete sentences at a much higher rate on average (91%) than their SNLI counterparts (74%). We observe that the two spoken genres differ in this-with FACE-TO-FACE showing more complete sentences (91%) than TELEPHONE (71%)-and speculate that the lack of visual feedback in a telephone setting may result in a high incidence of interrupted or otherwise incomplete sentences.\nHypothesis sentences in MultiNLI generally cannot be derived from their premise sentences using only trivial editing strategies. While 2.5% of the hypotheses in SNLI differ from their premises by deletion, only 0.9% of those in MultiNLI (170 examples total) are constructed in this way. Similarly, in SNLI, 1.6% of hypotheses differ from their premises by addition, substitution, or shuffling a single word, while in MultiNLI this only happens in 1.2% of examples. The percentage of The first two models produce separate vector representations for each sentence and compute label predictions for pairs of representations. To do this, they concatenate the representations for premise and hypothesis, their difference, and their element-wise product, following Mou et al. (2016b), and pass the result to a single tanh layer followed by a three-way softmax classifier.\nAll models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014). Out-of-vocabulary (OOV) words are initialized randomly and word embeddings are fine-tuned during training. The models use 300D hidden states, as in most prior work on SNLI. We use Dropout (Srivastava et al., 2014) for regularization. For ESIM, we use a dropout rate of 0.5, following the paper. For CBOW and BiLSTM models, we tune Dropout on the SNLI dev. set and find that a drop rate of 0.1 works well. We use the Adam (Kingma and Ba, 2015) optimizer with default parameters.\nWe train models on SNLI, MultiNLI, and a mixture; Table 4 shows the results. In the mixed setting, we use the full MultiNLI training set and randomly select 15% of the SNLI training set at each epoch, ensuring that each available genre is seen during training with roughly equal frequency.\nWe also train a separate CBOW model on each individual genre to establish the degree to which simple models already allow for effective transfer across genres, using a dropout rate of 0.2. When training on SNLI, a single random sample of 15% of the original training set is used. For each genre represented in the training set, the model that performs best on it was trained on that genre; a model trained only on SNLI performs worse on every genre than comparable models trained on any genre from MultiNLI. Models trained on a single genre from MultiNLI perform well on similar genres; for example, the model trained on TELEPHONE attains the best accuracy (63%) on FACE-TO-FACE, which was nearly one point better than it received on itself. SLATE seems to be a difficult and relatively unusual genre and performance on it is relatively poor in this setting; when averaging over runs trained on SNLI and all genres in the matched section of the training set, average performance on SLATE was only 57.5%. Sentences in SLATE cover a wide range of topics and phenomena, making it hard to do well on, but also forcing models trained on it be broadly capable; the model trained on SLATE achieves the highest accuracy of any model on 9/11 (55.6%) and VERBATIM (57.2%), and relatively high accuracy on TRAVEL (57.4%) and GOVERNMENT (58.3%). We also observe that our models perform similarly on both the matched and mismatched test sets of MultiNLI. We expect genre mismatch issues to become more conspicuous as models are developed that can better fit MultiNLI's training genres.\n\n4.1 Data Collection\nIn data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015). Drawing an example from Bowman et al., the pair \"a boat sank in the Pacific Ocean\" and \"a boat sank in the Atlantic Ocean\" can be labeled either CONTRADICTION or NEU-TRAL depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world. This uncertainty can present a serious problem for inter-annotator agreement, since it is not clear that it is possible to define an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert).\nBowman et al. attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descriptions; but, as we engage with the much more abstract writing that is found in, for example, government documents, there is no reason to assume a priori that any similar prompt and annotation strategy can work. We are surprised to find that this is not a major issue. Through a relatively straightforward trial-and-error piloting phase, followed by discussion with our annotators, we manage to design prompts for abstract genres that yield high inter-annotator agreement scores nearly identical to those of SNLI (see Table 2). These high scores suggest that our annotators agreed on a single task definition, and were able to apply it consistently across genres.\n\n4.2 Overall Difficulty\nAs expected, both the increase in the diversity of linguistic phenomena in MultiNLI and its longer average sentence length conspire to make MultiNLI dramatically more difficult than SNLI. Our three baseline models perform better on SNLI than MultiNLI by about 15% when trained on the respective datasets. All three models achieve accuracy above 80% on the SNLI test set when trained only on SNLI. However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on MultiNLI's test sets. When we train models on MultiNLI and downsampled SNLI, we see an expected significant improvement on SNLI, but no significant change in performance on the MultiNLI test sets, suggesting including SNLI in training doesn't drive substantial improvement. These results attest to MultiNLI's difficulty, and with its relatively high inter-annotator agreement, suggest that it presents a problem with substantial headroom for future work.\n\n4.3 Analysis by Linguistic Phenomenon\nTo better understand the types of language understanding skills that MultiNLI tests, we analyze the collected corpus using a set of annotation tags chosen to reflect linguistic phenomena which are known to be potentially difficult. We use two methods to assign tags to sentences. First, we use the Penn Treebank (PTB; Marcus et al., 1993) part-of-speech tag set (via the included Stanford Parser parses) to automatically isolate sentences containing a range of easily-identified phenomena like comparatives. Second, we isolate sentences that contain hand-chosen key words indicative of additional interesting phenomena.\nThe hand-chosen tag set covers the following phenomena: QUANTIFIERS contains single words with quantificational force (see, for example, Heim and Kratzer, 1998; Szabolcsi, 2010, e.g., many, all, few, some); BELIEF VERBS contains sentence-embedding verbs denoting mental states (e.g., know, believe, think), including irregular past tense forms; TIME TERMS contains single words with abstract temporal interpretation, (e.g., then, today) and month names and days of the week; DISCOURSE MARKERS contains words that facilitate discourse coherence (e.g., yet, however, but, thus, despite); PRESUPPOSITION TRIGGERS contains words with lexical presuppositions (Stalnaker, 1974; Schlenker, 2016, e.g., again, too,  anymore 3 ); CONDITIONALS contains the word if. Table 5 presents the frequency of the tags in SNLI and MultiNLI, and model accuracy on MultiNLI (trained only on MultiNLI).\nThe distributions of labels within each tagged subset of the corpus roughly mirrors the balanced overall distribution. The most frequent class overall (in this case, ENTAILMENT) occurs with a frequency of roughly one third (see  and sentences exceeding 20 words. Sentences that contain negation are slightly more likely than average to be labeled CONTRADICTION, reflecting a similar finding in SNLI, while long sentences are slightly more likely to be labeled ENTAILMENT.\nNone of the baseline models perform substantially better on any tagged set than they do on the corpus overall, with average model accuracies on sentences containing specific tags falling within about 3 points of overall averages. Using baseline model test accuracy overall as a metric (see Table 4), our baseline models had the most trouble on sentences containing comparatives or superlatives (losing 3-4 points each). Despite the fact that 17% of sentence pairs in the corpus contained at least one instance of comparative or superlative, our baseline models don't utilize the information present in these sentences to predict the correct label for the pair, although presence of a comparative or superlative is slightly more predictive of a NEUTRAL label.\nMoreover, the baseline models perform below average on discourse markers, such as despite and however, losing roughly 2 to 3 points each. Unsurprisingly, the attention-based ESIM model performs better than the other two on sentences with greater than 20 words. Additionally, our baseline models do show slight improvements in accuracy on negation, suggesting that they may be tracking it as a predictor of CONTRADICTION.\n\n5 Conclusion\nNatural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural language sentences. Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English. This paper presents a new dataset that offers dramatically greater linguistic difficulty and diversity, and also serves as a benchmark for cross-genre domain adaptation.\nOur new corpus, MultiNLI, improves upon SNLI in its empirical coverage-because it includes a representative sample of text and speech from ten different genres, as opposed to just simple image captions-and its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena. This greater diversity is reflected in the dramatically lower baseline model performance on MultiNLI than on SNLI (see Table 5) and comparable interannotator agreement, suggesting that MultiNLI has a lot of headroom remaining for future work.\nThe MultiNLI corpus was first released in draft form in the first half of 2017, and in the time since its initial release, work by others (Conneau et al., 2017) has shown that NLI can also be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models, with models trained on SNLI and MultiNLI substantially outperforming all prior models on a suite of established transfer learning benchmarks. We hope that this corpus will continue to serve for many years as a resource for the development and evaluation of methods for sentence understanding.\n\nFootnotes:\n1: http://www.anc.org/\n2: https://9-11commission.gov/\n3: Because their high frequency in the corpus, extremely common triggers like the were excluded from this tag.\n\nReferences:\n\n- Johan Bos and Katja Markert. 2005. Recog- nising textual entailment with logical infer- ence. In Proceedings of the 2005 Confer- ence on Empirical Methods in Natural Lan- guage Processing (EMNLP). pages 628-635. http://www.aclweb.org/anthology/H05-1079.- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associ- ation for Computational Linguistics, pages 632-642. https://doi.org/10.18653/v1/D15-1075.\n\n- Samuel R. Bowman, Jon Gauthier, Abhinav Ras- togi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. 2016. A fast unified model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers). Association for Computational Linguistics, pages 1466-1477. https://doi.org/10.18653/v1/P16-1139.\n\n- Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers). Association for Computational Linguistics, pages 1657-1668. https://doi.org/10.18653/v1/P17-1152.\n\n- Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein- hard Stolle, and Daniel G. Bobrow. 2003. Entail- ment, intensionality and text understanding. In Pro- ceedings of the Human Language Technology-North American Association for Computational Linguis- tics 2003 Workshop on Text Meaning. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364 .\n\n- Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evalu- ating predictive uncertainty, visual object classifica- tion, and recognising textual entailment, Springer, pages 177-190.\n\n- Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proceedings of As- sociation for Computational Linguistics-08: Human Language Technology. Association for Computational Linguistics, pages 1039-1047. http://www.aclweb.org/anthology/P08-1118.\n\n- Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff- man, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2014. DeCAF: A deep convolutional activation fea- ture for generic visual recognition. In Proceedings of the International Conference on Machine Learn- ing (ICML).\n\n- Charles Fillmore, Nancy Ide, Daniel Jurafsky, and Catherine Macleod. 1998. An American National Corpus: A proposal. In Proceedings of the First An- nual Conference on Language Resources and Eval- uation. pages 965-969.\n\n- Yaroslav Fyodorov, Yoad Winter, and Nissim Francez. 2000. A natural logic inference system. In Proceed- ings of the 2nd Workshop on Inference in Computa- tional Semantics.\n\n- Irene Heim and Angelika Kratzer. 1998. Semantics in generative grammar. Blackwell Publishers.\n\n- Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735-1780.\n\n- Nancy Ide and Catherine Macleod. 2001. The Amer- ican National Corpus: A standardized resource of American English. In Proceedings of Corpus Lin- guistics. Lancaster University Centre for Computer Corpus Research on Language, volume 3, pages 1- 7.\n\n- Nancy Ide and Keith Suderman. 2006. Inte- grating linguistic resources: The national corpus model. In Proceedings of the Fifth International Conference on Language Re- sources and Evaluation (LREC). European Language Resources Association (ELRA). http://www.aclweb.org/anthology/L06-1138.\n\n- Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Repre- sentations (ICLR).\n\n- Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL. https://doi.org/10.3115/1075096.1075150.\n\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classification with deep convo- lutional neural networks. In Advances in Neural In- formation Processing Systems 25, pages 1097-1105.\n\n- Bill MacCartney and Christopher D Manning. 2009. An extended model of natural logic. In Proceed- ings of the of the Eighth International Conference on Computational Semantics. pages 140-156.\n\n- Catherine Macleod, Nancy Ide, and Ralph Grishman. 2000. The American National Corpus: A standard- ized resource for American English. In Conference on Language Resources and Evaluation (LREC).\n\n- Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computa- tional linguistics 19(2):313-330.\n\n- Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Associ- ation for Computational Linguistics, pages 1-8. https://doi.org/10.3115/v1/S14-2001.\n\n- Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. 2014b. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Twelfth International Conference on Language Resources and Evaluation (LREC).\n\n- Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016a. How transferable are neural networks in NLP applications? In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). Associ- ation for Computational Linguistics, pages 479-489. https://doi.org/10.18653/v1/D16-1046.\n\n- Lili Mou, Men Rui, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016b. Natural language inference by tree-based convolution and heuristic matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Associa- tion for Computational Linguistics, pages 130-136. https://doi.org/10.18653/v1/P16-2022.\n\n- Tsendsuren Munkhdalai and Hong Yu. 2017. Neu- ral semantic encoders. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguis- tics: Volume 1, Long Papers. Association for Computational Linguistics, pages 397-407.\n\n- http://www.aclweb.org/anthology/E17-1038. Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel R Bowman. 2017. The repeval 2017 shared task: Multi-genre natural language inference with sentence representations. In Proceedings of RepEval 2017: The Second Workshop on Evaluat- ing Vector Space Representations for NLP..\n\n- Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 2249-2255. https://doi.org/10.18653/v1/D16-1244.\n\n- Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP). Association for Computational Linguistics, pages 1532-1543. https://doi.org/10.3115/v1/D14-1162.\n\n- Philippe Schlenker. 2016. The Cambridge Handbook of Formal Semantics, Cam- bridge University Press, chapter The Se- mantics/Pragmatics Interface, pages 664-727. https://doi.org/10.1017/CBO9781139236157.023.\n\n- Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. of Machine Learning Re- search (JMLR) 15:1929-1958.\n\n- Robert Stalnaker. 1974. Semantics and Philosophy, New York, NY: New York University Press, chap- ter Pragmatic Presupposition, pages 329-355.\n\n- Anna Szabolcsi. 2010. Quantification. Cambridge University Press.\n\n- Shuohang Wang and Jing Jiang. 2016. Learn- ing natural language inference with LSTM. In Proceedings of the 2016 Conference of the North American Chapter of the Asso- ciation for Computational Linguistics: Hu- man Language Technologies. Association for Computational Linguistics, pages 1442-1451. https://doi.org/10.18653/v1/N16-1170.\n\n- Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descrip- tions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Associa- tion of Computational Linguistics 2:67-78. http://www.aclweb.org/anthology/Q14-1006.\n\n- Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Pro- ceedings of the European Conference on Computer Vision (ECCV). pages 818-833.\n\n", "annotations": {"Abstract": [{"begin": 80, "end": 1249, "idx": 0}], "Head": [{"begin": 1252, "end": 1266, "n": "1", "idx": 0}, {"begin": 3654, "end": 3680, "idx": 1}, {"begin": 7099, "end": 7111, "n": "2", "idx": 2}, {"begin": 7114, "end": 7133, "n": "2.1", "idx": 3}, {"begin": 10246, "end": 10267, "idx": 4}, {"begin": 13952, "end": 13976, "n": "2.2", "idx": 5}, {"begin": 20565, "end": 20584, "n": "4.1", "idx": 6}, {"begin": 22204, "end": 22226, "n": "4.2", "idx": 7}, {"begin": 23151, "end": 23188, "n": "4.3", "idx": 8}, {"begin": 26342, "end": 26354, "n": "5", "idx": 9}], "ReferenceToBib": [{"begin": 2157, "end": 2180, "target": "#b9", "idx": 0}, {"begin": 2181, "end": 2205, "target": "#b4", "idx": 1}, {"begin": 2206, "end": 2228, "target": "#b0", "idx": 2}, {"begin": 2229, "end": 2248, "target": "#b5", "idx": 3}, {"begin": 2249, "end": 2278, "target": "#b17", "idx": 4}, {"begin": 3019, "end": 3039, "target": "#b1", "idx": 5}, {"begin": 3253, "end": 3275, "target": "#b32", "idx": 6}, {"begin": 3276, "end": 3296, "target": "#b26", "idx": 7}, {"begin": 3305, "end": 3330, "target": "#b24", "idx": 8}, {"begin": 3363, "end": 3382, "target": "#b23", "idx": 9}, {"begin": 3383, "end": 3403, "target": "#b2", "idx": 10}, {"begin": 3404, "end": 3422, "target": "#b3", "idx": 11}, {"begin": 5591, "end": 5616, "target": "#b16", "idx": 12}, {"begin": 5617, "end": 5641, "target": "#b34", "idx": 13}, {"begin": 5642, "end": 5663, "target": "#b7", "idx": 14}, {"begin": 5807, "end": 5825, "target": "#b22", "idx": 15}, {"begin": 7827, "end": 7849, "target": "#b8", "idx": 16}, {"begin": 7850, "end": 7871, "target": "#b18", "idx": 17}, {"begin": 7872, "end": 7894, "target": "#b12", "idx": 18}, {"begin": 7895, "end": 7929, "idx": 19}, {"begin": 10223, "end": 10243, "target": "#b33", "idx": 20}, {"begin": 12606, "end": 12629, "target": "#b21", "idx": 21}, {"begin": 14569, "end": 14599, "idx": 22}, {"begin": 15061, "end": 15086, "target": "#b15", "idx": 23}, {"begin": 18008, "end": 18026, "target": "#b23", "idx": 24}, {"begin": 18197, "end": 18221, "target": "#b27", "idx": 25}, {"begin": 18411, "end": 18436, "target": "#b29", "idx": 26}, {"begin": 18644, "end": 18665, "target": "#b14", "idx": 27}, {"begin": 20790, "end": 20812, "target": "#b6", "idx": 28}, {"begin": 20813, "end": 20835, "target": "#b20", "idx": 29}, {"begin": 20836, "end": 20856, "target": "#b1", "idx": 30}, {"begin": 23507, "end": 23527, "target": "#b19", "idx": 31}, {"begin": 23946, "end": 23969, "target": "#b10", "idx": 32}, {"begin": 23970, "end": 24014, "idx": 33}, {"begin": 27626, "end": 27648, "idx": 34}], "ReferenceToFootnote": [{"begin": 7938, "end": 7939, "target": "#foot_0", "idx": 0}, {"begin": 8674, "end": 8675, "target": "#foot_1", "idx": 1}], "SectionFootnote": [{"begin": 28083, "end": 28258, "idx": 0}], "ReferenceString": [{"begin": 28275, "end": 28528, "id": "b0", "idx": 0}, {"begin": 28530, "end": 28871, "id": "b1", "idx": 1}, {"begin": 28875, "end": 29269, "id": "b2", "idx": 2}, {"begin": 29273, "end": 29616, "id": "b3", "idx": 3}, {"begin": 29620, "end": 30123, "id": "b4", "idx": 4}, {"begin": 30127, "end": 30395, "id": "b5", "idx": 5}, {"begin": 30399, "end": 30706, "id": "b6", "idx": 6}, {"begin": 30710, "end": 30976, "id": "b7", "idx": 7}, {"begin": 30980, "end": 31198, "id": "b8", "idx": 8}, {"begin": 31202, "end": 31373, "id": "b9", "idx": 9}, {"begin": 31377, "end": 31470, "id": "b10", "idx": 10}, {"begin": 31474, "end": 31578, "id": "b11", "idx": 11}, {"begin": 31582, "end": 31829, "id": "b12", "idx": 12}, {"begin": 31833, "end": 32121, "id": "b13", "idx": 13}, {"begin": 32125, "end": 32289, "id": "b14", "idx": 14}, {"begin": 32293, "end": 32423, "id": "b15", "idx": 15}, {"begin": 32427, "end": 32633, "id": "b16", "idx": 16}, {"begin": 32637, "end": 32827, "id": "b17", "idx": 17}, {"begin": 32831, "end": 33023, "id": "b18", "idx": 18}, {"begin": 33027, "end": 33207, "id": "b19", "idx": 19}, {"begin": 33211, "end": 33635, "id": "b20", "idx": 20}, {"begin": 33639, "end": 33934, "id": "b21", "idx": 21}, {"begin": 33938, "end": 34269, "id": "b22", "idx": 22}, {"begin": 34273, "end": 34637, "id": "b23", "idx": 23}, {"begin": 34641, "end": 34904, "id": "b24", "idx": 24}, {"begin": 34908, "end": 35234, "id": "b25", "idx": 25}, {"begin": 35238, "end": 35563, "id": "b26", "idx": 26}, {"begin": 35567, "end": 35883, "id": "b27", "idx": 27}, {"begin": 35887, "end": 36093, "id": "b28", "idx": 28}, {"begin": 36097, "end": 36316, "id": "b29", "idx": 29}, {"begin": 36320, "end": 36461, "id": "b30", "idx": 30}, {"begin": 36465, "end": 36530, "id": "b31", "idx": 31}, {"begin": 36534, "end": 36867, "id": "b32", "idx": 32}, {"begin": 36871, "end": 37171, "id": "b33", "idx": 33}, {"begin": 37175, "end": 37354, "id": "b34", "idx": 34}], "ReferenceToTable": [{"begin": 13686, "end": 13687, "target": "#tab_3", "idx": 0}, {"begin": 13983, "end": 13984, "target": "#tab_1", "idx": 1}, {"begin": 16602, "end": 16603, "target": "#tab_5", "idx": 2}, {"begin": 18757, "end": 18758, "target": "#tab_6", "idx": 3}, {"begin": 22063, "end": 22064, "target": "#tab_3", "idx": 4}, {"begin": 24571, "end": 24572, "target": "#tab_8", "idx": 5}, {"begin": 25457, "end": 25458, "target": "#tab_6", "idx": 6}, {"begin": 27370, "end": 27371, "target": "#tab_8", "idx": 7}], "Footnote": [{"begin": 28094, "end": 28116, "id": "foot_0", "n": "1", "idx": 0}, {"begin": 28117, "end": 28147, "id": "foot_1", "n": "2", "idx": 1}, {"begin": 28148, "end": 28258, "id": "foot_2", "n": "3", "idx": 2}], "Paragraph": [{"begin": 90, "end": 1249, "idx": 0}, {"begin": 1267, "end": 1983, "idx": 1}, {"begin": 1984, "end": 2917, "idx": 2}, {"begin": 2918, "end": 3531, "idx": 3}, {"begin": 3532, "end": 3652, "idx": 4}, {"begin": 3681, "end": 3827, "idx": 5}, {"begin": 3828, "end": 3843, "idx": 6}, {"begin": 3851, "end": 3901, "idx": 7}, {"begin": 3902, "end": 4612, "idx": 8}, {"begin": 4613, "end": 5073, "idx": 9}, {"begin": 5074, "end": 6155, "idx": 10}, {"begin": 6156, "end": 6954, "idx": 11}, {"begin": 6955, "end": 7097, "idx": 12}, {"begin": 7134, "end": 7529, "idx": 13}, {"begin": 7530, "end": 8094, "idx": 14}, {"begin": 8095, "end": 9272, "idx": 15}, {"begin": 9273, "end": 9625, "idx": 16}, {"begin": 9626, "end": 9931, "idx": 17}, {"begin": 9932, "end": 10244, "idx": 18}, {"begin": 10268, "end": 11546, "idx": 19}, {"begin": 11547, "end": 11999, "idx": 20}, {"begin": 12000, "end": 12424, "idx": 21}, {"begin": 12425, "end": 13217, "idx": 22}, {"begin": 13218, "end": 13950, "idx": 23}, {"begin": 13977, "end": 14316, "idx": 24}, {"begin": 14317, "end": 14600, "idx": 25}, {"begin": 14601, "end": 14843, "idx": 26}, {"begin": 14844, "end": 14888, "idx": 27}, {"begin": 14889, "end": 14977, "idx": 28}, {"begin": 14978, "end": 15087, "idx": 29}, {"begin": 15088, "end": 15162, "idx": 30}, {"begin": 15163, "end": 15370, "idx": 31}, {"begin": 15371, "end": 15542, "idx": 32}, {"begin": 15543, "end": 16299, "idx": 33}, {"begin": 16300, "end": 16584, "idx": 34}, {"begin": 16585, "end": 17248, "idx": 35}, {"begin": 17249, "end": 18114, "idx": 36}, {"begin": 18115, "end": 18700, "idx": 37}, {"begin": 18701, "end": 18990, "idx": 38}, {"begin": 18991, "end": 20563, "idx": 39}, {"begin": 20585, "end": 21405, "idx": 40}, {"begin": 21406, "end": 22202, "idx": 41}, {"begin": 22227, "end": 23149, "idx": 42}, {"begin": 23189, "end": 23808, "idx": 43}, {"begin": 23809, "end": 24688, "idx": 44}, {"begin": 24689, "end": 25160, "idx": 45}, {"begin": 25161, "end": 25919, "idx": 46}, {"begin": 25920, "end": 26340, "idx": 47}, {"begin": 26355, "end": 26879, "idx": 48}, {"begin": 26880, "end": 27487, "idx": 49}, {"begin": 27488, "end": 28081, "idx": 50}], "SectionHeader": [{"begin": 0, "end": 1249, "idx": 0}], "SectionReference": [{"begin": 28260, "end": 37356, "idx": 0}], "Sentence": [{"begin": 90, "end": 289, "idx": 0}, {"begin": 290, "end": 400, "idx": 1}, {"begin": 401, "end": 505, "idx": 2}, {"begin": 506, "end": 773, "idx": 3}, {"begin": 774, "end": 1029, "idx": 4}, {"begin": 1030, "end": 1141, "idx": 5}, {"begin": 1142, "end": 1186, "idx": 6}, {"begin": 1187, "end": 1249, "idx": 7}, {"begin": 1267, "end": 1446, "idx": 8}, {"begin": 1447, "end": 1830, "idx": 9}, {"begin": 1831, "end": 1983, "idx": 10}, {"begin": 1984, "end": 2097, "idx": 11}, {"begin": 2098, "end": 2502, "idx": 12}, {"begin": 2503, "end": 2756, "idx": 13}, {"begin": 2757, "end": 2917, "idx": 14}, {"begin": 2918, "end": 3423, "idx": 15}, {"begin": 3424, "end": 3531, "idx": 16}, {"begin": 3532, "end": 3565, "idx": 17}, {"begin": 3566, "end": 3652, "idx": 18}, {"begin": 3681, "end": 3827, "idx": 19}, {"begin": 3828, "end": 3843, "idx": 20}, {"begin": 3851, "end": 3901, "idx": 21}, {"begin": 3902, "end": 4329, "idx": 22}, {"begin": 4330, "end": 4612, "idx": 23}, {"begin": 4613, "end": 4861, "idx": 24}, {"begin": 4862, "end": 5073, "idx": 25}, {"begin": 5074, "end": 5352, "idx": 26}, {"begin": 5353, "end": 5664, "idx": 27}, {"begin": 5665, "end": 5826, "idx": 28}, {"begin": 5827, "end": 6016, "idx": 29}, {"begin": 6017, "end": 6155, "idx": 30}, {"begin": 6156, "end": 6414, "idx": 31}, {"begin": 6415, "end": 6615, "idx": 32}, {"begin": 6616, "end": 6722, "idx": 33}, {"begin": 6723, "end": 6954, "idx": 34}, {"begin": 6955, "end": 7040, "idx": 35}, {"begin": 7041, "end": 7097, "idx": 36}, {"begin": 7134, "end": 7388, "idx": 37}, {"begin": 7389, "end": 7529, "idx": 38}, {"begin": 7530, "end": 7733, "idx": 39}, {"begin": 7734, "end": 8094, "idx": 40}, {"begin": 8095, "end": 9272, "idx": 41}, {"begin": 9273, "end": 9625, "idx": 42}, {"begin": 9626, "end": 9931, "idx": 43}, {"begin": 9932, "end": 10090, "idx": 44}, {"begin": 10091, "end": 10244, "idx": 45}, {"begin": 10268, "end": 10688, "idx": 46}, {"begin": 10689, "end": 10797, "idx": 47}, {"begin": 10798, "end": 10939, "idx": 48}, {"begin": 10940, "end": 11123, "idx": 49}, {"begin": 11124, "end": 11451, "idx": 50}, {"begin": 11452, "end": 11546, "idx": 51}, {"begin": 11547, "end": 11717, "idx": 52}, {"begin": 11718, "end": 11753, "idx": 53}, {"begin": 11754, "end": 11999, "idx": 54}, {"begin": 12000, "end": 12195, "idx": 55}, {"begin": 12196, "end": 12256, "idx": 56}, {"begin": 12257, "end": 12424, "idx": 57}, {"begin": 12425, "end": 12543, "idx": 58}, {"begin": 12544, "end": 12772, "idx": 59}, {"begin": 12773, "end": 12857, "idx": 60}, {"begin": 12858, "end": 13042, "idx": 61}, {"begin": 13043, "end": 13217, "idx": 62}, {"begin": 13218, "end": 13443, "idx": 63}, {"begin": 13444, "end": 13527, "idx": 64}, {"begin": 13528, "end": 13679, "idx": 65}, {"begin": 13680, "end": 13791, "idx": 66}, {"begin": 13792, "end": 13950, "idx": 67}, {"begin": 13977, "end": 14058, "idx": 68}, {"begin": 14059, "end": 14149, "idx": 69}, {"begin": 14150, "end": 14179, "idx": 70}, {"begin": 14180, "end": 14316, "idx": 71}, {"begin": 14317, "end": 14600, "idx": 72}, {"begin": 14601, "end": 14698, "idx": 73}, {"begin": 14699, "end": 14843, "idx": 74}, {"begin": 14844, "end": 14888, "idx": 75}, {"begin": 14889, "end": 14977, "idx": 76}, {"begin": 14978, "end": 15059, "idx": 77}, {"begin": 15060, "end": 15087, "idx": 78}, {"begin": 15088, "end": 15162, "idx": 79}, {"begin": 15163, "end": 15235, "idx": 80}, {"begin": 15236, "end": 15370, "idx": 81}, {"begin": 15371, "end": 15492, "idx": 82}, {"begin": 15493, "end": 15542, "idx": 83}, {"begin": 15543, "end": 15707, "idx": 84}, {"begin": 15708, "end": 15861, "idx": 85}, {"begin": 15862, "end": 16299, "idx": 86}, {"begin": 16300, "end": 16385, "idx": 87}, {"begin": 16386, "end": 16535, "idx": 88}, {"begin": 16536, "end": 16584, "idx": 89}, {"begin": 16585, "end": 16637, "idx": 90}, {"begin": 16638, "end": 16967, "idx": 91}, {"begin": 16968, "end": 17248, "idx": 92}, {"begin": 17249, "end": 17377, "idx": 93}, {"begin": 17378, "end": 17535, "idx": 94}, {"begin": 17536, "end": 17713, "idx": 95}, {"begin": 17714, "end": 17870, "idx": 96}, {"begin": 17871, "end": 18114, "idx": 97}, {"begin": 18115, "end": 18222, "idx": 98}, {"begin": 18223, "end": 18329, "idx": 99}, {"begin": 18330, "end": 18395, "idx": 100}, {"begin": 18396, "end": 18456, "idx": 101}, {"begin": 18457, "end": 18517, "idx": 102}, {"begin": 18518, "end": 18578, "idx": 103}, {"begin": 18579, "end": 18627, "idx": 104}, {"begin": 18628, "end": 18700, "idx": 105}, {"begin": 18701, "end": 18777, "idx": 106}, {"begin": 18778, "end": 18990, "idx": 107}, {"begin": 18991, "end": 19179, "idx": 108}, {"begin": 19180, "end": 19270, "idx": 109}, {"begin": 19271, "end": 19498, "idx": 110}, {"begin": 19499, "end": 19732, "idx": 111}, {"begin": 19733, "end": 19994, "idx": 112}, {"begin": 19995, "end": 20324, "idx": 113}, {"begin": 20325, "end": 20432, "idx": 114}, {"begin": 20433, "end": 20563, "idx": 115}, {"begin": 20585, "end": 20857, "idx": 116}, {"begin": 20858, "end": 21150, "idx": 117}, {"begin": 21151, "end": 21405, "idx": 118}, {"begin": 21406, "end": 21750, "idx": 119}, {"begin": 21751, "end": 21807, "idx": 120}, {"begin": 21808, "end": 22066, "idx": 121}, {"begin": 22067, "end": 22202, "idx": 122}, {"begin": 22227, "end": 22414, "idx": 123}, {"begin": 22415, "end": 22531, "idx": 124}, {"begin": 22532, "end": 22623, "idx": 125}, {"begin": 22624, "end": 22716, "idx": 126}, {"begin": 22717, "end": 22968, "idx": 127}, {"begin": 22969, "end": 23149, "idx": 128}, {"begin": 23189, "end": 23420, "idx": 129}, {"begin": 23421, "end": 23468, "idx": 130}, {"begin": 23469, "end": 23696, "idx": 131}, {"begin": 23697, "end": 23808, "idx": 132}, {"begin": 23809, "end": 24564, "idx": 133}, {"begin": 24565, "end": 24688, "idx": 134}, {"begin": 24689, "end": 24807, "idx": 135}, {"begin": 24808, "end": 24951, "idx": 136}, {"begin": 24952, "end": 25160, "idx": 137}, {"begin": 25161, "end": 25390, "idx": 138}, {"begin": 25391, "end": 25580, "idx": 139}, {"begin": 25581, "end": 25919, "idx": 140}, {"begin": 25920, "end": 26057, "idx": 141}, {"begin": 26058, "end": 26180, "idx": 142}, {"begin": 26181, "end": 26340, "idx": 143}, {"begin": 26355, "end": 26531, "idx": 144}, {"begin": 26532, "end": 26709, "idx": 145}, {"begin": 26710, "end": 26879, "idx": 146}, {"begin": 26880, "end": 27244, "idx": 147}, {"begin": 27245, "end": 27487, "idx": 148}, {"begin": 27488, "end": 27930, "idx": 149}, {"begin": 27931, "end": 28081, "idx": 150}], "ReferenceToFigure": [{"begin": 2354, "end": 2355, "idx": 0}, {"begin": 6962, "end": 6964, "idx": 1}, {"begin": 11249, "end": 11250, "idx": 2}, {"begin": 12954, "end": 12955, "idx": 3}], "Div": [{"begin": 90, "end": 1249, "idx": 0}, {"begin": 1252, "end": 3652, "idx": 1}, {"begin": 3654, "end": 7097, "idx": 2}, {"begin": 7099, "end": 7112, "idx": 3}, {"begin": 7114, "end": 10244, "idx": 4}, {"begin": 10246, "end": 13950, "idx": 5}, {"begin": 13952, "end": 20563, "idx": 6}, {"begin": 20565, "end": 22202, "idx": 7}, {"begin": 22204, "end": 23149, "idx": 8}, {"begin": 23151, "end": 26340, "idx": 9}, {"begin": 26342, "end": 28081, "idx": 10}], "SectionMain": [{"begin": 1249, "end": 28081, "idx": 0}]}}