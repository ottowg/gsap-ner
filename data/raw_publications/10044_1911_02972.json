{"text": "Blockwise Self-Attention for Long Document Understanding\n\nAbstract:\nWe present BlockBERT, a lightweight and efficient BERT model for better modeling longdistance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short-or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.\n\nMain:\n\n\n\n1 Introduction\nRecent emergence of the pre-training and finetuning paradigm, exemplified by methods like ELMo (Peters et al., 2018), GPT-2/3 (Radford et al., 2019; Brown et al., 2020), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019), has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has re-established the new state-of-theart baselines across various tasks, such as question answering (Devlin et al., 2019), coreference resolution (Joshi et al., 2019b), relation extraction (Soares et al., 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few.\nBuilding such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive.  Devlin et al. (2019) report that it takes four days to pre-train BERT-Base/BERT-Large on 4/16 Cloud TPUs. In order to reduce the pre-training time of RoBERTa to 1 day, Liu et al. (2019) use 1,024 V100 GPUs. One crucial factor contributing to the long training time is the memory consumption of these deep models, as it directly affects the batch size. Although the fine-tuning stage is relatively inexpensive, the memory issue still restricts the scenarios in which BERT can be used. For instance, \"it is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB-16GB of RAM, because the maximum batch size that can fit in memory is too small. 1 \"\nAlthough one may think that model size is the main contributor to the large memory consumption, our analysis (Section 2.1) shows that one of the main bottlenecks is actually dot-product selfattention, operated in multiple layers of Transformers (Vaswani et al., 2017), the building block of BERT. As the attention operation is quadratic to the sequence length, this fundamentally limits the maximum length of the input sequence, and thus restricts the model capacity in terms of capturing long-distance dependencies. As a result, downstream tasks have to either truncate their sequences to leading tokens (Nogueira and Cho, 2019) or split their sequences with a sliding window (Joshi et al., 2019a,b). Ad-hoc handling of long sequences is also required in the pre-training stage, such as updating the model using only short sequences in the early stage (Devlin et al., 2019).\nCommon strategies for reducing memory consumption, unfortunately, do not work. For instance, shrinking the model by lowering the number of layers L, attention heads A, or hidden units H leads to significant performance degradation (Vaswani et al., 2017; Devlin et al., 2019) and does not address the long sequence issue. Alternatively, general low-memory training techniques, such as microbatching (Huang et al., 2018) and gradient checkpointing (Chen et al., 2016) essentially trade off training time for memory consumption, prolongs the already lengthy training process.\nIn this work, we explore a different strategy, sparsifying the attention layers, intending to design a lightweight and effective BERT that can model long sequences in a memory-efficient way. Our BlockBERT extends BERT by introducing sparse block substructures into attention matrices to reduce both memory consumption and the number of floating-point operations (FLOPs), which also enables attention heads to capture either shortor long-range contextual information. Compared to the previous method that also enforces sparsity (Child et al., 2019), our approach is much simpler mathematically and very easy to implement. More importantly, the results of experiments conducted on several benchmark question answering datasets with various paragraph lengths show that BlockBERT performs comparably or even better than the original BERT-family models, while enjoying an 18.7-36.1% reduction in memory usage, a 12.0-25.1% reduction in training time, and a 27.8% reduction in inference time.\nThe rest of the paper is organized as follows. Section 2 gives a brief introduction of the BERT model, along with an in-depth analysis of its memory usage during training time. We describe our proposed model in Section 3 and contrast it with existing methods that aim for creating a lighter model. Section 4 presents the experimental results and ablation studies, followed by a survey of other related work in Section 5 and the conclusion in Section 6.\n\n2 Background: Memory Bottleneck in Training BERT\nWe briefly review BERT and introduce its memory profiling in this section. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT (Devlin et al., 2019) consists of multiple layers of bidirectional Transformers (Vaswani et al., 2017), where each Transformer encoder has a multi-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in (Devlin et al., 2019), we denote the number of Transformer layers by L, the number of hidden units by H, the number of attention heads by A, the sequence length by N , and the batch size by B. We also assume the feed-forward hidden unit size to be 4H. 2\n\n2.1 Memory Profiling\nTraining BERT is a memory-intensive process. In order to identify the bottleneck, we follow the memory model proposed by Sohoni et al. (2019), where memory usage throughout neural network training is categorized into three main types: (1) Model memory is used to store model parameters;\n(2) Optimizer memory is the additional memory used by the specific learning algorithm during the process;\n(3) Activation memory consists of the outputs of each layer, which are cached for reuse in backpropagation to compute gradients. Take BERT-Base training as an example. The model has 110 million parameters, so model memory occupies 0.2 GB if parameters are stored in half-precision floating-point format (FP16). For Adam (Kingma and Ba, 2014), the optimizer needs additional memory to store the gradients, first moments, and second moments of model parameters. If stored using the same precision, the optimizer memory should be three times of model memory. 3 o calculate the exact size of activation memory is not trivial because it depends heavily on the implementation of the toolkit. Instead, we measure it empirically by training BERT-Base using Adam with a memory profiler (more details are provided in Appendix A.2).\nWe use 32 NVIDIA V100 GPUs for training. Every single GPU thus consumes a minibatch of size b = B/32 = 8. Figure 1(a) shows the profiling result for a single GPU, where the model/optimizer/activation memory consumes 0.21/1.03/8.49 GB, respectively. We can see that activation memory accounts for the vast majority of the total GPU memory (87.6%) and is thus the bottleneck. Notice that although our analysis is done on BERT-Base, it can also be generalized to BERT-Large and other models such as RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019).\n\n2.3 Techniques for Reducing Traing Memory\nObserving that activation memory is the training bottleneck, we discuss common memory reduction techniques below.\nLow Precision (Micikevicius et al., 2017) Low precision is to use half-precision/mixed-precision for training neural networks. This technique has been widely used in Transformer training (Ott et al., 2019; Liu et al., 2019). In this work, we already assume to use mixed-precision training by default, as indicated in the aforementioned analysis.\nMicrobatching (Huang et al., 2018) Microbatching is to split a batch into small microbatches (which can be fit into memory), and then run forward and backward passes on them separately with gradients for each micro-batch accumulated. Because it runs forward/backward pass multiple times for a single batch, it trades off time for memory.\nGradient Checkpointing (Chen et al., 2016) Gradient checkpointing saves memory by only caching activations of a subset of layers. The un-cached activations will be recomputed during backpropagation from the latest checkpoint. This strategy trades off time for memory by repeating computations and will obviously extend training time.\nKnowledge Distillation (Hinton et al., 2015) Knowledge distillation aims to compress and transfer knowledge from a teacher model to a simpler student model. However, knowledge distillation relies on a teacher model (which is still expensive in training time) and usually suffers from a certain degree of performance degradation.  (Ding et al., 2020) presents an alternative idea based on cognitive theory to construct a workingmemory by identifying key sentences, which enables multi-step reasoning. However, common techniques are still limited in reducing both the training time and memory usage. In this paper, we investigate how to optimize the dot-product attention layers and introduce our approach next.\n\n3 Model: BlockBERT\nFollowing (Vaswani et al., 2017), the dot-product attention in Transformer is defined as:Attention(Q, K, V ) = softmax QK \u221a d V ,\nwhere Q, K, V \u2208 R N \u00d7d with N to be the sequence length and d to be a hidden dimension. As we can see, the inner product between Q and K consumes O(N 2 ) memory. One simple way to reduce the memory consumption of attention is to sparsify the attention matrix. Suppose we have a masking matrix M \u2208 {0, 1} N \u00d7N , we define a masked version of attention as follows:Attention(Q, K, V , M ) = softmax QK \u221a d M V ,\nwith operator defined by(A M )ij = Aij if Mij = 1 \u2212\u221e if Mij = 0 .\nIn this work, we design M to be a sparse block matrix, which not only reduces memory and the number of floating-point operations (FLOPs) but also benefits from efficient dense matrix support from deep learning frameworks, such as PyTorch and Tensorflow. More formally, we split the length-N input sequence into n blocks, with each block of length N n . 4 The N \u00d7 N attention matrix is then partitioned into n \u00d7 n blocks, where each block matrix is of the size N n \u00d7 N n . We define a sparse block matrix M by a permutation \u03c0 of {1, 2, \u2022 \u2022 \u2022 , n}:Mij = 1 if \u03c0 (i\u22121)n N + 1 = (j\u22121)n N + 1 ,\n(2)\nBy writing Q, K, V as block matrices, such thatQ = [Q 1 \u2022 \u2022 \u2022 Q n ] , K = [K 1 \u2022 \u2022 \u2022 K n ] and V = [V 1 \u2022 \u2022 \u2022 V n ]\nand pluging them into Equation 1, we can formally define Blockwise Attention as follows:Blockwise-Attention(Q, K, V , M ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 softmax Q 1 K \u03c0(1) \u221a d V \u03c0(1)\n. . .softmax QnK \u03c0(n) \u221a d V \u03c0(n) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb . ()\nEquation 3 only needs to compute and storeQ i K \u03c0(i) (i = 1, \u2022 \u2022 \u2022 n), each has size N n \u00d7 N n .\nIn other words, BlockBERT reduces both O(N 2 ) memory consumption and FLOPs by a factor of n,since N n \u00d7 N n \u00d7 n = N \u00d7N n .\n\n3.1 Blockwise Multi-Head Attention\nAnalogous to Multi-head Attention (Vaswani et al., 2017), we allow queries, keys, and values to be projected multiple times and perform blockwise attentions in parallel. Moreover, different blockwise attention heads can use different masking matrices. The outputs of multiple heads are then concatenated and aggregated with another linear projection. Let A be the number of attention heads and H the number of hidden units. Blockwise multi-head attention is formally defined as follows: 4 We assume N can be divided by n. If not, we pad the input sequence to make N divisible.Blockwise-Multi-head-Attention(Q, K, V ) =Concat(head1, \u2022 \u2022 \u2022 headA)W O ,\n\nMasking Matrices\n\n\nBlockwise Attention\nLinear Linear LinearConcat Linear Q K V Mask n=3 n=2 (1, 2) (2, 1) (1, 2, 3) (2, 3, 1) (3, 1, 2)\nFigure 2 : Architecture of Blockwise Multi-head Attention, which acts as building blocks of BlockBERT. The key idea is to introduce a sparse block masking matrix to the N \u00d7 N attention matrix. The right panel shows the masking matrices we use when n = 2, 3. For n = 2, the masking matrices are defined by permutation (1, 2), (2, 1) and have 50% non-zeros. For n = 3, the masking matrices are defined by permutation (1, 2, 3), (2, 3, 1), and (3, 1, 2) and have 33.33% non-zeros.\nwhere for each head i, i = 1, 2, \u2022 \u2022 \u2022 , A, headi = Blockwise-Attention(QW Q i , KW K i , V W V i , Mi), with d = H A , W Q i , W K i , W V i \u2208 R\nH\u00d7d and the projection matrix W O \u2208 R H\u00d7H . Each masking matrix M i is determined by a permutation \u03c0 i according to Equation 2. In particular, we choose \u03c0 from permutations generated by shifting one position: \u03c3 = (2, 3, \u2022 \u2022 \u2022 , n, 1), i.e., we select \u03c0 \u2208 {\u03c3, \u03c3 2 , \u2022 \u2022 \u2022 , \u03c3 n }. For example, with 12 attention heads (A = 12) and 2 blocks (n = 2), we can assign 10 heads to permutation (1, 2) and the other 2 heads to permutation (2, 1). Figure 2 illustrates the blockwise multi-head attention with block number n \u2208 {2, 3}. Blockwise sparsity captures both local and long-distance dependencies in a memoryefficiency way, which is crucial for long-document understanding tasks. For instance, the identity permutation, i.e., (1, 2, \u2022 \u2022 \u2022 , n), enables each token to attend to its nearby tokens in self-attention, while other permutations allow tokens within the same block attending to tokens in another block. Our proposed BlockBERT essentially replaces the multihead attention layers in Transformer/BERT with blockwise multi-head attention.\n\n3.2 Analysis of Memory Usage Reduction\nTo validate our claim that BlockBERT with n \u00d7 n blocks can reduce the O(N 2 ) memory usage by a factor of n, we perform the same memory profiling as described in sections 2.1 and 2.2. Again, We fix the number of tokens in each GPU (b \u00d7 N = 4096) and choose N from {128, 256, 512, 1024, 2048}. 5 s we can see from Figure 3 and Table 1, the empirical results align well with the theoretical values. When we set the number of blocks to be 2 and 3 for BlockBERT, the estimated O(N 2 ) activation memory decreases to 1/2 and 1/3 of BERT's O(N 2 ) activation memory, respectively. As shown in Table 2, for the sequence length N = 512, BlockBERT with 2 and 3 blocks saves 18.7% and 23.8% overall memory, respectively. The saving is more significant for longer sequences. When N = 1024, the overall memory reduction of BlockBERT with 2 and 3 blocks is 27.3% and 36.1%, respectively.\n\n4 Experiments\nWe evaluate the pre-training and fine-tuning performance of BlockBERT. In particular, when n = 2, we denote 10:2 to be the configuration which assigns 10 heads to permutation (1, 2) and 2 to permutation (2, 1); when n = 3, we denote 8:2:2 to be the configuration which assigns 8, 2, 2 heads to permutation (1, 2, 3), (2, 3, 1), and (3, 1, 2), respectively. We compare BlockBERT with the following baselines:\nGoogle BERT Google BERT is the official pretrained model from (Devlin et al., 2019).\nSparseBERT We pre-train BERT models with its Transformer encoder replaced by a Sparse Transformer encoder (Child et al., 2019). We set its sparsity hyper-parameters stride = 128 and expressivity c = 32. 6 The attention masking matrix used in Sparse Transformer and more implementation details are discussed in Appendix A.3. A similar architecture was adopted in GPT-3 (Brown et al., 2020).\n\n4.1 Pre-training\nAll the models follow the BERT-Base setting, i.e., 2. Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockBERT (n = 2) reduces the training time from RoBERTa's 9.7 days to 7.5 days.L\n\n4.2 Fine-tuning Tasks\nWe evaluate BlockBERT on several question answering tasks, including SQuAD 1.1/2.0  (Rajpurkar et al., 2018) and five other tasks from the MrQA shared task 7 -HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). Since MrQA does not have an official test set, we follow Joshi et al. (2019a) to split the development set evenly to build a new development set and test set. These QA datasets have different paragraph length distributions and are thus ideal for testing the effectiveness of BlockBERT 8. For example, SQuAD, NaturalQA, and HotpotQA consist of mostly short paragraphs (shorter than 512), while paragraphs in SearchQA (average length 1,004) and TriviaQA (average length 934) have around 1,000 tokens. When the input sequence is longer than N , we follow the common practice (Joshi et al., 2019a) to split it using a sliding window of size N and stride 128. This means that for SearchQA and TriviaQA, a model with N = 512 can only capture half of the context, while a model with N = 1024 can accept the whole paragraph as input.\nFor all models, we adopt the same fine-tuning QA setup from Devlin et al. (2019). The tokenized paragraph (p 1 , \u2022 \u2022 \u2022 , p s ) and question (q 1 , \u2022 \u2022 \u2022 , q t ) are concatenated to be a sequence[CLS]q 1 \u2022 \u2022 \u2022 q t [SEP]p 1 \u2022 \u2022 \u2022 p s [SEP].\nThe sequence is then fed into the pre-trained model with two extra linear layers for predicting the start and end positions of the answer spans. The detailed fine-tuning setting is listed in Appendix A.4. Table 3 and Table 4 report the experimental results.\nBlockBERT (n=2) v.s. RoBERTa-1seq Comparing BlockBERT with RoBERTa-1seq when N = 512, we observe an absolute F1 difference from 0.04 (in NaturalQA) to 1.18 (in NewsQA), with an average of 0.55. For N = 1024, BlockBERT achieves more comparable or even better performance to RoBERTa-1seq, In SearchQA, NewsQA and HotpotQA, BlockBERT achieves absolute F1 Yang et al. (2019). For BlockBERT models, their attention head configurations are the same as Table 2. improvement of 0.39, 0.44 and 0.23, respectively. BlockBERT v.s. SparseBERT For N = 512, it is interesting that BlockBERT with 3 blocks (density 33.33%) performs better then SparseBERT (density 44.20%) in both SQuAD and MrQA tasks. Similar results can be observed for N = 1024, too. These results show that off-diagonal masking matrices, e.g., the masking matrix defined by permutation (2, 3, 1) and (3, 1, 2), play crucial roles in BlockBERT. Furthermore, BlockBERT with 2 blocks achieve a more significant improvement.\nEffect of Long Sequence Pre-training Our observations are twofold: (1) Long sequence pre-training benefits long sequence fine-tuning. In TriviaQA and SearchQA, of which paragraph lengths are around 1024, pre-training models with N = 1024 achieve significantly better performance. (2) The heterogeneity of pre-training and fine-tuning sequence length may hurt performance. For example, in SQuAD, we do not see significant performance gain by using pre-trained models with N = 1024; in HotpotQA and NewsQA, longer sequence pretraining even hurts performance.\nEffect of #Blocks It is not surprising that BlockBERT with 2 blocks (n = 2) performs better than that with 3 blocks (n = 3), because it keeps more attention matrix entries. The biggest difference is in SQuAD 2.0 and NewsQA with N = 1024, where we observe an absolute loss of 1.6 F1 by increasing block number from 2 to 3.\nEfficient inference with BlockBERT We benchmark test efficiency of RoBERTa and BlockBERT. The benchmark code follows huggingface 9. All experiments are run 30 times on a 32GB V100 GPU with half precision (FP16). We report the average running time in Table 5. As we can see, BlockBERT does achieve speedup and memory reduction during test time. Take 8\u00d71024, i.e., batch size B = 8, sequence length N = 1024, as an example, we can see that BlockBERT with 2 blocks saves 27.8% of test time, and BlockBERT with 3 blocks saves more (30.4%). As for memory, we can observe that RoBERTa cannot handle an input of size 16\u00d71024, while it is possible for BlockBERT to work on it.\nIn summary, not only BlockBERT saves training/inference time and memory, but it also has a competitive and sometimes better performance, especially for tasks with longer sequences. This demonstrates the effectiveness of our blockwise multi-head attention approach.\n\n4.3 Ablation Study\nWe fix the assignment of attention heads in the above experiments. For example, BlockBERT with sequence length N = 512 and 2 blocks is trained with ten heads using permutation (1, 2) and the other two using permutation (2, 1). However, there are other ways to assign twelve attention heads, e.g., seven heads for permutation (1, 2) and the other five for permutation (2, 1). It would be interesting to see how the assignment of heads affects model performance. In this section, we grid search attention head assignments and plot their best validation performance in 1.2M training steps. The results are shown in Figure 4.\nOur observations are threefold: (1) Identity permutations, i.e., (1, 2) and (1, 2, 3), are important. As shown in Figure 4, all optimal solutions assign considerable attention heads to block-diagonal matrices, since those matrices enable each token to attend to its nearby tokens; (2) Non-identity permutations follow the rule of \"vital few and trivial many.\"\nAlthough identity permutations are important, assigning all attention heads to them (corresponding to 12:0 and 12:0:0 in Figure 4) significantly hurts performance, since the model can not learn longterm dependencies with only identity permutation;\n(3) Pre-training performance and fine-tuning performance are correlated but not always consistent. When n = 3, pre-training performance suggests 10:1:1 to be the best head assignment -ten heads for permutation (1, 2, 3), one head for (2, 3, 1) and one head for (3, 1, 2), but we observe that the configuration of 8:2:2 achieves better performance in fine-tuning tasks.\n\n5 Related Work\nIn this section, we review the related work of memory optimization for neural network training and recent efforts to simplify Transformer and BERT.\n\n5.1 Low-memory neural networks training\nDue to the large size of model parameters and deep architectures, modern neural networks training requires significant amounts of computing resources. As a result, there is an increasing interest in training neural networks with low memory (Sohoni et al., 2019). Mainstream techniques mostly address this problem with a better system or engineering design, such as low-precision training (Micikevicius et al., 2017), microbatching (Huang et al., 2018) and gradient checkpointing (Chen et al., 2016). Alternatively, there also exists some research focusing on the theoretical aspect, including the recently proposed lottery ticket hypothesis (Frankle and Carbin, 2018).\n\n5.2 Efficient Transformer\nSince the invention of Transformer (Vaswani et al., 2017) and its successful application to masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process. We summarize these attempts as follows:\nAttention layer simplification There are currently two lines of research trying to simplify the multi-head attention layers. The first one focuses on attention matrix sparsification. Notable examples include Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019) 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019), Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). However, due to the insufficient support for sparse tensors from the current deep learning platforms, some of them have to represent a sparse matrix using a dense matrix with a binary mask or rely on customized CUDA kernels (Gray et al., 2017). As a result, the speed-up or reduction in memory consumption is sometimes limited in practice. The second line of research prunes redundant attention heads. Examples include (Voita et al., 2019) and (Michel et al., 2019). Our BlockBERT model belongs to the first category, as we sparsify the attention matrices to be block sparse matrix.\nReducing model size for pre-training Knowledge distillation (Hinton et al., 2015) In the aforementioned efficient Transformers, the model quality is often demonstrated by comparable language model perplexity, or equivalently the bits per word/byte. It is often implicitly assumed that similar language model perplexity implies similar pre-training model quality, namely the same performance on the downstream tasks. We would like to point out that this assumption does not necessarily hold. For example, the experiments on the Enwik8 dataset by Child et al. (2019) demonstrates that Sparse Transformer \"surpasses the 1.03 state-of-the-art (bits per byte) for a similarly-sized Transformer-XL and matching the 0.99 (bits per byte) of a model trained with more than double the number of parameters\". However, if we compare SparseBERT (pre-training model with Sparse Transformer backbone) against XLNet (Yang et al., 2019) (pre-training model with Transformer-XL backbone) in SQuAD, Table 3 shows that XLNet still outperforms SparseBERT significantly. Therefore, we believe that it is necessary to conduct a comprehensive study and evaluation of existing efficient Transformer models when used for masked language model pre-training. Limited by resources, in this work, we mainly compare BlockBERT to pre-training using Sparse Transformer (Child et al., 2019), which is the earliest attempt to design efficient Transformer models and also the key contributor to the success of GPT-3 (Brown et al., 2020). We plan to benchmark more models in the future.\n\n6 Conclusion\nIn this work, we study the lightweight BERT model with the goal of achieving both efficiency and effectiveness. We profile and analyze the memory bottlenecks of BERT and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length. To reduce both time and memory consumption, we present BlockBERT, which sparsifies the attention matrices to be sparse block matrices. The proposed model achieves time and memory saving without significant loss of performance.\nIn the future, we plan to benchmark more efficient Transfomers in language model pre-training and fine-tuning. We also would like to explore more applications of BlockBERT on NLP tasks involving long sequences such as coreference resolution (Joshi et al., 2019b) and document-level machine translation (Miculicich et al., 2018), and also non-NLP tasks such as protein sequence modeling (Rives et al., 2019; Rao et al., 2019).\n\nFootnotes:\n2: The default parameter settings for BERT-Base and BERT-Large can be found in Appendix A.1\n3: 3  In the current PyTorch Adam implementation, the first and second moments are stored in single precision. Consequently, BERT's optimizer memory (1 GB) is five times of model memory (0.2 GB).\n5: We use GPUs of 16 GB memory for profiling. BERT with N = 2048 fails due to an out-of-memory error.RoBERTa-2seq & RoBERTa-1seqWe compare with two versions of RoBERTa (Liu et al., 2019). RoBERTa-2seq is trained with both masked language model (MLM) task and next sentence prediction (NSP) task, while RoBERTa-1seq refers to the pre-training model with only the MLM task.\n6: We adopt Sparse Transformer implemented by Fairseq, which first computes the N \u00d7 N attention matrix, and then masks it to be a sparse one. This implementation cannot avoid the O(N 2 ) attention computation, and thus has a similar training time/memory cost to RoBERTa.\n7: mrqa.github.io\n8: The detailed paragraph length distributions can be found in Appendix A.5\n9: github.com/huggingface/transformers/ blob/master/examples/benchmarks.py\n\nReferences:\n\n- Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.- Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n\n- Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\n\n- Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long se- quences with sparse transformers. arXiv preprint arXiv:1904.10509. Gonc \u00b8alo M Correia, Vlad Niculae, and Andr\u00e9 FT Mar- tins. 2019. Adaptively sparse transformers. arXiv preprint arXiv:1909.00015.\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL-HLT' 2019, pages 4171-4186.\n\n- Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Cogltx: Applying bert to long texts. In NeurIPS '20.\n\n- Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179.\n\n- Jonathan Frankle and Michael Carbin. 2018. The lot- tery ticket hypothesis: Finding sparse, trainable neu- ral networks. arXiv preprint arXiv:1803.03635.\n\n- Scott Gray, Alec Radford, and Diederik P Kingma. 2017. Gpu kernels for block-sparse weights. arXiv preprint arXiv:1711.09224.\n\n- Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star- transformer. In NAACL-HLT' 2019, pages 1315- 1325.\n\n- Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n\n- Yanping Huang, Yonglong Cheng, Dehao Chen, Hy- oukJoong Lee, Jiquan Ngiam, Quoc V Le, and Zhifeng Chen. 2018. Gpipe: Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965.\n\n- Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351.\n\n- Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2019a. Spanbert: Improving pre-training by representing and predict- ing spans. arXiv preprint arXiv:1907.10529.\n\n- Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL' 17, pages 1601-1611.\n\n- Mandar Joshi, Omer Levy, Daniel S Weld, and Luke Zettlemoyer. 2019b. Bert for coreference reso- lution: Baselines and analysis. arXiv preprint arXiv:1908.09091.\n\n- Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\n- Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.\n\n- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a bench- mark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.\n\n- Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\n\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300.\n\n- Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing the locality and breaking the mem- ory bottleneck of transformer on time series forecast- ing. arXiv preprint arXiv:1907.00235.\n\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\n- Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650.\n\n- Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. arXiv preprint arXiv:1710.03740. Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-level neural machine translation with hierarchical attention net- works. In EMNLP' 18, pages 2947-2954.\n\n- Rodrigo Nogueira and Kyunghyun Cho. 2019. Pas- sage re-ranking with bert. arXiv preprint arXiv:1901.04085.\n\n- Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensi- ble toolkit for sequence modeling. arXiv preprint arXiv:1904.01038.\n\n- Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227- 2237.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. arXiv preprint arXiv:1806.03822.\n\n- Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. 2019. Evaluating protein transfer learn- ing with tape. In Advances in Neural Information Processing Systems, pages 9686-9698.\n\n- Alexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, and Rob Fergus. 2019. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioRxiv, page 622803.\n\n- Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learn- ing. arXiv preprint arXiv:1906.03158.\n\n- Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christopher R\u00e9. 2019. Low-memory neural network training: A technical report. arXiv preprint arXiv:1904.10631.\n\n- Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo- janowski, and Armand Joulin. 2019. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799.\n\n- Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling task- specific knowledge from bert into simple neural net- works. arXiv preprint arXiv:1903.12136.\n\n- Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017. Newsqa: A machine compre- hension dataset. In Proceedings of the 2nd Work- shop on Representation Learning for NLP, pages 191-200.\n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\n- Elena Voita, David Talbot, Fedor Moiseev, Rico Sen- nrich, and Ivan Titov. 2019. Analyzing multi- head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418.\n\n- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237.\n\n- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP' 18.\n\n", "annotations": {"ReferenceToTable": [{"begin": 14498, "end": 14499, "target": "#tab_1", "idx": 0}, {"begin": 14759, "end": 14760, "target": "#tab_3", "idx": 1}, {"begin": 16008, "end": 16009, "target": "#tab_3", "idx": 2}, {"begin": 17817, "end": 17818, "target": "#tab_5", "idx": 3}, {"begin": 18304, "end": 18305, "target": "#tab_3", "idx": 4}, {"begin": 19963, "end": 19964, "target": "#tab_6", "idx": 5}, {"begin": 25512, "end": 25513, "target": "#tab_4", "idx": 6}], "ReferenceToFootnote": [{"begin": 6185, "end": 6186, "target": "#foot_0", "idx": 0}, {"begin": 7158, "end": 7159, "target": "#foot_1", "idx": 1}, {"begin": 14459, "end": 14460, "target": "#foot_2", "idx": 2}, {"begin": 15752, "end": 15753, "target": "#foot_3", "idx": 3}, {"begin": 16362, "end": 16363, "target": "#foot_4", "idx": 4}, {"begin": 16814, "end": 16815, "target": "#foot_5", "idx": 5}, {"begin": 19836, "end": 19837, "target": "#foot_6", "idx": 6}], "SectionMain": [{"begin": 821, "end": 27026, "idx": 0}], "ReferenceToFormula": [{"begin": 13210, "end": 13211, "idx": 0}], "SectionReference": [{"begin": 28140, "end": 36351, "idx": 0}], "SectionHeader": [{"begin": 0, "end": 821, "idx": 0}], "Div": [{"begin": 68, "end": 813, "idx": 0}, {"begin": 824, "end": 5471, "idx": 1}, {"begin": 5473, "end": 6186, "idx": 2}, {"begin": 6188, "end": 7977, "idx": 3}, {"begin": 7979, "end": 9862, "idx": 4}, {"begin": 9864, "end": 11637, "idx": 5}, {"begin": 11639, "end": 12323, "idx": 6}, {"begin": 12325, "end": 12342, "idx": 7}, {"begin": 12344, "end": 14125, "idx": 8}, {"begin": 14127, "end": 15040, "idx": 9}, {"begin": 15042, "end": 15938, "idx": 10}, {"begin": 15940, "end": 16182, "idx": 11}, {"begin": 16184, "end": 20640, "idx": 12}, {"begin": 20642, "end": 22259, "idx": 13}, {"begin": 22261, "end": 22423, "idx": 14}, {"begin": 22425, "end": 23133, "idx": 15}, {"begin": 23135, "end": 26075, "idx": 16}, {"begin": 26077, "end": 27026, "idx": 17}], "Head": [{"begin": 824, "end": 838, "n": "1", "idx": 0}, {"begin": 5473, "end": 5521, "n": "2", "idx": 1}, {"begin": 6188, "end": 6208, "n": "2.1", "idx": 2}, {"begin": 7979, "end": 8020, "n": "2.3", "idx": 3}, {"begin": 9864, "end": 9882, "n": "3", "idx": 4}, {"begin": 11639, "end": 11673, "n": "3.1", "idx": 5}, {"begin": 12325, "end": 12341, "idx": 6}, {"begin": 12344, "end": 12363, "idx": 7}, {"begin": 14127, "end": 14165, "n": "3.2", "idx": 8}, {"begin": 15042, "end": 15055, "n": "4", "idx": 9}, {"begin": 15940, "end": 15956, "n": "4.1", "idx": 10}, {"begin": 16184, "end": 16205, "n": "4.2", "idx": 11}, {"begin": 20642, "end": 20660, "n": "4.3", "idx": 12}, {"begin": 22261, "end": 22275, "n": "5", "idx": 13}, {"begin": 22425, "end": 22464, "n": "5.1", "idx": 14}, {"begin": 23135, "end": 23160, "n": "5.2", "idx": 15}, {"begin": 26077, "end": 26089, "n": "6", "idx": 16}], "Paragraph": [{"begin": 68, "end": 813, "idx": 0}, {"begin": 839, "end": 1726, "idx": 1}, {"begin": 1727, "end": 2582, "idx": 2}, {"begin": 2583, "end": 3458, "idx": 3}, {"begin": 3459, "end": 4031, "idx": 4}, {"begin": 4032, "end": 5018, "idx": 5}, {"begin": 5019, "end": 5471, "idx": 6}, {"begin": 5522, "end": 6186, "idx": 7}, {"begin": 6209, "end": 6495, "idx": 8}, {"begin": 6496, "end": 6601, "idx": 9}, {"begin": 6602, "end": 7423, "idx": 10}, {"begin": 7424, "end": 7977, "idx": 11}, {"begin": 8021, "end": 8134, "idx": 12}, {"begin": 8135, "end": 8480, "idx": 13}, {"begin": 8481, "end": 8818, "idx": 14}, {"begin": 8819, "end": 9152, "idx": 15}, {"begin": 9153, "end": 9862, "idx": 16}, {"begin": 9883, "end": 9972, "idx": 17}, {"begin": 10013, "end": 10375, "idx": 18}, {"begin": 10422, "end": 10446, "idx": 19}, {"begin": 10488, "end": 11034, "idx": 20}, {"begin": 11077, "end": 11080, "idx": 21}, {"begin": 11081, "end": 11128, "idx": 22}, {"begin": 11197, "end": 11285, "idx": 23}, {"begin": 11365, "end": 11370, "idx": 24}, {"begin": 11417, "end": 11459, "idx": 25}, {"begin": 11514, "end": 11607, "idx": 26}, {"begin": 11674, "end": 12250, "idx": 27}, {"begin": 12364, "end": 12384, "idx": 28}, {"begin": 12461, "end": 12938, "idx": 29}, {"begin": 12939, "end": 12960, "idx": 30}, {"begin": 13085, "end": 14125, "idx": 31}, {"begin": 14166, "end": 15040, "idx": 32}, {"begin": 15056, "end": 15463, "idx": 33}, {"begin": 15464, "end": 15548, "idx": 34}, {"begin": 15549, "end": 15938, "idx": 35}, {"begin": 15957, "end": 16181, "idx": 36}, {"begin": 16206, "end": 17354, "idx": 37}, {"begin": 17355, "end": 17549, "idx": 38}, {"begin": 17594, "end": 17851, "idx": 39}, {"begin": 17852, "end": 18827, "idx": 40}, {"begin": 18828, "end": 19384, "idx": 41}, {"begin": 19385, "end": 19706, "idx": 42}, {"begin": 19707, "end": 20375, "idx": 43}, {"begin": 20376, "end": 20640, "idx": 44}, {"begin": 20661, "end": 21282, "idx": 45}, {"begin": 21283, "end": 21642, "idx": 46}, {"begin": 21643, "end": 21890, "idx": 47}, {"begin": 21891, "end": 22259, "idx": 48}, {"begin": 22276, "end": 22423, "idx": 49}, {"begin": 22465, "end": 23133, "idx": 50}, {"begin": 23161, "end": 23513, "idx": 51}, {"begin": 23514, "end": 24525, "idx": 52}, {"begin": 24526, "end": 26075, "idx": 53}, {"begin": 26090, "end": 26600, "idx": 54}, {"begin": 26601, "end": 27026, "idx": 55}], "ReferenceToBib": [{"begin": 934, "end": 955, "target": "#b27", "idx": 0}, {"begin": 965, "end": 987, "target": "#b28", "idx": 1}, {"begin": 988, "end": 1007, "target": "#b1", "idx": 2}, {"begin": 1014, "end": 1035, "target": "#b4", "idx": 3}, {"begin": 1043, "end": 1062, "target": "#b39", "idx": 4}, {"begin": 1072, "end": 1090, "target": "#b22", "idx": 5}, {"begin": 1102, "end": 1120, "target": "#b19", "idx": 6}, {"begin": 1538, "end": 1559, "target": "#b4", "idx": 7}, {"begin": 1584, "end": 1605, "target": "#b15", "idx": 8}, {"begin": 1627, "end": 1647, "target": "#b32", "idx": 9}, {"begin": 1668, "end": 1686, "target": "#b20", "idx": 10}, {"begin": 1687, "end": 1710, "target": "#b25", "idx": 11}, {"begin": 1890, "end": 1910, "target": "#b4", "idx": 12}, {"begin": 2058, "end": 2075, "target": "#b22", "idx": 13}, {"begin": 2828, "end": 2850, "target": "#b37", "idx": 14}, {"begin": 3188, "end": 3212, "target": "#b25", "idx": 15}, {"begin": 3260, "end": 3283, "idx": 16}, {"begin": 3436, "end": 3457, "target": "#b4", "idx": 17}, {"begin": 3690, "end": 3712, "target": "#b37", "idx": 18}, {"begin": 3713, "end": 3733, "target": "#b4", "idx": 19}, {"begin": 3857, "end": 3877, "target": "#b11", "idx": 20}, {"begin": 3905, "end": 3924, "target": "#b2", "idx": 21}, {"begin": 4559, "end": 4579, "target": "#b3", "idx": 22}, {"begin": 5690, "end": 5711, "target": "#b4", "idx": 23}, {"begin": 5770, "end": 5792, "target": "#b37", "idx": 24}, {"begin": 5933, "end": 5954, "target": "#b4", "idx": 25}, {"begin": 6330, "end": 6350, "target": "#b33", "idx": 26}, {"begin": 6922, "end": 6943, "target": "#b16", "idx": 27}, {"begin": 7928, "end": 7946, "target": "#b22", "idx": 28}, {"begin": 7957, "end": 7976, "target": "#b39", "idx": 29}, {"begin": 8149, "end": 8176, "target": "#b24", "idx": 30}, {"begin": 8322, "end": 8340, "target": "#b26", "idx": 31}, {"begin": 8341, "end": 8358, "target": "#b22", "idx": 32}, {"begin": 8495, "end": 8515, "target": "#b11", "idx": 33}, {"begin": 8842, "end": 8861, "target": "#b2", "idx": 34}, {"begin": 9176, "end": 9197, "target": "#b10", "idx": 35}, {"begin": 9483, "end": 9502, "target": "#b5", "idx": 36}, {"begin": 9893, "end": 9915, "target": "#b37", "idx": 37}, {"begin": 11708, "end": 11730, "target": "#b37", "idx": 38}, {"begin": 15526, "end": 15547, "target": "#b4", "idx": 39}, {"begin": 15655, "end": 15675, "target": "#b3", "idx": 40}, {"begin": 15911, "end": 15937, "idx": 41}, {"begin": 16290, "end": 16314, "target": "#b29", "idx": 42}, {"begin": 16374, "end": 16393, "target": "#b40", "idx": 43}, {"begin": 16402, "end": 16426, "target": "#b36", "idx": 44}, {"begin": 16436, "end": 16455, "target": "#b6", "idx": 45}, {"begin": 16466, "end": 16486, "target": "#b14", "idx": 46}, {"begin": 16501, "end": 16527, "target": "#b18", "idx": 47}, {"begin": 16586, "end": 16606, "target": "#b13", "idx": 48}, {"begin": 17101, "end": 17122, "target": "#b13", "idx": 49}, {"begin": 17415, "end": 17435, "target": "#b4", "idx": 50}, {"begin": 18204, "end": 18222, "target": "#b39", "idx": 51}, {"begin": 22705, "end": 22726, "target": "#b33", "idx": 52}, {"begin": 22853, "end": 22880, "target": "#b24", "idx": 53}, {"begin": 22896, "end": 22916, "target": "#b11", "idx": 54}, {"begin": 22944, "end": 22963, "target": "#b2", "idx": 55}, {"begin": 23106, "end": 23132, "target": "#b7", "idx": 56}, {"begin": 23196, "end": 23218, "target": "#b37", "idx": 57}, {"begin": 23288, "end": 23309, "target": "#b4", "idx": 58}, {"begin": 23310, "end": 23331, "target": "#b28", "idx": 59}, {"begin": 23332, "end": 23350, "target": "#b39", "idx": 60}, {"begin": 23351, "end": 23368, "target": "#b22", "idx": 61}, {"begin": 23369, "end": 23386, "target": "#b19", "idx": 62}, {"begin": 23739, "end": 23757, "target": "#b9", "idx": 63}, {"begin": 23778, "end": 23798, "target": "#b3", "idx": 64}, {"begin": 23805, "end": 23829, "target": "#b34", "idx": 65}, {"begin": 23854, "end": 23871, "target": "#b21", "idx": 66}, {"begin": 23882, "end": 23903, "target": "#b17", "idx": 67}, {"begin": 23919, "end": 23941, "target": "#b0", "idx": 68}, {"begin": 24167, "end": 24186, "target": "#b8", "idx": 69}, {"begin": 24362, "end": 24382, "target": "#b38", "idx": 70}, {"begin": 24387, "end": 24408, "target": "#b23", "idx": 71}, {"begin": 24586, "end": 24607, "target": "#b10", "idx": 72}, {"begin": 25071, "end": 25090, "target": "#b3", "idx": 73}, {"begin": 25426, "end": 25445, "target": "#b39", "idx": 74}, {"begin": 25862, "end": 25882, "target": "#b3", "idx": 75}, {"begin": 26000, "end": 26026, "idx": 76}, {"begin": 26842, "end": 26862, "target": "#b15", "idx": 77}, {"begin": 26903, "end": 26928, "target": "#b24", "idx": 78}, {"begin": 26987, "end": 27007, "target": "#b31", "idx": 79}, {"begin": 27008, "end": 27025, "target": "#b30", "idx": 80}, {"begin": 27495, "end": 27513, "target": "#b22", "idx": 81}], "ReferenceString": [{"begin": 28155, "end": 28283, "id": "b0", "idx": 0}, {"begin": 28285, "end": 28523, "id": "b1", "idx": 1}, {"begin": 28527, "end": 28670, "id": "b2", "idx": 2}, {"begin": 28674, "end": 28955, "id": "b3", "idx": 3}, {"begin": 28959, "end": 29151, "id": "b4", "idx": 4}, {"begin": 29155, "end": 29264, "id": "b5", "idx": 5}, {"begin": 29268, "end": 29468, "id": "b6", "idx": 6}, {"begin": 29472, "end": 29625, "id": "b7", "idx": 7}, {"begin": 29629, "end": 29754, "id": "b8", "idx": 8}, {"begin": 29758, "end": 29902, "id": "b9", "idx": 9}, {"begin": 29906, "end": 30037, "id": "b10", "idx": 10}, {"begin": 30041, "end": 30262, "id": "b11", "idx": 11}, {"begin": 30266, "end": 30462, "id": "b12", "idx": 12}, {"begin": 30466, "end": 30664, "id": "b13", "idx": 13}, {"begin": 30668, "end": 30856, "id": "b14", "idx": 14}, {"begin": 30860, "end": 31020, "id": "b15", "idx": 15}, {"begin": 31024, "end": 31137, "id": "b16", "idx": 16}, {"begin": 31141, "end": 31267, "id": "b17", "idx": 17}, {"begin": 31271, "end": 31588, "id": "b18", "idx": 18}, {"begin": 31592, "end": 31802, "id": "b19", "idx": 19}, {"begin": 31806, "end": 31967, "id": "b20", "idx": 20}, {"begin": 31971, "end": 32205, "id": "b21", "idx": 21}, {"begin": 32209, "end": 32444, "id": "b22", "idx": 22}, {"begin": 32448, "end": 32571, "id": "b23", "idx": 23}, {"begin": 32575, "end": 32998, "id": "b24", "idx": 24}, {"begin": 33002, "end": 33108, "id": "b25", "idx": 25}, {"begin": 33112, "end": 33320, "id": "b26", "idx": 26}, {"begin": 33324, "end": 33672, "id": "b27", "idx": 27}, {"begin": 33676, "end": 33816, "id": "b28", "idx": 28}, {"begin": 33820, "end": 33966, "id": "b29", "idx": 29}, {"begin": 33970, "end": 34207, "id": "b30", "idx": 30}, {"begin": 34211, "end": 34462, "id": "b31", "idx": 31}, {"begin": 34466, "end": 34654, "id": "b32", "idx": 32}, {"begin": 34658, "end": 34853, "id": "b33", "idx": 33}, {"begin": 34857, "end": 35012, "id": "b34", "idx": 34}, {"begin": 35016, "end": 35208, "id": "b35", "idx": 35}, {"begin": 35212, "end": 35463, "id": "b36", "idx": 36}, {"begin": 35467, "end": 35697, "id": "b37", "idx": 37}, {"begin": 35701, "end": 35916, "id": "b38", "idx": 38}, {"begin": 35920, "end": 36128, "id": "b39", "idx": 39}, {"begin": 36132, "end": 36349, "id": "b40", "idx": 40}], "Sentence": [{"begin": 68, "end": 175, "idx": 0}, {"begin": 176, "end": 423, "idx": 1}, {"begin": 424, "end": 559, "idx": 2}, {"begin": 560, "end": 642, "idx": 3}, {"begin": 643, "end": 813, "idx": 4}, {"begin": 839, "end": 1205, "idx": 5}, {"begin": 1206, "end": 1386, "idx": 6}, {"begin": 1387, "end": 1726, "idx": 7}, {"begin": 1727, "end": 1813, "idx": 8}, {"begin": 1814, "end": 1888, "idx": 9}, {"begin": 1889, "end": 1995, "idx": 10}, {"begin": 1996, "end": 2096, "idx": 11}, {"begin": 2097, "end": 2241, "idx": 12}, {"begin": 2242, "end": 2373, "idx": 13}, {"begin": 2374, "end": 2580, "idx": 14}, {"begin": 2581, "end": 2582, "idx": 15}, {"begin": 2583, "end": 2879, "idx": 16}, {"begin": 2880, "end": 3099, "idx": 17}, {"begin": 3100, "end": 3284, "idx": 18}, {"begin": 3285, "end": 3458, "idx": 19}, {"begin": 3459, "end": 3537, "idx": 20}, {"begin": 3538, "end": 3779, "idx": 21}, {"begin": 3780, "end": 4031, "idx": 22}, {"begin": 4032, "end": 4222, "idx": 23}, {"begin": 4223, "end": 4498, "idx": 24}, {"begin": 4499, "end": 4652, "idx": 25}, {"begin": 4653, "end": 5018, "idx": 26}, {"begin": 5019, "end": 5065, "idx": 27}, {"begin": 5066, "end": 5195, "idx": 28}, {"begin": 5196, "end": 5316, "idx": 29}, {"begin": 5317, "end": 5471, "idx": 30}, {"begin": 5522, "end": 5596, "idx": 31}, {"begin": 5597, "end": 5902, "idx": 32}, {"begin": 5903, "end": 6186, "idx": 33}, {"begin": 6209, "end": 6253, "idx": 34}, {"begin": 6254, "end": 6495, "idx": 35}, {"begin": 6496, "end": 6601, "idx": 36}, {"begin": 6602, "end": 6730, "idx": 37}, {"begin": 6731, "end": 6769, "idx": 38}, {"begin": 6770, "end": 6912, "idx": 39}, {"begin": 6913, "end": 7061, "idx": 40}, {"begin": 7062, "end": 7159, "idx": 41}, {"begin": 7160, "end": 7287, "idx": 42}, {"begin": 7288, "end": 7423, "idx": 43}, {"begin": 7424, "end": 7464, "idx": 44}, {"begin": 7465, "end": 7654, "idx": 45}, {"begin": 7655, "end": 7672, "idx": 46}, {"begin": 7673, "end": 7797, "idx": 47}, {"begin": 7798, "end": 7977, "idx": 48}, {"begin": 8021, "end": 8134, "idx": 49}, {"begin": 8135, "end": 8261, "idx": 50}, {"begin": 8262, "end": 8359, "idx": 51}, {"begin": 8360, "end": 8480, "idx": 52}, {"begin": 8481, "end": 8714, "idx": 53}, {"begin": 8715, "end": 8818, "idx": 54}, {"begin": 8819, "end": 8948, "idx": 55}, {"begin": 8949, "end": 9044, "idx": 56}, {"begin": 9045, "end": 9152, "idx": 57}, {"begin": 9153, "end": 9309, "idx": 58}, {"begin": 9310, "end": 9481, "idx": 59}, {"begin": 9482, "end": 9652, "idx": 60}, {"begin": 9653, "end": 9750, "idx": 61}, {"begin": 9751, "end": 9862, "idx": 62}, {"begin": 9883, "end": 9972, "idx": 63}, {"begin": 10013, "end": 10100, "idx": 64}, {"begin": 10101, "end": 10174, "idx": 65}, {"begin": 10175, "end": 10272, "idx": 66}, {"begin": 10273, "end": 10375, "idx": 67}, {"begin": 10422, "end": 10446, "idx": 68}, {"begin": 10488, "end": 10741, "idx": 69}, {"begin": 10742, "end": 10842, "idx": 70}, {"begin": 10843, "end": 10959, "idx": 71}, {"begin": 10960, "end": 11034, "idx": 72}, {"begin": 11077, "end": 11080, "idx": 73}, {"begin": 11081, "end": 11128, "idx": 74}, {"begin": 11197, "end": 11285, "idx": 75}, {"begin": 11365, "end": 11370, "idx": 76}, {"begin": 11417, "end": 11459, "idx": 77}, {"begin": 11514, "end": 11607, "idx": 78}, {"begin": 11674, "end": 11843, "idx": 79}, {"begin": 11844, "end": 11925, "idx": 80}, {"begin": 11926, "end": 12024, "idx": 81}, {"begin": 12025, "end": 12097, "idx": 82}, {"begin": 12098, "end": 12195, "idx": 83}, {"begin": 12196, "end": 12250, "idx": 84}, {"begin": 12364, "end": 12384, "idx": 85}, {"begin": 12461, "end": 12563, "idx": 86}, {"begin": 12564, "end": 12653, "idx": 87}, {"begin": 12654, "end": 12718, "idx": 88}, {"begin": 12719, "end": 12816, "idx": 89}, {"begin": 12817, "end": 12927, "idx": 90}, {"begin": 12928, "end": 12938, "idx": 91}, {"begin": 12939, "end": 12960, "idx": 92}, {"begin": 13085, "end": 13128, "idx": 93}, {"begin": 13129, "end": 13364, "idx": 94}, {"begin": 13365, "end": 13608, "idx": 95}, {"begin": 13609, "end": 13761, "idx": 96}, {"begin": 13762, "end": 13993, "idx": 97}, {"begin": 13994, "end": 14125, "idx": 98}, {"begin": 14166, "end": 14349, "idx": 99}, {"begin": 14350, "end": 14460, "idx": 100}, {"begin": 14461, "end": 14562, "idx": 101}, {"begin": 14563, "end": 14740, "idx": 102}, {"begin": 14741, "end": 14876, "idx": 103}, {"begin": 14877, "end": 14929, "idx": 104}, {"begin": 14930, "end": 15026, "idx": 105}, {"begin": 15027, "end": 15040, "idx": 106}, {"begin": 15056, "end": 15126, "idx": 107}, {"begin": 15127, "end": 15412, "idx": 108}, {"begin": 15413, "end": 15463, "idx": 109}, {"begin": 15464, "end": 15548, "idx": 110}, {"begin": 15549, "end": 15676, "idx": 111}, {"begin": 15677, "end": 15753, "idx": 112}, {"begin": 15754, "end": 15872, "idx": 113}, {"begin": 15873, "end": 15938, "idx": 114}, {"begin": 15957, "end": 16010, "idx": 115}, {"begin": 16011, "end": 16072, "idx": 116}, {"begin": 16073, "end": 16181, "idx": 117}, {"begin": 16206, "end": 16288, "idx": 118}, {"begin": 16289, "end": 16528, "idx": 119}, {"begin": 16529, "end": 16687, "idx": 120}, {"begin": 16688, "end": 16816, "idx": 121}, {"begin": 16817, "end": 17027, "idx": 122}, {"begin": 17028, "end": 17183, "idx": 123}, {"begin": 17184, "end": 17354, "idx": 124}, {"begin": 17355, "end": 17436, "idx": 125}, {"begin": 17437, "end": 17549, "idx": 126}, {"begin": 17594, "end": 17738, "idx": 127}, {"begin": 17739, "end": 17851, "idx": 128}, {"begin": 17852, "end": 17872, "idx": 129}, {"begin": 17873, "end": 18045, "idx": 130}, {"begin": 18046, "end": 18223, "idx": 131}, {"begin": 18224, "end": 18356, "idx": 132}, {"begin": 18357, "end": 18371, "idx": 133}, {"begin": 18372, "end": 18538, "idx": 134}, {"begin": 18539, "end": 18589, "idx": 135}, {"begin": 18590, "end": 18750, "idx": 136}, {"begin": 18751, "end": 18827, "idx": 137}, {"begin": 18828, "end": 18961, "idx": 138}, {"begin": 18962, "end": 19107, "idx": 139}, {"begin": 19108, "end": 19199, "idx": 140}, {"begin": 19200, "end": 19384, "idx": 141}, {"begin": 19385, "end": 19557, "idx": 142}, {"begin": 19558, "end": 19706, "idx": 143}, {"begin": 19707, "end": 19796, "idx": 144}, {"begin": 19797, "end": 19838, "idx": 145}, {"begin": 19839, "end": 19918, "idx": 146}, {"begin": 19919, "end": 19965, "idx": 147}, {"begin": 19966, "end": 20050, "idx": 148}, {"begin": 20051, "end": 20242, "idx": 149}, {"begin": 20243, "end": 20375, "idx": 150}, {"begin": 20376, "end": 20556, "idx": 151}, {"begin": 20557, "end": 20640, "idx": 152}, {"begin": 20661, "end": 20727, "idx": 153}, {"begin": 20728, "end": 20887, "idx": 154}, {"begin": 20888, "end": 21035, "idx": 155}, {"begin": 21036, "end": 21121, "idx": 156}, {"begin": 21122, "end": 21247, "idx": 157}, {"begin": 21248, "end": 21282, "idx": 158}, {"begin": 21283, "end": 21384, "idx": 159}, {"begin": 21385, "end": 21642, "idx": 160}, {"begin": 21643, "end": 21890, "idx": 161}, {"begin": 21891, "end": 21989, "idx": 162}, {"begin": 21990, "end": 22259, "idx": 163}, {"begin": 22276, "end": 22423, "idx": 164}, {"begin": 22465, "end": 22615, "idx": 165}, {"begin": 22616, "end": 22727, "idx": 166}, {"begin": 22728, "end": 22964, "idx": 167}, {"begin": 22965, "end": 23133, "idx": 168}, {"begin": 23161, "end": 23473, "idx": 169}, {"begin": 23474, "end": 23513, "idx": 170}, {"begin": 23514, "end": 23638, "idx": 171}, {"begin": 23639, "end": 23696, "idx": 172}, {"begin": 23697, "end": 23942, "idx": 173}, {"begin": 23943, "end": 24187, "idx": 174}, {"begin": 24188, "end": 24282, "idx": 175}, {"begin": 24283, "end": 24344, "idx": 176}, {"begin": 24345, "end": 24409, "idx": 177}, {"begin": 24410, "end": 24525, "idx": 178}, {"begin": 24526, "end": 24774, "idx": 179}, {"begin": 24775, "end": 24941, "idx": 180}, {"begin": 24942, "end": 25016, "idx": 181}, {"begin": 25017, "end": 25323, "idx": 182}, {"begin": 25324, "end": 25574, "idx": 183}, {"begin": 25575, "end": 25756, "idx": 184}, {"begin": 25757, "end": 26027, "idx": 185}, {"begin": 26028, "end": 26075, "idx": 186}, {"begin": 26090, "end": 26201, "idx": 187}, {"begin": 26202, "end": 26373, "idx": 188}, {"begin": 26374, "end": 26508, "idx": 189}, {"begin": 26509, "end": 26600, "idx": 190}, {"begin": 26601, "end": 26711, "idx": 191}, {"begin": 26712, "end": 27026, "idx": 192}], "ReferenceToFigure": [{"begin": 7537, "end": 7541, "idx": 0}, {"begin": 12468, "end": 12469, "idx": 1}, {"begin": 13530, "end": 13531, "idx": 2}, {"begin": 14486, "end": 14487, "idx": 3}, {"begin": 21280, "end": 21281, "idx": 4}, {"begin": 21404, "end": 21405, "idx": 5}, {"begin": 21771, "end": 21772, "idx": 6}], "Abstract": [{"begin": 58, "end": 813, "idx": 0}], "SectionFootnote": [{"begin": 27028, "end": 28138, "idx": 0}], "Footnote": [{"begin": 27039, "end": 27130, "id": "foot_0", "n": "2", "idx": 0}, {"begin": 27131, "end": 27326, "id": "foot_1", "n": "3", "idx": 1}, {"begin": 27327, "end": 27698, "id": "foot_2", "n": "5", "idx": 2}, {"begin": 27699, "end": 27969, "id": "foot_3", "n": "6", "idx": 3}, {"begin": 27970, "end": 27987, "id": "foot_4", "n": "7", "idx": 4}, {"begin": 27988, "end": 28063, "id": "foot_5", "n": "8", "idx": 5}, {"begin": 28064, "end": 28138, "id": "foot_6", "n": "9", "idx": 6}]}}